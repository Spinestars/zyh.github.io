<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>正则表达式-修饰符</title>
      <link href="posts/1a0d1bbd/"/>
      <url>posts/1a0d1bbd/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p><code>?修饰符1修饰符2</code> 启用修饰符功能</p><p><code>?-修饰符1修饰符2</code>  关闭修饰符功能</p><p>(?i) 表示所在位置右侧的表达式开启忽略大小写模式</p><p>(?s) 表示所在位置右侧的表达式开启单行模式。<br>更改句点字符 (.) 的含义，以使它与每个字符（而不是除 \n 之外的所有字符）匹配。<br>注意：(?s)通常在匹配有换行的文本时使用</p><p>(?m) 表示所在位置右侧的表示式开启指定多行模式。</p><h2 id="i">(?i)</h2><p>以 nginx 为例，这里的 (?i) 表示忽略 <code>/aoo/(.*)</code> 里的大小写，也就是说 <code>/Aoo/(.*)</code> 也可以匹配</p><pre><code class="language-bash">rewrite &quot;(?i)/aoo/(.*)&quot; /$1 break;</code></pre><h2 id="m">(?m)</h2><p>如果一个字符串中包含<code>\n</code>这种换行符，若正则前缀如果不加<code>(?m)</code>，则正则匹配的时候会认为是一行。</p><p>例如，以 js 为例</p><p>不加 m 则无法匹配到，因为 <code>This is an\n antzone good</code> 被当作一行</p><pre><code class="language-javascript">var str=&quot;This is an\n antzone good&quot;; var reg=/an$/;console.log(str.match(reg));# 输出null</code></pre><p>加了 m 则可以匹配到，因为 <code>This is an</code> 被当作一行</p><pre><code class="language-javascript">var str=&quot;This is an\n antzone good&quot;; var reg=/an$/m;console.log(str.match(reg));# 输出[    &quot;an&quot;]</code></pre><h2 id="多次调用">多次调用</h2><p><code>(?i)a((?-i)a)</code>可以匹配Aa和aa，不能匹配AA和aA</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jira-备份文档</title>
      <link href="posts/a4903823/"/>
      <url>posts/a4903823/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p>系统备份均备份到本地磁盘，需自行写脚本备份到对象存储。</p><h2 id="Jira备份Web配置">Jira备份Web配置</h2><p>系统-服务，如果没有备份服务，则需要新加备份服务。</p><p>备份服务-类：<code>com.atlassian.jira.service.services.export.ExportService</code></p><p><img src="/posts/a4903823/image-20220324110110748.png" alt="image-20220324110110748"></p><p>💥这里的时间间隔是UTC+8，而我服务器时间是UTC+0</p><p>✨这个Jira的WEB备份不包含附件，附件需要自行脚本备份</p><h2 id="Confluence备份Web配置">Confluence备份Web配置</h2><p>一般配置-预定作业-备份系统</p><p><img src="/posts/a4903823/image-20220324110436538.png" alt="image-20220324110436538"></p><p>💥这里Cron表达式是UTC+0</p><h2 id="将备份复制到S3">将备份复制到S3</h2><h3 id="计划任务">计划任务</h3><pre><code>#jira自动备份时间是 utc 11:00, 因此上传到s3定到11:1515 11 * * * root /export/atlassian/application-data/jira/export/start &gt; /export/atlassian/application-data/jira/export/start.log 2&gt;&amp;1#jira confluence 自动备份时间是 utc0 12:00, 因此上传到S3定到13:0000 13 * * * root /export/confluence_data/backups/start &gt; /export/confluence_data/backups/start.log 2&gt;&amp;1</code></pre><h3 id="备份脚本">备份脚本</h3><p>✨脚本应放在jira或者confluence_data的备份目录内，方式就是扫描目录里的已备份文件，通过文件名获取备份文件的时间，删除7天之前的备份，然后同步到s3</p><p>/export/atlassian/application-data/jira/export/start</p><pre><code class="language-bash">#!/bin/bashexport AWS_DEFAULT_REGION='cn-north-1'basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`BucketName=''BucketPath='backup/jira'# 确定gitlab备份文件地址cd $&#123;basedir&#125;ls *.zip | while read line;do        FileCTime=`echo $&#123;line&#125; | awk -F '.' '&#123;gsub(/_/,&quot; &quot;,$1);print $1&#125;'`        Old7day=`date -d &quot;-7 days&quot; &quot;+%s&quot;`        FileCTimeStamp=`date -d &quot;$FileCTime&quot; +%s`        if [ $FileCTimeStamp -lt $Old7day ];then              [[ -f $&#123;line&#125; ]] &amp;&amp; rm -rf $&#123;line&#125; &amp;&amp; echo &quot;Delete old7day file $&#123;basedir&#125;/$&#123;line&#125;&quot;        fidoneecho &quot;Start: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; s3://$&#123;BucketName&#125;/$&#123;BucketPath&#125;&quot;aws s3 sync $&#123;basedir&#125;/ s3://$&#123;BucketName&#125;/$&#123;BucketPath&#125;/ --delete --exclude '*.log'echo &quot;End: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; oss://$&#123;BucketName&#125;/$&#123;BucketPath&#125;&quot;CurrentTime=`date +%Y%m%d`cd ../data/attachments/ &amp;&amp; touch $&#123;CurrentTime&#125;_attachments.tgztar --exclude=$&#123;CurrentTime&#125;_attachments.tgz -zcvf $&#123;CurrentTime&#125;_attachments.tgz . &amp;&amp; aws s3 cp $&#123;CurrentTime&#125;_attachments.tgz s3://$&#123;BucketName&#125;/$&#123;BucketPath&#125;/rm -rf $&#123;CurrentTime&#125;_attachments.tgz</code></pre><p>/export/confluence_data/backups/start</p><pre><code class="language-bash">#!/bin/bashexport AWS_DEFAULT_REGION='cn-north-1'basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`BucketName=''BucketPath='backup/jira_confluence'# 确定gitlab备份文件地址cd $&#123;basedir&#125;ls *.zip | while read line;do        FileCTime=`echo $&#123;line&#125; | awk -F&quot;.&quot; '&#123;print $1&#125;' | awk -F&quot;-&quot; '&#123;tmp=$2;gsub(/_/,&quot;-&quot;,tmp);print tmp&#125;'`        Old7day=`date -d &quot;-7 days&quot; &quot;+%s&quot;`        FileCTimeStamp=`date -d &quot;$FileCTime&quot; +%s`        if [ $FileCTimeStamp -lt $Old7day ];then              [[ -f $&#123;line&#125; ]] &amp;&amp; rm -rf $&#123;line&#125; &amp;&amp; echo &quot;Delete old7day file $&#123;basedir&#125;/$&#123;line&#125;&quot;        fidoneecho &quot;Start: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; s3://$&#123;BucketName&#125;/$&#123;BucketPath&#125;&quot;aws s3 sync $&#123;basedir&#125;/ s3://$&#123;BucketName&#125;/$&#123;BucketPath&#125;/ --delete --exclude '*.log'echo &quot;End: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; oss://$&#123;BucketName&#125;/$&#123;BucketPath&#125;&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> vlan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pgsql-基础命令</title>
      <link href="posts/684ab210/"/>
      <url>posts/684ab210/</url>
      
        <content type="html"><![CDATA[<h1>授权命令</h1><pre><code class="language-bash"># 授权只读grant select on all tables in schema &lt;schema_name&gt; to &lt;username&gt;;grant usage on schema &lt;schema_name&gt; to &lt;username&gt;;alter default privileges in schema &lt;schema_name&gt; grant select on tables to &lt;username&gt;;# 授权读写grant all on all tables in schema &lt;schema_name&gt; to &lt;username&gt;;grant all on schema &lt;schema_name&gt; to &lt;username&gt;;alter default privileges in schema &lt;schema_name&gt; grant all on tables to &lt;username&gt;;</code></pre><p>🤷‍♂️默认授权规则，必须是schema所有者修改才会生效。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> pgsql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pgsql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络-SNAT和DNAT[转]</title>
      <link href="posts/40d986c3/"/>
      <url>posts/40d986c3/</url>
      
        <content type="html"><![CDATA[<p>DNAT（Destination Network Address Translation,目的地址转换) 通常被叫做目的映谢。</p><p>而SNAT（Source Network Address Translation，源地址转换）通常被叫做源映谢。</p><p>这是我们在设置Linux网关或者防火墙时经常要用来的两种方式。</p><h1>IP包的结构</h1><p><img src="/posts/40d986c3/image-20211226154016611.png" alt="image-20211226154016611"></p><p>在任何一个IP数据包中，都会有Source IP Address与Destination IP Address这两个字段，数据包所经过的路由器也是根据这两个字段是判定数据包是由什么地方发过来的，它要将数据包发到什么地方去。而iptables的DNAT与SNAT就是根据这个原理，对Source IP Address与Destination IP Address进行修改。</p><h1>iptables数据流</h1><p><img src="/posts/40d986c3/image-20211226154032075.png" alt="image-20211226154032075"></p><p>正菱形的区域是对数据包进行判定转发的地方。在这里，系统会根据IP数据包中的DIP对数据包进行分发。如果destination ip adress是本机地址，数据将会被转交给INPUT链。如果不是本机地址，则交给FORWARD链检测。</p><p>✨【DNAT】是在【PREROUTING链】中进行。</p><p>比如我们要把访问202.103.96.112的访问转发到192.168.0.112上：</p><pre><code class="language-bash">iptables -t nat -A PREROUTING -d 202.103.96.112 -j DNAT --to-destination 192.168.0.112</code></pre><p>其实就是将已经达到这台Linux网关上的数据包上的 DIP：202.103.96.112 修改为 SERVER_IP：192.168.0.112 然后交给系统路由进行转发。</p><p>✨【SNAT】是在【POSTROUTING链】中进行。</p><pre><code class="language-bash">iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j SNAT --to-source 58.20.51.66</code></pre><p>这个语句就是告诉系统把即将要流出本机的数据的SIP修改成为58.20.51.66。这样，数据包在达到目的机器以后，目的机器会将包返回到58.20.51.66也就是本机。如果不做这个操作，那么你的数据包在传递的过程中，reply的包肯定会丢失。</p><p>注意，DNAT target只能用在nat表的PREOUTING 和 OUTPUT 链中，或者是被这两条链调用的链里。但还要注意的是，包含DNAT target的连不能被除此之外的其他链调用，如POSTROUTING。</p><pre><code class="language-bash">iptables -t nat -A PREROUTING -p tcp -d 15.45.23.67 --dport 80 -j DNAT --to-destination 192.168.1.1-192.168.1.10</code></pre><p>Explanation指定要写入IP头的地址，这也是包要被转发到的地方。上面的例子就是把所有发往地址15.45.23.67的包都转发到一段LAN使用的私有地址中，即192.168.1.1到192.168.1.10。如前所述，在这种情况下，每个流都会被随机分配一个要转发到的地址，但同一个流总是使用同一个地址。我们也可以只指定一个IP地址作为参数，这样所有包都被转发到同一台机子。我们还可以在地址后指定一个或一个范围的端口。比如：–to-destination 192.168.1.1:80或192.168.1.1:80-100。SNAT的语法和这个target的一样，只是目的不同罢了。要注意，只有先用–protocol指定了TCP或UDP协议，才能使用端口。</p><h1>DNAT的应用：网站发布</h1><p>防火墙：</p><ul><li><p>对外的IP：$GW_WAN_IP。</p></li><li><p>内网的IP：$GW_LAN_IP。</p></li></ul><p>WEB服务器：</p><ul><li>内网的IP：$HTTP_LAN_IP</li></ul><p>NAT表的PREROUTING链中添加DNAT规则：</p><pre><code class="language-bash">iptables -t nat -A PREROUTING --dst$GW_INET_IP -p tcp --dport 80 -j DNAT / --to-destination $HTTP_LAN_IP</code></pre><p>以上规则可以满足，外网客户端通过$GW_WAN_IP访问内网WEB服务器，但内网客户端无法通过$GW_WAN_IP访问WEB服务器。</p><h1>外网客户端可以通过$GW_WAN_IP访问的原因</h1><p>外网上访问我们服务器的那台机子的IP地址记为$EXT_CLIENT：</p><ol><li><p>包从地址为$EXT_CLIENT的机子出发，去往地址为$GW_WAN_IP的机子。</p></li><li><p>包到达防火墙。防火墙DNAT（也就是转发）这个包，而且包会经过很多其他的链检验及处理。</p></li><li><p>包离开防火墙向$HTTP_LAN_IP前进。包到达HTTP服务器，服务器就会通过防火墙给以回应，当然，这要求把防火墙作为HTTP到达$EXT_CLIENT的网关。一般情况下，防火墙就是HTTP服务器的缺省网关。</p></li><li><p>防火墙再对返回包做Un-DNAT（就是照着DNAT的步骤反过来做一遍），这样就好像是防火墙自己回复了那个来自外网的请求包。返回包回到$EXT_CLIENT。</p></li></ol><h1>内网客户端无法通过$GW_WAN_IP访问的原因</h1><p>假设客户机的IP为$LAN_CLIENT，其他设置同上：</p><ol><li><p>包离开$LAN_CLIENT，去往$GW_WAN_IP。</p></li><li><p>包到达防火墙。包被DNAT，而且还会经过其他的处理。此时数据包SIP是：$LAN_CLIENT，DIP是：$HTTP_LAN_IP</p></li><li><p>包离开防火墙，到达HTTP服务器。HTTP服务器试图回复这个包，它在看到包是来自同一个网络的一台机子，因此它会把回复包直接发送到$LAN_CLIENT。此时回复包SIP是：$GW_WAN_IP，DIP是：$HTTP_LAN_IP</p></li><li><p>回复包到达客户机，但客户端发现恢复包的SIP不是当初请求的$GW_WAN_IP。这样，它就会把这个包扔掉而去等待“真正”的回复包。</p></li></ol><h1>内网客户端无法通过$GW_WAN_IP访问的解决办法</h1><h2 id="针对请求包添加SNAT规则">针对请求包添加SNAT规则</h2><pre><code class="language-bash">iptables -t -nat -A POSTROUTING -p tcp --dst$HTTP_LAN_IP --dport 80 -j SNAT / --to-source $GW_LAN_IP</code></pre><p>✨按运行的顺序POSTROUTING链是所有链中最后一个，因此包到达这条链时，已经被做过DNAT操作了，所以我们在规则里要基于内网的地址$HTTP_LAN_IP（包的目的地）来匹配包。</p><p>☣ 这种方式，会导致WEB服务器的请求日志的源IP都是$GW_LAN_IP。因为来自Internet包在防火墙内先后经过了DNAT和SNAT处理，才能到达HTTP服务器，所以HTTP服务器就认为包是防火墙发来的，而不知道真正的源头是其他的IP。</p><h2 id="DNS服务">DNS服务</h2><p>通过单独的DNS服务器，内网客户使用网站域名访问HTTP服务器时，DNS将域名解析成内网地址。客户机就可以直接去访问HTTP服务器的内网地址了，从而避免了通过防火墙的操作。</p><h2 id="DMZ区域">DMZ区域</h2><p>通过构建DMZ区域，将WEB服务器和客户端内网进行分离。</p><h1>防火墙通过$GW_WAN_IP访问</h1><p>从防火墙发出的请求包不会经过PREROUTING和POSTROUTING，而是经过OUTPUT。 我们要在nat表的OUTPUT链中添加下面的规则：</p><pre><code class="language-bash">iptables -t nat -A OUTPUT --dst $GW_INET_IP -p tcp --dport 80 -j DNAT / --to-destination $HTTP_LAN_IP</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go-013文件先读再覆盖写</title>
      <link href="posts/38c126f9/"/>
      <url>posts/38c126f9/</url>
      
        <content type="html"><![CDATA[<h2 id="涉及模块">涉及模块</h2><ul><li>os</li><li>fmt</li><li>io/ioutil</li></ul><h2 id="代码">代码</h2><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;os&quot;    &quot;io/ioutil&quot;)func main()&#123;    filePath := &quot;/home/zyh/test&quot;    if fd, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, 0644); err != nil &#123;        fmt.Printf(&quot;打开文件失败：%s\n&quot;,filePath)    &#125; else &#123;        fdbyte, _ := ioutil.ReadAll(fd)        fmt.Printf(&quot;原始内容：%s\n&quot;, fdbyte)        fd.Truncate(0)        fd.Seek(0,0)        fd.WriteString(&quot;456!&quot;)        fd.Close()    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell☞read嵌套</title>
      <link href="posts/b06f599c/"/>
      <url>posts/b06f599c/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>今天碰到一个群友咨询一个问题，代码如下：</p><pre><code class="language-bash">cat xxx | while read line;do    read -p &quot;y|n:&quot; yn    echo $yn    echo $linedone</code></pre><p>他的目标预期是，循环体里的<code>read</code>可以接收来自键盘的输入。</p><p>但是，实际情况是，循环体里的<code>read</code>自动接收了<code>xxx</code>的数据。</p><h1>原因</h1><p><code>read</code>默认是从标准输入里接收数据，因此两个<code>read</code>均采用了默认行为。</p><h1>解决</h1><p>修改循环体外<code>read</code>的默认输入源，变更为其它文件描述符。</p><p>代码如下：</p><pre><code class="language-bash">while read -u 6 line;do    read -p &quot;y|n:&quot; yn    echo $yn    echo $linedone 6&lt;xxx </code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>windows-wsl问题点</title>
      <link href="posts/348cba53/"/>
      <url>posts/348cba53/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p>记录 wsl 使用期间遇到的问题</p><h2 id="列表">列表</h2><h3 id="1-发行版安装之后，wsl无法启动">1. 发行版安装之后，wsl无法启动</h3><p>需升级wsl。</p><pre><code class="language-powershell">wsl --update</code></pre><h3 id="2-wsl里无法正常解析">2. wsl里无法正常解析</h3><p>wsl的ubuntu系统中，默认<code>/etc/resolv.conf</code>通过<code>/etc/wsl.conf</code>控制，其wsl的系统通过windows的dns来解析</p><p>如果你发现生成正常，但是就是无法解析，尝试在windows中重置一下网络</p><pre><code class="language-powershell">netsh winsock resetnetsh int ip reset allnetsh winhttp reset proxyipconfig /flushdns</code></pre><p>上述操作完毕后，重启windows系统。</p><h3 id="3-wsl-dmesg-里报-no-channels-available-for-device-drvfs">3. wsl dmesg 里报 no channels available for device drvfs</h3><p>如果 /mnt/ 下的 windows 盘符挂载点都是空目录，则删除 /mnt/ 下的 windows 盘符挂载点，然后重启 ubuntu</p><h3 id="4-iwr-useb-get-scoop-sh-iex-出错">4. iwr -useb <a href="http://get.scoop.sh">get.scoop.sh</a> | iex 出错</h3><p>iex : 使用“2”个参数调用“DownloadFile”时发生异常:“基础连接已经关闭: 发送时发生错误。”</p><p>国际网络不稳定，重新执行。</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> windows </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows </tag>
            
            <tag> wsl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞location</title>
      <link href="posts/4c513678/"/>
      <url>posts/4c513678/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>开发有个小需求，因此需要添加一个额外的location</p><pre><code class="language-bash">location /game_log/v1/  ##新加的额外locationlocation ~* /v1/(.*)</code></pre><p>配置完后，发现不生效，查看日志，发现匹配到了第二个location。</p><h2 id="location写法">location写法</h2><p>优先级从高到低排序：</p><p>location =  /path  精确匹配<code>/path</code></p><p>location ^~ /path  匹配<code>/path</code>开头的</p><p>location ~  /path  匹配正则<code>/path</code>，区分大小写</p><p>location ~* /path  匹配正则<code>/path</code>，不区分大小写</p><p>location    /path  匹配<code>/path</code>开头的</p><p>location    /      匹配所有</p><h2 id="原因">原因</h2><ol><li>优先级</li></ol><p>因为 <code>~*</code>优先级大于<code>空</code>，因此先匹配 <code>location ~* /v1/(.*)</code></p><ol start="2"><li>正则</li></ol><p><code>/v1/(.*)</code> 并不是匹配以<code>/v1/</code>开头的，而是只要包含<code>/v1/</code>即可匹配</p><p>因<code>/game_log/v1/</code>包含<code>v1</code>，所以被 <code>location ~* /v1/(.*)</code>捕获</p><h2 id="解决">解决</h2><p>方法1:</p><p>将<code>location /game_log/v1/</code>改为<code>location ^~ /game_log/v1/</code></p><p>方法2:</p><p>将<code>location ~* /v1/(.*)</code>改为<code>location ~* ^/v1/(.*)</code></p>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞20网络cni</title>
      <link href="posts/68c615a7/"/>
      <url>posts/68c615a7/</url>
      
        <content type="html"><![CDATA[<h1>IP</h1><p>kubernetes 中的 ip 以 pod 为单位分配，一个 pod 内部所有的容器共享网络命名空间。</p><h1>CNI</h1><p>cni 提供了容器的插件化网络方案，即定义容器网络操作和配置的规范，并通过插件实现规范。</p><p>cni 自身仅关注容器和网络对象。当容器创建的时候，cni就会创建网络，当容器销毁的时候，cni就会删除网络。</p><p>cni 插件分为两部分：</p><ul><li>cni插件：负责配置网络</li><li>ipam插件：负责分配IP地址</li></ul><h1>网络策略</h1><p>network policy 可以对 pod 间的网络通信进行限制以及准入控制，从而控制可以访问 pod 的客户端。</p><p>network controller 通过 cni 插件实现用户定义的 network policy。</p><h1>待续</h1>]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞19日志系统EFK</title>
      <link href="posts/ce23a4a1/"/>
      <url>posts/ce23a4a1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch#running-efk-stack-in-production">kubernetes/cluster/addons/fluentd-elasticsearch at master · kubernetes/kubernetes (github.com)</a></p><p>EFK包含三个组件：</p><ol><li>fluentd 采集器。采集日志</li><li>elasticsearch 搜索引擎。处理日志</li><li>kibana 展示。展示日志</li></ol><h2 id="数据过程">数据过程:</h2><p>Pod日志 ☞ fluentd ☞ Elasticsearch ☞ Kibana</p><p>👙如果pod的日志是stdout和stderr，则日志的生命周期和pod的生命周期是一致的。fluentd无法采集到已经删除的pod的日志。</p><p>更加复杂和优化的数据流：</p><p>Fluentd+Kafka+Logstash+Elasticsearch+Kibana</p><h2 id="结构">结构</h2><p>集群模式，节点级收集器收集日志。</p><p>其结构图如下：</p><p><img src="/posts/ce23a4a1/image-20210813111558385.png" alt="image-20210813111558385"></p><h2 id="ES集群要求">ES集群要求</h2><p><img src="/posts/ce23a4a1/image-20220501214746600.png" alt="image-20220501214746600"></p><h2 id="安装步骤">安装步骤</h2><p>我们通过 helm 包管理器进行安装。其地址为：<a href="https://artifacthub.io/">Artifact Hub</a></p><p>⚠️需要注意的是，elasticsearch和kibana版本要保持一致</p><h3 id="elasticsearch">elasticsearch</h3><h4 id="部署">部署</h4><h4 id="证书和授权密码">证书和授权密码</h4><p>ES 7以上默认启用X-PACK，而X-PACK的启用，需要证书授权</p><ol><li>生成证书文件</li></ol><pre><code class="language-bash">docker run --name elastic-certs -i -w /app elasticsearch:7.12.0 /bin/sh -c &quot;elasticsearch-certutil ca --out /app/elastic-stack-ca.p12 --pass '' \&amp;&amp; elasticsearch-certutil cert --name security-master --dns security-master --ca /app/elastic-stack-ca.p12 --ca-pass '' --pass '' --out /app/elastic-certificates.p12&quot;docker cp elastic-certs:/app/elastic-certificates.p12 . &amp;&amp; docker rm -f elastic-certsopenssl pkcs12 -nodes -passin pass:'' -in elastic-certificates.p12 -out elastic-certificate.pem</code></pre><ol start="2"><li>创建证书对象和集群密码</li></ol><pre><code class="language-bash">kubectl create secret -n logging generic elastic-certs --from-file=elastic-certificates.p12kubectl create secret -n logging generic elastic-auth --from-literal=username=elastic --from-literal=password=ydzsio321</code></pre><h4 id="安装ES集群">安装ES集群</h4><p><a href="https://artifacthub.io/packages/helm/elastic/elasticsearch">https://artifacthub.io/packages/helm/elastic/elasticsearch</a></p><p>添加repo到本地，下载chart包到本地，并解压</p><pre><code class="language-bash">helm repo add elastic https://helm.elastic.cohelm repo updatehelm fetch elastic/elasticsearch --version 7.12.0tar xf elasticsearch*.tgz &amp;&amp; cd elasticsearch</code></pre><p>这里安装ES的三个角色，分别是：</p><ul><li>master 负责集群间的管理工作</li><li>data 负责存储数据</li><li>client 负责代理 ElasticSearch Cluster 集群，负载均衡</li></ul><blockquote><p>关于角色的介绍：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#node-roles">Node | Elasticsearch Guide | Elastic</a></p></blockquote><p>自定义配置：<a href="https://artifacthub.io/packages/helm/elastic/elasticsearch#configuration">https://artifacthub.io/packages/helm/elastic/elasticsearch#configuration</a></p><p>👙需要先配置 storageclass，假设这里的 storageclass 是 nfs-storage</p><ol><li>master 节点配置</li></ol><p>在 Chart 目录下面创建用于 Master 节点安装配置的 values 文件</p><pre><code class="language-yaml"># values-master.yaml## 设置集群名称clusterName: &quot;elasticsearch&quot;## 设置节点名称nodeGroup: &quot;master&quot;## 设置角色roles:  master: &quot;true&quot;  ingest: &quot;false&quot;  data: &quot;false&quot;# ============镜像配置============## 指定镜像与镜像版本image: &quot;elasticsearch&quot;imageTag: &quot;7.12.0&quot;## 副本数replicas: 3# ============资源配置============## JVM 配置参数esJavaOpts: &quot;-Xmx1g -Xms1g&quot;## 部署资源配置(生成环境一定要设置大些)resources:  requests:    cpu: &quot;2000m&quot;    memory: &quot;2Gi&quot;  limits:    cpu: &quot;2000m&quot;    memory: &quot;2Gi&quot;## 数据持久卷配置persistence:  enabled: true## 存储数据大小配置volumeClaimTemplate:  storageClassName: nfs-storage  accessModes: [&quot;ReadWriteOnce&quot;]  resources:    requests:      storage: 5Gi# ============安全配置============## 设置协议，可配置为 http、httpsprotocol: http## 证书挂载配置，这里我们挂入上面创建的证书secretMounts:  - name: elastic-certs    secretName: elastic-certs    path: /usr/share/elasticsearch/config/certs## 允许您在/usr/share/elasticsearch/config/中添加任何自定义配置文件,例如 elasticsearch.yml## ElasticSearch 7.x 默认安装了 x-pack 插件，部分功能免费，这里我们配置下## 下面注掉的部分为配置 https 证书，配置此部分还需要配置 helm 参数 protocol 值改为 httpsesConfig:  elasticsearch.yml: |    xpack.security.enabled: true    xpack.security.transport.ssl.enabled: true    xpack.security.transport.ssl.verification_mode: certificate    xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    # xpack.security.http.ssl.enabled: true    # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12## 环境变量配置，这里引入上面设置的用户名、密码 secret 文件extraEnvs:  - name: ELASTIC_USERNAME    valueFrom:      secretKeyRef:        name: elastic-auth        key: username  - name: ELASTIC_PASSWORD    valueFrom:      secretKeyRef:        name: elastic-auth        key: password# ============调度配置============## 设置调度策略## - hard：只有当有足够的节点时 Pod 才会被调度，并且它们永远不会出现在同一个节点上## - soft：尽最大努力调度antiAffinity: &quot;soft&quot;tolerations:  - operator: &quot;Exists&quot; ##容忍全部污点</code></pre><ol start="2"><li>data 节点配置</li></ol><pre><code class="language-yaml"># values-data.yaml# ============设置集群名称============## 设置集群名称clusterName: &quot;elasticsearch&quot;## 设置节点名称nodeGroup: &quot;data&quot;## 设置角色roles:  master: &quot;false&quot;  ingest: &quot;true&quot;  data: &quot;true&quot;# ============镜像配置============## 指定镜像与镜像版本image: &quot;elasticsearch&quot;imageTag: &quot;7.12.0&quot;## 副本数(建议设置为3，我这里资源不足只用了1个副本)replicas: 1# ============资源配置============## JVM 配置参数esJavaOpts: &quot;-Xmx1g -Xms1g&quot;## 部署资源配置(生成环境一定要设置大些)resources:  requests:    cpu: &quot;1000m&quot;    memory: &quot;2Gi&quot;  limits:    cpu: &quot;1000m&quot;    memory: &quot;2Gi&quot;## 数据持久卷配置persistence:  enabled: true## 存储数据大小配置volumeClaimTemplate:  storageClassName: nfs-storage  accessModes: [&quot;ReadWriteOnce&quot;]  resources:    requests:      storage: 10Gi# ============安全配置============## 设置协议，可配置为 http、httpsprotocol: http## 证书挂载配置，这里我们挂入上面创建的证书secretMounts:  - name: elastic-certs    secretName: elastic-certs    path: /usr/share/elasticsearch/config/certs## 允许您在/usr/share/elasticsearch/config/中添加任何自定义配置文件,例如 elasticsearch.yml## ElasticSearch 7.x 默认安装了 x-pack 插件，部分功能免费，这里我们配置下## 下面注掉的部分为配置 https 证书，配置此部分还需要配置 helm 参数 protocol 值改为 httpsesConfig:  elasticsearch.yml: |    xpack.security.enabled: true    xpack.security.transport.ssl.enabled: true    xpack.security.transport.ssl.verification_mode: certificate    xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    # xpack.security.http.ssl.enabled: true    # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12## 环境变量配置，这里引入上面设置的用户名、密码 secret 文件extraEnvs:  - name: ELASTIC_USERNAME    valueFrom:      secretKeyRef:        name: elastic-auth        key: username  - name: ELASTIC_PASSWORD    valueFrom:      secretKeyRef:        name: elastic-auth        key: password# ============调度配置============## 设置调度策略## - hard：只有当有足够的节点时 Pod 才会被调度，并且它们永远不会出现在同一个节点上## - soft：尽最大努力调度antiAffinity: &quot;soft&quot;## 容忍配置tolerations:  - operator: &quot;Exists&quot; ##容忍全部污点</code></pre><ol start="3"><li>client 节点配置</li></ol><pre><code class="language-yaml"># values-client.yaml# ============设置集群名称============## 设置集群名称clusterName: &quot;elasticsearch&quot;## 设置节点名称nodeGroup: &quot;client&quot;## 设置角色roles:  master: &quot;false&quot;  ingest: &quot;false&quot;  data: &quot;false&quot;# ============镜像配置============## 指定镜像与镜像版本image: &quot;elasticsearch&quot;imageTag: &quot;7.12.0&quot;## 副本数replicas: 1# ============资源配置============## JVM 配置参数esJavaOpts: &quot;-Xmx1g -Xms1g&quot;## 部署资源配置(生成环境一定要设置大些)resources:  requests:    cpu: &quot;1000m&quot;    memory: &quot;2Gi&quot;  limits:    cpu: &quot;1000m&quot;    memory: &quot;2Gi&quot;## 数据持久卷配置persistence:  enabled: false# ============安全配置============## 设置协议，可配置为 http、httpsprotocol: http## 证书挂载配置，这里我们挂入上面创建的证书secretMounts:  - name: elastic-certs    secretName: elastic-certs    path: /usr/share/elasticsearch/config/certs## 允许您在/usr/share/elasticsearch/config/中添加任何自定义配置文件,例如 elasticsearch.yml## ElasticSearch 7.x 默认安装了 x-pack 插件，部分功能免费，这里我们配置下## 下面注掉的部分为配置 https 证书，配置此部分还需要配置 helm 参数 protocol 值改为 httpsesConfig:  elasticsearch.yml: |    xpack.security.enabled: true    xpack.security.transport.ssl.enabled: true    xpack.security.transport.ssl.verification_mode: certificate    xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    # xpack.security.http.ssl.enabled: true    # xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12    # xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/certs/elastic-certificates.p12## 环境变量配置，这里引入上面设置的用户名、密码 secret 文件extraEnvs:  - name: ELASTIC_USERNAME    valueFrom:      secretKeyRef:        name: elastic-auth        key: username  - name: ELASTIC_PASSWORD    valueFrom:      secretKeyRef:        name: elastic-auth        key: password# ============Service 配置============service:  type: NodePort  nodePort: &quot;30200&quot;</code></pre><ul><li>添加污点容忍，允许调度到master节点上。</li></ul><blockquote><p>正常情况下，你不应该让其调度到master节点上。</p></blockquote><pre><code class="language-yaml">tolerations:  - key: &quot;role.kubernetes.io/master&quot;    operator: &quot;Exists&quot;    effect: &quot;NoSchedule&quot;</code></pre><ul><li>开始安装</li></ul><pre><code class="language-bash">kubectl create ns logginghelm upgrade --install es-master -f values-master.yaml --namespace logging .helm upgrade --install es-data -f values-data.yaml --namespace logging .helm upgrade --install es-client -f values-client.yaml --namespace logging .</code></pre><ul><li>校验</li></ul><pre><code class="language-bash">kubectl run cirros-$RANDOM  -it --rm --restart=Never --image=cirros -- curl --user elastic:ydzsio321 -H 'Content-Type: application/x-ndjson' http://10.96.105.164:9200/&#123;  &quot;name&quot; : &quot;elasticsearch-master-2&quot;,  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,  &quot;cluster_uuid&quot; : &quot;pihlND64SqKGge_RneVeFg&quot;,  &quot;version&quot; : &#123;    &quot;number&quot; : &quot;7.12.0&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;docker&quot;,    &quot;build_hash&quot; : &quot;78722783c38caa25a70982b5b042074cde5d3b3a&quot;,    &quot;build_date&quot; : &quot;2021-03-18T06:17:15.410153305Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;8.8.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot;  &#125;,  &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;</code></pre><h4 id="可能出现的问题">可能出现的问题</h4><pre><code class="language-bash"># kubectl describe pod elasticsearch-master-0 -n loggingQ: pod has unbound immediate PersistentVolumeClaimsA: 检查 pvc 状态。Q: Insufficient cpuA: cpu 资源不足，无法调度, 调低 cpu 资源的 request。</code></pre><p>👙statefulset 资源在更新的时候，是倒叙更新，例如这里有三个es-master，则先更新 elasticsearch-master-2，所以在修正配置之后，应该优先观察 elasticsearch-master-2 的状态。</p><h3 id="kibana"><a href="https://artifacthub.io/packages/helm/elastic/kibana/7.14.0">kibana</a></h3><h4 id="部署-2">部署</h4><p>下载chart包到本地，并解压</p><pre><code class="language-bash">helm pull elastic/kibana --untar --version 7.12.0cd kibana</code></pre><p>修改 values.yaml</p><pre><code class="language-yaml">---elasticsearchHosts: &quot;http://elasticsearch-client:9200&quot;replicas: 1extraEnvs:  - name: &quot;ELASTICSEARCH_USERNAME&quot;    valueFrom:      secretKeyRef:        name: elastic-auth        key: username  - name: &quot;ELASTICSEARCH_PASSWORD&quot;    valueFrom:      secretKeyRef:        name: elastic-auth        key: passwordimage: &quot;docker.elastic.co/kibana/kibana&quot;imageTag: &quot;7.12.0&quot;imagePullPolicy: &quot;IfNotPresent&quot;resources:  requests:    cpu: &quot;256m&quot;    memory: &quot;1Gi&quot;  limits:    cpu: &quot;500m&quot;    memory: &quot;1Gi&quot;protocol: httpserverHost: &quot;0.0.0.0&quot;healthCheckPath: &quot;/app/kibana&quot;podSecurityContext:  fsGroup: 1000securityContext:  capabilities:    drop:    - ALL  # readOnlyRootFilesystem: true  runAsNonRoot: true  runAsUser: 1000httpPort: 5601updateStrategy:  type: &quot;Recreate&quot;kibanaConfig:  kibana.yml: |    i18n.locale: &quot;zh-CN&quot;service:  type: NodePort  port: 5601  nodePort: &quot;30601&quot;  httpPortName: httpingress:  enabled: falsereadinessProbe:  failureThreshold: 3  initialDelaySeconds: 10  periodSeconds: 10  successThreshold: 3  timeoutSeconds: 5</code></pre><p>👙也可以将 svc 改为 LB 模式。</p><pre><code class="language-yaml">service:  type: LoadBalancer  loadBalancerIP: &quot;&quot;  port: 5601  nodePort: &quot;&quot;  labels: &#123;&#125;  annotations:    &#123;&#125;    # cloud.google.com/load-balancer-type: &quot;Internal&quot;    # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0    #service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;    # service.beta.kubernetes.io/openstack-internal-load-balancer: &quot;true&quot;    # service.beta.kubernetes.io/cce-load-balancer-internal-vpc: &quot;true&quot;</code></pre><blockquote><p>根据你的环境修改，我这里环境用了metallb组件，用来模拟LB，所以annotations不需要添加任何注释</p></blockquote><p>启动</p><pre><code class="language-bash">helm upgrade --install kibana -f values.yaml --namespace logging .</code></pre><h4 id="kibana-webui配置">kibana webui配置</h4><p>访问：<a href="http://10.200.16.101:30601/">http://10.200.16.101:30601/</a></p><p>添加模式分区</p><p>路径：Stack Management-&gt;Index patterns</p><p>Index pattern name: <code>logstash*</code></p><p>Time field: <code>@timestamp</code></p><h3 id="fluentd"><a href="https://artifacthub.io/packages/helm/kokuwa/fluentd-elasticsearch">fluentd</a></h3><p>官方文档：<a href="http://docs.fluentd.org">docs.fluentd.org</a></p><p>👙td-agent 软件是一个官方RPM发行版</p><p>fluentd 主要运行步骤如下：</p><ul><li>首先 Fluentd 从多个日志源获取数据</li><li>结构化并且标记这些数据</li><li>然后根据匹配的标签将数据发送到多个目标服务去</li></ul><p><img src="/posts/ce23a4a1/image-20220501214725265.png" alt="image-20220501214725265"></p><h4 id="日志源配置¶">日志源配置<a href="https://www.qikqiak.com/k8strain2/logging/efk/#%E6%97%A5%E5%BF%97%E6%BA%90%E9%85%8D%E7%BD%AE">¶</a></h4><p>官方文档：<a href="http://docs.fluentd.org/input">docs.fluentd.org/input</a></p><p>比如我们这里为了收集 Kubernetes 节点上的所有容器日志，就需要做如下的日志源配置：</p><pre><code>&lt;source&gt;  @id fluentd-containers.log             # 日志源ID  @type tail                             # Fluentd 内置的输入方式，通过 tail 插件不停地从源文件中获取新的日志。  path /var/log/containers/*.log         # 挂载的服务器Docker容器日志地址  pos_file /var/log/es-containers.log.pos # position 位置文件，记录fluent的读取位置  tag raw.kubernetes.*                   # 设置日志标签, 在 fluent 的 filter 过滤配置中根据标签过滤日志  read_from_head true                    # 从日志文件开头读取  &lt;parse&gt;                                # 多行格式化成JSON    @type multi_format                   # 使用 multi-format-parser 解析器插件    &lt;pattern&gt;                            # 匹配配置段      format json                        # JSON 解析器      time_key time                      # 指定事件时间的时间字段      time_format %Y-%m-%dT%H:%M:%S.%NZ  # 时间格式    &lt;/pattern&gt;    &lt;pattern&gt;                            # 多行日志匹配模式      format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/      time_format %Y-%m-%dT%H:%M:%S.%N%:z    &lt;/pattern&gt;  &lt;/parse&gt;&lt;/source&gt;</code></pre><p>上面配置部分参数说明如下：</p><ul><li>id：表示引用该日志源的唯一标识符，该标识可用于进一步过滤和路由结构化日志数据</li><li>type：Fluentd 内置的指令，<code>tail</code> 表示 Fluentd 从上次读取的位置通过 tail 不断获取数据，另外一个是 <code>http</code> 表示通过一个 GET 请求来收集数据。</li><li>path：<code>tail</code> 类型下的特定参数，告诉 Fluentd 采集 <code>/var/log/containers</code> 目录下的所有日志，这是 docker 在 Kubernetes 节点上用来存储运行容器 stdout 输出日志数据的目录。</li><li>pos_file：检查点，如果 Fluentd 程序重新启动了，它将使用此文件中的位置来恢复日志数据收集。</li><li>tag：用来将日志源与目标或者过滤器匹配的自定义字符串，Fluentd 匹配源/目标标签来路由日志数据。</li></ul><h4 id="路由配置¶">路由配置<a href="https://www.qikqiak.com/k8strain2/logging/efk/#%E8%B7%AF%E7%94%B1%E9%85%8D%E7%BD%AE">¶</a></h4><p>上面是日志源的配置，接下来看看如何将日志数据发送到 Elasticsearch：</p><pre><code>&lt;match **&gt;  @id elasticsearch  @type elasticsearch  @log_level info  include_tag_key true  type_name fluentd  host &quot;#&#123;ENV['OUTPUT_HOST']&#125;&quot;  port &quot;#&#123;ENV['OUTPUT_PORT']&#125;&quot;  logstash_format true  &lt;buffer&gt;    @type file    path /var/log/fluentd-buffers/kubernetes.system.buffer    flush_mode interval    retry_type exponential_backoff    flush_thread_count 2    flush_interval 5s    retry_forever    retry_max_interval 30    chunk_limit_size &quot;#&#123;ENV['OUTPUT_BUFFER_CHUNK_LIMIT']&#125;&quot;    queue_limit_length &quot;#&#123;ENV['OUTPUT_BUFFER_QUEUE_LIMIT']&#125;&quot;    overflow_action block  &lt;/buffer&gt;&lt;/match&gt;</code></pre><ul><li>match：标识一个目标标签，后面是一个匹配日志源的正则表达式，我们这里想要捕获所有的日志并将它们发送给 Elasticsearch，所以需要配置成<code>**</code>。</li><li>id：目标的一个唯一标识符。</li><li>type：支持的输出插件标识符，我们这里要输出到 Elasticsearch，所以配置成 elasticsearch，这是 Fluentd 的一个内置插件。</li><li>log_level：指定要捕获的日志级别，我们这里配置成 <code>info</code>，表示任何该级别或者该级别以上（INFO、WARNING、ERROR）的日志都将被路由到 Elsasticsearch。</li><li>host/port：定义 Elasticsearch 的地址，也可以配置认证信息，我们的 Elasticsearch 不需要认证，所以这里直接指定 host 和 port 即可。</li><li>logstash_format：Elasticsearch 服务对日志数据构建反向索引进行搜索，将 logstash_format 设置为 <code>true</code>，Fluentd 将会以 logstash 格式来转发结构化的日志数据。</li><li>Buffer： Fluentd 允许在目标不可用时进行缓存，比如，如果网络出现故障或者 Elasticsearch 不可用的时候。缓冲区配置也有助于降低磁盘的 IO。</li></ul><h4 id="过滤¶">过滤<a href="https://www.qikqiak.com/k8strain2/logging/efk/#%E8%BF%87%E6%BB%A4">¶</a></h4><p>由于 Kubernetes 集群中应用太多，也还有很多历史数据，所以我们可以只将某些应用的日志进行收集，比如我们只采集具有 <code>logging=true</code> 这个 Label 标签的 Pod 日志，这个时候就需要使用 filter，如下所示：</p><pre><code># 删除无用的属性，将 remove_keys 指定的 key 从包含 raw.kubernetes.* 标签的日志中移除。&lt;filter raw.kubernetes.**&gt;  @type record_transformer  remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash&lt;/filter&gt;# 只保留具有logging=true标签的Pod日志&lt;filter raw.kubernetes.**&gt;  @id filter_log  @type grep  &lt;regexp&gt;    key $.kubernetes.labels.logging    pattern ^true$  &lt;/regexp&gt;&lt;/filter&gt;</code></pre><h4 id="部署-3">部署</h4><p>👙fluentd 输出到 es，需要 es 插件，因此应该选用包含 es 插件的 fluentd 镜像。</p><p>下载chart包到本地，并解压</p><pre><code class="language-bash">helm repo add kokuwa https://kokuwaio.github.io/helm-chartshelm search repo kokuwa/fluentd-elasticsearch -lhelm pull --untar kokuwa/fluentd-elasticsearch --version 11.14.0cd fluentd-elasticsearch</code></pre><ul><li>编辑 values.yaml</li></ul><ol><li>挂载日志文件到容器</li></ol><pre><code class="language-yaml">hostLogDir:  varLog: /var/log  #dockerContainers: /var/lib/docker/containers  #dockerContainers: /export/docker-data-root/containers  #dockerContainers: /var/log/containers  libSystemdDir: /usr/lib64</code></pre><p>👙 默认 fluentd 会在容器里访问 /var/log/containers/*.log ，但这个目录下其实是软连接，指向了 /var/log/pods/。</p><p>通过 varLog: /var/log 挂载后：</p><p>如果 cri 是 docker，则 /var/log/pods 会进一步软连接到  /var/lib/docker/containers 下，因为 /var/lib/docker/containers 并不在 /var/log 下。因此，需要额外挂载 dockerContainers: /var/lib/docker/containers 到容器里。</p><p>如果 cri 是 containerd，则 /var/log/pods 会直接写入实际文件。因此，这时候 dockerContainers: 无需挂载。但考虑到 helm charts 目前模板没有适配 containerd，因此依然填入 dockerContainers: /var/log/pods</p><ol start="2"><li>修改 flentd 的输出配置，这里是 elasticsearch</li></ol><p>👙这里的配置，会在ES里创建 k8s-%Y.%M.%d 的索引，并存储所有收集的日志。</p><pre><code class="language-yaml">elasticsearch:  auth:    enabled: true    user: elastic    password: null    existingSecret:      name: elastic-auth      key: password  includeTagKey: true  setOutputHostEnvVar: true  # If setOutputHostEnvVar is false this value is ignored  hosts: [&quot;elasticsearch-client:9200&quot;]  indexName: &quot;fluentd&quot;  # 索引前缀  logstash:    enabled: true    prefix: &quot;k8s&quot;    prefixSeparator: &quot;-&quot;    dateformat: &quot;%Y.%m.%d&quot;  # 自动创建生命周期策略, 默认是关闭  ilm:    enabled: false    policy_id: logstash-policy    policy: &#123;&#125;      # example for ilm policy config      # phases:      #   hot:      #     min_age: 0ms      #     actions:      #       rollover:      #         max_age: 30d      #         max_size: 20gb      #       set_priority:      #           priority: 100      #   delete:      #     min_age: 60d      #     actions:      #       delete:    policies: &#123;&#125;      # example for ilm policies config      # ilm_policy_id1: &#123;&#125;      # ilm_policy_id2: &#123;&#125;    policy_overwrite: false  # 自动创建索引模板, 默认是关闭  template:    enabled: false    overwrite: false    useLegacy: true    name: fluentd-template    file: fluentd-template.json    content: |-      &#123;        &quot;index_patterns&quot;: [            &quot;k8s-*&quot;        ],        &quot;settings&quot;: &#123;            &quot;index&quot;: &#123;                &quot;number_of_replicas&quot;: &quot;3&quot;            &#125;        &#125;      &#125;</code></pre><p>添加master节点的容忍，允许 fluentd 部署到 master 节点上</p><pre><code class="language-yaml">tolerations:  - key: node-role.kubernetes.io/master    operator: Exists    effect: NoSchedule</code></pre><p>添加节点选择器，仅部署到需要搜集日志的节点上</p><pre><code class="language-yaml">nodeSelector:  beta.kubernetes.io/fluentd-ds-ready: &quot;true&quot;</code></pre><p>👙如果节点需要收集日志，则给节点打标签</p><pre><code class="language-bash">kubectl label nodes &lt;node_name&gt; beta.kubernetes.io/fluentd-ds-ready=true</code></pre><p>开始安装，或者更新</p><pre><code class="language-bash">helm upgrade --install fluentd --namespace logging -f values.yaml .</code></pre><h4 id="测试">测试</h4><p>👙默认情况下，这个 helm chart 的 template/configmap.yaml 里并没有过滤日志，可以在 configmap.yaml 中   <code>containers.input.conf: |-</code>  部分的最后加入下面的配置，从而通过给容器添加标签来决定是否收集容器日志。</p><p>这里的两段，分别是删除日志事件里的无用标签和只保留包含 <code>logging=&quot;true&quot;</code> 标签的pod日志。</p><pre><code class="language-yaml">    # 删除一些多余的属性    &lt;filter kubernetes.**&gt;      @type record_transformer      remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash    &lt;/filter&gt;    # 只保留具有logging=true标签的Pod日志    &lt;filter kubernetes.**&gt;      @id filter_log      @type grep      &lt;regexp&gt;        key $.kubernetes.labels.logging        pattern ^true$      &lt;/regexp&gt;    &lt;/filter&gt;        # 忽略 default 命名空间下的日志    # 这里的 kubernetes.var.log.containers.*_default_* 依然是匹配日志事件中的 tag 字段标签    &lt;match kubernetes.var.log.containers.*_default_*&gt;      @type null    &lt;/match&gt;</code></pre><p>测试pod demo</p><p>👙这个demo会自动输出时间到 stdout</p><pre><code class="language-bash">2022-05-01T11:17:25.355158001+08:00 stdout F 131: Sun May  1 03:17:25 UTC 20222022-05-01T11:17:26.355906645+08:00 stdout F 132: Sun May  1 03:17:26 UTC 20222022-05-01T11:17:27.356902598+08:00 stdout F 133: Sun May  1 03:17:27 UTC 20222022-05-01T11:17:28.357654227+08:00 stdout F 134: Sun May  1 03:17:28 UTC 20222022-05-01T11:17:29.35856036+08:00 stdout F 135: Sun May  1 03:17:29 UTC 20222022-05-01T11:17:30.359381367+08:00 stdout F 136: Sun May  1 03:17:30 UTC 2022</code></pre><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: counter  labels:    logging: &quot;true&quot; # 一定要具有该标签才会被采集spec:  containers:    - name: count      image: busybox      args:        [          /bin/sh,          -c,          'i=0; while true; do echo &quot;$i: $(date)&quot;; i=$((i+1)); sleep 1; done',        ]</code></pre><h4 id="校验-elasticsearch">校验 elasticsearch</h4><p>确认 elasticsearch 是否接收到来自 fluentd 发送的数据</p><pre><code class="language-bash">kubectl run cirros-$RANDOM  -it --rm --restart=Never --image=cirros -- curl --user elastic:ydzsio321 -H 'Content-Type: application/x-ndjson' http://10.96.105.164:9200/_cat/indices | grep k8s===yellow open k8s-2022.05.01                  APoJ5ABzRn67xl72_x4zdQ 1 1     86     0 178.1kb 178.1kb</code></pre><p><img src="/posts/ce23a4a1/image-20220501214656032.png" alt="image-20220501214656032"></p><h4 id="问题">问题</h4><pre><code class="language-bash">Q: fluentd 不停重启，Elasticsearch buffers found stuck longer than 300 seconds.A: </code></pre><h2 id="其他-kibana-配置">其他 kibana 配置</h2><p>配置一个展示k8s error错误的图标</p><p><img src="/posts/ce23a4a1/image-20210814122316905.png" alt="image-20210814122316905"></p><p><img src="/posts/ce23a4a1/image-20210814122130824.png" alt="image-20210814122130824"></p><h2 id="加入kafka">加入kafka</h2><h3 id="部署-4">部署</h3><pre><code class="language-bash">helm repo add bitnami https://charts.bitnami.com/bitnamihelm repo updatehelm pull bitnami/kafka --untar --version 12.17.5cd kafka</code></pre><p>配置文件 values-prod.yaml</p><pre><code class="language-yaml"># values-prod.yaml## Persistence parameters##persistence:  enabled: true  storageClass: &quot;nfs-client&quot;  accessModes:    - ReadWriteOnce  size: 5Gi  ## Mount point for persistence  mountPath: /bitnami/kafka# 配置zk volumeszookeeper:  enabled: true  persistence:    enabled: true    storageClass: &quot;nfs-client&quot;    accessModes:      - ReadWriteOnce    size: 8Gi</code></pre><p>启动</p><pre><code class="language-bash">helm upgrade --install kafka -f values-prod.yaml --namespace logging .</code></pre><h3 id="校验kafka">校验kafka</h3><pre><code class="language-bash">kubectl run kafka-client --restart='Never' --image docker.io/bitnami/kafka:2.8.0-debian-10-r0 --namespace logging --command -- sleep infinitypod/kafka-client created# 生产者$ kubectl exec --tty -i kafka-client --namespace logging -- bashI have no name!@kafka-client:/$ kafka-console-producer.sh --broker-list kafka-0.kafka-headless.logging.svc.cluster.local:9092 --topic test&gt;hello kafka on k8s# 消费者$ kubectl exec --tty -i kafka-client --namespace logging -- bashI have no name!@kafka-client:/$ kafka-console-consumer.sh --bootstrap-server kafka.logging.svc.cluster.local:9092 --topic test --from-beginninghello kafka on k8s</code></pre><h2 id="fluentd添加kafka插件">fluentd添加kafka插件</h2><pre><code class="language-dockerfile">FROM quay.io/fluentd_elasticsearch/fluentd:v3.2.0RUN echo &quot;source 'https://mirrors.tuna.tsinghua.edu.cn/rubygems/'&quot; &gt; Gemfile &amp;&amp; gem install bundlerRUN gem install fluent-plugin-kafka -v 0.16.1 --no-document</code></pre><h3 id="配置fluentd，将output指向kafka">配置fluentd，将output指向kafka</h3><pre><code class="language-yaml"># fluentd-configmap.yamlkind: ConfigMapapiVersion: v1metadata:  name: fluentd-conf  namespace: loggingdata:  ......  output.conf: |-    &lt;match **&gt;      @id kafka      @type kafka2      @log_level info      # list of seed brokers      brokers kafka-0.kafka-headless.logging.svc.cluster.local:9092      use_event_time true      # topic settings      topic_key k8slog      default_topic messages  # 注意，kafka中消费使用的是这个topic      # buffer settings      &lt;buffer k8slog&gt;        @type file        path /var/log/td-agent/buffer/td        flush_interval 3s      &lt;/buffer&gt;      # data type settings      &lt;format&gt;        @type json      &lt;/format&gt;      # producer settings      required_acks -1      compression_codec gzip    &lt;/match&gt;</code></pre><h3 id="校验kafka是否有数据">校验kafka是否有数据</h3><pre><code class="language-bash">$ kubectl exec --tty -i kafka-client --namespace logging -- bashI have no name!@kafka-client:/$ kafka-console-consumer.sh --bootstrap-server kafka.logging.svc.cluster.local:9092 --topic messages --from-beginning&#123;&quot;stream&quot;:&quot;stdout&quot;,&quot;docker&quot;:&#123;&#125;,&quot;kubernetes&quot;:&#123;&quot;container_name&quot;:&quot;count&quot;,&quot;namespace_name&quot;:&quot;default&quot;,&quot;pod_name&quot;:&quot;counter&quot;,&quot;container_image&quot;:&quot;busybox:latest&quot;,&quot;host&quot;:&quot;node1&quot;,&quot;labels&quot;:&#123;&quot;logging&quot;:&quot;true&quot;&#125;&#125;,&quot;message&quot;:&quot;43883: Tue Apr 27 12:16:30 UTC 2021\n&quot;&#125;......</code></pre><h2 id="加入logstash">加入logstash</h2><h3 id="部署-5">部署</h3><pre><code class="language-bash">helm pull elastic/logstash --untar --version 7.12.0cd logstash</code></pre><h3 id="配置">配置</h3><pre><code class="language-yaml"># values-prod.yamlfullnameOverride: logstashpersistence:  enabled: truelogstashConfig:  logstash.yml: |    http.host: 0.0.0.0    # 如果启用了xpack，需要做如下配置    xpack.monitoring.enabled: true    xpack.monitoring.elasticsearch.hosts: [&quot;http://elasticsearch-client:9200&quot;]    xpack.monitoring.elasticsearch.username: &quot;elastic&quot;    xpack.monitoring.elasticsearch.password: &quot;ydzsio321&quot;# 要注意下格式logstashPipeline:  logstash.conf: |    input &#123; kafka &#123; bootstrap_servers =&gt; &quot;kafka-0.kafka-headless.logging.svc.cluster.local:9092&quot; codec =&gt; json consumer_threads =&gt; 3 topics =&gt; [&quot;messages&quot;] &#125; &#125;    filter &#123;&#125;  # 过滤配置（比如可以删除key、添加geoip等等）    output &#123; elasticsearch &#123; hosts =&gt; [ &quot;elasticsearch-client:9200&quot; ] user =&gt; &quot;elastic&quot; password =&gt; &quot;ydzsio321&quot; index =&gt; &quot;logstash-k8s-%&#123;+YYYY.MM.dd&#125;&quot; &#125; stdout &#123; codec =&gt; rubydebug &#125; &#125;volumeClaimTemplate:  accessModes: [&quot;ReadWriteOnce&quot;]  storageClassName: nfs-storage  resources:    requests:      storage: 1Gi</code></pre><p>👙stdout { codec =&gt; rubydebug } 仅当测试中添加</p><p>启动</p><pre><code class="language-bash">$ helm upgrade --install logstash -f values-prod.yaml --namespace logging .</code></pre><h3 id="校验">校验</h3><pre><code class="language-bash">kubectl logs -f logstash-0 -n logging......&#123;&quot;docker&quot; =&gt; &#123;&#125;,&quot;stream&quot; =&gt; &quot;stdout&quot;,&quot;message&quot; =&gt; &quot;46921: Tue Apr 27 13:07:15 UTC 2021\n&quot;,&quot;kubernetes&quot; =&gt; &#123;            &quot;host&quot; =&gt; &quot;node1&quot;,          &quot;labels&quot; =&gt; &#123;    &quot;logging&quot; =&gt; &quot;true&quot;&#125;,        &quot;pod_name&quot; =&gt; &quot;counter&quot;,&quot;container_image&quot; =&gt; &quot;busybox:latest&quot;,  &quot;container_name&quot; =&gt; &quot;count&quot;,  &quot;namespace_name&quot; =&gt; &quot;default&quot;&#125;,&quot;@timestamp&quot; =&gt; 2021-04-27T13:07:15.761Z,&quot;@version&quot; =&gt; &quot;1&quot;&#125;</code></pre><h2 id="最终的工具栈">最终的工具栈</h2><p>Fluentd+Kafka+Logstash+Elasticsearch+Kibana</p><pre><code class="language-bash">$ kubectl get pods -n loggingNAME                            READY   STATUS    RESTARTS   AGEelasticsearch-client-0          1/1     Running   0          128melasticsearch-data-0            1/1     Running   0          128melasticsearch-master-0          1/1     Running   0          128mfluentd-6k52h                   1/1     Running   0          61mfluentd-cw72c                   1/1     Running   0          61mfluentd-dn4hs                   1/1     Running   0          61mkafka-0                         1/1     Running   3          134mkafka-client                    1/1     Running   0          125mkafka-zookeeper-0               1/1     Running   0          134mkibana-kibana-66f97964b-qqjgg   1/1     Running   0          128mlogstash-0                      1/1     Running   0          13m</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> k8s </tag>
            
            <tag> EFK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker-常用的指令</title>
      <link href="posts/634b48fb/"/>
      <url>posts/634b48fb/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><p>记录一些常用的指令</p><h1>列表</h1><h2 id="1-查看镜像环境变量">1. 查看镜像环境变量</h2><pre><code>docker run -it --rm [image_name] env</code></pre><h2 id="2-删除">2. 删除</h2><h3 id="删除所有dangling但没有被引用的镜像">删除所有dangling但没有被引用的镜像</h3><pre><code class="language-bash">docker image prune</code></pre><h3 id="删除所有没有被引用的本地卷">删除所有没有被引用的本地卷</h3><pre><code class="language-bash">docker volume prune</code></pre><h3 id="删除所有停止的容器">删除所有停止的容器</h3><pre><code class="language-bash">docker container prune</code></pre><h3 id="清空所有-未使用-未运行-的docker资源对象，包括容器、卷、网络、镜像">清空所有[未使用/未运行]的docker资源对象，包括容器、卷、网络、镜像</h3><pre><code class="language-bash">docker system prune --all --force --volumes</code></pre><h3 id="清空所有的docker资源对象，包括容器、卷、网络、镜像">清空所有的docker资源对象，包括容器、卷、网络、镜像</h3><p>👻这是极度可怕的指令，仅当你要初始化docker的时候才应该使用。</p><pre><code class="language-bash">docker container stop $(docker container ls -a -q) &amp;&amp; docker system prune --all --force --volumes</code></pre><h2 id="3-it-rm启动的容器，如何临时退出">3. <code>-it --rm</code>启动的容器，如何临时退出</h2><p>下述指令可以让你临时退出容器，而不会触发<code>--rm</code></p><pre><code class="language-bash">ctrl+P+q</code></pre><p>你可以通过 exec 从新进入容器，不过在此之后，你通过exit退出容器，也不会触发<code>--rm</code></p><p>只有执行<code>docker stop [container_name]</code>才会触发删除操作。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker☞非root用户执行程序</title>
      <link href="posts/604e367f/"/>
      <url>posts/604e367f/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>容器内以root用户运行，则就可以看到挂载点内root用户的文件，这并不安全。</p><p>但同时，容器里的用户需要与宿主机里的用户，拥有相同的ID，否则容器里的用户无法拥有足够的权限访问挂载点</p><h2 id="创建宿主机用户">创建宿主机用户</h2><p>对应容器里的用户</p><pre><code class="language-bash">groupadd --gid 60000 testuseradd --uid 60000 -g test --no-create-home test</code></pre><h2 id="容器使用方式">容器使用方式</h2><pre><code class="language-bash">FROM busybox:latestADD 1.txt /app/WORKDIR /appRUN addgroup --gid 60000 test &amp;&amp; adduser -u 60000 -G test -D -H testUSER test</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>harbor☞安装</title>
      <link href="posts/9573a87d/"/>
      <url>posts/9573a87d/</url>
      
        <content type="html"><![CDATA[<h2 id="硬件和依赖软件">硬件和依赖软件</h2><p><a href="https://goharbor.io/docs/2.3.0/install-config/installation-prereqs/">Harbor docs | Harbor Installation Prerequisites (goharbor.io)</a></p><h3 id="硬件依赖">硬件依赖</h3><table><thead><tr><th style="text-align:left">Resource</th><th style="text-align:left">Minimum</th><th style="text-align:left">Recommended</th></tr></thead><tbody><tr><td style="text-align:left">CPU</td><td style="text-align:left">2 CPU</td><td style="text-align:left">4 CPU</td></tr><tr><td style="text-align:left">Mem</td><td style="text-align:left">4 GB</td><td style="text-align:left">8 GB</td></tr><tr><td style="text-align:left">Disk</td><td style="text-align:left">40 GB</td><td style="text-align:left">160 GB</td></tr></tbody></table><h3 id="软件依赖">软件依赖</h3><table><thead><tr><th style="text-align:left">Software</th><th style="text-align:left">Version</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">Docker engine</td><td style="text-align:left">Version 17.06.0-ce+ or higher</td><td style="text-align:left">For installation instructions, see <a href="https://docs.docker.com/engine/installation/">Docker Engine documentation</a></td></tr><tr><td style="text-align:left">Docker Compose</td><td style="text-align:left">Version 1.18.0 or higher</td><td style="text-align:left">For installation instructions, see <a href="https://docs.docker.com/compose/install/">Docker Compose documentation</a></td></tr><tr><td style="text-align:left">Openssl</td><td style="text-align:left">Latest is preferred</td><td style="text-align:left">Used to generate certificate and keys for Harbor</td></tr></tbody></table><blockquote><p><a href="https://github.com/Spinestars/shell/blob/main/install/centos7_init.sh">shell/centos7_init.sh at main · Spinestars/shell (github.com)</a></p></blockquote><h3 id="网络依赖">网络依赖</h3><table><thead><tr><th style="text-align:left">Port</th><th style="text-align:left">Protocol</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">443</td><td style="text-align:left">HTTPS</td><td style="text-align:left">Harbor portal and core API accept HTTPS requests on this port. You can change this port in the configuration file.</td></tr><tr><td style="text-align:left">4443</td><td style="text-align:left">HTTPS</td><td style="text-align:left">Connections to the Docker Content Trust service for Harbor. Only required if Notary is enabled. You can change this port in the configuration file.</td></tr><tr><td style="text-align:left">80</td><td style="text-align:left">HTTP</td><td style="text-align:left">Harbor portal and core API accept HTTP requests on this port. You can change this port in the configuration file.</td></tr></tbody></table><h2 id="下载-配置">下载/配置</h2><p>harbor通过安装包里的安装脚本和安装配置文件，来生成各组件容器所需的配置，各组件容器通过docker-compose来启动.</p><ul><li>下载在线安装包并解压</li></ul><p><a href="https://github.com/goharbor/harbor/releases">https://github.com/goharbor/harbor/releases</a></p><ul><li>修改解压后的配置文件</li></ul><pre><code class="language-bash">cp harbor.yml.tmpl harbor.ymlmkdir -p /export/docker-data-harbor/log</code></pre><p>配置文件中，主要的修改如下</p><p>✨这里的配置并非 harbor 各程序组件直接调用的配置，而是 harbor 安装脚本会根据这个配置，动态的生成之后 harbor 所需各项程序组件的配置和数据</p><pre><code class="language-yaml"># The IP address or hostname to access admin UI and registry service.# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.# &lt;域名&gt;hostname: &lt;域名&gt;# http related confighttp:  # port for http, default is 80. If https enabled, this port will redirect to https port  port: 10080# https related confighttps:  # https port for harbor, default is 443  port: 10443  # The path of cert and key files for nginx &lt;宿主机路径,不能是软链接&gt;，nginx容器会将这份证书复制到容器里  certificate: /export/cert/apps/xxx.com/fullchain.cer  private_key: /export/cert/apps/xxx.com/xxx.com.key   ...harbor_admin_password: &lt;web界面管理员密码&gt;# Harbor DB configurationdatabase:  # The password for the root user of Harbor DB. Change this before any production use.  password: &lt;数据库密码&gt;  # The maximum number of connections in the idle connection pool. If it &lt;=0, no idle connections are retained.  max_idle_conns: 100  # The maximum number of open connections to the database. If it &lt;= 0, then there is no limit on the number of open connections.  # Note: the default number of connections is 1024 for postgres of harbor.  max_open_conns: 900# The default data volume, default: /data  &lt;宿主机路径，harbor所有容器的数据目录的根目录&gt;data_volume: /export/docker-data-harbor...# Log configurationslog:  # options are debug, info, warning, error, fatal  level: info  # configs for logs in local storage  local:    # Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.    rotate_count: 50    # Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.    # If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G    # are all valid.    rotate_size: 200M    # The directory on your host that store log,     location: /export/docker-data-harbor/log</code></pre><h3 id="https的配置">https的配置</h3><p>证书配置分为两部分，一个是nginx的，一个是docker的，两地证书一样，只不过docker需要额外的<code>ca.cert</code></p><ul><li>docker位置(可用软连接)</li></ul><pre><code class="language-bash">cp yourdomain.com.cert /etc/docker/certs.d/yourdomain.com/  # 如果你更换了默认端口，则复制到 /etc/docker/certs.d/yourdomain.com:port/cp yourdomain.com.key /etc/docker/certs.d/yourdomain.com/cp ca.crt /etc/docker/certs.d/yourdomain.com/</code></pre><ul><li>nginx位置(不可用软连接)</li></ul><p>nginx的证书会从<code>harbor.yml</code>中的<code>https.certificate</code> <code>https.private_key</code>处复制到 <code>&lt;path_to_data_volume&gt;/secret/cert/ </code></p><h2 id="安装-启动-关闭">安装/启动/关闭</h2><h3 id="预安装，并启动">预安装，并启动</h3><p>生成 docker-compose 配置，并创建相关数据目录，以及容器所需的依赖数据</p><pre><code>./prepare &amp;&amp; docker-compose up -d</code></pre><h3 id="直接安装，并启动">直接安装，并启动</h3><pre><code>./install.sh</code></pre><h3 id="关闭">关闭</h3><pre><code class="language-bash">docker-compose down</code></pre><h2 id="其它">其它</h2><h3 id="证书更换">证书更换</h3><p>nginx需要更换 &lt;path_to_data_volume&gt;/secret/cert/ 下的文件，并重启nginx容器</p><p>docker需要更换 /etc/docker/certs.d/yourdomain.com/ 下的文件，应该无需重启</p><h2 id="web配置-策略">web配置-策略</h2><p>删除策略的具体需求：</p><ol><li>策略执行时，删除所有的含有 untag 镜像</li><li>策略执行时，保留最近7天含有 tag 镜像</li></ol><p><img src="/posts/9573a87d/image-20210823180632775.png" alt="image-20210823180632775"></p><p>策略的测试，可以通过【模拟运行】+运行后的【日志】来确定策略是否满足期望。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> harbor </category>
          
      </categories>
      
      
        <tags>
            
            <tag> harbor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vcsa☞存储设备有vmfs分区，但数据存储丢失</title>
      <link href="posts/eab024cf/"/>
      <url>posts/eab024cf/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>一个esxi老是断连，根据日志判断是主板硬盘接口有点问题，所以将硬盘换到了另一个主机。结果主机启动起来之后，vcsa里可以看到存储设备，就是数据存储不可访问，删除之后，再扫描也找不到。</p><h2 id="过程">过程</h2><p>开启esxi的ssh服务，进入之后，发现分区啥的都还在，看起来是没有挂载成功。</p><h3 id="检查分区">检查分区</h3><p>发现序号8就是没有挂载上的vmfs分区，且检查没有错误</p><p>voma -m vmfs -f check -d &lt;分区文件&gt;</p><pre><code>voma -m vmfs -f check -d /vmfs/devices/disks/naa.50014ee213cfd76d:8===Running VMFS Checker version 2.1 in check modeInitializing LVM metadata, Basic Checks will be doneChecking for filesystem activityPerforming filesystem liveness check..\Scanning for VMFS-6 host activity (4096 bytes/HB, 1024 HBs).         Scsi 2 reservation successfulPhase 1: Checking VMFS header and resource files   Detected VMFS-6 file system (labeled:'10-200-16-4-hdd') with UUID:60a7b864-03293672-7e51-8cdcd42142e2, Version 6:82Phase 2: Checking VMFS heartbeat regionMarking Journal addr (0, 0) in usePhase 3: Checking all file descriptors.   Found stale lock [type 10c00001 offset 7561216 v 24, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1109         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7577600 v 30, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1119         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7585792 v 87, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 999         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7602176 v 42, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1114         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7733248 v 47, hb offset 3899392         gen 77, mode 1, owner 60cb1a4b-1ed0702e-cebf-8cdcd42142e2 mtime 664         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7749632 v 49, hb offset 3899392         gen 77, mode 1, owner 60cb1a4b-1ed0702e-cebf-8cdcd42142e2 mtime 699         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7856128 v 70, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 986         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7864320 v 71, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 993         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7872512 v 72, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1004         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7880704 v 73, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1028         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7888896 v 74, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1046         num 0 gblnum 0 gblgen 0 gblbrk 0]Phase 4: Checking pathname and connectivity.Phase 5: Checking resource reference counts.Total Errors Found:           0</code></pre><blockquote><p>输出信息里 Phase 1: 部分显示了vmfs分区的label和UUID</p></blockquote><h3 id="挂载vmfs分区，并加入开机启动">挂载vmfs分区，并加入开机启动</h3><p>esxcfg-volume -M &lt;分区UUID&gt;</p><pre><code class="language-bash">esxcfg-volume -M 60a7b864-03293672-7e51-8cdcd42142e2===Mounting volume 60a7b864-03293672-7e51-8cdcd42142e2</code></pre>]]></content>
      
      
      <categories>
          
          <category> 虚拟化 </category>
          
          <category> vmware </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vmware </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git☞常用命令</title>
      <link href="posts/f0c6f6c3/"/>
      <url>posts/f0c6f6c3/</url>
      
        <content type="html"><![CDATA[<h2 id="版本">版本</h2><pre><code class="language-bash"># 查看当前 commit idgit rev-parse HEAD# 放弃当前 commit id 之后的修改git reset --hard</code></pre><h2 id="暂存">暂存</h2><pre><code># 临时将修改暂存到堆栈列，并初始化到最后一个 commit id## 当存在多个暂存的时候，你需要自定义 messagegit stash save &lt;message&gt;# 取出堆栈列相应的暂存，并应用到暂存对应的 commit id git stash apply stash@&#123;X&#125;# 删除堆栈列相应的暂存git stash drop stash@&#123;X&#125;# 临时将修改暂存到堆栈列，添加一个默认名，并初始化到最后一个 commit id## 命名规范：stash@&#123;num&#125;: WIP on &lt;branch_name&gt; ： &lt;latest_commit_id&gt; &lt;latest_commit_message&gt;git stash# 取出堆栈列最上层的暂存，并应用到暂存对应的 commit id，同时删除暂存git stash pop</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus-07k8s-coredns</title>
      <link href="posts/e55f0a09/"/>
      <url>posts/e55f0a09/</url>
      
        <content type="html"><![CDATA[<h2 id="检查-metrics-接口">检查 metrics 接口</h2><pre><code class="language-bash">➜   kubectl describe cm coredns -n kube-systemName:         corednsNamespace:    kube-systemLabels:       &lt;none&gt;Annotations:  &lt;none&gt;Data====Corefile:----.:53 &#123;    errors    health &#123;       lameduck 5s    &#125;    ready    kubernetes cluster.local in-addr.arpa ip6.arpa &#123;       pods insecure       fallthrough in-addr.arpa ip6.arpa       ttl 30    &#125;    prometheus :9153    forward . /etc/resolv.conf &#123;       max_concurrent 1000    &#125;    cache 30    loop    reload    loadbalance&#125;Events:  &lt;none&gt;</code></pre><p><code>prometheus :9153</code> 表示开启</p><h2 id="确认-coredns-的-pod-地址">确认 coredns 的 pod 地址</h2><pre><code class="language-bash">➜   kubectl get pod -n kube-system -o wide | grep corednscoredns-78fcd69978-dt4lx          1/1     Running   1 (34d ago)    121d   10.97.0.195     k8s01   &lt;none&gt;           &lt;none&gt;coredns-78fcd69978-nzb7m          1/1     Running   1 (34d ago)    121d   10.97.0.183     k8s01   &lt;none&gt;           &lt;none&gt;</code></pre><h2 id="添加抓取配置">添加抓取配置</h2><pre><code class="language-yaml">apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-config  namespace: monitordata:  prometheus.yml: |    global:      scrape_interval: 15s      scrape_timeout: 15s    scrape_configs:    - job_name: 'prometheus'      static_configs:      - targets: ['localhost:9090']    - job_name: 'coredns'      static_configs:      - targets: ['10.97.0.195:9153', '10.97.0.183:9153']</code></pre><h2 id="重载prometheus">重载prometheus</h2><pre><code class="language-bash">curl -X POST http://prometheus_ip:9090/-/reload</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> etcd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus-08k8s-node</title>
      <link href="posts/1bafff65/"/>
      <url>posts/1bafff65/</url>
      
        <content type="html"><![CDATA[<h2 id="官方库">官方库</h2><p><a href="https://github.com/prometheus/node_exporter">https://github.com/prometheus/node_exporter</a></p><h2 id="daemonset方式部署">daemonset方式部署</h2><p>以daemonset方式部署 node_exporter</p><pre><code class="language-yaml"># prome-node-exporter.yamlapiVersion: apps/v1kind: DaemonSetmetadata:  name: node-exporter  namespace: monitor  labels:    app: node-exporterspec:  selector:    matchLabels:      app: node-exporter  template:    metadata:      labels:        app: node-exporter    spec:      hostPID: true      hostIPC: true      hostNetwork: true      nodeSelector:        kubernetes.io/os: linux      containers:      - name: node-exporter        image: prom/node-exporter:v1.3.1        args:        - --web.listen-address=$(HOSTIP):9100        - --path.procfs=/host/proc        - --path.sysfs=/host/sys        - --path.rootfs=/host/root        - --no-collector.hwmon # 禁用不需要的一些采集器        - --no-collector.nfs        - --no-collector.nfsd        - --no-collector.nvme        - --no-collector.dmi        - --no-collector.arp        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/containerd/.+|/var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)        - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$        ports:        - containerPort: 9100        env:        - name: HOSTIP          valueFrom:            fieldRef:              fieldPath: status.hostIP        resources:          requests:            cpu: 150m            memory: 180Mi          limits:            cpu: 150m            memory: 180Mi        securityContext:          runAsNonRoot: true          runAsUser: 65534        volumeMounts:        - name: proc          mountPath: /host/proc        - name: sys          mountPath: /host/sys        - name: root          mountPath: /host/root          mountPropagation: HostToContainer          readOnly: true      tolerations:      - operator: &quot;Exists&quot;      volumes:      - name: proc        hostPath:          path: /proc      - name: dev        hostPath:          path: /dev      - name: sys        hostPath:          path: /sys      - name: root        hostPath:          path: /</code></pre><h2 id="systemctl-宿主机直接安装">systemctl 宿主机直接安装</h2><pre><code class="language-bash"># yum包 https://copr.fedorainfracloud.org/coprs/ibotty/prometheus-exporters/curl -Lo /etc/yum.repos.d/_copr_ibotty-prometheus-exporters.repo https://copr.fedorainfracloud.org/coprs/ibotty/prometheus-exporters/repo/epel-7/ibotty-prometheus-exporters-epel-7.repo &amp;&amp; yum install node_exporter -y# 启动systemctl enable node_exporter --now</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> etcd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞17-2调度之亲和性和反亲和性</title>
      <link href="posts/4d69fc4a/"/>
      <url>posts/4d69fc4a/</url>
      
        <content type="html"><![CDATA[<h2 id="nodeSelector">nodeSelector</h2><p>最简单的调度方式<code>nodeSelector </code>方式，仅需给Pod提供，就可以让Pod调度到对应的节点上。例如：</p><p>查看node标签</p><pre><code class="language-bash">➜  ~ kubectl get node k8s01 --show-labelsNAME    STATUS   ROLES                  AGE    VERSION   LABELSk8s01   Ready    control-plane,master   564d   v1.22.4   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=</code></pre><p>Pod通过spec.nodeSelector选择标签</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  nodeSelector:    disktype: ssd</code></pre><p>🩱缺点：万一所有节点都不符合条件，则会Pod会卡住无法调度。</p><hr><h2 id="亲和性和反亲和性策略">亲和性和反亲和性策略</h2><pre><code class="language-bash">➜  ~ kubectl explain pod.spec.affinityKIND:     PodVERSION:  v1RESOURCE: affinity &lt;Object&gt;DESCRIPTION:     If specified, the pod's scheduling constraints     Affinity is a group of affinity scheduling rules.FIELDS:   nodeAffinity &lt;Object&gt;     Describes node affinity scheduling rules for the pod.   podAffinity  &lt;Object&gt;     Describes pod affinity scheduling rules (e.g. co-locate this pod in the     same node, zone, etc. as some other pod(s)).   podAntiAffinity      &lt;Object&gt;     Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod     in the same node, zone, etc. as some other pod(s)).</code></pre><p>节点只有亲和性 nodeAffinity：其意思是如果有符合条件的node，就将pod调度到这个node上</p><p>Pod分为亲和性 podAffinity、反亲和性 podAntiAffinity，以及拓扑网格topologyKey：</p><ul><li>对于 podAffinity 和 podAntiAffinity，k8s将其限定在一个称为``topologyKey<code>的概念。通过</code>topologyKey`，可以将节点划分为若干拓扑网格。</li><li>eg：例如 topologyKey: “<a href="http://kubernetes.io/hostname">kubernetes.io/hostname</a>”，则表示按主机名划分拓扑网格。此时：<ul><li>podAffinity ：表示在划分的每一个拓扑网格内，若已经有符合条件的pod，则将pod调度到<code>topologyKey</code>内</li><li>podAntiAffinity：表示在划分的每一个拓扑网格内，若已经有符合条件的pod，则不可调度到<code>topologyKey</code>内</li></ul></li></ul><h2 id="调度策略类型">调度策略类型</h2><p>支持nodeAffinity、podAffinity、podAntiAffinity</p><pre><code class="language-bash">➜  ~ kubectl explain pod.spec.affinity.podAffinityKIND:     PodVERSION:  v1RESOURCE: podAffinity &lt;Object&gt;DESCRIPTION:     Describes pod affinity scheduling rules (e.g. co-locate this pod in the     same node, zone, etc. as some other pod(s)).     Pod affinity is a group of inter pod affinity scheduling rules.FIELDS:   preferredDuringSchedulingIgnoredDuringExecution      &lt;[]Object&gt;     The scheduler will prefer to schedule pods to nodes that satisfy the     affinity expressions specified by this field, but it may choose a node that     violates one or more of the expressions. The node that is most preferred is     the one with the greatest sum of weights, i.e. for each node that meets all     of the scheduling requirements (resource request, requiredDuringScheduling     affinity expressions, etc.), compute a sum by iterating through the     elements of this field and adding &quot;weight&quot; to the sum if the node has pods     which matches the corresponding podAffinityTerm; the node(s) with the     highest sum are the most preferred.   requiredDuringSchedulingIgnoredDuringExecution       &lt;[]Object&gt;     If the affinity requirements specified by this field are not met at     scheduling time, the pod will not be scheduled onto the node. If the     affinity requirements specified by this field cease to be met at some point     during pod execution (e.g. due to a pod label update), the system may or     may not try to eventually evict the pod from its node. When there are     multiple elements, the lists of nodes corresponding to each podAffinityTerm     are intersected, i.e. all terms must be satisfied.</code></pre><ul><li><p>硬限制：requiredDuringScheduling<strong>IgnoredDuringExecution</strong> 调度必须满足条件+忽略执行期间条件变化</p><p>表示【必须】满足设定的条件才可以调度，如果没有满足条件的，就不停重试。其中IgnoreDuringExecution表示pod部署后运行期间，如果不再满足设定的条件，pod也会继续运行。</p></li><li><p>软限制：preferredDuringScheduling<strong>IgnoredDuringExecution</strong> 调度尽可能满足条件+忽略执行期间条件变化</p><p>表示【尽可能】满足设定的条件才可以调度，如果没有满足条件的，就忽略这些条件，按照正常逻辑部署。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果不再满足设定的条件，pod也会继续运行。</p><ul><li>软限制有权重概念，也就是说软限制可以同时设置多个条件，并根据权重来优先考虑条件。</li></ul></li></ul><p>💖策略可以组合使用</p><h3 id="未来可能支持的调度策略">未来可能支持的调度策略</h3><ul><li><p>requiredDuringScheduling<strong>RequiredDuringExecution</strong> 调度必须满足条件+不可忽略执行期间条件变化<br>表示【必须】满足设定的条件才可以调度，如果没有满足条件的，就不停重试。其中RequiredDuringExecution表示pod部署后运行期间，如果不再满足设定的条件，则被驱逐重新调度。</p></li><li><p>preferredDuringScheduling<strong>RequiredDuringExecution</strong> 调度尽可能满足条件+不可忽略执行期间条件变化<br>表示【尽可能】满足设定的条件才可以调度，如果没有满足条件的，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示pod部署之后运行的时候，如果不再满足设定的条件，则被驱逐重新调度。</p></li></ul><h2 id="案例1">案例1</h2><p>实现目标：每一个节点（topologyKey）上，均只有一个web-server和一个redis</p><p>redis的清单</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: redis-cachespec:  selector:    matchLabels:      app: store  replicas: 3  template:    metadata:      labels:        app: store    spec:      affinity:        podAntiAffinity: # 反亲和性：以节点名划分拓扑域，若拓扑域内已有app=store的Pod，则不可再将本Pod调度进此拓扑域          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - store            topologyKey: &quot;kubernetes.io/hostname&quot;      containers:      - name: redis-server        image: redis:3.2-alpine</code></pre><p>web-server的清单</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: web-serverspec:  selector:    matchLabels:      app: web-store  replicas: 3  template:    metadata:      labels:        app: web-store    spec:      affinity:        podAntiAffinity: # 反亲和性：以节点名划分拓扑域，若拓扑域内已有app=web-store的Pod，则不可再将本Pod调度进此拓扑域          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - web-store            topologyKey: &quot;kubernetes.io/hostname&quot;        podAffinity: # 亲和性：以节点名划分拓扑域，若拓扑域内已有app=store的pod,则可将本Pod调度进此拓扑域          requiredDuringSchedulingIgnoredDuringExecution:          - labelSelector:              matchExpressions:              - key: app                operator: In                values:                - store            topologyKey: &quot;kubernetes.io/hostname&quot;      containers:      - name: web-app        image: nginx:1.12-alpine</code></pre><p>💛如果要将超出节点数的Pod尽可能的均衡负载，则Pod反亲和应该使用<code>preferredDuringSchedulingIgnoredDuringExecution</code>，这可以确保在每一个节点都部署Pod后，依然可以将Pod部署进去。</p><pre><code class="language-yaml">    spec:      affinity:        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:          - weight: 1            podAffinityTerm:              labelSelector:                matchExpressions:                - key: app                  operator: In                  values:                  - web-store              topologyKey: &quot;kubernetes.io/hostname&quot;</code></pre><h2 id="案例2">案例2</h2><p>线上服务器组专用节点</p><pre><code class="language-bash"># 添加污点，非prod服务不可调度到此节点kubectl taint nodes k8s001 dedicated=prod:NoSchedule# 添加标签kubectl label nodes k8s001 dedicated=prod</code></pre><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    dedicated: prodspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: dedicated            operator: In            values:             - prod  tolerations:  - key: &quot;dedicated&quot;    operator: &quot;Equal&quot;    value: &quot;prod&quot;    effect: &quot;NoSchedule&quot;  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent</code></pre><p>affinity.nodeAffinity 节点亲和确保了必须调度到拥有 dedicated=prod 标签的节点，而 k8s001 拥有此标签。</p><ul><li>🩱注意：当节点的dedicated != prod的时候，Pod将【不会】重新调度到其它满足条件的节点上。</li></ul><p>tolerations 确保了 nginx pod 可以调度到拥有 dedicated=prod:NoSchedule 污点的节点，而 k8s001 拥有此污点。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>helm☞01入门</title>
      <link href="posts/51015304/"/>
      <url>posts/51015304/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>helm： k8s的包管理器，这里是v3版本</p><p>大致流程：Chart仓库&lt;-helm-&gt;存储到helm客户端本地-&gt;config-&gt;kubeconfig-&gt;Kube Apiserver-&gt;Release</p><p>chart仓库：存储chart程序包，不包含程序，是一个资源描述/模板 ，是一个http服务器。</p><p>helm：本地客户端</p><p>config：提供chart包所需的变量，构建特定的chart包配置，对应chart包里的values.yaml文件</p><p>Release：特定的chart包的本地实例化对象，部署于目标集群上的一个实例</p><p>当chart更新后，helm可以滚动更新对应的Release实例</p><h2 id="部署">部署</h2><h3 id="下载Helm包">下载Helm包</h3><p><a href="https://helm.sh/zh/docs/intro/install/">Helm | 安装Helm</a></p><p><a href="https://github.com/helm/helm/releases">Releases · helm/helm (github.com)</a></p><p>Helm包是一个二进制程序，直接解压就可以用，将解压后的 helm 文件放置在 /usr/bin/ 下即可</p><p>你也可以通过脚本一键安装最新版本的helm: <code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash</code></p><h2 id="添加源">添加源</h2><p>添加微软家的源，并命名为 stable</p><pre><code class="language-bash">helm repo add stable http://mirror.azure.cn/kubernetes/charts/</code></pre><p>列出已有的源</p><pre><code class="language-bash">helm repo list</code></pre><p>查询stable源包含的包</p><pre><code class="language-bash">helm search repo stable</code></pre><h2 id="常用命令">常用命令</h2><h3 id="release命令-v3版本">release命令(v3版本)</h3><ul><li><p>install</p><ul><li><code>helm install &lt;release_name&gt; &lt;chart_name&gt; &lt;flag&gt;</code></li><li><code>helm install myelk elastic/elasticsearch --namespace elk -f values.yaml</code></li></ul><blockquote><p>相同的包，不同的实例，之间是不相干独立管理的。</p></blockquote></li><li><p>delete</p><ul><li><pre><code class="language-bash">helm uninstall &lt;release_name&gt; &lt;flag&gt;</code></pre></li></ul><blockquote><p>flag 如果指定–keep-history，则属于标记删除，依然可以查到状态。在这种情况下，你可以通过 helm rollback 恢复</p></blockquote></li><li><p>upgrade：更新release版本，更新release配置</p><ul><li><code>helm upgrade &lt;release_name&gt; &lt;chart_name&gt; &lt;flag&gt;</code></li><li>chart_name 通过search命令查找</li><li>flag:<ul><li>–recreate-pods 重建所有Pod.</li></ul></li></ul></li><li><p>rollback：回滚release版本</p></li><li><p>list</p></li><li><p>history：获取release历史版本</p></li><li><p>status：获取release状态信息</p></li><li><p>get：获取release已设置的值。</p><ul><li><pre><code class="language-bash">helm get values &lt;release-name&gt;</code></pre></li></ul></li></ul><h3 id="chart命令-v3版本">chart命令(v3版本)</h3><ul><li><p>search 通过一个关键词查找一个chat。</p><ul><li><code>helm search repo metallb</code></li></ul></li><li><p>create 创建一个chart，将包含一些必要的文件</p></li><li><p>fetch 下载压缩包但不安装。常用来安装前自定义配置，例如values.yaml</p></li><li><p>get 下载</p></li><li><p>inspect</p></li><li><p>package 打包一个chart</p><ul><li><code>helm package &lt;chart根目录&gt;</code></li></ul></li><li><p>verify 验证一个chart</p></li></ul><h2 id="定制化">定制化</h2><p>就是通过定制化包的<code>values.yaml</code></p><p>通过下列命令导出包所支持的所有参数:</p><pre><code class="language-bash">helm show value &lt;pkg.name&gt;</code></pre><pre><code class="language-bash">➜   helm show values apphub/nginx  | grep -v '#' | grep -v '^$'image:  registry: docker.io  repository: bitnami/nginx  tag: 1.16.1-debian-10-r0  pullPolicy: IfNotPresentreplicaCount: 1podAnnotations: &#123;&#125;affinity: &#123;&#125;nodeSelector: &#123;&#125;tolerations: &#123;&#125;resources:  limits: &#123;&#125;  requests: &#123;&#125;livenessProbe:  httpGet:    path: /    port: http  initialDelaySeconds: 30  timeoutSeconds: 5  failureThreshold: 6readinessProbe:  httpGet:    path: /    port: http  initialDelaySeconds: 5  timeoutSeconds: 3  periodSeconds: 5service:  type: LoadBalancer  port: 80  httpsPort: 443  nodePorts:    http: &quot;&quot;    https: &quot;&quot;  annotations: &#123;&#125;  externalTrafficPolicy: Clusteringress:  enabled: false  certManager: false  hostname: example.local  annotations: &#123;&#125;  tls:    - hosts:        - example.local      secretName: example.local-tls  secrets:metrics:  enabled: false  image:    registry: docker.io    repository: bitnami/nginx-exporter    tag: 0.5.0-debian-10-r0    pullPolicy: IfNotPresent  podAnnotations:    prometheus.io/scrape: &quot;true&quot;    prometheus.io/port: &quot;9113&quot;  service:    port: 9113    annotations:      prometheus.io/scrape: &quot;true&quot;      prometheus.io/port: &quot;&#123;&#123; .Values.metrics.service.port &#125;&#125;&quot;  resources:    limits: &#123;&#125;    requests: &#123;&#125;  serviceMonitor:    enabled: false</code></pre><p>可以看到配置并非是k8s的标准格式。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> helm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> helm </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>helm☞02chart</title>
      <link href="posts/710e3e49/"/>
      <url>posts/710e3e49/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://helm.sh/docs/chart_template_guide/getting_started/">Helm | Getting Started</a></p><p>chart包目录结构：</p><pre><code class="language-bash">wordpress/  Chart.yaml          # 必选：包含当前 chart 信息的 YAML 文件  values.yaml         # 必选：chart 的默认配置 values  charts/             # 必选：包含该 chart 依赖的所有 chart 的目录  crds/               # 必选：Custom Resource Definitions  templates/          # 必选：模板目录，与 values 结合使用时，将渲染生成 Kubernetes 资源清单文件  LICENSE             # 可选：包含 chart 的 license 的文本文件  README.md           # 可选：一个可读性高的 README 文件  values.schema.json  # 可选: 一个作用在 values.yaml 文件上的 JSON 模式  templates/NOTES.txt # 可选: 包含简短使用的文本文件</code></pre><h2 id="创建框架">创建框架</h2><p>helm create &lt;chart包名&gt;</p><h2 id="必要文件">必要文件</h2><h3 id="Chart-yaml">Chart.yaml</h3><p>Chart.yaml 文件包含chart的描述。您可以从模板中访问它。 charts/ 目录可能包含其他chart（我们称之为subcharts）</p><p>一份nginx的Chat.yaml示例：</p><pre><code class="language-bash">apiVersion: v1                                                                                         appVersion: 1.16.1description: Chart for the nginx serverengine: gotplhome: http://www.nginx.orgicon: https://bitnami.com/assets/stacks/nginx/img/nginx-stack-220x234.pngkeywords:- nginx- http- web- www- reverse proxymaintainers:- email: containers@bitnami.com  name: Bitnaminame: nginxsources:- https://github.com/bitnami/bitnami-docker-nginxversion: 5.1.5</code></pre><h3 id="Templates目录">Templates目录</h3><p>包含以<code>.yaml</code>结尾的模板文件和以<code>.txt</code>结尾的NOTES.txt文件</p><h4 id="NOTES-txt">NOTES.txt</h4><p>Release安装过程中的输出文本</p><p>Release命令<code>helm status</code>信息</p><h4 id="模板文件">模板文件</h4><blockquote><p>go模板语法</p></blockquote><p><code>&#123;&#123; template "myapp.fullname" &#125;&#125;</code> 引用模板信息，这里的<code>myapp.fullname</code>即chart的名</p><p><code>&#123;&#123; .Values.replicaCount &#125;&#125;</code> 表示 Values.yaml 文件里的顶级 key: replicaCount</p><h2 id="验证">验证</h2><pre><code class="language-bash"># chart 根路径helm lint .</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> helm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> helm </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞17-1调度之node污染和pod容忍</title>
      <link href="posts/932d1f08/"/>
      <url>posts/932d1f08/</url>
      
        <content type="html"><![CDATA[<h1>引用</h1><p>概念文档：<a href="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">污点和容忍度 | Kubernetes</a></p><p>命令文档：<a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint">Kubectl Reference Docs (kubernetes.io)</a></p><h1>污点/容忍概念</h1><p>Node上的污点规则，即Node告知调度器只有可以容忍污点规则的Pod才可以调度过来。</p><p>Node的污点规则由两部分组成：污点:污点效果。</p><ul><li><p>污点就是键值对，用来描述一种场景。例如网络不通、硬件标识（低性能HDD磁盘）、环境（测试环境）</p></li><li><p>污点效果是判定规则，用来判断是否可以调度到节点上。</p></li></ul><p>Pod上的容忍规则，即Pod告知调度器可以容忍Node上设定的污点规则。</p><p>🍖容忍规则和污点规则都可以添加多个。</p><h1>污点/容忍命令</h1><p>Node的污点规则：</p><pre><code class="language-bash"># 添加污点## 标签对:污染关键词kubectl taint nodes &lt;node.name&gt; KEY_1=VAL_1:TAINT_EFFECT_1 KEY_N=VAL_N:TAINT_EFFECT_N# 移除污点kubectl taint nodes &lt;node.name&gt; KEY_1-# 查看节点污染信息，若无污染则为 nonekubectl describe nodes &lt;node.name&gt; | grep TaintsTaints:             node-role.kubernetes.io/master:NoSchedule</code></pre><p>Pod容忍规则：</p><pre><code class="language-yaml">tolerations:- key: &quot;key1&quot;  operator: &quot;Equal&quot;  value: &quot;value1&quot;  effect: &quot;NoSchedule&quot;</code></pre><p>operator 可以是Equal等于或者Exists存在。</p><h1>调度逻辑</h1><p>首先，Pod的容忍规则需要完全等同于污点规则，即污点和污点效果都一致才算是匹配成功。但存在特殊情况：</p><ul><li>仅定义了<code>operator:Exists</code>，则表示匹配任意污点规则。</li><li>仅定义了污点，没有定义污点效果，则表示匹配这个污点的所有效果。</li></ul><p>其次，当Node的污点规则有多个的时候，调度器会将Pod的容忍规则与之一一匹配。就如同小学题里那种连线题一样。</p><p>最后，若都可以匹配，则Pod可以调度进去。但只要存在一个不匹配的污点规则，那么调度器就需要依据这个污点规则的污点效果来判断Pod的进/退：</p><ul><li>进：针对还未创建的Pod，是否将Pod调度进去。</li><li>退：针对已经创建的Pod，是否将Pod驱逐滚蛋。</li></ul><h1>污点效果</h1><p>一般情况，若Node存在下列污点效果，调度器的逻辑是：</p><p><code>NoSchedule</code> 表示调度器不可调度【还未创建的】Pod到此节点。</p><p><code>PreferNoSchedule</code>表示尽量不要调度【还未创建的】Pod到此节点。</p><p><code>NoExecute</code> 表示绝不允许调度Pod到此节点，哪怕Pod【已经创建】都会被驱逐。</p><p>针对【NoExecute】，若可以【匹配NoExecute】的Pod里包含了额外的宽恕期规则<code>tolerationSeconds</code>，则Pod【仅可以】在<code>tolerationSeconds</code>宽恕期之内运行。</p><p>🤦‍♂️经测试，即使是污点规则添加之后的Pod，也只可以在<code>tolerationSeconds</code>内运行。因此这个宽恕期不如称之为【可运行时间】。</p><p><a href="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions">https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions</a></p><p>例如：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  tolerations:  - key: &quot;example-key&quot;    operator: &quot;Exists&quot;    effect: &quot;NoExecute&quot;    tolerationSeconds: 3600</code></pre><p>上述配置表示，当 pod 所在节点被添加了一个<code>example-key:NoExecute</code> 污点的时候，pod 将不会被驱逐，而是可以继续存活3600秒，如果还未到3600秒，污点就被移除，则 pod 驱逐也会停止。</p><h1>自动污点规则</h1><p>当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：</p><ul><li><code>node.kubernetes.io/not-ready:NoExecute</code>：节点未准备好。这相当于节点状态 <code>Ready</code> 的值为 “<code>False</code>”。</li><li><code>node.kubernetes.io/unreachable:NoExecute</code>：节点控制器访问不到节点. 这相当于节点状态 <code>Ready</code> 的值为 “<code>Unknown</code>”。</li><li><code>node.kubernetes.io/memory-pressure:NoSchedule</code>：节点存在内存压力。</li><li><code>node.kubernetes.io/disk-pressure:NoSchedule</code>：节点存在磁盘压力。</li><li><code>node.kubernetes.io/pid-pressure:NoSchedule</code>: 节点的 PID 压力。</li><li><code>node.kubernetes.io/network-unavailable:NoSchedule</code>：节点网络不可用。</li><li><code>node.kubernetes.io/unschedulable:NoSchedule</code>: 节点不可调度。</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个 “外部” 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li></ul><p>在节点被驱逐的时候，节点控制器或者 kubelet 会添加带有 <code>NoExecute</code> 污点效果的相关内置污点。异常恢复的时候，会自动移除污点。</p><blockquote><p>🤷‍♂️关于用<code>kubectl drain</code>执行驱逐的时候，<a href="http://xn--nodenode-309lrnu35cx15cpfcfz3aulp50vcrk.kubernetes.io/unschedulable:NoSchedule%E3%80%82%E4%BD%86%E5%AE%9E%E9%99%85%E6%95%88%E6%9E%9C%E6%98%AF">node显示的污点只有一个node.kubernetes.io/unschedulable:NoSchedule。但实际效果是</a><code>node.kubernetes.io/unschedulable:NoExecute</code>。即驱逐命令会驱逐掉已存在的Pod。可能<code>kubectl drain</code>会添加内置的特殊污点效果。</p><pre><code class="language-bash">➜   kubectl drain k8s03 --ignore-daemonsets --delete-emptydir-data➜   kubectl describe node k8s03 | grep TaintsTaints:             node.kubernetes.io/unschedulable:NoSchedule</code></pre></blockquote><p>如果你想让某个pod在节点出现问题（例如节点网络故障）后，依然被调度在当前节点保持1小时，那么你可以添加下列容忍规则：</p><pre><code class="language-yaml">tolerations:- key: &quot;node.kubernetes.io/network-unavailable&quot;  operator: &quot;Exists&quot;  effect: &quot;NoExecute&quot;  tolerationSeconds: 3600</code></pre><h1>自动容忍规则</h1><p>Kubernetes 会自动给 Pod 添加内置污点 <code>node.kubernetes.io/not-ready:NoExecute</code> 的容忍度并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为 <code>node.kubernetes.io/not-ready</code> 的容忍度。</p><p>Kubernetes 会自动给 Pod 添加内置污点 <code>node.kubernetes.io/unreachable:NoExecute</code> 的容忍度并配置 <code>tolerationSeconds=300</code>，除非用户提供的 Pod 配置中已经已存在了 key 为 <code>node.kubernetes.io/unreachable</code> 的容忍度。</p><p>这意味着在其中一种问题被检测到时 Pod 默认能够继续停留在当前节点运行 5 分钟。</p><p>但是，对于DaemonSet的Pod，则不会配置<code>tolerationSeconds</code>。这意味着，Node出现上述问题后，这类Pod将永久运行不被驱逐。</p><p>DaemonSet的自动附加容忍度：</p><pre><code class="language-yaml">node.kubernetes.io/disk-pressure:NoSchedule op=Existsnode.kubernetes.io/memory-pressure:NoSchedule op=Existsnode.kubernetes.io/network-unavailable:NoSchedule op=Existsnode.kubernetes.io/pid-pressure:NoSchedule op=Existsnode.kubernetes.io/unschedulable:NoSchedule op=Existsnode.kubernetes.io/not-ready:NoExecute op=Existsnode.kubernetes.io/unreachable:NoExecute op=Exists</code></pre><p>🍖DaemonSet中除了上述自动容忍度，还经常见到万能容忍度<code>:NoSchedule op=Exists 或者 op=Exists</code>。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞18日志系统</title>
      <link href="posts/451721ed/"/>
      <url>posts/451721ed/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>k8s的集群日志收集结构，一般分为下面几种：</p><ol><li>集群节点级模式，节点程序收集系统（k8s和容器）底层日志，并传输到日志系统。</li><li>伪边车容器模式，边车容器程序只负责将共享卷里的文件日志读取并传输到节点的stdout和stderr，然后节点级Pod收集器获取日志并传输到日志系统。</li><li>边车容器模式，边车容器程序通过共享卷从应用程序那收集日志，然后传输到日志系统。</li><li>应用容器模式，主容器程序直接将日志流式传输到日志系统。</li></ol><p>k8s的日志收集分两部分：</p><ul><li>自身日志，通过集群节点级模式收集</li><li>应用日志，根据情况选择收集方式</li></ul><h2 id="自身日志">自身日志</h2><p>采用daemonset方式在每一个节点上部署一个日志收集程序。对节点的日志目录采集：</p><ul><li>节点级日志：/var/log</li></ul><h2 id="应用日志">应用日志</h2><h3 id="集群节点级模式（模式1）">集群节点级模式（模式1）</h3><p>采用daemonset方式在每一个节点上部署一个日志收集程序。对节点的日志目录采集：</p><ul><li>应用级日志：/var/lib/docker/containers</li></ul><p>例如：官方的示例EFK，fluentd（节点级收集器）=》elasticsearch =》 kibana</p><p>✨这种方式，需要主容器程序将日志输出到stdout和stderr。</p><h3 id="伪边车容器模式（模式2）">伪边车容器模式（模式2）</h3><p>前提：主容器程序需要将日志以文件形式放在共享卷中。</p><p>通过边车容器程序将日志文件重新读取并输出到stdout，比如用<code>tail -n+1 -f log.file</code>；</p><p>然后通过集群节点级模式的方式收集日志。</p><p>💥相比于模式1，主容器程序写入一次日志文件，同时边车容器程序又重新读取文件日志，对磁盘负担增大。</p><p>✨这种方式，目的是解决日志文件在本地，但又不想给每一个pod部署一个日志收集程序。毕竟日志收集程序占用资源多。</p><p>总的来说是：磁盘压力大，CPU和内存额外开销小。</p><p>一个Pod例子：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:    name: counterspec:    containers:    - name: count        image: busybox        args:        - /bin/sh        - -c    - &gt;            i=0;            while true;do                echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;                echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;                i=$((i+1));                sleep 1;            done        volumeMounts:        - name: varlog            mountPath: /var/log    - name: count-log-1        image: busybox        args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']        volumeMounts:        - name: varlog            mountPath: /var/log    - name: count-log-2        image: busybox        args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']        volumeMounts:        - name: varlog            mountPath: /var/log  volumes:  - name: varlog      emptyDir: &#123;&#125;</code></pre><h3 id="边车容器模式（模式3）">边车容器模式（模式3）</h3><p>前提：主容器程序需要将日志以文件形式放在共享卷（例如通过emtyDir共享日志目录）。</p><p>通过边车容器程序将文件日志重新读取并发送到远程日志系统，例如用filebeat；</p><p>💥相比于模式1，主容器程序写入一次日志文件，同时边车容器程序又重新读取文件日志，对磁盘负担增大。</p><p>💥相比于模式2，无需将日志在写入到节点级，磁盘负担压力小了50%。但filebeat程序CPU和内存消耗大。</p><p>每一个运行的pod中添加一个日志收集容器filebeat，通过共享卷共享日志目录收集应用日志，将收集后的日志数据传输到logstash</p><p>logstash 通过 filebeat 配置的标签创建不同的索引</p><h2 id="filebeat示例">filebeat示例</h2><p>节点级DaemonSet类型filebeat，收集k8s节点日志并传输到kafka</p><p><a href="https://www.elastic.co/guide/en/beats/filebeat/7.17/running-on-kubernetes.html">https://www.elastic.co/guide/en/beats/filebeat/7.17/running-on-kubernetes.html</a></p><pre><code class="language-yaml">apiVersion: apps/v1kind: DaemonSetmetadata:  name: k8s-logs  namespace: kube-systemspec:  selector:    matchLabels:      project: k8s      app: filebeat  template:    metadata:      labels:        project: k8s        app: filebeat    spec:      hostAliases:      - ip: &quot;10.200.16.51&quot;        hostnames:        - &quot;data01&quot;      - ip: &quot;10.200.16.52&quot;        hostnames:        - &quot;data02&quot;      - ip: &quot;10.200.16.53&quot;        hostnames:        - &quot;data03&quot;      containers:      - name: filebeat        image: docker.elastic.co/beats/filebeat:7.17.1        args: [          &quot;-c&quot;, &quot;/etc/filebeat.yml&quot;,          &quot;-e&quot;,        ]        resources:          requests:            cpu: 100m            memory: 100Mi          limits:            cpu: 500m            memory: 500Mi        securityContext:          runAsUser: 0        volumeMounts:        - name: filebeat-config          mountPath: /etc/filebeat.yml          subPath: filebeat.yml        - name: k8s-logs          mountPath: /messages      volumes:      - name: filebeat-config        configMap:          name: k8s-logs-filebeat-config      - name: k8s-logs        hostPath:          path: /var/log/messages          type: File---apiVersion: v1kind: ConfigMapmetadata:  name: k8s-logs-filebeat-config  namespace: kube-systemdata:  filebeat.yml: |-    filebeat.shutdown_timeout: 5s    close_removed: true    clean_removed: true    filebeat.inputs:    - type: log      enabled: true      paths:      - /messages      tags: [&quot;k8s&quot;,&quot;messages&quot;]      fields:        log_topic: zz.it.elk.k8s.messages      fields_under_root: true    output.kafka:      hosts: [&quot;data01:8123&quot;, &quot;data01:8123&quot;, &quot;data01:8123&quot;]      username: elk      password: 123456      sasl.mechanism: 'SCRAM-SHA-256'      topic: '%&#123;[log_topic]&#125;'      partition.round_robin:        reachable_only: false      required_acks: 1      compression: gzip      max_message_bytes: 1000000</code></pre><p>logstash 配置</p><pre><code class="language-yaml">input &#123;  beats &#123;    port =&gt; 5044  &#125;&#125;filter &#123;&#125;output &#123;  elasticsearch &#123;    hosts =&gt; [&quot;http://xxx:9200&quot;]    index =&gt; &quot;k8s-syslog-%&#123;+YYYY.MM.dd&#125;&quot;  &#125;&#125;# 调试用stdout &#123;  codec =&gt; rubydebug&#125;</code></pre><h3 id="节点级filebeat">节点级filebeat</h3><pre><code class="language-yaml">- type: log  paths:    - /messages  fields:    app: k8s    type: module  fields_under_root: true  output.logstash:    hosts: ['xxx:5044']</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞04mysql监控</title>
      <link href="posts/6178b477/"/>
      <url>posts/6178b477/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://awesome-prometheus-alerts.grep.to/rules#mysql">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a></p><p><a href="https://github.com/prometheus/mysqld_exporter">prometheus/mysqld_exporter: Exporter for MySQL server metrics (github.com)</a></p><p><a href="https://registry.hub.docker.com/r/prom/mysqld-exporter/">prom/mysqld-exporter (docker.com)</a></p><h2 id="安装">安装</h2><h3 id="添加mysql账户">添加mysql账户</h3><pre><code class="language-bash">CREATE USER 'exporter'@'&lt;局域网授信IP&gt;' IDENTIFIED BY 'exporter@123';GRANT PROCESS, REPLICATION CLIENT ON *.* TO 'exporter'@'&lt;局域网授信IP&gt;';GRANT SELECT ON performance_schema.* TO 'exporter'@'&lt;局域网授信IP&gt;';</code></pre><pre><code class="language-bash">docker run -d --net promsnet --name mysqld-exporter -p 9104:9104 --link=my_mysql_container:&lt;被监控的mysql容器名&gt;  \  -e DATA_SOURCE_NAME=&quot;exporter:exporter@123@(&lt;被监控的mysql容器名 or 被监控的mysql实例地址&gt;:3306)/&quot; prom/mysqld-exporter</code></pre><blockquote><p>–link 可选，关联被监控的容器数据库</p></blockquote><h2 id="prometheus-主配置">prometheus 主配置</h2><pre><code class="language-yaml">scrape_configs:  - job_name: 'mysql-001'    static_configs:      - targets: ['mysqld-exporter:9104'] # mysqld-exporter 采集器地址        labels:          instance: &lt;mysql_server_path&gt;:3306 # 变更采集后的标签instance为mysql实例地址</code></pre><h2 id="prometheus-告警配置">prometheus 告警配置</h2><pre><code class="language-yaml">groups:  - name: mysql-alert    rules:    - alert: MysqlDown      expr: mysql_up == 0      for: 0m      labels:        severity: critical      annotations:        summary: MySQL down (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;MySQL instance is down on &#123;&#123; $labels.instance &#125;&#125;\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: MysqlTooManyConnections(&gt;80%)      expr: avg by (instance) (rate(mysql_global_status_threads_connected[1m])) / avg by (instance) (mysql_global_variables_max_connections) * 100 &gt; 80      for: 2m      labels:        severity: warning      annotations:        summary: MySQL too many connections (&gt; 80%) (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;More than 80% of MySQL connections are in use on &#123;&#123; $labels.instance &#125;&#125;\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: MysqlRestarted      expr: mysql_global_status_uptime &lt; 60      for: 0m      labels:        severity: info      annotations:        summary: MySQL restarted (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;MySQL has just been restarted, less than one minute ago on &#123;&#123; $labels.instance &#125;&#125;.\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: MysqlHighThreadsRunning      expr: avg by (instance) (rate(mysql_global_status_threads_running[1m])) / avg by (instance) (mysql_global_variables_max_connections) * 100 &gt; 60      for: 2m      labels:        severity: warning      annotations:        summary: MySQL high threads running (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;More than 60% of MySQL connections are in running state on &#123;&#123; $labels.instance &#125;&#125;\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h2 id="grafana">grafana</h2><p><a href="https://grafana.com/grafana/dashboards/7362">https://grafana.com/grafana/dashboards/7362</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞04redis监控</title>
      <link href="posts/978d0c1d/"/>
      <url>posts/978d0c1d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://awesome-prometheus-alerts.grep.to/rules#redis">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a></p><p><a href="https://github.com/oliver006/redis_exporter">oliver006/redis_exporter: Prometheus Exporter for Redis Metrics. Supports Redis 2.x, 3.x, 4.x, 5.x and 6.x (github.com)</a></p><p><a href="https://hub.docker.com/r/oliver006/redis_exporter/">oliver006/redis_exporter (docker.com)</a></p><h2 id="安装">安装</h2><pre><code class="language-bash">docker run -d --net promsnet --name redis_exporter -p 9121:9121 oliver006/redis_exporter -web.listen-address 0.0.0.0:9121 --redis.addr=</code></pre><blockquote><p>–redis.addr= 后面不用加任何地址，就是这么写的，如果不加这个选项，则 redis_exporter 会自动添加 redis://localhost:6379，如果 redis_exporter 部署的宿主机无法访问 redis://localhost:6379，则会导致 proms 报一个实例 down 掉。</p><p>具体redis_exporter需要监控的redis地址，在prometheus中配置即可。</p></blockquote><h2 id="prometheus-主配置">prometheus 主配置</h2><pre><code class="language-yaml">scrape_configs:  - job_name: 'redis'    metrics_path: /scrape    relabel_configs:    - source_labels: [__address__]      target_label: __param_target    - source_labels: [__param_target]      target_label: instance    - target_label: __address__      replacement: &lt;redis_exporter地址&gt;:9121    static_configs:    - targets:      - redis://&lt;被监控的redis地址01&gt;:6379      - redis://&lt;被监控的redis地址02&gt;:6379  - job_name: 'redis_exporter'    static_configs:      - targets:        - redis_exporter:9121</code></pre><h2 id="prometheus-告警配置">prometheus 告警配置</h2><pre><code class="language-yaml">groups:  - name: redis-alert    rules:    - alert: RedisDown      expr: redis_up == 0      for: 0m      labels:        severity: critical      annotations:        summary: Redis down (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis instance is down\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisOutOfSystemMemory      expr: redis_memory_used_bytes / redis_total_system_memory_bytes * 100 &gt; 90      for: 2m      labels:        severity: warning      annotations:        summary: Redis out of system memory (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis is running out of system memory (&gt; 90%)\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisOutOfConfiguredMaxmemory      expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 &gt; 90      for: 2m      labels:        severity: warning      annotations:        summary: Redis out of configured maxmemory (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis is running out of configured maxmemory (&gt; 90%)\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisTooManyConnections      expr: redis_connected_clients &gt; 100      for: 2m      labels:        severity: warning      annotations:        summary: Redis too many connections (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis instance has too many connections\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisNotEnoughConnections      expr: redis_connected_clients &lt; 5      for: 2m      labels:        severity: warning      annotations:        summary: Redis not enough connections (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis instance should have more connections (&gt; 5)\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisRejectedConnections      expr: increase(redis_rejected_connections_total[1m]) &gt; 0      for: 0m      labels:        severity: critical      annotations:        summary: Redis rejected connections (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Some connections to Redis has been rejected\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h2 id="grafana">grafana</h2><p><a href="http://www.eryajf.net/go?url=https://grafana.com/dashboards/763">https://grafana.com/dashboards/763</a></p><h2 id="redis-exporter-作为边车容器">redis_exporter 作为边车容器</h2><pre><code class="language-yaml"># prome-redis.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: redis  namespace: monitorspec:  selector:    matchLabels:      app: redis  template:    metadata:      labels:        app: redis    spec:      containers:      - name: redis        image: redis:4        resources:          requests:            cpu: 100m            memory: 100Mi        ports:        - containerPort: 6379      - name: redis-exporter        image: oliver006/redis_exporter:latest        resources:          requests:            cpu: 100m            memory: 100Mi        ports:        - containerPort: 9121---kind: ServiceapiVersion: v1metadata:  name: redis  namespace: monitorspec:  selector:    app: redis  ports:  - name: redis    port: 6379    targetPort: 6379  - name: prom    port: 9121    targetPort: 9121</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞06rabbitmq监控</title>
      <link href="posts/7fd13722/"/>
      <url>posts/7fd13722/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://awesome-prometheus-alerts.grep.to/rules#rabbitmq">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a></p><p><a href="https://www.rabbitmq.com/prometheus.html">Monitoring with Prometheus &amp; Grafana — RabbitMQ</a></p><p><a href="https://grafana.com/grafana/dashboards/10991">RabbitMQ-Overview dashboard for Grafana | Grafana Labs</a></p><h2 id="安装">安装</h2><pre><code class="language-bash">rabbitmq-plugins enable rabbitmq_prometheusrabbitmqctl -q set_cluster_name test@rabbitmq</code></pre><p>15692 端口是 prometheus 采集器的暴露端口，docker方式默认采集插件已经开启。</p><p>宿主机里执行 <code>curl -s localhost:15692/metrics | head -n 3</code>确保可以返回采集器数据。</p><h2 id="prometheus-主配置">prometheus 主配置</h2><pre><code class="language-yaml">scrape_configs:  - job_name: 'rabbitmq-001'    static_configs:      - targets: ['&lt;rabbitmq地址&gt;:15692'] </code></pre><h2 id="prometheus-告警配置">prometheus 告警配置</h2><pre><code class="language-yaml">groups:  - name: mysql-alert    rules:      - alert: RabbitmqNodeDown        expr: sum(rabbitmq_build_info) &lt; 3        for: 0m        labels:          severity: critical        annotations:          summary: Rabbitmq node down (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Less than 3 nodes running in RabbitMQ cluster\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqNodeNotDistributed        expr: erlang_vm_dist_node_state &lt; 3        for: 0m        labels:          severity: critical        annotations:          summary: Rabbitmq node not distributed (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Distribution link state is not 'up'\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqInstancesDifferentVersions        expr: count(count(rabbitmq_build_info) by (rabbitmq_version)) &gt; 1        for: 1h        labels:          severity: warning        annotations:          summary: Rabbitmq instances different versions (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Running different version of Rabbitmq in the same cluster, can lead to failure.\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqMemoryHigh        expr: rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes * 100 &gt; 90        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq memory high (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A node use more than 90% of allocated RAM\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqFileDescriptorsUsage        expr: rabbitmq_process_open_fds / rabbitmq_process_max_fds * 100 &gt; 90        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq file descriptors usage (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A node use more than 90% of file descriptors\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqTooMuchUnack        expr: sum(rabbitmq_queue_messages_unacked) BY (queue) &gt; 1000        for: 1m        labels:          severity: warning        annotations:          summary: Rabbitmq too much unack (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Too much unacknowledged messages\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqTooMuchConnections        expr: rabbitmq_connections &gt; 1000        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq too much connections (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;The total connections of a node is too high\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqNoQueueConsumer        expr: rabbitmq_queue_consumers &lt; 1        for: 1m        labels:          severity: warning        annotations:          summary: Rabbitmq no queue consumer (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A queue has less than 1 consumer\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqUnroutableMessages        expr: increase(rabbitmq_channel_messages_unroutable_returned_total[1m]) &gt; 0 or increase(rabbitmq_channel_messages_unroutable_dropped_total[1m]) &gt; 0        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq unroutable messages (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A queue has unroutable messages\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h2 id="grafana">grafana</h2><p><a href="https://grafana.com/grafana/dashboards/10991">https://grafana.com/grafana/dashboards/10991</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rabbitmq </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyuncli☞安装和基本调用</title>
      <link href="posts/6df9bb76/"/>
      <url>posts/6df9bb76/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>aliyun cli指令还不是太完善，并不是完全支持所有的阿里云产品，对标的是awscli</p><p>因此，建议时刻关注 <a href="https://github.com/aliyun/aliyun-cli/releases">https://github.com/aliyun/aliyun-cli/releases</a> 产品变化页面。</p><h1>安装</h1><pre><code class="language-bash">wget 'https://aliyuncli.alicdn.com/aliyun-cli-linux-latest-amd64.tgz'tar xf aliyun-cli-linux-latest-amd64.tgz</code></pre><p>💁近期的重要变化</p><p>3.0.40 - 3.0.42版本加入配置变量</p><p><code>ALIBABACLOUD_ACCESS_KEY_ID</code>, <code>ALICLOUD_ACCESS_KEY_ID</code></p><p><code>ALIBABACLOUD_ACCESS_KEY_SECRET</code>, <code>ALICLOUD_ACCESS_KEY_SECRET</code></p><p><code>ALIBABACLOUD_REGION_ID</code>, <code>ALICLOUD_REGION_ID</code></p><h1>单用户授权配置</h1><pre><code class="language-bash"># 查看当前配置aliyun configure listProfile             | Credential         | Valid   | Region           | Language---------           | ------------------ | ------- | ---------------- | --------default             | AK:***             | Invalid |                  | enecsRamRoleProfile * | EcsRamRole:gitlab  | Valid   | cn-zhangjiakou   | en# 切换配置aliyun configure set --profile &lt;profile_name&gt;# 添加配置，例如 ecsramrole 模式# https://help.aliyun.com/document_detail/121193.html?spm=a2c4g.11186623.3.4.35af3ae51h8w8Maliyun configure set \   --profile ecsRamRoleProfile \  --mode EcsRamRole \  --ram-role-name RoleName \  --region cn-hangzhou</code></pre><h1>全局用户授权配置（role模式）</h1><pre><code class="language-bash">cat &lt;&lt; 'EOF'| sudo tee /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;EOF</code></pre><p>✨通过访问<code>curl -sq 'http://100.100.100.200/latest/meta-data/ram/security-credentials/&lt;ram_role_name&gt;'</code>可以获取到临时token。临时授权信息如下：</p><pre><code class="language-json">&#123;  &quot;AccessKeyId&quot; : &quot;STS.abcde&quot;,  &quot;AccessKeySecret&quot; : &quot;abcde12345&quot;,  &quot;Expiration&quot; : &quot;2022-03-28T08:50:26Z&quot;,  &quot;SecurityToken&quot; : &quot;abcde1234567890&quot;,  &quot;LastUpdated&quot; : &quot;2022-03-28T02:50:26Z&quot;,  &quot;Code&quot; : &quot;Success&quot;&#125;</code></pre><p>在实际使用用，发现在ECS里请求上述地址会出现超时问题。。。所以建议实际使用中，本地缓存临时授权并判断授权过期时间。</p><h1>kubernetes中配置EcsRamRole</h1><pre><code class="language-yaml">      annotations:        k8s.aliyun.com/eci-ram-role-name: role_name</code></pre><h1>alpine Docker中调用aliyun命令</h1><pre><code class="language-bash">apk add --no-cache libc6-compat musl</code></pre><h1>基本使用</h1><p>以 oss 为例：</p><pre><code class="language-bash"># 导入角色source /etc/profile.d/ecs_role.sh## 设定 oss 的 endpoint 地址EndpointLan=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;EndpointWan=&quot;http://oss-$&#123;Region&#125;.aliyuncs.com&quot;##查询## 默认查询是递归查询，-d 只查询一层aliyun oss ls oss://test/ -d -e $&#123;EndpointLan&#125;##基本的上传或下载##上传文件 a.file 到 oss://test/ aliyun oss cp a.file oss://test/ -e $&#123;EndpointLan&#125;##基本的递归上传##上传目录 abc 下的文件到 oss://test/ 下，如果有重复内容，则需要加入 --forcealiyun oss cp abc oss://test/ --recursive -e $&#123;EndpointLan&#125;##复杂的递归上传##上传目录 abc 下的 .lzo 结尾的文件到 oss://test/ 下.##严禁在源目录里执行 --recursive 参数.##即禁止执行 aliyun oss cp . oss://test/ --recursive aliyun oss cp abc/ oss://test/ --include='*.lzo' --update --recursive -e $&#123;EndpointLan&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_output##同步目录 sync 指令变更##同步目录 abc 下的文件到 oss://test/ 下，如有重复，则忽略;同时删除目标目录下本地没有的文件aliyun oss sync abc/ oss://test/ --update --delete --force -e $&#123;EndpointLan&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_output</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
            <tag> cli </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>css☞背景图居中拉伸平铺</title>
      <link href="posts/fa5680b3/"/>
      <url>posts/fa5680b3/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-css">background: url('backend.jpg') center center no-repeat;background-attachment: fixed;  background-size: cover;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> css </category>
          
      </categories>
      
      
        <tags>
            
            <tag> css </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络☞链路聚合</title>
      <link href="posts/6f53a6b8/"/>
      <url>posts/6f53a6b8/</url>
      
        <content type="html"><![CDATA[<h2 id="目的">目的</h2><p>扩宽网络，突破单路</p><p>例如原本是单路千兆，扩成双路并发千兆</p><p>文档里的聚合类型均为动态聚合</p><h2 id="H3C交换机-副-其它交换机">H3C交换机(副) - 其它交换机</h2><pre><code class="language-bash"># 创建连接其它交换机的聚合接口# 聚合接口序号 1# 聚合接口设置为 trunk 模式， 并允许所有 vlan 通过# 聚合接口模式设置为动态聚合interface Bridge-Aggregation 1 port link-type trunk port trunk permit vlan all link-aggregation mode dynamic # 添加普通端口 1-4 到聚合接口# 设置普通端口为 trunk 模式，并允许所有 vlan 通过interface range GigabitEthernet1/0/1 to GigabitEthernet1/0/4 port link-type trunk port trunk permit vlan all port link-aggregation group 1</code></pre><blockquote><p>关于普通端口是否设置 link-type，很多文档里没有提及，但是我配置中，不设置网络就不通</p><p>若 H3C 连接的是终端，则将端口模式改为 access，并设置仅允许通过的 vlan</p></blockquote><h2 id="华为交换机-主-其它交换机">华为交换机(主) - 其它交换机</h2><pre><code class="language-bash"># 创建连接其它交换机的聚合接口# 聚合接口序号1# 聚合接口设置为 trunk 模式， 并允许所有 vlan 通过# 聚合接口模式设置为动态聚合interface Eth-Trunk 1 port link-type trunk port trunk allow-pass vlan 2 to 4094 mode lacp# 添加普通端口 1-4 到聚合接口# 设置普通端口的活动优先级为 100，默认端口优先级是30000+, 优先级数字越小，优先级越高interface GigabitEthernet0/0/1 eth-trunk 1 lacp priority 100interface GigabitEthernet0/0/2 eth-trunk 1 lacp priority 100interface GigabitEthernet0/0/3 eth-trunk 1 lacp priority 100interface GigabitEthernet0/0/4 eth-trunk 1 lacp priority 100</code></pre><blockquote><p>若华为连接的是终端，则将端口模式改为 access，并设置仅允许通过的 vlan</p></blockquote><h2 id="群辉">群辉</h2><p>前提：</p><p>交换机一侧已经配置好了动态聚合</p><p>控制面板-网络-网络界面-新增-创建 Bond</p><ol><li>设置聚合方式，选<code>IEEE 802.3ad 动态 Link Aggregationl</code></li><li>选择参与聚合的网络端口</li><li>设定聚合接口的ip地址</li></ol><p>保存设置后，群辉会重载。</p><p>如果不幸因配置故障，导致群辉无法访问，那么你可以通过群辉硬件上的 reset 按钮进行管理员账户和网络配置重置，需要注意的是，仅可按下4秒，在第一次听到滴声后，就立马松开。</p><h2 id="vsphere-vcenter">vsphere vcenter</h2><p>先看一个设置的拓扑，左边虚拟机16.50，默认只有一个虚拟网卡，右边lag1是一个上行聚合组</p><p><img src="/posts/6f53a6b8/image-20210527180252600.png" alt="image-20210527180252600"></p><p>登录 vsphere 网络控制台，点击<code>网络</code>，新建一个分布式虚拟交换机，期间所有属性全默认（名字你可以自己起，其它保持全默认就行）</p><p><img src="/posts/6f53a6b8/image-20210525145742419.png" alt="image-20210525145742419"></p><p>既然是一个虚拟交换机，那么肯定需要配置一个上联至物理交换机的聚合trunk链路（lag1）和下联虚拟机的聚合access链路（DportGroup-vlan2016）</p><h3 id="创建上联至物理交换机的聚合trunk链路">创建上联至物理交换机的聚合trunk链路</h3><p>选中所建立的 DSwitch - 配置 - LACP - 新建，创建虚拟交换机下的上联聚合接口lag1，lag1有4个端口，并且这4个端口对应4个宿主机网卡，负载平衡模式需要和物理交换机聚合接口设置的负载协议一致</p><p>假设这里都选择 src-dst-ip，即源和目标ip地址。</p><blockquote><p>需要注意的是，华为交换机普通型号不支持增强负载方案，也就是不支持sport和dspot负载。例如S5700EI就不支持。</p></blockquote><p>VLAN中继范围，我们选择中继所有，这个和物理交换机聚合接口设置保持一致即可。</p><p><img src="/posts/6f53a6b8/image-20210525151556021.png" alt="image-20210525151556021"></p><p>按照web页面的提示</p><p><img src="/posts/6f53a6b8/image-20210525155005563.png" alt="image-20210525155005563"></p><h3 id="创建一个下行端口组，用于关联虚拟机的网卡">创建一个下行端口组，用于关联虚拟机的网卡</h3><p>你有几个虚拟网卡，就开几个端口数，也可以多开，用不用看你自己。</p><p>VLAN ID就是指这个下行端口组允许通过的VLAN。</p><p><img src="/posts/6f53a6b8/image-20210527182054562.png" alt="image-20210527182054562"></p><h3 id="将exsi物理主机加入到DSwitch，并将DSwitch的聚合接口里的端口与exsi物理网卡绑定">将exsi物理主机加入到DSwitch，并将DSwitch的聚合接口里的端口与exsi物理网卡绑定</h3><p><img src="/posts/6f53a6b8/image-20210527175600067.png" alt="image-20210527175600067"></p><p>选中所建立的 DSwitch - 右键 - 添加和管理主机 - 添加主机</p><p>在添加和管理主机-管理物理适配器中，选择物理网卡并绑定到虚拟交换机的上联链路聚合组ige1里的虚拟端口，随你怎么绑定，反正把你的物理网卡都绑定上即可。</p><p>其余的迁移虚拟机网络之类的可以先不做。</p><p>到此，DSwitch和物理交换机之间的聚合应该已经打通了。</p><p>在物理交换机那边查看聚合接口，确保所有参与聚合的端口都是 selected 状态。</p><h3 id="将虚拟机网卡关联到虚拟交换机DSwitch的下行端口组">将虚拟机网卡关联到虚拟交换机DSwitch的下行端口组</h3><p>在本文档中，即将网络适配器关联到 DportGroup-vlan2016。</p><p>最后，在虚拟机网卡配置中，将网络适配器从VM Network改为下行端口组</p><p><img src="/posts/6f53a6b8/image-20210525160148265.png" alt="image-20210525160148265"></p><p>或者将虚拟机从 VM Network交换机迁出到 DSwitch 交换机</p><blockquote><p>需要注意的是，这种方式会将虚拟机下所有虚拟网卡全部迁移。</p></blockquote><p><img src="/posts/6f53a6b8/image-20210525160429071.png" alt="image-20210525160429071"></p><h3 id="测试虚拟机的网卡速度">测试虚拟机的网卡速度</h3><p>通过 iptraf-ng 命令可以看到，已经突破了100MBps（聚合前，exsi宿主机到物理交换机之前是100MBps）</p><p><img src="/posts/6f53a6b8/image-20210527181349144.png" alt="image-20210527181349144"></p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 链路聚合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络-华为路由器vlan划分</title>
      <link href="posts/f68976bc/"/>
      <url>posts/f68976bc/</url>
      
        <content type="html"><![CDATA[<h2 id="配置示例">配置示例</h2><p>路由器上直接划分vlan，下挂二层交换机的简单网络示例。</p><pre><code>vlan batch 100 150 200vlan 100 description 806vlan 150 description wifivlan 200 description 807interface Vlanif1 ip address 192.168.1.1 255.255.255.0 dhcp select interface dhcp server dns-list 192.168.1.1 sa application-statistic enable#interface Vlanif100 description 806-if ip address 192.168.100.1 255.255.255.0 dhcp select interface dhcp server excluded-ip-address 192.168.100.3 192.168.100.20 dhcp server dns-list 192.168.100.1 sa application-statistic enable#interface Vlanif150 description wifi-if ip address 192.168.150.1 255.255.255.0 dhcp select interface dhcp server excluded-ip-address 192.168.150.3 192.168.150.20 dhcp server dns-list 192.168.150.1 sa application-statistic enable#interface Vlanif200 description 807-if ip address 192.168.200.1 255.255.255.0 dhcp select interface dhcp server excluded-ip-address 192.168.200.2 192.168.200.20 dhcp server dns-list 192.168.200.1 sa application-statistic enable#interface GigabitEthernet0/0/1 port hybrid pvid vlan 100 undo port hybrid vlan 1 port hybrid untagged vlan 100 150 200#interface GigabitEthernet0/0/2 port hybrid pvid vlan 200 undo port hybrid vlan 1 port hybrid untagged vlan 100 150 200#interface GigabitEthernet0/0/3 port hybrid pvid vlan 150 undo port hybrid vlan 1 port hybrid untagged vlan 100 150 200#interface GigabitEthernet0/0/4 pppoe-client dial-bundle-number 1 sa application-statistic enable#</code></pre><h2 id="命令解析">命令解析</h2><pre><code> # 定义vlan接口100 # 设定接口地址为 192.168.100.1 并作为网关 # 开启 dhcp interface Vlanif100 description 806-if ip address 192.168.100.1 255.255.255.0 dhcp select interface dhcp server excluded-ip-address 192.168.100.3 192.168.100.20 dhcp server dns-list 192.168.100.1 sa application-statistic enable  # 定义接口模式是hybrid # 设定缺省vlan是100 # 允许携带100、150、200的报文以untagged方式通过接口,便于下挂access口 port link-type hybrid port hybrid pvid vlan 100 undo port hybrid vlan 1 port hybrid untagged vlan 100 150 200</code></pre><h2 id="三种端口模式规则">三种端口模式规则</h2><p><img src="/posts/f68976bc/Access%E7%AB%AF%E5%8F%A3%E6%94%B6%E5%8F%91%E8%A7%84%E5%88%99.png" alt="Access端口收发规则"></p><p><img src="/posts/f68976bc/Trunk%E7%AB%AF%E5%8F%A3%E6%94%B6%E5%8F%91%E6%95%B0%E6%8D%AE%E5%B8%A7%E7%9A%84%E8%A7%84%E5%88%99.png" alt="Trunk端口收发数据帧的规则"></p><p><img src="/posts/f68976bc/Hybrid%E7%AB%AF%E5%8F%A3%E6%94%B6%E5%8F%91%E6%95%B0%E6%8D%AE%E5%B8%A7%E7%9A%84%E8%A7%84%E5%88%99.png" alt="Hybrid端口收发数据帧的规则"></p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络 </tag>
            
            <tag> vlan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开发-认证机制[转载]</title>
      <link href="posts/248f8ccf/"/>
      <url>posts/248f8ccf/</url>
      
        <content type="html"><![CDATA[<p>来源：<a href="https://baijiahao.baidu.com/s?id=1695005220854995140&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1695005220854995140&amp;wfr=spider&amp;for=pc</a></p><p><strong>什么是认证（Authentication）</strong></p><p>通俗地讲就是验证当前用户的身份，证明“你是你自己”（比如：你每天上下班打卡，都需要通过指纹打卡，当你的指纹和系统里录入的指纹相匹配时，就打卡成功）</p><p>互联网中的认证：</p><p>用户名密码登录邮箱发送登录链接手机号接收验证码只要你能收到邮箱/验证码，就默认你是账号的主人<strong>什么是授权（Authorization）</strong></p><p>用户授予第三方应用访问该用户某些资源的权限你在安装手机应用的时候，APP 会询问是否允许授予权限（访问相册、地理位置等权限）你在访问微信小程序时，当登录时，小程序会询问是否允许授予权限（获取昵称、头像、地区、性别等个人信息）实现授权的方式有：cookie、session、token、OAuth<strong>什么是凭证（Credentials）</strong></p><p>实现认证和授权的前提是需要一种媒介（证书） 来标记访问者的身份在战国时期，商鞅变法，发明了照身帖。照身帖由官府发放，是一块打磨光滑细密的竹板，上面刻有持有人的头像和籍贯信息。国人必须持有，如若没有就被认为是黑户，或者间谍之类的。在现实生活中，每个人都会有一张专属的居民身份证，是用于证明持有人身份的一种法定证件。通过身份证，我们可以办理手机卡/银行卡/个人贷款/交通出行等等，这就是认证的凭证。在互联网应用中，一般网站（如掘金）会有两种模式，游客模式和登录模式。游客模式下，可以正常浏览网站上面的文章，一旦想要点赞/收藏/分享文章，就需要登录或者注册账号。当用户登录成功后，服务器会给该用户使用的浏览器颁发一个令牌（token），这个令牌用来表明你的身份，每次浏览器发送请求时会带上这个令牌，就可以使用游客模式下无法使用的功能。<strong>什么是 Cookie</strong></p><p>HTTP 是无状态的协议（对于事务处理没有记忆能力，每次客户端和服务端会话完成时，服务端不会保存任何会话信息）：每个请求都是完全独立的，服务端无法确认当前访问者的身份信息，无法分辨上一次的请求发送者和这一次的发送者是不是同一个人。所以服务器与浏览器为了进行会话跟踪（知道是谁在访问我），就必须主动的去维护一个状态，这个状态用于告知服务端前后两个请求是否来自同一浏览器。而这个状态需要通过 cookie 或者 session 去实现。cookie 存储在客户端： cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。cookie 是不可跨域的： 每个 cookie 都会绑定单一的域名，无法在别的域名下获取使用，一级域名和二级域名之间是允许共享使用的（靠的是 domain）。cookie 重要的属性属性说明name=value键值对，设置 Cookie 的名称及相对应的值，都必须是字符串类型</p><p>如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。domain指定 cookie 所属域名，默认是当前域名path<strong><strong>指定 cookie 在哪个路径（路由）下生效，默认是 ‘/’。如果设置为 /abc，则只有 /abc 下的路由可以访问到该 cookie，如：/abc/read。maxAgecookie 失效的时间，单位秒。如果为整数，则该 cookie 在 maxAge 秒后失效。如果为负数，该 cookie 为临时 cookie ，关闭浏览器即失效，浏览器也不会以任何形式保存该 cookie 。如果为 0，表示删除该 cookie 。默认为 -1。比 expires 好用。expires过期时间，在设置的某个时间点后该 cookie 就会失效。一般浏览器的 cookie 都是默认储存的，当关闭浏览器结束这个会话的时候，这个 cookie 也就会被删除secure该 cookie 是否仅被使用安全协议传输。安全协议有 HTTPS，SSL等，在网络上传输数据之前先将数据加密。默认为false。当 secure 值为 true 时，cookie 在 HTTP 中是无效，在 HTTPS 中才有效。httpOnly</strong></strong>如果给某个 cookie 设置了 httpOnly 属性，则无法通过 JS 脚本 读取到该 cookie 的信息，但还是能通过 Application 中手动修改 cookie，所以只是在一定程度上可以防止 XSS 攻击，不是绝对的安全<strong>什么是 Session</strong></p><p>session 是另一种记录服务器和客户端会话状态的机制session 是基于 cookie 实现的，session 存储在服务器端，sessionId 会被存储到客户端的cookie 中</p><p><img src="https://pics5.baidu.com/feed/a8014c086e061d95df83d4b9429536d960d9cad0.jpeg?token=756fbdcecd06995c10607932d3940ff7" alt="img"></p><p>**session 认证流程：**用户第一次请求服务器的时候，服务器根据用户提交的相关信息，创建对应的 Session请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入到 Cookie 中，同时 Cookie 记录此 SessionID 属于哪个域名当用户第二次访问服务器的时候，请求会自动判断此域名下是否存在 Cookie 信息，如果存在自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。根据以上流程可知，SessionID 是连接 Cookie 和 Session 的一道桥梁，大部分系统也是根据此原理来验证用户登录状态。</p><p><strong>Cookie 和 Session 的区别</strong></p><p>安全性： Session 比 Cookie 安全，Session 是存储在服务器端的，Cookie 是存储在客户端的。存取值的类型不同：Cookie 只支持存字符串数据，想要设置其他类型的数据，需要将其转换成字符串，Session 可以存任意数据类型。有效期不同： Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭（默认情况下）或者 Session 超时都会失效。存储大小不同： 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie，但是当访问量过多，会占用过多的服务器资源。<strong>什么是 Token（令牌）</strong></p><p><strong>Acesss Token</strong></p><p>访问资源接口（API）时所需要的资源凭证简单 token 的组成： uid(用户唯一的身份标识)、time(当前时间的时间戳)、sign（签名，token 的前几位以哈希算法压缩成的一定长度的十六进制字符串）特点：</p><p>服务端无状态化、可扩展性好支持移动端设备安全支持跨程序调用token 的身份验证流程：</p><p><img src="https://pics2.baidu.com/feed/b812c8fcc3cec3fdcfe3b1e3efe9e8378694272b.jpeg?token=2d451727474617304b4216da9845ec5a" alt="img"></p><p>客户端使用用户名跟密码请求登录服务端收到请求，去验证用户名与密码验证成功后，服务端会签发一个 token 并把这个 token 发送给客户端客户端收到 token 以后，会把它存储起来，比如放在 cookie 里或者 localStorage 里客户端每次向服务端请求资源的时候需要带着服务端签发的 token服务端收到请求，然后去验证客户端请求里面带着的 token ，如果验证成功，就向客户端返回请求的数据每一次请求都需要携带 token，需要把 token 放到 HTTP 的 Header 里基于 token 的用户认证是一种服务端无状态的认证方式，服务端不用存放 token 数据。用解析 token 的计算时间换取 session 的存储空间，从而减轻服务器的压力，减少频繁的查询数据库token 完全由应用管理，所以它可以避开同源策略<strong>Refresh Token</strong></p><p>另外一种 token——refresh tokenrefresh token 是专用于刷新 access token 的 token。如果没有 refresh token，也可以刷新 access token，但每次刷新都要用户输入登录用户名与密码，会很麻烦。有了 refresh token，可以减少这个麻烦，客户端直接用 refresh token 去更新 access token，无需用户进行额外的操作。</p><p><img src="https://pics5.baidu.com/feed/5ab5c9ea15ce36d31923a6547b92068fe950b11c.jpeg?token=4adbc859277abb2801031a8393ac0d3d" alt="img"></p><p>Access Token 的有效期比较短，当 Acesss Token 由于过期而失效时，使用 Refresh Token 就可以获取到新的 Token，如果 Refresh Token 也失效了，用户就只能重新登录了。Refresh Token 及过期时间是存储在服务器的数据库中，只有在申请新的 Acesss Token 时才会验证，不会对业务接口响应时间造成影响，也不需要向像Session 一样一直保持在内存中以应对大量的请求。<strong>Token 和 Session 的区别</strong></p><p>Session 是一种记录服务器和客户端会话状态的机制，使服务端有状态化，可以记录会话信息。而 Token 是令牌，访问资源接口（API）时所需要的资源凭证。Token 使服务端无状态化，不会存储会话信息。Session 和 Token 并不矛盾，作为身份认证 Token 安全性比 Session 好，因为每一个请求都有签名还能防止监听以及重放攻击，而 Session 就必须依赖链路层来保障通讯安全了。如果你需要实现有状态的会话，仍然可以增加 Session 来在服务器端保存一些状态。所谓 Session 认证只是简单的把 User 信息存储到 Session 里，因为 SessionID 的不可预测性，暂且认为是安全的。而 Token ，如果指的是 OAuth Token 或类似的机制的话，提供的是 认证 和 授权 ，认证是针对用户，授权是针对 App 。其目的是让某 App 有权利访问某用户的信息。这里的 Token 是唯一的。不可以转移到其它 App上，也不可以转到其它用户上。Session 只提供一种简单的认证，即只要有此 SessionID ，即认为有此 User 的全部权利。是需要严格保密的，这个数据应该只保存在站方，不应该共享给其它网站或者第三方 App。所以简单来说：如果你的用户数据可能需要和第三方共享，或者允许第三方调用 API 接口，用 Token 。如果永远只是自己的网站，自己的 App，用什么就无所谓了。<strong>什么是 JWT</strong></p><p>JSON Web Token（简称 JWT）是目前最流行的跨域认证解决方案。是一种认证授权机制。JWT 是为了在网络应用环境间传递声明而执行的一种基于 JSON 的开放标准（RFC 7519）。JWT 的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源。比如用在用户登录上。可以使用 HMAC 算法或者是 RSA 的公/私密钥对 JWT 进行签名。因为数字签名的存在，这些传递的信息是可信的。阮一峰老师的 JSON Web Token 入门教程 讲的非常通俗易懂，这里就不再班门弄斧了<strong>生成 JWT</strong></p><p><a href="http://jwt.io/www.jsonwebtoken.io/">jwt.io/www.jsonwebtoken.io/</a></p><p><strong>JWT 的原理</strong></p><p><img src="https://pics7.baidu.com/feed/6a63f6246b600c3392c95c525b2d6d07d8f9a165.jpeg?token=83f3cd98d1fd7cc827dd0f4c3a8bc2cf" alt="img"></p><p><strong>JWT 认证流程：</strong></p><p>用户输入用户名/密码登录，服务端认证成功后，会返回给客户端一个 JWT客户端将 token 保存到本地（通常使用 localstorage，也可以使用 cookie）当用户希望访问一个受保护的路由或者资源的时候，需要请求头的 Authorization 字段中使用Bearer 模式添加 JWT，其内容看起来是下面这样Authorization: Bearer复制代码</p><p>服务端的保护路由将会检查请求头 Authorization 中的 JWT 信息，如果合法，则允许用户的行为因为 JWT 是自包含的（内部包含了一些会话信息），因此减少了需要查询数据库的需要因为 JWT 并不使用 Cookie 的，所以你可以使用任何域名提供你的 API 服务而不需要担心跨域资源共享问题（CORS）因为用户的状态不再存储在服务端的内存中，所以这是一种无状态的认证机制<strong>JWT 的使用方式</strong></p><p>客户端收到服务器返回的 JWT，可以储存在 Cookie 里面，也可以储存在 localStorage。<strong>方式一</strong></p><p>当用户希望访问一个受保护的路由或者资源的时候，可以把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP 请求头信息的 Authorization 字段里，使用 Bearer 模式添加 JWT。</p><p>GET /calendar/v1/events Host: <a href="http://api.example.com">api.example.com</a> Authorization: Bearer <token></token></p><p>用户的状态不会存储在服务端的内存中，这是一种 无状态的认证机制服务端的保护路由将会检查请求头 Authorization 中的 JWT 信息，如果合法，则允许用户的行为。由于 JWT 是自包含的，因此减少了需要查询数据库的需要JWT 的这些特性使得我们可以完全依赖其无状态的特性提供数据 API 服务，甚至是创建一个下载流服务。因为 JWT 并不使用 Cookie ，所以你可以使用任何域名提供你的 API 服务而不需要担心跨域资源共享问题（CORS）<strong>方式二</strong></p><p>跨域的时候，可以把 JWT 放在 POST 请求的数据体里。<strong>方式三</strong></p><p>通过 URL 传输</p><p><a href="http://www.example.com/user?token=xxx">http://www.example.com/user?token=xxx</a></p><p><strong>项目中使用 JWT</strong></p><p>**项目地址: <a href="https://github.com/yjdjiayou/jwt-demo">https://github.com/yjdjiayou/jwt-demo</a> **</p><p>Token 和 JWT 的区别</p><p><strong>相同：</strong></p><p>都是访问资源的令牌都可以记录用户的信息都是使服务端无状态化都是只有验证成功后，客户端才能访问服务端上受保护的资源<strong>区别：</strong></p><p>Token：服务端验证客户端发送过来的 Token 时，还需要查询数据库获取用户信息，然后验证 Token 是否有效。JWT：将 Token 和 Payload 加密后存储于客户端，服务端只需要使用密钥解密进行校验（校验也是 JWT 自己实现的）即可，不需要查询或者减少查询数据库，因为 JWT 自包含了用户信息和加密的数据。<strong>常见的前后端鉴权方式</strong></p><p>Session-CookieToken 验证（包括 JWT，SSO）OAuth2.0（开放授权）常见的加密算法</p><p><img src="https://pics1.baidu.com/feed/738b4710b912c8fcbef915fbc662ae4dd6882163.jpeg?token=33bdbdc5fbb6faaab7a8ba68b79d7f52" alt="img"></p><p>哈希算法(Hash Algorithm)又称散列算法、散列函数、哈希函数，是一种从任何一种数据中创建小的数字“指纹”的方法。哈希算法将数据重新打乱混合，重新创建一个哈希值。哈希算法主要用来保障数据真实性(即完整性)，即发信人将原始消息和哈希值一起发送，收信人通过相同的哈希函数来校验原始数据是否真实。哈希算法通常有以下几个特点：</p><p>正像快速：原始数据可以快速计算出哈希值逆向困难：通过哈希值基本不可能推导出原始数据输入敏感：原始数据只要有一点变动，得到的哈希值差别很大冲突避免：很难找到不同的原始数据得到相同的哈希值，宇宙中原子数大约在 10 的 60 次方到 80 次方之间，所以 2 的 256 次方有足够的空间容纳所有的可能，算法好的情况下冲突碰撞的概率很低：2 的 128 次方为 340282366920938463463374607431768211456，也就是 10 的 39 次方级别2 的 160 次方为 1.4615016373309029182036848327163e+48，也就是 10 的 48 次方级别2 的 256 次方为 1.1579208923731619542357098500869 × 10 的 77 次方，也就是 10 的 77 次方<strong>注意：</strong></p><p>以上不能保证数据被恶意篡改，原始数据和哈希值都可能被恶意篡改，要保证不被篡改，可以使用RSA 公钥私钥方案，再配合哈希值。哈希算法主要用来防止计算机传输过程中的错误，早期计算机通过前 7 位数据第 8 位奇偶校验码来保障（12.5% 的浪费效率低），对于一段数据或文件，通过哈希算法生成 128bit 或者 256bit 的哈希值，如果校验有问题就要求重传。<strong>常见问题</strong></p><p><strong>使用 cookie 时需要考虑的问题</strong></p><p>因为存储在客户端，容易被客户端篡改，使用前需要验证合法性不要存储敏感数据，比如用户密码，账户余额使用 httpOnly 在一定程度上提高安全性尽量减少 cookie 的体积，能存储的数据量不能超过 4kb设置正确的 domain 和 path，减少数据传输cookie 无法跨域一个浏览器针对一个网站最多存 20 个Cookie，浏览器一般只允许存放 300 个Cookie移动端对 cookie 的支持不是很好，而 session 需要基于 cookie 实现，所以移动端常用的是 token<strong>使用 session 时需要考虑的问题</strong></p><p>将 session 存储在服务器里面，当用户同时在线量比较多时，这些 session 会占据较多的内存，需要在服务端定期的去清理过期的 session当网站采用集群部署的时候，会遇到多台 web 服务器之间如何做 session 共享的问题。因为 session 是由单个服务器创建的，但是处理用户请求的服务器不一定是那个创建 session 的服务器，那么该服务器就无法拿到之前已经放入到 session 中的登录凭证之类的信息了。当多个应用要共享 session 时，除了以上问题，还会遇到跨域问题，因为不同的应用可能部署的主机不一样，需要在各个应用做好 cookie 跨域的处理。sessionId 是存储在 cookie 中的，假如浏览器禁止 cookie 或不支持 cookie 怎么办？ 一般会把 sessionId 跟在 url 参数后面即重写 url，所以 session 不一定非得需要靠 cookie 实现移动端对 cookie 的支持不是很好，而 session 需要基于 cookie 实现，所以移动端常用的是 token<strong>使用 token 时需要考虑的问题</strong></p><p>如果你认为用数据库来存储 token 会导致查询时间太长，可以选择放在内存当中。比如 redis 很适合你对 token 查询的需求。token 完全由应用管理，所以它可以避开同源策略token 可以避免 CSRF 攻击(因为不需要 cookie 了)移动端对 cookie 的支持不是很好，而 session 需要基于 cookie 实现，所以移动端常用的是 token<strong>使用 JWT 时需要考虑的问题</strong></p><p>因为 JWT 并不依赖 Cookie 的，所以你可以使用任何域名提供你的 API 服务而不需要担心跨域资源共享问题（CORS）JWT 默认是不加密，但也是可以加密的。生成原始 Token 以后，可以用密钥再加密一次。JWT 不加密的情况下，不能将秘密数据写入 JWT。JWT 不仅可以用于认证，也可以用于交换信息。有效使用 JWT，可以降低服务器查询数据库的次数。JWT 最大的优势是服务器不再需要存储 Session，使得服务器认证鉴权业务可以方便扩展。但这也是 JWT 最大的缺点：由于服务器不需要存储 Session 状态，因此使用过程中无法废弃某个 Token 或者更改 Token 的权限。也就是说一旦 JWT 签发了，到期之前就会始终有效，除非服务器部署额外的逻辑。JWT 本身包含了认证信息，一旦泄露，任何人都可以获得该令牌的所有权限。为了减少盗用，JWT的有效期应该设置得比较短。对于一些比较重要的权限，使用时应该再次对用户进行认证。JWT 适合一次性的命令认证，颁发一个有效期极短的 JWT，即使暴露了危险也很小，由于每次操作都会生成新的 JWT，因此也没必要保存 JWT，真正实现无状态。为了减少盗用，JWT 不应该使用 HTTP 协议明码传输，要使用 HTTPS 协议传输。<strong>使用加密算法时需要考虑的问题</strong></p><p>绝不要以明文存储密码永远使用 哈希算法 来处理密码，绝不要使用 Base64 或其他编码方式来存储密码，这和以明文存储密码是一样的，使用哈希，而不要使用编码。编码以及加密，都是双向的过程，而密码是保密的，应该只被它的所有者知道， 这个过程必须是单向的。哈希正是用于做这个的，从来没有解哈希这种说法， 但是编码就存在解码，加密就存在解密。绝不要使用弱哈希或已被破解的哈希算法，像 MD5 或 SHA1 ，只使用强密码哈希算法。绝不要以明文形式显示或发送密码，即使是对密码的所有者也应该这样。如果你需要 “忘记密码” 的功能，可以随机生成一个新的 一次性的（这点很重要）密码，然后把这个密码发送给用户。<strong>分布式架构下 session 共享方案</strong></p><p><strong>1. session 复制</strong></p><p>任何一个服务器上的 session 发生改变（增删改），该节点会把这个 session 的所有内容序列化，然后广播给所有其它节点，不管其他服务器需不需要 session ，以此来保证 session 同步优点： 可容错，各个服务器间 session 能够实时响应。</p><p>缺点： 会对网络负荷造成一定压力，如果 session 量大的话可能会造成网络堵塞，拖慢服务器性能。</p><p><strong>2. 粘性 session /IP 绑定策略</strong></p><p>采用 Ngnix 中的 ip_hash 机制，将某个 ip的所有请求都定向到同一台服务器上，即将用户与服务器绑定。 用户第一次请求时，负载均衡器将用户的请求转发到了 A 服务器上，如果负载均衡器设置了粘性 session 的话，那么用户以后的每次请求都会转发到 A 服务器上，相当于把用户和 A 服务器粘到了一块，这就是粘性 session 机制。优点： 简单，不需要对 session 做任何处理。</p><p>缺点： 缺乏容错性，如果当前访问的服务器发生故障，用户被转移到第二个服务器上时，他的 session 信息都将失效。</p><p>适用场景： 发生故障对客户产生的影响较小；服务器发生故障是低概率事件。实现方式： 以 Nginx 为例，在 upstream 模块配置 ip_hash 属性即可实现粘性 session。</p><p><strong>3. session 共享（常用）</strong></p><p>使用分布式缓存方案比如 Memcached 、Redis 来缓存 session，但是要求 Memcached 或 Redis 必须是集群把 session 放到 Redis 中存储，虽然架构上变得复杂，并且需要多访问一次 Redis ，但是这种方案带来的好处也是很大的：实现了 session 共享；可以水平扩展（增加 Redis 服务器）；服务器重启 session 不丢失（不过也要注意 session 在 Redis 中的刷新/失效机制）；不仅可以跨服务器 session 共享，甚至可以跨平台（例如网页端和 APP 端）</p><p><img src="https://pics6.baidu.com/feed/8b82b9014a90f603dd02486203738f13b151edcb.jpeg?token=5a12b412255809e84b67881b61476094" alt="img"></p><p><strong>4. session 持久化</strong></p><p>将 session 存储到数据库中，保证 session 的持久化优点： 服务器出现问题，session 不会丢失</p><p>缺点： 如果网站的访问量很大，把 session 存储到数据库中，会对数据库造成很大压力，还需要增加额外的开销维护数据库。</p><p><strong>只要关闭浏览器 ，session 真的就消失了？</strong></p><p>不对。对 session 来说，除非程序通知服务器删除一个 session，否则服务器会一直保留，程序一般都是在用户做 log off 的时候发个指令去删除 session。然而浏览器从来不会主动在关闭之前通知服务器它将要关闭，因此服务器根本不会有机会知道浏览器已经关闭，之所以会有这种错觉，是大部分 session 机制都使用会话 cookie 来保存 session id，而关闭浏览器后这个 session id 就消失了，再次连接服务器时也就无法找到原来的 session。如果服务器设置的 cookie 被保存在硬盘上，或者使用某种手段改写浏览器发出的 HTTP 请求头，把原来的 session id 发送给服务器，则再次打开浏览器仍然能够打开原来的 session。恰恰是由于关闭浏览器不会导致 session 被删除，迫使服务器为 session 设置了一个失效时间，当距离客户端上一次使用 session 的时间超过这个失效时间时，服务器就认为客户端已经停止了活动，才会把 session 删除以节省存储空间。</p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞03k8s内置部署</title>
      <link href="posts/bced4977/"/>
      <url>posts/bced4977/</url>
      
        <content type="html"><![CDATA[<h1>通过 prometheus-operator</h1><h2 id="部署文档">部署文档</h2><p><a href="https://prometheus-operator.dev/docs/prologue/quick-start/">https://prometheus-operator.dev/docs/prologue/quick-start/</a></p><p>如果采用的是kubeadm安装的k8s，或许会用到</p><p><a href="https://prometheus-operator.dev/docs/kube-prometheus-on-kubeadm/#kubeadm-pre-requisites">https://prometheus-operator.dev/docs/kube-prometheus-on-kubeadm/#kubeadm-pre-requisites</a></p><p>提到的信息。</p><h2 id="架构图">架构图</h2><p><img src="https://www.qikqiak.com/img/posts/prometheus-operator.png" alt="promtheus opeator"></p><p>这里的 servicemonitor 资源对象很关键</p><h2 id="监控的东西">监控的东西</h2><ul><li>cluster state via kube-state-metrics</li><li>nodes via the node_exporter</li><li>kubelets</li><li>apiserver</li><li>kube-scheduler</li><li>kube-controller-manager</li></ul><h2 id="基本步骤">基本步骤</h2><h3 id="拉取代码">拉取代码</h3><pre><code class="language-shell">git clone https://github.com/prometheus-operator/kube-prometheus.git</code></pre><h3 id="部署到k8s">部署到k8s</h3><p>ℹ️资源会部署在monitoring命名空间中</p><pre><code class="language-shell">kubectl create -f manifests/setup# 等待上述命令资源跑完kubectl create -f manifests/# 等待所有 pod 创建完毕kubectl get pod -n monitoring</code></pre><h3 id="添加ingress配置">添加ingress配置</h3><p>ℹ️需先部署完 ingress，例如</p><pre><code class="language-bash">kubectl get svc -n ingress-nginx===NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                      AGEingress-nginx-controller             LoadBalancer   10.96.210.139   10.200.16.11   80:32489/TCP,443:30936/TCP   88mingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;         443/TCP                      88m</code></pre><p>上述 ingress-nginx-controller 已经分到了 EXTERNAL-IP：10.200.16.11</p><p>部署下面的 ingress 配置</p><pre><code class="language-yaml">kind: IngressapiVersion: networking.k8s.io/v1metadata:  name: prometheus-ingress  namespace: monitoring  annotations:    kubernetes.io/ingress.class: &quot;nginx&quot;spec:  rules:  - host: grafana.it.local    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: grafana            port:              number: 3000  - host: proms.it.local    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: prometheus-k8s            port:              number: 9090  - host: alert.it.local    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: alertmanager-main            port:              number: 9093    </code></pre><p>解析 grafana.it.local 和 proms.it.local 到 svc 对象 ingress-nginx-controller 关联的 EXTERNAL-IP.</p><p>最后通过 <a href="http://grafana.it.local">http://grafana.it.local</a> 和 <a href="http://proms.it.local">http://proms.it.local</a> 访问</p><p>其中 grafana 的默认账户密码都是 admin，效果如图：</p><p><img src="/posts/bced4977/image-20210319171647029.png" alt="image-20210319171647029"></p><p>其中 prometheus 的效果如图：</p><p><img src="/posts/bced4977/image-20210319171728505.png" alt="image-20210319171728505"></p><h3 id="添加告警">添加告警</h3><p>配置相关可以在 kube-prometheus/manifests/alertmanager-secret.yaml 中找到</p><pre><code class="language-bash">apiVersion: v1kind: Secretmetadata:  labels:    alertmanager: main    app.kubernetes.io/component: alert-router    app.kubernetes.io/name: alertmanager    app.kubernetes.io/part-of: kube-prometheus    app.kubernetes.io/version: 0.21.0  name: alertmanager-main  namespace: monitoringstringData:  alertmanager.yaml: |-    global:      resolve_timeout: 5m      http_config: &#123;&#125;      smtp_hello: localhost      smtp_require_tls: true      pagerduty_url: https://events.pagerduty.com/v2/enqueue      opsgenie_api_url: https://api.opsgenie.com/      wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/      victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/    route:      receiver: Default      group_by:      - namespace      routes:      - receiver: Watchdog        match:          alertname: Watchdog      - receiver: Critical        match:          severity: critical      group_wait: 30s      group_interval: 5m      repeat_interval: 12h    inhibit_rules:    - source_match:        severity: critical      target_match_re:        severity: warning|info      equal:      - namespace      - alertname    - source_match:        severity: warning      target_match_re:        severity: info      equal:      - namespace      - alertname    receivers:    - name: Default      webhook_configs:      - url: &quot;...&quot;    - name: Critical      webhook_configs:      - url: &quot;...&quot;    - name: Watchdog    templates: []type: Opaque</code></pre><p>如上命令所示，添加 receivers ，这里均采用 webhook 方式</p><p>部署新配置，并reload alertmanager</p><pre><code class="language-bash">kubectl apply -f alertmanager-secret.yaml curl -X POST http://&lt;alertmanager_addr&gt;/-/reload</code></pre><h3 id="修改默认的prometheus规则">修改默认的prometheus规则</h3><p>默认的规则位于 prometheusrule 资源对象中</p><pre><code class="language-bash">kubectl get prometheusrule -n monitoringNAME                              AGEalertmanager-main-rules           6d22hkube-prometheus-rules             6d22hkube-state-metrics-rules          6d22hkubernetes-monitoring-rules       6d22hnode-exporter-rules               6d22hprometheus-k8s-prometheus-rules   6d22hprometheus-operator-rules         6d22h</code></pre><p>通过 kubectl edit 修改即可</p><h3 id="可能出现的问题">可能出现的问题</h3><h4 id="kube-controller-manager和kube-scheduler状态为down">kube-controller-manager和kube-scheduler状态为down</h4><p>prometheus页面中可能会看到有一些错误，两个核心组件kube-controller-manager和kube-scheduler是down</p><p><img src="/posts/bced4977/image-20210319174713804.png" alt="image-20210319174713804"></p><p>其原因在于，prometheus-operator的ServiceMonitor资源对象指定的svc不存在</p><pre><code class="language-bash">[root@k8s01 my-yaml]# kubectl get servicemonitor -n monitoringNAME                      AGEalertmanager              2d19hblackbox-exporter         2d19hcoredns                   2d19hgrafana                   2d19hkube-apiserver            2d19hkube-controller-manager   2d19hkube-scheduler            2d19hkube-state-metrics        2d19hkubelet                   2d19hnode-exporter             2d19hprometheus-adapter        2d19hprometheus-k8s            2d19hprometheus-operator       2d19h</code></pre><p>以kube-scheduler为例</p><pre><code class="language-bash">[root@k8s01 my-yaml]# kubectl get servicemonitor kube-scheduler -n monitoring -o yamlapiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:......spec:  endpoints:  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token    interval: 30s    port: https-metrics    scheme: https    tlsConfig:      insecureSkipVerify: true  jobLabel: app.kubernetes.io/name  namespaceSelector:    matchNames:    - kube-system  selector:    matchLabels:      app.kubernetes.io/name: kube-scheduler</code></pre><p>可以看到 <code>kube-scheduler</code> 的 <code>servicemonitor</code> 指向拥有 <code>app.kubernetes.io/name: kube-scheduler</code> 和 <code>ports.name: https-metrics</code> 的 svc</p><h4 id="建立servicemonitor所需的svc">建立servicemonitor所需的svc</h4><p>查看服务pod的标签</p><pre><code class="language-bash">[root@k8s01 my-yaml]# kubectl get pod -n kube-system | grep kube-schedulerkube-scheduler-k8s01            1/1     Running   0          36mkube-scheduler-k8s02            1/1     Running   0          18mkube-scheduler-k8s03            1/1     Running   0          16m[root@k8s01 my-yaml]# kubectl get pod kube-scheduler-k8s01  -n kube-system -o yaml | grep -A 2 labels  labels:    component: kube-scheduler    tier: control-plane--        f:labels:          .: &#123;&#125;          f:component: &#123;&#125;</code></pre><p>如上可以看到，在kubeadm安装的k8s中，kube-scheduler的 labels 是 component: kube-scheduler。</p><p>最后生成svc配置</p><pre><code class="language-bash">apiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-scheduler-prometheus  labels:    app.kubernetes.io/name: kube-scheduler  # 关键spec:  selector:    component: kube-scheduler  # 关键  ports:  - name: https-metrics  # 关键    port: 10259  # 关键    targetPort: 10259  # 关键    protocol: TCP---apiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-controller-manager-prometheus  labels:    app.kubernetes.io/name: kube-controller-managerspec:  selector:    component: kube-controller-manager  ports:  - name: https-metrics    port: 10257    targetPort: 10257    protocol: TCP</code></pre><h4 id="修改服务监听地址">修改服务监听地址</h4><p>默认kubeadm安装的kube-controller-manager和kube-scheduler监听地址都是127.0.0.1，这导致无法被采集，因此需要改成0.0.0.0。</p><p>修改 static pod 配置即可。</p><pre><code class="language-bash">sed -e &quot;s/- --address=127.0.0.1/- --address=0.0.0.0/&quot; -i /etc/kubernetes/manifests/kube-controller-manager.yamlsed -e &quot;s/- --address=127.0.0.1/- --address=0.0.0.0/&quot; -i /etc/kubernetes/manifests/kube-scheduler.yaml</code></pre><p>修改完，k8s会自动重建相应的pod。</p><h1>通过 prometheus 配置清单</h1><h2 id="主配置简写">主配置简写</h2><pre><code class="language-yaml"># prometheus-cm.yamlapiVersion: v1kind: ConfigMapmetadata:  name: prometheus-config  namespace: monitordata:  prometheus.yml: |    global:      scrape_interval: 15s      scrape_timeout: 15s    scrape_configs:    - job_name: 'prometheus'      static_configs:      - targets: ['localhost:9090']</code></pre><h2 id="存储">存储</h2><p>本地卷</p><pre><code class="language-yaml"># prometheus-pv.yamlapiVersion: v1kind: PersistentVolumemetadata:  name: prometheus-local  labels:    app: prometheusspec:  accessModes:  - ReadWriteOnce  capacity:    storage: 20Gi  storageClassName: local-storage  local:    path: /data/k8s/prometheus     # 数据存储路径需要提前在k8s节点上创建  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:          - node1                  # 利用节点亲和性存放在 node1 节点上  persistentVolumeReclaimPolicy: Retain     # PVC 删除的时候，PV保留---# prometheus-pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: prometheus-data  namespace: monitorspec:  selector:    matchLabels:      app: prometheus              # 绑定到 app: prometheus 的 pv 上  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 20Gi  storageClassName: local-storage</code></pre><h2 id="prometheus所需的授权服务账户">prometheus所需的授权服务账户</h2><pre><code class="language-yaml"># prometheus-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheusrules:- apiGroups:  - &quot;&quot;  resources:  - nodes  - services  - endpoints  - pods  - nodes/proxy  verbs:  - get  - list  - watch- apiGroups:  - &quot;extensions&quot;  resources:    - ingresses  verbs:  - get  - list  - watch- apiGroups:  - &quot;&quot;  resources:  - configmaps  - nodes/metrics  verbs:  - get- nonResourceURLs:  - /metrics  verbs:  - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prometheusroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheussubjects:- kind: ServiceAccount  name: prometheus  namespace: monitor</code></pre><h2 id="主程序">主程序</h2><pre><code class="language-yaml"># prometheus-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: prometheus  namespace: monitor  labels:    app: prometheusspec:  selector:    matchLabels:      app: prometheus  template:    metadata:      labels:        app: prometheus    spec:      serviceAccountName: prometheus      initContainers:          # 初始化 prometheus 的存储路径权限      - name: fix-permissions        image: busybox        command: [chown, -R, &quot;nobody:nobody&quot;, /prometheus]        volumeMounts:        - name: data          mountPath: /prometheus      containers:      - image: prom/prometheus:v2.34.0        name: prometheus        args:        - &quot;--config.file=/etc/prometheus/prometheus.yml&quot;        - &quot;--storage.tsdb.path=/prometheus&quot;  # 指定tsdb数据路径        - &quot;--storage.tsdb.retention.time=24h&quot;        - &quot;--web.enable-admin-api&quot;  # 控制对admin HTTP API的访问，其中包括删除时间序列等功能        - &quot;--web.enable-lifecycle&quot;  # 支持热更新，直接执行 curl -X POST localhost:9090/-/reload立即生效        ports:        - containerPort: 9090          name: http        volumeMounts:        - name: config-volume          mountPath: &quot;/etc/prometheus&quot;        - name: data          mountPath: &quot;/prometheus&quot;        resources:          requests:            cpu: 100m            memory: 512Mi          limits:  # 根据抓取情况调整资源上限。            cpu: 100m            memory: 512Mi      volumes:      - name: data        persistentVolumeClaim:          claimName: prometheus-data      - name: config-volume        configMap:          name: prometheus-config</code></pre><h2 id="暴漏服务">暴漏服务</h2><pre><code class="language-yaml"># prometheus-svc.yamlapiVersion: v1kind: Servicemetadata:  name: prometheus  namespace: monitor  labels:    app: prometheusspec:  selector:    app: prometheus  type: NodePort  ports:    - name: web      port: 9090      targetPort: http</code></pre><p>通过查询nodeport端口来访问</p><pre><code class="language-bash">➜ kubectl get svc -n monitorNAME         TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGEprometheus   NodePort   10.96.194.29   &lt;none&gt;        9090:30980/TCP   13h</code></pre><p>通过 <code>http://任意节点IP:30980</code> 访问 prometheus 的 webui 服务了</p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞showdoc部署</title>
      <link href="posts/9a7bc037/"/>
      <url>posts/9a7bc037/</url>
      
        <content type="html"><![CDATA[<p>ℹ️ 配置引用的存储是 nfs</p><pre><code class="language-yaml">kind: IngressapiVersion: networking.k8s.io/v1metadata:  name: showdoc-ingress  namespace: it  annotations:    kubernetes.io/ingress.class: &quot;nginx&quot;spec:  rules:  - host: showdoc.xxx.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: showdoc-nginx-svc            port:              number: 80---kind: ServiceapiVersion: v1metadata:  name: showdoc-nginx-svc  namespace: itspec:  selector:    app: showdoc-nginx-pod  ports:  - protocol: TCP    port: 80    targetPort: 80    name: showdoc-nginx-svc---kind: DeploymentapiVersion: apps/v1metadata:  name: showdoc-nginx-dep  namespace: itspec:  replicas: 1  selector:    matchLabels:      app: showdoc-nginx-pod  template:    metadata:      labels:        app: showdoc-nginx-pod    spec:      containers:      - name: showdoc-nginx-pod        image: nginx        ports:        - containerPort: 80        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: showdoc-vol          subPath: showdoc.xxx.com/data          mountPath: /app        - name: showdoc-vc          subPath: nginx.conf          mountPath: /etc/nginx/nginx.conf        - name: showdoc-vc          subPath: showdoc.xxx.com.conf          mountPath: /etc/nginx/conf.d/showdoc.xxx.com.conf        - name: showdoc-vc          subPath: fastcgi.conf          mountPath: /etc/nginx/fastcgi.conf      volumes:      - name: showdoc-vol        persistentVolumeClaim:          claimName: showdoc-pvc      - name: showdoc-vc        configMap:          name: showdoc-cm---kind: ServiceapiVersion: v1metadata:  name: showdoc-php-svc  namespace: itspec:  selector:    app: showdoc-php-pod  ports:  - protocol: TCP    port: 9000    targetPort: 9000    name: showdoc-php-svc---kind: DeploymentapiVersion: apps/v1metadata:  name: showdoc-php-dep  namespace: itspec:  replicas: 1  selector:    matchLabels:      app: showdoc-php-pod  template:    metadata:      labels:        app: showdoc-php-pod    spec:      containers:      - name: showdoc-php-pod        #image: php:7.2-fpm        image: bitnami/php-fpm        ports:        - containerPort: 9000        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: showdoc-vol          subPath: showdoc.xxx.com/data          mountPath: /app      volumes:      - name: showdoc-vol        persistentVolumeClaim:          claimName: showdoc-pvc---kind: ConfigMapapiVersion: v1metadata:  name: showdoc-cm  namespace: itdata:  nginx.conf: |    user nginx;    worker_processes auto;    error_log /dev/stderr;    pid /run/nginx.pid;    include /usr/share/nginx/modules/*.conf;    events &#123;        worker_connections 1024;    &#125;    http &#123;        log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '                          '$status $body_bytes_sent &quot;$http_referer&quot; '                          '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';        access_log  /dev/stdout  main;        sendfile            on;        tcp_nopush          on;        tcp_nodelay         on;        keepalive_timeout   65;        types_hash_max_size 2048;        include             /etc/nginx/mime.types;        default_type        application/octet-stream;        include /etc/nginx/conf.d/*.conf;    &#125;  showdoc.xxx.com.conf: |    server        &#123;            listen 80;            server_name showdoc.xxx.com;            index index.html index.php;            root /app;            location ~ [^/]\.php(/|$)            &#123;                try_files $uri =404;                fastcgi_pass  showdoc-php-svc:9000;                fastcgi_index index.php;                include fastcgi.conf;            &#125;            location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$            &#123;                expires      1d;            &#125;            location ~ .*\.(js|css)?$            &#123;                expires      12h;            &#125;            location ~ /.well-known &#123;                allow all;            &#125;            location ~ /\.            &#123;                deny all;            &#125;        &#125;  fastcgi.conf: |    fastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;    fastcgi_param  QUERY_STRING       $query_string;    fastcgi_param  REQUEST_METHOD     $request_method;    fastcgi_param  CONTENT_TYPE       $content_type;    fastcgi_param  CONTENT_LENGTH     $content_length;    fastcgi_param  SCRIPT_NAME        $fastcgi_script_name;    fastcgi_param  REQUEST_URI        $request_uri;    fastcgi_param  DOCUMENT_URI       $document_uri;    fastcgi_param  DOCUMENT_ROOT      $document_root;    fastcgi_param  SERVER_PROTOCOL    $server_protocol;    fastcgi_param  REQUEST_SCHEME     $scheme;    fastcgi_param  HTTPS              $https if_not_empty;    fastcgi_param  GATEWAY_INTERFACE  CGI/1.1;    fastcgi_param  SERVER_SOFTWARE    nginx/$nginx_version;    fastcgi_param  REMOTE_ADDR        $remote_addr;    fastcgi_param  REMOTE_PORT        $remote_port;    fastcgi_param  SERVER_ADDR        $server_addr;    fastcgi_param  SERVER_PORT        $server_port;    fastcgi_param  SERVER_NAME        $server_name;    fastcgi_param  REDIRECT_STATUS    200;---kind: PersistentVolumeapiVersion: v1metadata:  name:  showdoc-pv  # 定义 pv 名字, 会被 pvc 引用  namespace: itspec:  claimRef:    name: showdoc-pvc    namespace: it  capacity:    storage: 10Gi  # 定义大小  accessModes:  - ReadWriteMany   # 定义访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /mnt/data001/nfs-k8s   # 定义 nfs 共享路径    server: 10.200.16.250        # 定义 nfs 服务器地址---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: showdoc-pvc  namespace: itspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 10Gi  volumeName: showdoc-pv</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> showdoc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s-22自定义hpa</title>
      <link href="posts/74e7acf4/"/>
      <url>posts/74e7acf4/</url>
      
        <content type="html"><![CDATA[<h2 id="原理">原理</h2><p>除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使用 Prometheus Adapter，Prometheus 用于监控应用的负载和集群本身的各种指标，Prometheus Adapter 可以帮我们使用 Prometheus 收集的指标并使用它们来制定扩展策略，这些指标都是通过 APIServer 暴露的，而且 HPA 资源对象也可以很轻易的直接使用。</p><h2 id="结构图">结构图</h2><p><img src="/posts/74e7acf4/image-20220502144924565.png" alt="image-20220502144924565"></p><h2 id="待监控的demo">待监控的demo</h2><p>👙这个demo通过业界约定俗称的注解暴漏metrics接口给prometheus，因此prometheus需要先配置kubernetes_sd_config自动发现的endpoints角色功能。</p><p>hpa-prome-demo.yaml</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: hpa-prom-demospec:  selector:    matchLabels:      app: nginx-server  template:    metadata:      labels:        app: nginx-server    spec:      containers:        - name: nginx-demo          image: cnych/nginx-vts:v1.0          resources:            limits:              cpu: 50m            requests:              cpu: 50m          ports:            - containerPort: 80              name: http---apiVersion: v1kind: Servicemetadata:  name: hpa-prom-demo  annotations:    prometheus.io/scrape: &quot;true&quot;    prometheus.io/port: &quot;80&quot;    prometheus.io/path: &quot;/status/format/prometheus&quot;spec:  ports:    - port: 80      targetPort: 80      name: http  selector:    app: nginx-server  type: NodePort</code></pre><p>在这个demo中，nginx暴漏了一个请求总数的指标<code>nginx_vts_server_requests_total </code>，我们通过这个指标来扩缩。</p><h3 id="检查暴露指标">检查暴露指标</h3><pre><code class="language-bash">curl http://10.200.16.101:30233/status/format/prometheus</code></pre><h2 id="prometheus-adapter">prometheus-adapter</h2><p>我们将 Prometheus-Adapter 安装到集群中，并通过 Prometheus-Adapter 配置规则来查询 Prometheus 数据从而跟踪 Pod 的请求。</p><p>我们可以将 Prometheus 中的任何一个指标都用于 HPA，但是前提是你得通过查询语句将它拿到（包括指标名称和其对应的值）。</p><h3 id="规则流程">规则流程</h3><p>Prometheus-Adapter 的官方文档：</p><p><a href="https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/config.md">https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/config.md</a></p><p>发现Discovery：发现基础指标</p><p>关联Association：将基础指标中的标签与k8s的资源对应起来</p><p>重命名Naming：构建HPA所需要的查询指标名</p><p>查询指标语句Querying：编写HPA所需要的指标数据查询语句，它是一个 go 模板</p><h3 id="规则示例">规则示例</h3><p>定义一个pod级别的qps指标，目的是让hpa监视的pod的qps超过阈值的时候，就进行扩容，低于阈值的时候就缩容。</p><pre><code class="language-yaml">rules:  - seriesQuery: &quot;nginx_vts_server_requests_total&quot;    seriesFilters: []    resources:      overrides:        namespace:           resource: namespace        pod_name:          resource: pod    name: # 构建新的指标名 nginx_vts_server_requests_per_second      matches: &quot;^(.*)_total&quot;        as: &quot;$&#123;1&#125;_per_second&quot;    metricsQuery: (sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[1m])) by (&lt;&lt;.GroupBy&gt;&gt;))</code></pre><p>👙上述规则，对应规则流程的四个部分：</p><p>第一个部分是发现Discovery： seriesQuery，获取 nginx 的请求总数</p><p>第二个部分是关联Association：resources ，关联指标里的 pod_name 和 namespace 标签到 k8s 资源</p><p>第三个部分是重命名Naming：name，构建HPA查询指标名 nginx_vts_server_requests_per_second</p><p>第四个部分是查询指标语句Querying：metricsQuery，编写 nginx_vts_server_requests_per_second 所需的 PromQL</p><p>更详细的解析：</p><ul><li><p>seriesQuery 就是 PromQL 语句，不过这个是基础指标。</p></li><li><p>resources 是将 prometheus 的指标与 k8s 的资源进行关联。</p></li><li><ul><li>overrides 下的 namespace 和 pod_name 是 prometheus 里面指标的标签名。因此这里需要根据 prometheus 查询的指标数据来填写。</li><li>overrides.&lt;指标标签&gt;.resource 下的 namespace 和 pod  是与&lt;指标标签&gt;对应的 k8s 对象类型。</li></ul></li><li><p>metricsQuery</p></li><li><ul><li>&lt;&lt;.Series&gt;&gt; 指的就是 nginx_vts_server_requests_total</li><li>&lt;&lt;.LabelMatchers&gt;&gt; 作用根据 resources 的资源关联，从而在进行查询 k8s 对象暴漏的指标时，自动的将k8s对象信息代入到查询语句指标标签中。例如在例子中的 demo pod 名叫 hpa-prom-demo，位于namespace中，则这里的语句转换后就是 nginx_vts_server_requests_total{pod_name=“hpa-prom-demo-xxx”, namespace=“default”}</li><li>&lt;&lt;.GroupBy&gt;&gt; 根据 resources 指定指标标签进行分组，例如： pod_name</li></ul></li></ul><h3 id="部署">部署</h3><p>添加 repo，拉取 chart</p><p>👙prometheus-community repo 下有很多 chart</p><pre><code class="language-bash">$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts$ helm repo update$ helm pull --untar prometheus-community/prometheus-adapter$ cd prometheus-adapter</code></pre><p>构建 helm hpa-prome-adapter-values.yaml</p><pre><code class="language-yaml">rules:  default: false  custom:    - seriesQuery: &quot;nginx_vts_server_requests_total&quot;      resources:        overrides:          namespace:            resource: namespace          pod_name:            resource: pod      name:        matches: &quot;^(.*)_total&quot;        as: &quot;$&#123;1&#125;_per_second&quot;      metricsQuery: (sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[1m])) by (&lt;&lt;.GroupBy&gt;&gt;))prometheus:  url: http://thanos-querier.kube-mon.svc.cluster.local</code></pre><p>安装</p><pre><code class="language-bash">$ helm upgrade --install prometheus-adapter -f hpa-prome-adapter-values.yaml --namespace monitor .NAME: prometheus-adapterLAST DEPLOYED: Mon Mar 29 18:52:44 2021NAMESPACE: kube-monSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:prometheus-adapter has been deployed.In a few minutes you should be able to list metrics using the following command(s):  kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1</code></pre><h3 id="校验规则">校验规则</h3><p>prometheus-adapter 会创建一个 APIService 类型：<a href="http://v1beta1.custom.metrics.k8s.io">v1beta1.custom.metrics.k8s.io</a></p><p>通过这个 APIService 接口，查询构建的规则以及指标数据都会转到 prometheus-adapter 的 svc 对象。</p><pre><code class="language-bash">➜  prometheus-adapter git:(main) ✗ kubectl get --raw=&quot;/apis/custom.metrics.k8s.io/v1beta1&quot; | jq&#123;  &quot;kind&quot;: &quot;APIResourceList&quot;,  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;groupVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,  &quot;resources&quot;: []&#125;</code></pre><p>👙如上，输出结果，就是服务正常，但是规则没生效。这个常见于规则流程中的 Association 资源关联有问题，需要检查规则中资源关联的 promeheus 指标标签是否存在。</p><p>正确的输出</p><pre><code class="language-yaml">➜  prometheus-adapter git:(main) ✗ kubectl get --raw=&quot;/apis/custom.metrics.k8s.io/v1beta1&quot; | jq&#123;  &quot;kind&quot;: &quot;APIResourceList&quot;,  &quot;apiVersion&quot;: &quot;v1&quot;,  &quot;groupVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,  &quot;resources&quot;: [    &#123;      &quot;name&quot;: &quot;pods/nginx_vts_server_requests_per_second&quot;,      &quot;singularName&quot;: &quot;&quot;,      &quot;namespaced&quot;: true,      &quot;kind&quot;: &quot;MetricValueList&quot;,      &quot;verbs&quot;: [        &quot;get&quot;      ]    &#125;,    &#123;      &quot;name&quot;: &quot;namespaces/nginx_vts_server_requests_per_second&quot;,      &quot;singularName&quot;: &quot;&quot;,      &quot;namespaced&quot;: false,      &quot;kind&quot;: &quot;MetricValueList&quot;,      &quot;verbs&quot;: [        &quot;get&quot;      ]    &#125;  ]&#125;</code></pre><p>通过APIService获取指标</p><pre><code class="language-bash">➜  prometheus-adapter git:(main) ✗ kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second&quot; | jq .&#123;  &quot;kind&quot;: &quot;MetricValueList&quot;,  &quot;apiVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,  &quot;metadata&quot;: &#123;    &quot;selfLink&quot;: &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second&quot;  &#125;,  &quot;items&quot;: [    &#123;      &quot;describedObject&quot;: &#123;        &quot;kind&quot;: &quot;Pod&quot;,        &quot;namespace&quot;: &quot;default&quot;,        &quot;name&quot;: &quot;hpa-prom-demo-bbb6c65bb-zbmzd&quot;,        &quot;apiVersion&quot;: &quot;/v1&quot;      &#125;,      &quot;metricName&quot;: &quot;nginx_vts_server_requests_per_second&quot;,      &quot;timestamp&quot;: &quot;2022-05-02T04:12:45Z&quot;,      &quot;value&quot;: &quot;266m&quot;,      &quot;selector&quot;: null    &#125;  ]&#125;</code></pre><p>👙这里 value: 266m 指的是 qps：0.266</p><h2 id="HPA">HPA</h2><h3 id="配置示例">配置示例</h3><p>监视 deployment/hpa-prom-demo 的所有 pod，当所有的 pod 的 nginx_vts_server_requests_per_second 指标超出阈值或者低于阈值的时候，就进行扩缩容。</p><pre><code class="language-yaml"># hpa-prome.yamlapiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata:  name: nginx-custom-hpaspec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: hpa-prom-demo  minReplicas: 2  maxReplicas: 5  metrics:  - type: Pods    pods:      metric:        name: nginx_vts_server_requests_per_second      target:        type: AverageValue        averageValue: 10 # 或者用 10000m        # m 除以 1000        # target 500 milli-requests per second,        # which is 1 request every two seconds        # averageValue: 500m</code></pre><p>👙需要注意的是 apiVersion 版本。自定义HPA需要用v2版本。通过命令查询版本：</p><pre><code class="language-bash">kubectl api-versions | grep autoscaling</code></pre><h3 id="测试命令">测试命令</h3><p>死循环访问监控demo</p><pre><code class="language-bash">➜  kube-prometheus-myself git:(main) ✗ while true; do wget -q -O- http://10.200.16.101:30233; done</code></pre><h3 id="测试结果">测试结果</h3><pre><code class="language-bash">➜  kube-prometheus-myself git:(main) ✗ kubectl describe hpa nginx-custom-hpaName:                                              nginx-custom-hpaNamespace:                                         defaultLabels:                                            &lt;none&gt;Annotations:                                       &lt;none&gt;CreationTimestamp:                                 Mon, 02 May 2022 14:09:46 +0800Reference:                                         Deployment/hpa-prom-demoMetrics:                                           ( current / target )  &quot;nginx_vts_server_requests_per_second&quot; on pods:  266m / 10Min replicas:                                      2Max replicas:                                      5Deployment pods:                                   2 current / 2 desiredConditions:  Type            Status  Reason            Message  ----            ------  ------            -------  AbleToScale     True    ReadyForNewScale  recommended size matches current size  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from pods metric nginx_vts_server_requests_per_second  ScalingLimited  True    TooFewReplicas    the desired replica count is less than the minimum replica countEvents:  Type    Reason             Age    From                       Message  ----    ------             ----   ----                       -------  Normal  SuccessfulRescale  25m    horizontal-pod-autoscaler  New size: 2; reason: Current number of replicas below Spec.MinReplicas  Normal  SuccessfulRescale  7m28s  horizontal-pod-autoscaler  New size: 3; reason: pods metric nginx_vts_server_requests_per_second above target  Normal  SuccessfulRescale  58s    horizontal-pod-autoscaler  New size: 2; reason: All metrics below target</code></pre><p>使用查询语句<code>(sum(rate(nginx_vts_server_requests_total&#123;&#125;[1m])) by (pod_name))</code>观察Prometheus状态</p><p><img src="/posts/74e7acf4/image-20220502145017245.png" alt="image-20220502145017245"></p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> hpa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>windows☞win10迁移数据</title>
      <link href="posts/88eb7b6/"/>
      <url>posts/88eb7b6/</url>
      
        <content type="html"><![CDATA[<ol><li><p>桌面文件备份</p></li><li><p>子系统备份</p><p>如果用的子系统是Ubuntu，那么目录是</p><p>C:\Users\&lt;家目录&gt;\AppData\Local\Packages\CanonicalGroupLimited.Ubuntu***</p><p>将整个文件夹备份</p></li><li><p>其它分区数据备份</p></li></ol><p>如果有必要，则清理当前电脑的无用资源，直接整个系统盘进行Ghost克隆还原也是可以的</p><ol start="4"><li>新系统WSL安装</li></ol><pre><code class="language-powershell">wsl --list --onlinewsl --install -d Ubuntu-20.04</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> windows </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ubuntu </tag>
            
            <tag> windows10 </tag>
            
            <tag> 系统 </tag>
            
            <tag> 迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞02外置prometheus监控k8s</title>
      <link href="posts/e172b6e2/"/>
      <url>posts/e172b6e2/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><p>prometheus 通过k8s的serviceaccount来获取授权，从而在k8s的apiserver那里获取 k8s 指标，指标需要从三个方面获取：</p><ul><li>kubelet 内置的 cadvisor</li><li>kubernetes-apiserver.metrics-server (需要额外安装)</li><li>kube-state-metrics (第三方APP，需要提前自行安装)</li></ul><p>而上述三个方面，均需要 prometheus 去访问 k8s apiserver。这里需要提前创建授权token.</p><ul><li>cAdvisor：cAdvisor是Google开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持Docker容器，Kubernetes中，我们不需要单独去安装，cAdvisor作为kubelet内置的一部分程序可以直接使用</li><li>kube-state-metrics：通过监听API Server生成有关资源对象的状态指标，比如Deployment、Node、Pod，需要注意的是kube-state-metrics只是简单的提供一个metrics数据，并不会存储这些指标数据，所以我们可以使用Prometheus来抓取这些数据然后存储</li><li>metrics-server：是一个集群范围内的资源数据局和工具，是Heapster的代替品，同样的，metrics-server也只是显示数据，并不提供数据存储服务。他当前的核心作用是：为HPA等组件提供决策指标支持。也可以将接收到的数据存储到influxdb进行存储,简单来说,如果想基础监控,那么就要安装这个组件</li></ul><p>不过kube-state-metrics和metrics-server之前还有很大不同的，二者主要区别如下</p><p>1.kube-state-metrics主要关注的是业务相关的一些元数据，比如Deployment、Pod、副本状态等</p><p>2.metrics-service主要关注的是资源度量API的实现，比如CPU、文件描述符、内存、请求延时等指标</p><h1>授权认证</h1><h2 id="kube-apiserver-client访问证书">kube-apiserver-client访问证书</h2><p>💥如果prometheus配置了<code>kubernetes_sd_configs.tls_config.insecure_skip_verify: true</code>，即禁用服务器证书认证，则无需这个步骤。</p><pre><code class="language-bash">openssl genrsa -out prometheus.key 2048openssl req -new -key prometheus.key -out prometheus.csr -subj &quot;/CN=prometheus/O=it&quot;RequestStr=`cat prometheus.csr | base64 | tr -d &quot;\n&quot;`# 提交申请到k8scat &lt;&lt;EOF | kubectl apply -f -apiVersion: certificates.k8s.io/v1beta1kind: CertificateSigningRequestmetadata:  name: prometheusspec:  groups:  - system:authenticated  request: $&#123;RequestStr&#125;  signerName: kubernetes.io/kube-apiserver-client  usages:  - client authEOF# 如果不出错，可以审批kubectl certificate approve prometheus# 导出证书kubectl get csr prometheus -o jsonpath='&#123;.status.certificate&#125;' | base64 --decode &gt; prometheus.crt# 检查证书有效期openssl x509 -in prometheus.crt -noout -dates</code></pre><blockquote><p>保存好证书文件 prometheus.crt，prometheus配置所需</p></blockquote><h2 id="创建服务账户-集群角色-集群角色绑定对象">创建服务账户/集群角色/集群角色绑定对象</h2><p>用于查询k8s资源。</p><pre><code class="language-bash">kubectl create ns monitor</code></pre><p>prometheus-rabc.yaml</p><pre><code class="language-yaml">apiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheusrules:- apiGroups:  - &quot;&quot;  resources:  - nodes  - services  - endpoints  - pods  - nodes/proxy  verbs:  - get  - list  - watch- apiGroups:  - &quot;extensions&quot;  resources:    - ingresses  verbs:  - get  - list  - watch- apiGroups:  - &quot;&quot;  resources:  - configmaps  - nodes/metrics  verbs:  - get- nonResourceURLs:  - /metrics  verbs:  - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prometheusroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheussubjects:- kind: ServiceAccount  name: prometheus  namespace: monitor</code></pre><p>因prometheus访问k8s需要拿到bearer_token，因此还需要获取服务账户prometheus的Tokens。</p><p>获取服务账户prometheus存储Token的secret对象名</p><pre><code class="language-bash">➜   kubectl get sa prometheus -n monitor -o jsonpath=&#123;'.secrets[0].name'&#125; | moreprometheus-token-2gljj</code></pre><p>将Token进行解密</p><pre><code class="language-bash">➜   kubectl get secret prometheus-token-2gljj -n monitor -o jsonpath=&#123;'.data.token'&#125; | base64 -d &gt; prometheus.bearer_token</code></pre><blockquote><p>保存好解密后的 prometheus.bearer_token，prometheus配置所需</p></blockquote><h1>安装Metrics-server</h1><p>添加 metrics-server  <a href="https://github.com/kubernetes-sigs/metrics-server#configuration">https://github.com/kubernetes-sigs/metrics-server#configuration</a></p><pre><code class="language-bash">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml# 修改metrics-server容器参数部分，添加额外的启动参数(arg)args:  - --kubelet-preferred-address-types=InternalIP  - --kubelet-insecure-tlskubectl apply -f components.yamlkubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;</code></pre><blockquote><p>kubectl top 指令需要指标才能输出</p></blockquote><h1>安装kube-state-metrics</h1><p>部署方式：<a href="https://github.com/kubernetes/kube-state-metrics/tree/master/examples/standard">点我</a></p><p>指标文档：<a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs">https://github.com/kubernetes/kube-state-metrics/tree/master/docs</a></p><pre><code class="language-bash"># 文件克隆到本地后的列表清单：➜  standard git:(master) ✗  git clone https://github.com/kubernetes/kube-state-metrics.gittotal 20-rw-r--r-- 1 zyh zyh  381 Mar 12 12:07 cluster-role-binding.yaml-rw-r--r-- 1 zyh zyh 1744 Mar 12 12:07 cluster-role.yaml-rw-r--r-- 1 zyh zyh 1134 Mar 12 17:29 deployment.yaml-rw-r--r-- 1 zyh zyh  197 Mar 12 12:07 service-account.yaml-rw-r--r-- 1 zyh zyh  410 Mar 12 12:07 service.yaml</code></pre><blockquote><p>需要注意的是，<a href="http://deployment.xn--yamlkube-state-metricsk8s-3b63b637k9tejna412ib11erhyf.gcr.io/kube-state-metrics/kube-state-metrics%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%9C%B0%E5%9D%80%E5%9B%BD%E5%86%85%E6%B2%A1%E6%B3%95%E8%AE%BF%E9%97%AE%E3%80%82%E5%9B%A0%E6%AD%A4%E9%9C%80%E8%A6%81%E6%9B%BF%E6%8D%A2%E4%B8%BA%E5%85%B6%E5%AE%83%E5%9C%B0%E5%9D%80%E3%80%82">deployment.yaml中kube-state-metrics的容器地址是k8s.gcr.io/kube-state-metrics/kube-state-metrics，这个地址国内没法访问。因此需要替换为其它地址。</a></p></blockquote><p>可以将项目clone下来，然后根据项目源代码里的Dockerfile来，自行build一个镜像。</p><pre><code class="language-bash">ARG GOVERSION=1.17ARG GOARCH=amd64FROM golang:$&#123;GOVERSION&#125; as builderARG GOARCHENV GOARCH=$&#123;GOARCH&#125;WORKDIR /go/src/k8s.io/kube-state-metrics/COPY . /go/src/k8s.io/kube-state-metrics/RUN make build-localFROM gcr.io/distroless/static:latest-$&#123;GOARCH&#125;COPY --from=builder /go/src/k8s.io/kube-state-metrics/kube-state-metrics /USER nobodyENTRYPOINT [&quot;/kube-state-metrics&quot;, &quot;--port=8080&quot;, &quot;--telemetry-port=8081&quot;]EXPOSE 8080 8081</code></pre><p><code>GOARCH</code>指定环境，其余无需修改。</p><h2 id="根据情况修改-kube-state-metrics-的-service-对象">根据情况修改 kube-state-metrics 的 service 对象</h2><p>集群外的prometheus无法直接访问集群内的kube-state-metrics服务，因此需要暴漏kube-state-metrics。</p><p>默认配置里，service对象是ClusterIP，仅针对了集群内部。</p><ul><li>prometheus + 阿里云 ASK</li></ul><p>通过内网负载均衡器提供。</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  labels:    app.kubernetes.io/component: exporter    app.kubernetes.io/name: kube-state-metrics    app.kubernetes.io/version: 2.3.0  name: kube-state-metrics  namespace: kube-system  annotations:    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-spec: slb.s1.small    service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-additional-resource-tags: &quot;project=cms-saas&quot;spec:  type: LoadBalancer  ports:  - name: http-metrics    port: 8080    targetPort: http-metrics  - name: telemetry    port: 8081    targetPort: telemetry  selector:    app.kubernetes.io/name: kube-state-metrics</code></pre><h2 id="安装">安装</h2><pre><code class="language-bash">➜kubectl apply -f .</code></pre><h2 id="检查服务是否正常">检查服务是否正常</h2><pre><code class="language-bash">curl ip:8080/metrics</code></pre><h2 id="遇到的问题">遇到的问题</h2><p>性能不够（250m/512Mi，针对3000+ Job对象)，导致接口数据返回过慢，引发prometheus数据采集失败。</p><h1>配置Prometheus</h1><p><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config</a></p><p>通过<code>kubernetes_sd_configs</code>.<code>role: node</code>发现 kubernetes 的节点上的kubelet地址，并将其作为刮取数据源</p><p>通过<code>kubernetes_sd_configs</code>.<code>role: endpoints</code>发现 kubernetes service 对象的 endpoint 地址，从而找到 kube-state-metrics 的外部端点地址。</p><p>✨配置里需要将api_server替换成k8s的api_server服务地址.</p><pre><code class="language-yaml">  - job_name: 'k8s-cadvisor' # 抓容器, cadvisor被整合在kubelet中    scheme: https    metrics_path: /metrics    tls_config:      #ca_file: /etc/prometheus/conf/prometheus.crt      insecure_skip_verify: true    bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    kubernetes_sd_configs:    - role: node      api_server: &quot;https://&lt;api_server_ip&gt;:6443&quot;      tls_config:        #ca_file: /etc/prometheus/conf/prometheus.crt        insecure_skip_verify: true      bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    relabel_configs:    - action: labelmap # 标签映射：抓取regex正则匹配的标签名，复制为replacement，期间标签值不变（replacement为$1时可以不写）      regex: __meta_kubernetes_node_label_(.+)      replacement: $1    metric_relabel_configs:    - source_labels: [instance]      separator: ;      regex: (.+)      target_label: node      replacement: $1      action: replace    - source_labels: [pod_name] # 兼容老集群，老集群曾用标签 pod_name      separator: ;      regex: (.+)      target_label: pod      replacement: $1      action: replace    - source_labels: [container_name] # 兼容老集群，老集群曾用标签 container_name      separator: ;      regex: (.+)      target_label: container      replacement: $1      action: replace        - job_name: 'kubernetes-apiservers'    scheme: https    tls_config:      #ca_file: /etc/prometheus/conf/prometheus.crt      insecure_skip_verify: true    bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    kubernetes_sd_configs:    - role: endpoints      api_server: &quot;https://&lt;api_server_ip&gt;:6443&quot;      tls_config:        #ca_file: /etc/prometheus/conf/prometheus.crt        insecure_skip_verify: true      bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    relabel_configs:    - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]      action: keep      regex: default;kubernetes;https  - job_name: &quot;kubernetes-state-metrics&quot; # 抓工作负载资源    scheme: http    tls_config:      #ca_file: /etc/prometheus/conf/prometheus.crt      insecure_skip_verify: true    bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    kubernetes_sd_configs:    - role: endpoints      api_server: &quot;https://&lt;api_server_ip&gt;:6443&quot;      tls_config:        #ca_file: /etc/prometheus/conf/prometheus.crt        insecure_skip_verify: true      bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    relabel_configs:    - source_labels: [__meta_kubernetes_service_name] # 将自动发现的endpoint只保留kube-state-metrics      regex: kube-state-metrics      replacement: $1      action: keep    metric_relabel_configs: # 排除部分kube-state-metrics采集项目    - source_labels: [__name__]      regex: &quot;^go_.*|^kube_job_.*&quot;  # 如果删除 job 会导致 job 对象监控失败.      action: drop    - source_labels: [pod]      regex: &quot;^无需监控的Pod名称前缀-.*&quot;      action: drop</code></pre><h1>添加grafana模板</h1><p><a href="https://grafana.com/grafana/dashboards/13105">https://grafana.com/grafana/dashboards/13105</a></p><p><img src="/posts/e172b6e2/image-20210313180815651.png" alt="image-20210313180815651"></p><p>上图是我在阿里云ASK中测试的最终效果图</p><h2 id="调整模板">调整模板</h2><p>如果你发现模板中，【网络带宽】之类的没有数据，则需要看下此板块SQL里的标签过滤是否有误。</p><p>在我的环境里，模板中过滤的 name=‘^k8s_.*’ 无法匹配到Pod。因此删除SQL里的这个过滤即可。</p><h1>添加预警</h1><p>需要明确，我们需要监控什么</p><ol><li>k8s本身各组件状态</li><li>调度了多少个replicas？现在可用的有几个？</li><li>Pod是否启动成功</li><li>Pod重启速率？</li></ol><pre><code class="language-yaml">groups:- name: kubernetes  rules:  - alert: kube endpoint down    expr: (up&#123;job=&quot;kube-state-metrics&quot;&#125; or up&#123;job=&quot;kubernetes-apiservers&quot;&#125;) != 1  #0不正常，1正常    for: 5m  #持续时间 ，表示持续5分钟获取不到信息，则触发报警    labels:      severity: error      cluster: k8s    annotations:      summary: &quot;Instance:&#123;&#123; $labels.instance &#125;&#125;, Job &#123;&#123; $labels.job &#125;&#125; stop &quot;      sourcedata: '&#123;&#123; $labels.job &#125;&#125;'  - alert: JobFailed    expr: kube_job_status_failed == 1    for: 5m    labels:      severity: error      cluster: k8s    annotations:      summary: 'Namespace: &#123;&#123; $labels.namespace &#125;&#125;, Job: &#123;&#123; $labels.job &#125;&#125; run failed.'      sourcedata: '&#123;&#123; $labels.job &#125;&#125;'  - alert: PodReady    expr: kube_pod_container_status_ready != 1    for: 5m   #Ready持续5分钟，说明启动有问题    labels:      severity: warning      cluster: k8s    annotations:      summary: 'Namespace: &#123;&#123; $labels.namespace &#125;&#125;, Pod: &#123;&#123; $labels.pod &#125;&#125; not ready for 5m'      sourcedata: '&#123;&#123; $labels.job &#125;&#125;'  - alert: PodRestart    expr: kube_pod_container_status_last_terminated_reason == 1 and on(container) rate(kube_pod_container_status_restarts_total[5m]) * 300 &gt; 1    for: 5s    labels:      severity: error      cluster: k8s    annotations:      summary: 'namespace: &#123;&#123; $labels.namespace &#125;&#125;, pod: &#123;&#123; $labels.pod &#125;&#125; restart rate &gt; &#123;&#123; $value &#125;&#125;'      sourcedata: '&#123;&#123; $labels.job &#125;&#125;'  - alert: Deployment_replicas_available_num_low    expr: kube_deployment_status_replicas - kube_deployment_status_replicas_available &gt; 0    for: 1m    labels:      severity: warning      cluster: k8s    annotations:      summary: 'namespace: &#123;&#123; $labels.namespace &#125;&#125;, deployment: &#123;&#123; $labels.deployment &#125;&#125; available low'      sourcedata: '&#123;&#123; $labels.job &#125;&#125;'  - alert: Deployment_replicas_unavailable    expr: kube_deployment_status_replicas &gt; 0 and kube_deployment_status_replicas_available == 0    for: 1m    labels:      severity: error      cluster: k8s    annotations:      summary: 'namespace: &#123;&#123; $labels.namespace &#125;&#125;, deployment: &#123;&#123; $labels.deployment &#125;&#125; unavailable'      sourcedata: '&#123;&#123; $labels.job &#125;&#125;'</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyun-ack_ingress</title>
      <link href="posts/bd60be5f/"/>
      <url>posts/bd60be5f/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p><a href="https://help.aliyun.com/document_detail/398740.html#section-eph-gef-b2h">https://help.aliyun.com/document_detail/398740.html#section-eph-gef-b2h</a></p><h2 id="构建-nginx-ingress-controller-专属节点池">构建 nginx-ingress-controller 专属节点池</h2><p><a href="https://help.aliyun.com/document_detail/86750.htm?spm=a2c4g.11186623.0.0.198d3d01oJIf2x#task-1339886">https://help.aliyun.com/document_detail/86750.htm?spm=a2c4g.11186623.0.0.198d3d01oJIf2x#task-1339886</a></p><ol><li>创建 ingress-nginx 节点池，并设置节点角色标签 <code>node-role.kubernetes.io/ingress: true</code> 和污点<code>node-role.kubernetes.io/ingress=true:NoExecute</code></li></ol><p><img src="/posts/bd60be5f/image-20220424232115597.png" alt="image-20220424232115597"></p><ol start="2"><li>验证</li></ol><pre><code class="language-bash">➜   kubectl get nodeNAME                    STATUS   ROLES     AGE   VERSIONcn-beijing.10.0.0.39    Ready    ingress   65m   v1.20.11-aliyun.1cn-beijing.10.0.0.41    Ready    &lt;none&gt;    62m   v1.20.11-aliyun.1cn-beijing.10.0.1.234   Ready    &lt;none&gt;    62m   v1.20.11-aliyun.1</code></pre><p>✨cn-beijing.10.0.0.39 就是 ingress-nginx 节点池里的节点。</p><ol start="3"><li><p>集群详情页-运维管理-组件管理-Nginx Ingress Controller-安装</p></li><li><p>待 Nginx Ingress Controller 安装完毕后，添加新配置，将 nginx-ingress-controller 调度到 ingress-nginx 专属节点池</p></li></ol><p>191-193 以及 201-205 行为新增行</p><pre><code class="language-bash">191       nodeSelector:192         kubernetes.io/os: linux193         node-role.kubernetes.io/ingress: &quot;true&quot;194       priorityClassName: system-node-critical195       restartPolicy: Always196       schedulerName: default-scheduler197       securityContext: &#123;&#125;198       serviceAccount: ingress-nginx199       serviceAccountName: ingress-nginx200       terminationGracePeriodSeconds: 300201       tolerations:202       - effect: NoExecute203         key: node-role.kubernetes.io/ingress204         operator: Equal205         value: &quot;true&quot;</code></pre><p>✨默认阿里云的 nginx-ingress-controller 位于 kube-system 命名空间</p><h2 id="优化-nginx-ingress-controller">优化 nginx-ingress-controller</h2><p><a href="https://help.aliyun.com/document_detail/202125.htm?spm=a2c4g.11186623.0.0.198d3d01oJIf2x#task-2036582">https://help.aliyun.com/document_detail/202125.htm?spm=a2c4g.11186623.0.0.198d3d01oJIf2x#task-2036582</a></p><h2 id="nginx-ingress-lb-服务对象创建-SLB-失败">nginx-ingress-lb 服务对象创建 SLB 失败</h2><pre><code class="language-bash">kubectl get -n kube-system svc nginx-ingress-lb -o yaml &gt; nginx-ingress-lb.yaml</code></pre><p>清理掉生成的状态配置</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;service.beta.kubernetes.io/alibaba-cloud-loadbalancer-resource-group-id&quot;:&quot;rg-acfm3nywkge2z2a&quot;&#125;,&quot;labels&quot;:&#123;&quot;app&quot;:&quot;nginx-ingress-lb&quot;&#125;,&quot;name&quot;:&quot;nginx-ingress-lb&quot;,&quot;namespace&quot;:&quot;kube-system&quot;&#125;,&quot;spec&quot;:&#123;&quot;externalTrafficPolicy&quot;:&quot;Local&quot;,&quot;ipFamilyPolicy&quot;:&quot;SingleStack&quot;,&quot;ports&quot;:[&#123;&quot;name&quot;:&quot;http&quot;,&quot;port&quot;:80,&quot;targetPort&quot;:80&#125;,&#123;&quot;name&quot;:&quot;https&quot;,&quot;port&quot;:443,&quot;targetPort&quot;:443&#125;],&quot;selector&quot;:&#123;&quot;app&quot;:&quot;ingress-nginx&quot;&#125;,&quot;type&quot;:&quot;LoadBalancer&quot;&#125;&#125;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-resource-group-id: rg-acfm3nywkge2z2a  finalizers:  - service.k8s.alibaba/resources  labels:    app: nginx-ingress-lb  name: nginx-ingress-lb  namespace: kube-systemspec:  externalTrafficPolicy: Local  healthCheckNodePort: 31836  ports:  - name: http    nodePort: 31947    port: 80    protocol: TCP    targetPort: 80  - name: https    nodePort: 32283    port: 443    protocol: TCP    targetPort: 443  selector:    app: ingress-nginx  sessionAffinity: None  type: LoadBalancer</code></pre><p>删除 nginx-ingress-lb 然后重新创建</p><pre><code class="language-bash">kubectl delete -f nginx-ingress-lb.yaml &amp;&amp; kubectl apply -f nginx-ingress-lb.yaml</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
            <tag> kubernetes </tag>
            
            <tag> ingress </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞利用acme.sh管理免费的ssl证书申请</title>
      <link href="posts/d2fed0bf/"/>
      <url>posts/d2fed0bf/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>申请免费的ssl证书途径有很多，例如阿里云的1年期，或者freessl这种站点。</p><p>如果你想自己部署一个服务来管理并自动更新，则可以使用 <a href="http://acme.sh">acme.sh</a></p><p><a href="http://acme.sh">acme.sh</a> 是一个开源程序，托管在github上。</p><h2 id="安装">安装</h2><p><a href="https://github.com/acmesh-official/acme.sh/wiki/%E8%AF%B4%E6%98%8E#1-%E5%AE%89%E8%A3%85-acmesh">https://github.com/acmesh-official/acme.sh/wiki/说明#1-安装-acmesh</a></p><p>参考官方文档即可，很简单，不过国内经常受限于网络问题导致无法下载，因此你可以在 gitee 上下载</p><p><a href="https://gitee.com/neilpang/acme.sh?_from=gitee_search#https://github.com/acmesh-official/acme.sh/wiki/%E8%AF%B4%E6%98%8E">https://gitee.com/neilpang/acme.sh?_from=gitee_search#https://github.com/acmesh-official/acme.sh/wiki/说明</a></p><p>不过 gitee 上看起来并不是最新的</p><h2 id="使用">使用</h2><h3 id="申请-ssl">申请 ssl</h3><p>以阿里云的dns托管解析为例，给abc.com创建免费ssl证书 <a href="https://letsencrypt.org/zh-cn/">Let’s Encrypt</a></p><p>创建访问阿里云dns解析所需的ram策略，请直接使用管理员策略，但是可以附加源ip限制</p><p>💁说实话我单独授权云解析策略就是不行，至于原因没有具体去验证。所以如果你授权了管理员策略，务必添加源ip限制</p><pre><code class="language-bash">DnsAccessKey=DnsAccessSecret=export  Ali_Key=$&#123;DnsAccessKey&#125;export  Ali_Secret=$&#123;DnsAccessSecret&#125;D1=abc.comDnsMode=dns_aliacme.sh --issue --dns $&#123;DnsMode&#125; -d $&#123;D1&#125; -d *.$&#123;D1&#125; </code></pre><blockquote><p>–dns 指定域名所在的托管解析商</p><p>-d 证书关联的域名</p></blockquote><p>这个步骤会给域名添加新的txt记录，用来校验域名是否归你所有</p><p>但是在写这篇文章的时候，有新的问题出现，就是acme.sh默认调用cloudflare和google的dns去验证添加的记录是否已生效，而国内一些服务商访问国际接口那是一个稀烂🤕 所以可能会无限的卡在校验这里。</p><p>根据<a href="https://github.com/acmesh-official/acme.sh/wiki/dnscheck">官方文档</a>，你可以添加 --dnssleep 300 去忽略掉。</p><h3 id="部署ssl">部署ssl</h3><p>将生成的密钥和证书放在指定位置，这里的key和cer分别对应nginx所需的key和cert</p><pre><code class="language-bash">acme.sh  --installcert  -d $&#123;D1&#125; --key-file $&#123;D1_DIR&#125;/$&#123;D1&#125;.key --fullchain-file $&#123;D1_DIR&#125;/fullchain.cer</code></pre><h3 id="更新">更新</h3><p><a href="http://acme.sh">acme.sh</a> 会在用户级别的crontab中添加自动更新指令。通过<code>crontab -l</code>可以看到。</p><p>如果你需要手动强制更新，则可以执行</p><pre><code class="language-bash"> acme.sh --renew -d $&#123;D1&#125; -d *.$&#123;D1&#125;</code></pre><h3 id="配置">配置</h3><p><a href="http://acme.sh">acme.sh</a> 的配置文件默认放在 ~/.acme.sh/account.conf 中，一般来说需要用户提供的环境变量，都会以 <code>SAVED_</code>开头。</p><h3 id="通告">通告</h3><p>如果你想接收 <a href="http://acme.sh">acme.sh</a> 处理后的消息，则需要配置，不同的消息体可以看<a href="https://github.com/acmesh-official/acme.sh/wiki/notify">官方文档</a></p><p>主要分为三个部分：</p><ul><li>消息级别</li><li>消息模式</li><li>消息钩子</li></ul><p>以自定义的 smtp 为例，你需要提供以下变量</p><pre><code class="language-bash">export SMTP_FROM=export SMTP_TO=export SMTP_HOST=&quot;smtp.feishu.cn&quot;export SMTP_SECURE=&quot;tls&quot;export SMTP_PORT=&quot;587&quot;export SMTP_USERNAME=export SMTP_PASSWORD=# export SMTP_BIN=&quot;/path/to/python_or_curl&quot;export SMTP_TIMEOUT=&quot;30&quot;</code></pre><p>并执行</p><pre><code class="language-bash">acme.sh --set-notify --notify-level 3 --notify-mode 0 --notify-hook smtp --accountconf ~/.acme.sh/account.conf</code></pre><p>如此以来，<a href="http://acme.sh">acme.sh</a> 会将你配置的环境变量追加存入 account.conf</p><h2 id="其它问题">其它问题</h2><h3 id="单一托管商多账户的问题">单一托管商多账户的问题</h3><p>在写这篇文章的时候，<a href="http://acme.sh">acme.sh</a> 当前并不支持（19年作者提出有时间会搞 🤺 ），但是你可以通过配置多个 account.conf，并在计划任务中使用 --accountconf 来写入多条更新计划命令来实现配置文件级别的轮询处理。就像下面这样:</p><pre><code class="language-bash">34 0 * * * &quot;/home/it/.acme.sh&quot;/acme.sh --cron --home &quot;/home/it/.acme.sh&quot; --accountconf /home/it/.acme.sh/account.conf.aliyun34 1 * * * &quot;/home/it/.acme.sh&quot;/acme.sh --cron --home &quot;/home/it/.acme.sh&quot; --accountconf /home/it/.acme.sh/account.conf.aws34 2 * * * &quot;/home/it/.acme.sh&quot;/acme.sh --cron --home &quot;/home/it/.acme.sh&quot; --accountconf /home/it/.acme.sh/account.conf.godaddy</code></pre><p>当然因为域名在生成ssl证书的时候，acme.sh并没有给域名关联某个配置的记录。因此每一条更新计划任务均会处理所有的域名，这必然会产生一些错误信息（即：xxx托管商找不到xxx域名），不过这无关紧要。</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> acme.sh </tag>
            
            <tag> ssl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞使用expect自动登录ssh后远程会话不会跟随本地会话缩放的问题</title>
      <link href="posts/7b3cf3d8/"/>
      <url>posts/7b3cf3d8/</url>
      
        <content type="html"><![CDATA[<h2 id="原因">原因</h2><p>expect 执行的时候，并不会将窗口大小改变的信号传递给远程会话。所以在脚本中添加即可。</p><h2 id="解决">解决</h2><pre><code class="language-bash">#!/usr/bin/env expect #trap sigwinch spawnedtrap &#123; set rows [stty rows] set cols [stty columns] stty rows $rows columns $cols &lt; $spawn_out(slave,name)&#125; WINCH</code></pre><blockquote><p>在脚本开头加入 trap</p></blockquote><h2 id="来源">来源</h2><p><a href="http://blog.sina.com.cn/s/blog_a73a649601018sgl.html">http://blog.sina.com.cn/s/blog_a73a649601018sgl.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ssh </tag>
            
            <tag> expect </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyun-k8s_svc绑定elb</title>
      <link href="posts/6723fbb4/"/>
      <url>posts/6723fbb4/</url>
      
        <content type="html"><![CDATA[<h2 id="svc阿里云特殊注解">svc阿里云特殊注解</h2><pre><code>    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-spec: slb.s1.small      # 申请的elb资源类型    service.beta.kubernetes.io/alicloud-loadbalancer-address-type: intranet       # 申请的elb内外网类型(如果不显式指定，就是外网)    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-additional-resource-tags: &quot;A=B&quot; # 附加的阿里云资源标签    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id: &quot;lb-8vbblzrcxxxxxxxx&quot;  # 绑定已有的ELB-ID    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-force-override-listeners: &quot;true&quot; # 是否将svc声明的spec规则强制覆盖ELB已有规则</code></pre><h2 id="例子">例子</h2><p>通过绑定已有的elb，避免因elb变更，导致对外服务的ip发生变动。</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  name: svc-nginx-b  namespace: b  annotations:    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-additional-resource-tags: &quot;project=b&quot;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id: &quot;lb-8vbblzrcxxxxxxxx&quot;    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-force-override-listeners: &quot;true&quot;spec:  ports:    - name: http      protocol: TCP      port: 80      targetPort: 80    - name: https      protocol: TCP      port: 443      targetPort: 443  selector:    app: nginx-b  sessionAffinity: None  type: LoadBalancer---apiVersion: v1kind: Servicemetadata:  name: svc-fpm-b  namespace: bspec:  ports:    - name: tcp      protocol: TCP      port: 9000      targetPort: 9000  selector:    app: fpm-b  sessionAffinity: None  type: ClusterIP</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
            <tag> kubernetes </tag>
            
            <tag> elb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyun☞常见问题</title>
      <link href="posts/4cc38acc/"/>
      <url>posts/4cc38acc/</url>
      
        <content type="html"><![CDATA[<h1>adb for mysql</h1><h2 id="adb-访问-oss-文件不存在">adb 访问 oss 文件不存在</h2><p>adb 是分布式集群，调用oss的时候，源ip并不是adb实例地址所解析的vpc ip。因此ram的权限不能添加源ip限制。</p><h1>emr</h1><h2 id="hive-元数据使用外部rds的时候，构建表出错">hive 元数据使用外部rds的时候，构建表出错</h2><p>如果编码是utf8，变更为latin1</p><h2 id="hive删除表卡住，hive日志看不出异常">hive删除表卡住，hive日志看不出异常</h2><p>使用mysql8.0的时候出现，最终换成官方建议的5.7后，正常</p><h2 id="最新版emr采用role授权方式的时候，ecs-节点权限异常">最新版emr采用role授权方式的时候，ecs 节点权限异常</h2><p>role的规则必须是官方默认规则，任何缩小权限范围的规则都会异常。暂不知原因。</p><blockquote><p>说实话，官方默认的规则权限好大</p></blockquote><h2 id="HUE-服务不太正常">HUE 服务不太正常</h2><p>确认zookeeper是否安装</p><h1>CDN</h1><h2 id="非简单跨域-OPTIONS-无法通过">非简单跨域 OPTIONS 无法通过</h2><p>产生OPTIONS的原因是客户端请求存在自定义header，因此跨域的配置需要允许额外的自定义header</p><pre><code class="language-bash">Access-Control-Allow-Origin=&lt;允许的域&gt;Access-Control-Allow-Methods=GET,POST,PUT,OPTIONSAccess-Control-Allow-Headers=&lt;允许的自定义header&gt;</code></pre><h1>ASK</h1><h2 id="不创建nat网关，让pod可访问外网">不创建nat网关，让pod可访问外网</h2><p>Pod.metadata</p><pre><code class="language-yaml">  annotations:    k8s.aliyun.com/eci-with-eip: &quot;true&quot; # 每一个Pod都会创建一个独立的EIP    k8s.aliyun.com/eip-bandwidth: &quot;200&quot; # 200Mbps带宽</code></pre><blockquote><p><a href="https://help.aliyun.com/document_detail/128506.html">https://help.aliyun.com/document_detail/128506.html</a></p><p><a href="https://help.aliyun.com/document_detail/119199.html">https://help.aliyun.com/document_detail/119199.html</a></p></blockquote><h1>DTS</h1>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞21证书管理</title>
      <link href="posts/e902d670/"/>
      <url>posts/e902d670/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><ul><li>安装文档</li></ul><p><a href="https://cert-manager.io/docs/installation/helm/#installing-with-helm">https://cert-manager.io/docs/installation/helm/#installing-with-helm</a></p><ul><li>卸载文档</li></ul><p><a href="https://cert-manager.io/docs/installation/helm/#uninstalling">https://cert-manager.io/docs/installation/helm/#uninstalling</a></p><ul><li>helm repo文档</li></ul><p><a href="https://artifacthub.io/packages/helm/cert-manager/cert-manager">https://artifacthub.io/packages/helm/cert-manager/cert-manager</a></p><h2 id="主要对象">主要对象</h2><ul><li><p>Issuer 命名空间级别的发行者，用户自建</p></li><li><p>ClusterIssuer 集群级别的发行者，用户自建</p></li><li><p>Certificate 证书对象，用户自建</p></li><li><p>CertificateRequests 对象请求对象，根据Certificate创建</p></li><li><p>Orders 订单，根据Certificate创建</p></li><li><p>Challenges 挑战校验所有权对象，根据Certificate创建。若证书请求成功，则会自动删除。</p><blockquote><p>Certificates对象中的每一个dnsName都需要创建一个，并且Challenges对象是串行校验。如果网络不好，可能会耗时比较久。☠Challenges 是最有可能出错的对象，需要持续关注。例如 dns token 配置错误，或者网络错误，或者 webhook bug 导致无法正确请求 dns api。</p></blockquote></li></ul><h1>安装</h1><h2 id="创建CRD">创建CRD</h2><pre><code class="language-bash">➜ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.crds.yaml</code></pre><h2 id="添加Repo">添加Repo</h2><pre><code class="language-bash">➜ helm repo add jetstack https://charts.jetstack.io</code></pre><h2 id="安装cert-manager">安装cert-manager</h2><pre><code class="language-bash">➜ helm install cert-manager jetstack/cert-manager \--namespace cert-manager \--version v1.6.1 \--create-namespace \--set prometheus.enabled=true \--set webhook.timeoutSeconds=4</code></pre><h1>基本自测</h1><p>基本自测，不牵扯到真正的发行方以及第三方webhook。所以仅能保证cert-manager的基本正常。</p><pre><code class="language-yaml">➜ cat &lt;&lt; EOF | tee test-selfsigned.yamlapiVersion: v1kind: Namespacemetadata:  name: cert-manager-test---apiVersion: cert-manager.io/v1kind: Issuermetadata:  name: test-selfsigned  namespace: cert-manager-testspec:  selfSigned: &#123;&#125;---apiVersion: cert-manager.io/v1kind: Certificatemetadata:  name: selfsigned-cert  namespace: cert-manager-testspec:  commonName: example.com  secretName: selfsigned-cert-tls  issuerRef:    name: test-selfsignedEOF➜ kubectl apply -f test-selfsigned.yaml➜ kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges -n cert-manager-test -o wideNAME                                     READY   STATUS   AGEissuer.cert-manager.io/test-selfsigned   True             49sNAME                                          READY   SECRET                ISSUER            STATUS                                          AGEcertificate.cert-manager.io/selfsigned-cert   True    selfsigned-cert-tls   test-selfsigned   Certificate is up to date and has not expired   49sNAME                                                       APPROVED   DENIED   READY   ISSUER            REQUESTOR                                         STATUS                                         AGEcertificaterequest.cert-manager.io/selfsigned-cert-dwtdj   True                True    test-selfsigned   system:serviceaccount:cert-manager:cert-manager   Certificate fetched from issuer successfully   49s</code></pre><blockquote><p>输出 Certificate is up to date and has not expired和Certificate fetched from issuer successfully 即表示正常。</p></blockquote><p>如果没有输出，则检查各对象状态是否为True，出现False的一一排查</p><pre><code class="language-bash">➜ kubectl describe issuer -n cert-manager-test➜ kubectl describe certificaterequests -n cert-manager-test</code></pre><h1>证书申请</h1><p>申请证书，需要选用发行方（issuer）和域名所有权校验（Challenges ）方法。这里以ACME+DNS方式来配置。</p><p>证书申请对象流程：</p><p>order订单对象-&gt;</p><p>文档：</p><p><a href="https://cert-manager.io/docs/configuration/acme/dns01/">https://cert-manager.io/docs/configuration/acme/dns01/</a></p><p><a href="https://cert-manager.io/docs/configuration/acme/dns01/#webhook">https://cert-manager.io/docs/configuration/acme/dns01/#webhook</a></p><p>以aliyun dns为例。</p><h2 id="安装dns-webhook">安装dns webhook</h2><p>通过webhook，让cert-manager可以支持aliyun dns域名校验</p><pre><code class="language-bash">➜ helm repo add cert-manager-alidns-webhook https://devmachine-fr.github.io/cert-manager-alidns-webhook➜ helm repo update➜ helm install cert-manager-alidns-webhook cert-manager-alidns-webhook/alidns-webhook -n cert-manager --set groupName=zyh</code></pre><p>😒groupName必须和后面ClusterIssuer对象里的groupName保持一致。<a href="https://github.com/DEVmachine-fr/cert-manager-alidns-webhook/issues/11">https://github.com/DEVmachine-fr/cert-manager-alidns-webhook/issues/11</a></p><h2 id="构建域名解析服务商token">构建域名解析服务商token</h2><p>以阿里云dns解析服务为例</p><pre><code class="language-yaml">➜ kubectl create secret generic alidns-secrets --from-literal=&quot;access-token=&quot; --from-literal=&quot;secret-key=&quot; -n cert-manager</code></pre><blockquote><p>确保ram ak拥有dns解析服务的读写权限</p></blockquote><h2 id="构建发行方ClusterIssuer">构建发行方ClusterIssuer</h2><p>ClusterIssuer 对象用来指明你要用的证书发行方（集群级别）。以ACME发行方，并采用dns验证域名所有权。</p><p><a href="https://cert-manager.io/docs/configuration/acme/">https://cert-manager.io/docs/configuration/acme/</a></p><pre><code class="language-yaml"># 测试版本的发行方➜   cat &lt;&lt; EOF | tee clusterissuer-letsencrypt-staging-ali.yamlapiVersion: cert-manager.io/v1kind: ClusterIssuermetadata:  name: letsencrypt-staging-alispec:  acme:    email:     server: https://acme-staging-v02.api.letsencrypt.org/directory    privateKeySecretRef:      name: letsencrypt-staging-ali    solvers:    - dns01:        webhook:          config:            regionId: cn-beijing            accessTokenSecretRef:              key: access-token              name: alidns-secrets            secretKeySecretRef:              key: secret-key              name: alidns-secrets          groupName: zyh          solverName: alidns-solverEOF# 正式版本的发行方➜   cat &lt;&lt; EOF | tee clusterissuer-letsencrypt-prod-ali.yamlapiVersion: cert-manager.io/v1kind: ClusterIssuermetadata:  name: letsencrypt-prod-alispec:  acme:    email:     server: https://acme-v02.api.letsencrypt.org/directory    privateKeySecretRef:      name: letsencrypt-prod-ali    solvers:    - dns01:        webhook:          config:            regionId: cn-beijing            accessTokenSecretRef:              key: access-token              name: alidns-secrets            secretKeySecretRef:              key: secret-key              name: alidns-secrets          groupName: zyh          solverName: alidns-solverEOF</code></pre><blockquote><p>acme.server  定义ACME服务端</p><ul><li><a href="https://acme-staging-v02.api.letsencrypt.org/directory">https://acme-staging-v02.api.letsencrypt.org/directory</a>  测试服务器</li><li><a href="https://acme-v02.api.letsencrypt.org/directory">https://acme-v02.api.letsencrypt.org/directory</a>  线上服务器</li></ul><p>acme.email  acme 需定义一个ACME账户邮箱，【自己定义】</p><p><a href="http://acme.privateKeySecretRef.name">acme.privateKeySecretRef.name</a> 存储ACME账户私钥的secret对象，会自动创建</p><p>webhook.config 定义webhoob调用的dns token，【自己定义】</p><p>groupName 并不是证书包含的域名，指的是组织名，【自己定义】，需与webhook里保持一直。</p><p>solverName 指向webhook里定义的解析者，alidns-solver 貌似是写死的。</p></blockquote><h2 id="校验">校验</h2><p>确认  <a href="http://clusterissuer.cert-manager.io/letsencrypt-*-ali">clusterissuer.cert-manager.io/letsencrypt-*-ali</a> 是 True</p><pre><code class="language-bash">➜   kubectl get Issuers,ClusterIssuers -n cert-managerNAME                                                          READY   AGEissuer.cert-manager.io/cert-manager-alidns-webhook-ca         True    20hissuer.cert-manager.io/cert-manager-alidns-webhook-selfsign   True    20hNAME                                                    READY   AGEclusterissuer.cert-manager.io/letsencrypt-prod-ali      True    137mclusterissuer.cert-manager.io/letsencrypt-staging-ali   True    137m</code></pre><p>如果你发现它是False，则通过describe观察错误信息。在中国，可能出现的错误信息有：</p><ol><li><code>Error initializing issuer: context deadline exceeded</code> 这通常表明无法访问acme服务。这种情况下你需要耐心等待，直到注册成功。</li></ol><h2 id="构建证书Certificate">构建证书Certificate</h2><p>Certificate 用于申请并生成证书</p><pre><code class="language-bash">apiVersion: cert-manager.io/v1kind: Certificatemetadata:  name: example-com  namespace: dev-zyhspec:  secretName: example-com-tls  issuerRef:    name: letsencrypt-ali    kind: ClusterIssuer  dnsNames:  - '*.example.com'  - example.com  - example.org</code></pre><blockquote><p>namespace 证书使用的区域</p><p>secretName 存储证书的secret名，会自动创建</p><p>issuerRef 指定发行方</p><p>dnsNames 证书包含的域名</p></blockquote><h2 id="再次校验">再次校验</h2><pre><code class="language-bash">➜   kubectl get Certificates,CertificateRequests,Orders,Challenges -n dev-zyh</code></pre><h2 id="成功结果">成功结果</h2><pre><code class="language-bash">➜   kubectl get Certificates,CertificateRequests,Orders,Challenges -n dev-zyh -o wideNAME                                      READY   SECRET            ISSUER            STATUS                                          AGEcertificate.cert-manager.io/zyh-cool   True    zyh-cool-tls   letsencrypt-ali   Certificate is up to date and has not expired   15mNAME                                                   APPROVED   DENIED   READY   ISSUER            REQUESTOR                                         STATUS                                         AGEcertificaterequest.cert-manager.io/zyh-cool-jgj92   True                True    letsencrypt-ali   system:serviceaccount:cert-manager:cert-manager   Certificate fetched from issuer successfully   15mNAME                                                      STATE   ISSUER            REASON   AGEorder.acme.cert-manager.io/zyh-cool-jgj92-4122243896   valid   letsencrypt-ali            15m</code></pre><p>输出<code>Certificate is up to date and has not expired</code>即申请成功。</p><h2 id="额外">额外</h2><p>默认情况下，cert-manager-alidns-webhook 承诺自动更新证书。</p><p>而Certificates对象申请的证书时间默认为90天，提前30天更新。</p><p>你还可以通过<code>spec.duration</code>设定证书有效时间，则更新时间<code>spec.renewBefore</code>将默认设定为<code>spec.duration</code>的<code>2/3</code>。</p><p>不过如果用<code>letsencrypt</code>，则没有必要设定<code>spec.duration</code>，因为你只能申请最大90天。</p><h1>证书使用</h1><h2 id="通过ingress直接申请调用">通过ingress直接申请调用</h2><p>cert-manager 通过组件<code>ingress-shim</code>监控<code>Ingress</code>注解和<code>spec.tls</code>的配置从而自动创建 Certificate 对象，并调用<code>issuer</code>来申请证书。</p><p>前置条件：</p><ul><li>issuer 或者 clusterissuer 对象已创建</li></ul><p>特点：</p><ul><li>默认如果order彻底失败，则1小时之后将再次发起请求</li></ul><p>已知并经过验证的问题：</p><ul><li><p>如果<code>ingress注解</code>或者<code>ingress.spec.tls</code>发生了修改，但<code>ingress.spec.tls.hosts</code>完全没有改变，则不可直接apply。因为这将在同一时刻出现两份针对相同hosts发起的证书申请。这会导致其中一个申请永远不会成功并且重复发起，直至进入锁定期。</p><p><a href="https://github.com/jetstack/cert-manager/issues/1888">https://github.com/jetstack/cert-manager/issues/1888</a></p><p>💥多个ingress里永远不要出现完全相同的<code>ingress.spec.tls.hosts</code></p></li><li><p><code>spec.tls.hosts</code> 必须包含<code>spec.rules.hosts  </code>，否则 ingress 会将 Kubernetes Ingress Controller Fake Certificate 默认证书传递到 ingress-controller</p></li></ul><p>一个例子：</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  annotations:    # add an annotation indicating the issuer to use.    cert-manager.io/cluster-issuer: letsencrypt-ali    cert-manager.io/duration: 2160h # 90d    cert-manager.io/renew-before: 360h # 15d  name: myIngress  namespace: dev-zyhspec:  ingressClassName: nginx  rules:  - host: zyh.cool    http:      paths:      - pathType: Prefix        path: /        backend:          service:            name: myservice            port:              number: 80  tls: # &lt; placing a host in the TLS config will determine what ends up in the cert's subjectAltNames  - hosts:    - '*.zyh.cool'    - zyh.cool    secretName: zyh-cool-tls # &lt; cert-manager will store the created certificate in this secret.</code></pre><p><code>annotations.cert-manager.io/cluster-issuer: nameOfClusterIssuer</code> <code>ingress-shim</code>监视此注解，从而启动集群级别的证书发行方</p><p><code>tls.secretName</code> 指定存储证书的secret对象，它会自动创建</p><p><code>tls.hosts</code> 指定证书绑定的域名</p><h2 id="校验-2">校验</h2><pre><code class="language-bash">kubectl get Certificates,CertificateRequests,Orders,Challenges -n dev-zyh</code></pre><h2 id="一份证书申请正常的日志">一份证书申请正常的日志</h2><pre><code class="language-bash">2022-01-20T14:31:04.709965618+08:00 I0120 06:31:04.709580       1 conditions.go:201] Setting lastTransitionTime for Certificate &quot;zyh-cool-prod-tls&quot; condition &quot;Ready&quot; to 2022-01-20 06:31:04.709572525 +0000 UTC m=+84737.7244719392022-01-20T14:31:04.709985148+08:00 I0120 06:31:04.709732       1 trigger_controller.go:181] cert-manager/controller/certificates-trigger &quot;msg&quot;=&quot;Certificate must be re-issued&quot; &quot;key&quot;=&quot;dev-cms/zyh-cool-prod-tls&quot; &quot;message&quot;=&quot;Issuing certificate as Secret does not exist&quot; &quot;reason&quot;=&quot;DoesNotExist&quot;2022-01-20T14:31:04.709989519+08:00 I0120 06:31:04.709741       1 conditions.go:201] Setting lastTransitionTime for Certificate &quot;zyh-cool-prod-tls&quot; condition &quot;Issuing&quot; to 2022-01-20 06:31:04.709739172 +0000 UTC m=+84737.7246385452022-01-20T14:31:04.733560526+08:00 I0120 06:31:04.733475       1 controller.go:161] cert-manager/controller/certificates-trigger &quot;msg&quot;=&quot;re-queuing item due to optimistic locking on resource&quot; &quot;key&quot;=&quot;dev-cms/zyh-cool-prod-tls&quot; &quot;error&quot;=&quot;Operation cannot be fulfilled on certificates.cert-manager.io \&quot;zyh-cool-prod-tls\&quot;: the object has been modified; please apply your changes to the latest version and try again&quot;2022-01-20T14:31:04.733613304+08:00 I0120 06:31:04.733525       1 trigger_controller.go:181] cert-manager/controller/certificates-trigger &quot;msg&quot;=&quot;Certificate must be re-issued&quot; &quot;key&quot;=&quot;dev-cms/zyh-cool-prod-tls&quot; &quot;message&quot;=&quot;Issuing certificate as Secret does not exist&quot; &quot;reason&quot;=&quot;DoesNotExist&quot;2022-01-20T14:31:04.733620510+08:00 I0120 06:31:04.733538       1 conditions.go:201] Setting lastTransitionTime for Certificate &quot;zyh-cool-prod-tls&quot; condition &quot;Issuing&quot; to 2022-01-20 06:31:04.73353554 +0000 UTC m=+84737.7484349082022-01-20T14:31:04.833325827+08:00 I0120 06:31:04.833054       1 conditions.go:261] Setting lastTransitionTime for CertificateRequest &quot;zyh-cool-prod-tls-22n7r&quot; condition &quot;Approved&quot; to 2022-01-20 06:31:04.833048731 +0000 UTC m=+84737.8479481322022-01-20T14:31:04.863021821+08:00 I0120 06:31:04.862933       1 conditions.go:261] Setting lastTransitionTime for CertificateRequest &quot;zyh-cool-prod-tls-22n7r&quot; condition &quot;Ready&quot; to 2022-01-20 06:31:04.862927112 +0000 UTC m=+84737.8778264652022-01-20T14:31:17.954783254+08:00 I0120 06:31:17.954632       1 dns.go:88] cert-manager/controller/challenges/Present &quot;msg&quot;=&quot;presenting DNS01 challenge for domain&quot; &quot;dnsName&quot;=&quot;test.zyh.cool&quot; &quot;domain&quot;=&quot;test.zyh.cool&quot; &quot;resource_kind&quot;=&quot;Challenge&quot; &quot;resource_name&quot;=&quot;zyh-cool-prod-tls-22n7r-2700220904-1691659519&quot; &quot;resource_namespace&quot;=&quot;dev-cms&quot; &quot;resource_version&quot;=&quot;v1&quot; &quot;type&quot;=&quot;DNS-01&quot; 2022-01-20T14:32:27.475094724+08:00 I0120 06:32:27.474930       1 acme.go:209] cert-manager/controller/certificaterequests-issuer-acme/sign &quot;msg&quot;=&quot;certificate issued&quot; &quot;related_resource_kind&quot;=&quot;Order&quot; &quot;related_resource_name&quot;=&quot;zyh-cool-prod-tls-22n7r-2700220904&quot; &quot;related_resource_namespace&quot;=&quot;dev-cms&quot; &quot;related_resource_version&quot;=&quot;v1&quot; &quot;resource_kind&quot;=&quot;CertificateRequest&quot; &quot;resource_name&quot;=&quot;zyh-cool-prod-tls-22n7r&quot; &quot;resource_namespace&quot;=&quot;dev-cms&quot; &quot;resource_version&quot;=&quot;v1&quot; 2022-01-20T14:32:27.475161100+08:00 I0120 06:32:27.475001       1 conditions.go:250] Found status change for CertificateRequest &quot;zyh-cool-prod-tls-22n7r&quot; condition &quot;Ready&quot;: &quot;False&quot; -&gt; &quot;True&quot;; setting lastTransitionTime to 2022-01-20 06:32:27.474997969 +0000 UTC m=+84820.4898973122022-01-20T14:32:27.537540719+08:00 I0120 06:32:27.537414       1 conditions.go:190] Found status change for Certificate &quot;zyh-cool-prod-tls&quot; condition &quot;Ready&quot;: &quot;False&quot; -&gt; &quot;True&quot;; setting lastTransitionTime to 2022-01-20 06:32:27.537409387 +0000 UTC m=+84820.5523087522022-01-20T14:32:27.552649591+08:00 E0120 06:32:27.552582       1 controller.go:211] cert-manager/controller/challenges &quot;msg&quot;=&quot;challenge in work queue no longer exists&quot; &quot;error&quot;=&quot;challenge.acme.cert-manager.io \&quot;zyh-cool-prod-tls-22n7r-2700220904-1691659519\&quot; not found&quot;  2022-01-20T14:32:27.571542097+08:00 I0120 06:32:27.571149       1 controller.go:161] cert-manager/controller/certificates-readiness &quot;msg&quot;=&quot;re-queuing item due to optimistic locking on resource&quot; &quot;key&quot;=&quot;dev-cms/zyh-cool-prod-tls&quot; &quot;error&quot;=&quot;Operation cannot be fulfilled on certificates.cert-manager.io \&quot;zyh-cool-prod-tls\&quot;: the object has been modified; please apply your changes to the latest version and try again&quot;2022-01-20T14:32:27.571579091+08:00 I0120 06:32:27.571390       1 conditions.go:190] Found status change for Certificate &quot;zyh-cool-prod-tls&quot; condition &quot;Ready&quot;: &quot;False&quot; -&gt; &quot;True&quot;; setting lastTransitionTime to 2022-01-20 06:32:27.571386394 +0000 UTC m=+84820.586285748</code></pre><p>里面存在一些看起来是错误的日志，但其实并不是。</p><h2 id="非ingress直接申请调用">非ingress直接申请调用</h2><p>前置条件：</p><ul><li>证书已申请成功</li></ul><p>ingress配置</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: myIngress  namespace: dev-zyhspec:  ingressClassName: nginx  rules:  - host: zyh.cool    http:      paths:      - pathType: Prefix        path: /        backend:          service:            name: myservice            port:              number: 80  tls: # &lt; placing a host in the TLS config will determine what ends up in the cert's subjectAltNames  - hosts:    - '*.zyh.cool'    - zyh.cool    secretName: zyh-cool-tls # &lt; cert-manager will store the created certificate in this secret.</code></pre><h1>监控</h1><p>待续</p><h1>卸载</h1><h2 id="卸载cert-manager创建的资源">卸载cert-manager创建的资源</h2><pre><code class="language-bash">kubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces</code></pre><h2 id="卸载cert-manager">卸载cert-manager</h2><pre><code class="language-bash">helm --namespace cert-manager delete cert-managerkubectl delete namespace cert-manager</code></pre><h2 id="删除CRD">删除CRD</h2><pre><code class="language-bash">helm list -n cert-manager -o yaml | grep app_versionkubectl delete -f https://github.com/jetstack/cert-manager/releases/download/vX.Y.Z/cert-manager.crds.yaml</code></pre><blockquote><p>vX.Y.Z需要自定义</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> ssl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞003共享库</title>
      <link href="posts/e4df8a4f/"/>
      <url>posts/e4df8a4f/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p><a href="https://www.jenkins.io/zh/doc/book/pipeline/shared-libraries/">https://www.jenkins.io/zh/doc/book/pipeline/shared-libraries/</a></p><p>多个Jenkins工程的流水线可能包含重复代码，因此这些重复代码可以剥离出来从而被共享使用，这种东西叫共享库.</p><h2 id="基本结构">基本结构</h2><pre><code class="language-groovy">(root)+- src                       # Groovy 共享库代码|   +- org|       +- devops|           +- tools.groovy  # for org.devops.tools class，在Jenkinsfile里通过def tools = new org.devops.tools() 加载共享库代码+- vars|   +- devops.groovy          # for global 'devops' variable|   +- devops.txt             # help for 'devops' variable+- resources                  # resource files (external libraries only)|   +- org|       +- devops|           +- tools.json    # static helper data for org.devops.Bar</code></pre><h2 id="基本写法">基本写法</h2><p>/src/org/devops/tools.groovy</p><pre><code class="language-groovy">package org.devops// 格式化输出, 需要AnsiColor插件支持.// 接收内容，并输出彩色内容def myPrint(content, color)&#123;    colors = ['red'   : &quot;\033[40;31m #############$&#123;content&#125;############# \033[0m&quot;,              'green' : &quot;\033[40;32m #############$&#123;content&#125;############# \033[0m&quot;,              'yellow' : &quot;\033[40;33m #############$&#123;content&#125;############# \033[0m&quot;,              'blue'  : &quot;\033[47;34m #############$&#123;content&#125;############# \033[0m&quot;]    ansiColor('xterm') &#123;        println(colors[color])    &#125;&#125;// 封装chatBot请求， 需要 http request 插件// 接收http method、http地址、http请求体，返回请求结果def myChat(reqMode,reqUrl,reqBody)&#123;    result = httpRequest httpMode: reqMode,                accept: &quot;APPLICATION_JSON_UTF8&quot;,                contentType: &quot;APPLICATION_JSON_UTF8&quot;,                consoleLogResponseBody: true,                ignoreSslErrors: true,                requestBody: reqBody,                url: reqUrl                quiet: true    return result&#125;//根据标签获取目标机器ip, 并写入到 .hosts 文件中//ipType: PublicIpAddress PrivateIpAddressdef getEc2Ip(ipType, tagKey, tagValue, Region) &#123;    sh &quot;&quot;&quot;      export AWS_DEFAULT_REGION=$&#123;Regionssss&#125;      aws ec2 describe-instances --filters &quot;Name=tag:$&#123;tagKey&#125;,Values=$&#123;tagValue&#125;&quot; --query 'Reservations[*].Instances[*].[$&#123;ipType&#125;]' --output text &gt; .hosts      cat .hosts   &quot;&quot;&quot;&#125;def getRemoteIP(serverIP) &#123;    sh &quot;&quot;&quot;        echo $&#123;serverIP&#125; &gt; .hosts        cat .hosts    &quot;&quot;&quot;&#125;// 获取阿里云 acr 服务的动态 token, 得到全局变量 ACRUSER 和 ACRPWD// &#123;&quot;data&quot;:&#123;&quot;authorizationToken&quot;:&quot;abcde123456&quot;,&quot;tempUserName&quot;:&quot;cr_temp_user&quot;,&quot;expireDate&quot;:1612348203000&#125;&#125;def getAliAcrToken(url) &#123;    sh (        script: &quot;&quot;&quot;curl -sq $url &gt; acr.token;            cat acr.token | jq -r .'[&quot;data&quot;][&quot;tempUserName&quot;]' &gt; acr.user;            cat acr.token | jq -r .'[&quot;data&quot;][&quot;authorizationToken&quot;]' &gt;&gt; acr.pwd;        &quot;&quot;&quot;    )&#125;// 需要提前部署好私钥 /root/.remote.pem// 对应的公钥需要放置在目标机器的 $&#123;remoteUser&#125; 用户中// .hosts 存放在工作目录下def ansible(remoteUser,shellCommand) &#123;    sh &quot;&quot;&quot;        ansible -i .hosts --private-key /root/.remote.pem -u $&#123;remoteUser&#125; all -m shell -a &quot;$&#123;shellCommand&#125;&quot;    &quot;&quot;&quot;&#125;</code></pre><h2 id="关联pipeline-libraries">关联pipeline libraries</h2><p>jenkins 系统管理-系统配置-Global Pipeline Libraries</p><blockquote><p>这里将共享库放到git上，通过scm从git拉去共享库.</p></blockquote><p><img src="/posts/e4df8a4f/image-20211102114354246.png" alt="image-20211102114354246"></p><h2 id="调用pipeline-libraries">调用pipeline libraries</h2><p>jenkins工程的pipeline代码开头添加，即jenkinsfile文件中</p><pre><code class="language-groovy">pipeline &#123;    @Library('jenkinslib') _  //导入lib库, 'jenkinslib' 名就是上一步里Global Pipeline Libraries配置的名    def tools = new org.devops.tools()  //加载tools    agent &#123;        ...    &#125;    stages &#123;        stage(&quot;第一个阶段&quot;) &#123;            steps &#123;                tools.myPrint(&quot;第一个步骤&quot;,&quot;green&quot;)            &#125;        &#125;    &#125;    post &#123;        ...    &#125;&#125;</code></pre><h2 id="问题">问题</h2><p>可能会遇到jenkins web端编辑器提示无法解析，这种不用处理，只不过是编辑器不支持共享库。</p><p><img src="/posts/e4df8a4f/image-20220420113445664.png" alt="image-20220420113445664"></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞基于k8s生成动态的Jenkins agent</title>
      <link href="posts/3b5cd3e1/"/>
      <url>posts/3b5cd3e1/</url>
      
        <content type="html"><![CDATA[<h2 id="流程">流程</h2><p>jenkins-master -&gt; pipeline -&gt; kubernetes 插件 -&gt; kubernetes 集群 -&gt; jenkins-agent【k8s pod】 -&gt; jenkins-master</p><h2 id="所需插件">所需插件</h2><ul><li>kubernetes</li><li>kubernetes cli</li></ul><h2 id="凭证信息">凭证信息</h2><p>jenkins-master 通过凭证来动态的创建agent pod执行自动化工程。</p><h3 id="jenkins-master-位于kubernetes外">jenkins master 位于kubernetes外</h3><p>将拥有命名空间读写权限的用户的 .kube/config 里的加密信息获取凭据所需的证书信息</p><pre><code class="language-bash">certificate_authority_data=client_certificate_data=client_key_data=echo &quot;$&#123;certificate_authority_data&#125;&quot; | base64 -d &gt; ca.crtecho &quot;$&#123;client_certificate_data&#125;&quot; | base64 -d &gt; client.crtecho &quot;$&#123;client_key_data&#125;&quot; | base64 -d &gt; client.keyopenssl pkcs12 -export -out cert.pfx -inkey client.key -in client.crt -certfile ca.crt</code></pre><p>✨提示输入密码的时候，输入密码。</p><p>将 cert.pfx 传输到 jenkins 凭据中</p><p>![image-20210116152344875](jenkins☞基于k8s生成动态的Jenkins agent/image-20210116152344875.png)</p><p>💁你需要在凭据【密码】位置中输入你生成 cert.pfx 证书时输入的密码.</p><h3 id="jenkins-master-位于kubernetes内">jenkins master 位于kubernetes内</h3><ol><li>创建SA-RABC</li></ol><pre><code class="language-yaml">apiVersion: v1kind: ServiceAccountmetadata:  name: jenkins-admin  namespace: jenkins---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: jenkins-adminrules:  - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]    resources: [&quot;deployments&quot;]    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;services&quot;]    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;pods&quot;]    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;pods/exec&quot;]    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;pods/log&quot;]    verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]  - apiGroups: [&quot;&quot;]    resources: [&quot;secrets&quot;]    verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: jenkins-adminroleRef:  kind: ClusterRole  name: jenkins-admin  apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount  name: jenkins-admin  namespace: jenkins</code></pre><ol start="2"><li>jenkins-master调用sa</li></ol><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: jenkins  namespace: jenkinsspec:  replicas: 1  selector:    matchLabels:      app: jenkins  template:    metadata:      labels:        app: jenkins    spec:      terminationGracePeriodSeconds: 10      serviceAccount: jenkins-admin      ......</code></pre><h2 id="安装配置-kubernetes-插件">安装配置 kubernetes 插件</h2><ol><li><p>安装 <a href="https://plugins.jenkins.io/kubernetes/">https://plugins.jenkins.io/kubernetes/</a> 插件</p></li><li><p>添加一个新的插件配置</p></li></ol><h3 id="jenkins-master-位于集群外">jenkins master 位于集群外</h3><p>![image-20210116152033813](jenkins☞基于k8s生成动态的Jenkins agent/image-20210116152033813.png)</p><h3 id="jenkins-master-位于集群内">jenkins master 位于集群内</h3><p>只需填写如下信息</p><pre><code>名称：kuberneteskubernetes 地址：https://kubernetes.default.svc.cluster.localJenkins 地址： http://jenkins.jenkins.svc.cluster.local:8080Jenkins 通道： jenkins-jnlp.jenkins.svc.cluster.local:50000</code></pre><p>✨默认情况下，kubernetes集群会在default命名空间内，创建一个kubernetes的svc。</p><p>✨Jenkins地址根据实际的svc配置填写。</p><p>最后，确保插件配置【连接测试】显示成功连接</p><h2 id="配置-agent-pod-模板">配置  agent pod 模板</h2><h3 id="概念">概念</h3><p>关于 agent 的 pod 模板，默认插件已经提供了一个，只不过你看不到。因此哪怕你什么都不做，你 pipeline stages 也会在默认的 agent pod 里执行。</p><p>我们一般选择给 agent pod 添加一个额外的容器，这个额外的容器里包含了我们执行 stages 所需要的环境，例如叫 jenkinstools。</p><p>即最终，整个pod包含多个容器，分别是沟通 jenkins-master 的  JNLP 容器和执行我们工作流步骤的 jenkinstools 容器。</p><p>需要注意的是：</p><p>多容器 pod 模板有诸多的硬性限制：</p><p>Multiple containers can be defined in a pod. One of them is automatically created with name <code>jnlp</code>, and runs the Jenkins JNLP agent service, with args <code>$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;</code>, and will be the container acting as Jenkins agent.</p><p>Other containers must run a long running process, so the container does not exit. If the default entrypoint or command just runs something and exit then it should be overridden with something like <code>cat</code> with <code>ttyEnabled: true</code>.</p><p><strong>WARNING</strong> If you want to provide your own Docker image for the JNLP agent, you <strong>must</strong> name the container <code>jnlp</code> so it overrides the default one. Failing to do so will result in two agents trying to concurrently connect to the master.</p><ol><li>jnlp agent 容器会自动创建，哪怕你没有定义。</li><li>如果你自定义了 JNLP 服务所在的容器，那么负责连接 master 的 jnlp agent 容器名必须叫 jnlp。💥</li><li>工作容器（jenkinstools)，也就是我们用来执行 stages 的容器，必须运行一个持久的程序，以便于容器不会自动退出。例如 cat 命令并附加一个伪终端。</li><li>stages 阶段执行的时候，必须明确的指定工作容器（jenkinstools)，否则会默认在 jnlp 容器里执行. （这个不是很确定，但是我测试是这样）</li></ol><h3 id="设置基本Pod模板">设置基本Pod模板</h3><p>下图是我的 pod 模板配置（部分截图）</p><p>![image-20210126182630608](jenkins☞基于k8s生成动态的Jenkins agent/image-20210126182630608.png)</p><p>✨jenkinstools 即实际执行自动化工作的容器，通常可能是【代码打包容器】。</p><p>例如：若为go程序，则这里设置为go环境容器，若为java程序，则这里设置为maven环境容器等。</p><h3 id="优化模板里的代码环境容器">优化模板里的代码环境容器</h3><p>假设我们的程序是java，则对应的代码打包容器镜像可能是 maven:3.6-openjdk-11-slim。而 maven 在构建 jar 包的时候，通常会下载依赖包到 /root/.m2 (通过root执行)。我们为了避免每次所需的依赖文件，则可以通过构建一个多读写卷 nfs pvc 挂载到 /root/.m2。</p><p>✨这里的<code>挂载路径</code>会挂载到所有的容器中。</p><p>![image-20220327104422310](jenkins☞基于k8s生成动态的Jenkins agent/image-20220327104422310.png)</p><h3 id="添加包含docker服务容器">添加包含docker服务容器</h3><p>这种场景常用来在【agent pod】中将【代码打包容器】生成的程序通过【docker服务容器】打包成镜像并推送到镜像注册服务。</p><p>✨如果要在Pipeline中基于声明使用docker命令，则依赖docker和docker pipeline插件。</p><p>✨通过和代码打包容器保持相同的工作目录 /home/jenkins/agent，从而可以让docker容器查找到代码里的Dockerfile文件。</p><p>新加一个Pod模板：dockerbuild，只需要写名称，其它不用写。</p><p>✨如果存在【代码打包容器】的Pod模板，则在<code>父级的 Pod 模板名称</code>栏里添加。从而将【代码打包容器】加到 dockerbuild Pod 模板中。</p><p>![image-20220328180135103](jenkins☞基于k8s生成动态的Jenkins agent/image-20220328180135103.png)</p><p>将下列Pod配置复制到<code>Raw YAML for the Pod</code>，并将<code>Yaml merge strategy</code>设置为<code>Merge</code></p><pre><code class="language-yaml">apiVersion: &quot;v1&quot;kind: &quot;Pod&quot;spec:  hostAliases:  - ip: &quot;10.0.0.10&quot;    hostnames:    - &quot;gitlab.xxx.com&quot;  containers:  - args:    - &quot;--host=unix:///var/run/docker.sock&quot;    - &quot;--host=tcp://0.0.0.0:8000&quot;    command:    - &quot;dockerd&quot;    image: &quot;docker:stable-dind&quot;    imagePullPolicy: &quot;IfNotPresent&quot;    name: &quot;docker&quot;    resources:      limits: &#123;&#125;      requests: &#123;&#125;    securityContext:      privileged: true    tty: false    volumeMounts:    - mountPath: &quot;/home/jenkins/agent&quot;      name: &quot;workspace-volume&quot;      readOnly: false    workingDir: &quot;/home/jenkins/agent&quot;  hostNetwork: false  nodeSelector:    kubernetes.io/os: &quot;linux&quot;  restartPolicy: &quot;Never&quot;  volumes:  - emptyDir:      medium: &quot;&quot;    name: &quot;workspace-volume&quot;</code></pre><h4 id="pipeline">pipeline</h4><pre><code class="language-groovy">pipeline&#123;environment &#123;appName = &quot;server&quot; // 程序名，亦是镜像仓库名appVersion = &quot;v0.1.0&quot; // 程序版本，亦是镜像标签appGit = &quot;http://gitgg.xxx.com/crm/xxx.git&quot;registryCredential = &quot;harbor-jenkins-rw&quot; // 镜像注册表登录凭据名，需提前在jenkins凭据功能里添加registryUrl = &quot;https://harbor.xxx.com/&quot; // 镜像注册表地址registryProject = &quot;crm&quot; // 项目名/命名空间dockerImage = &quot;&quot; // 空的环境变量名&#125;    agent&#123;        kubernetes&#123;            inheritFrom &quot;dockerbuild&quot; // Agent Pod模板名    &#125;    stages &#123;stage(&quot;code: pull&quot;) &#123;steps &#123;git credentialsId: 'gitgg-apps-ro', branch: 'master', url: appGit&#125;&#125;        stage('docker: build') &#123;steps &#123;container('docker') &#123;script &#123;dockerImage = docker.build( registryRepository + &quot;/&quot; + appName + &quot;:&quot; +appVersion, &quot;docker&quot;)  // build方法接收两个参数，第一个是镜像tag，第二个是dockerfile在代码里的相对目录&#125;&#125;&#125;&#125;stage('docker: push') &#123;    steps &#123;        container('docker') &#123;            script &#123;                docker.withRegistry( registryUrl, registryCredential ) &#123; // 登录注册表                    dockerImage.push() // dockerImage 即上一阶段的 dockerImage 变量.                &#125;            &#125;        &#125;    &#125;&#125;    &#125;&#125;</code></pre><h3 id="添加包含kubectl命令的容器">添加包含kubectl命令的容器</h3><p>这种场景通常用来在【agent pod】中的【kubectl】容器执行 kubernetes 清单到kubernetes。</p><p>✨关于kubectl的容器，可以选用 kubesphere/kubectl 镜像.</p><h4 id="创建kubectl命令所需的kubeconfig凭证">创建kubectl命令所需的kubeconfig凭证</h4><p>凭证类型：secret file</p><p>凭证文件：拥有创建 kubernetes 清单的权限</p><h4 id="pipeline-2">pipeline</h4><pre><code class="language-groovy">pipeline&#123;    agent&#123;        kubernetes&#123;            //label &quot;jenkins-agent&quot; // pod 模板标签            //cloud 'kubernetes'  // 插件配置名            inheritFrom &quot;kubectl&quot; // 从标签为 kubectl 的 pod 模板继承        &#125;    &#125;    stages &#123;stage(&quot;git&quot;) &#123;steps &#123;git branch: 'master', url: 'https://github.com/abc-deployment.git' //从https://github.com/abc-deployment.git的master分支克隆代码&#125;&#125;stage('deploy') &#123;steps &#123;container('kubectl') &#123;                    withKubeConfig([credentialsId: 'k8s-cluster-admin-kubeconfig'])&#123; // 这个指令基于 Kubernetes CLI 插件，k8s-cluster-admin-kubeconfig 是 secret file 类型凭据，加载 kubeconfig                        sh 'kubectl apply -f kubernetes/'                    &#125;&#125;&#125;&#125;    &#125;&#125;</code></pre><h2 id="出错">出错</h2><h3 id="agent-pod一直启动中">agent pod一直启动中</h3><p>节点日志出现，<code>jenkins jnlp Waiting for agent to connect</code>，这个表示jenkins一直在等待jnlp链接。常见于 jnlp 没有设定 agent 启动命令或者 agent 无法链接到 jenkins master。</p><h3 id="jenkins-工程中显示-kubectl-指令卡住">jenkins 工程中显示 kubectl 指令卡住</h3><p>检查 withKubeConfig([credentialsId: ‘k8s-cluster-admin-kubeconfig’]) 凭证加载的 kubeconfig 是否可以正常执行清单。</p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞002Pipelines</title>
      <link href="posts/8a33e212/"/>
      <url>posts/8a33e212/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>纯图形化的配置，并不能很有效的支撑自动化，也不灵活，一般jenkins的工程会通过Jenkinsfile来配置pipeline。</p><p>pipeline 有声明和脚本两种方式，声明式官方比较推荐。当然声明也可以嵌入脚本。</p><p>可以在http://groovy.jsrun.net/等站点上调试语法。</p><p><a href="https://www.jenkins.io/zh/doc/book/pipeline/syntax/#%E6%B5%81%E6%B0%B4%E7%BA%BF%E8%AF%AD%E6%B3%95">流水线语法</a></p><p>基本流程如下：<br>Jenkins工程 =&gt; scm git =&gt; Jenkinsfile =&gt; pipeline</p><h2 id="脚本式语法">脚本式语法</h2><p>最后，脚本语言采用groovy语言编写。</p><blockquote><p>关于groovy的语法，可以简单的看看https://www.w3cschool.cn/groovy/</p></blockquote><h3 id="字符串截断">字符串截断</h3><pre><code class="language-groovy">&quot;abscd adfa dasfds ghisgirs fsdfgf&quot;.take(10)  //&quot;abscd adfa&quot;&quot;It's groovy, man&quot;.take(4)      //&quot;It's&quot;&quot;It's groovy, man&quot;.take(10000)  //&quot;It's groovy, man&quot; (no exception thrown)&quot;It's groovy, man&quot;.drop(15)         //&quot;n&quot;&quot;It's groovy, man&quot;.drop(5).take(6)  //&quot;groovy&quot;</code></pre><h3 id="字符串分割">字符串分割</h3><pre><code class="language-groovy">// https://abc.com/it/test.git 获取 it 字符串registryNamespace = env.GIT_URL.split(&quot;/|\\.&quot;)[-3]</code></pre><h2 id="声明式语法">声明式语法</h2><h2 id="agent">agent</h2><p>用于指明运行程序的节点</p><p>可用参数：</p><ul><li>label 指定标签</li><li>any 任意节点</li><li>None 默认节点</li><li>node 详细描述某个节点信息，可以包含label</li></ul><pre><code class="language-groovy">pipeline &#123;    agent &#123;        node &#123;        label &quot;master&quot; //指定【运行节点】的标签或者名称        customWorkspace &quot;$&#123;workspace&#125;&quot;        &#125;    &#125;    stages &#123;        stage &#123;            steps &#123;                pass            &#125;        &#125;    &#125;    post &#123;        always &#123;            pass        &#125;    &#125;&#125;agent &#123;    node &#123;        label &quot;master&quot; //指定【运行节点】的标签或者名称        customWorkspace &quot;$&#123;workspace&#125;&quot;    &#125;&#125;</code></pre><h2 id="stages">stages</h2><p>运行阶段，包含多个阶段，每个阶段包含步骤所需的代码</p><pre><code class="language-groovy">pipeline &#123;    agent any    stages &#123;        stage()&#123;            steps &#123;                pass            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="post">post</h2><p>根据stages运行状态，执行后续处理的代码</p><p>可用状态:</p><ul><li>always 总是执行</li><li>changed 任意状态对比上一次执行不一致的时候执行</li><li>failure 最终状态失败的时候执行</li><li>success 最终状态成功地时候执行</li><li>unstable 中间状态失败导致无法抵达最终阶段的时候执行</li><li>aborted 手动取消的时候执行</li></ul><pre><code class="language-groovy">pipeline &#123;    agent any    stages &#123;        stage &#123;            pass        &#125;    &#125;    post &#123;        always &#123;            pass        &#125;    &#125;&#125;</code></pre><p>除了上面的三大块，在agent后，你还可以添加一些额外的</p><h2 id="environment">environment</h2><p>环境变量</p><pre><code class="language-groovy">pipeline &#123;    agent    environment &#123;        key = &quot;123456&quot;    &#125;&#125;</code></pre><h2 id="options">options</h2><p>配置特定于流水线的选项</p><ul><li>buildDiscarder: 为最近的流水线运行的特定数量保存组件和控制台输出。</li><li>disableConcurrentBuilds: 不允许同时执行流水线。 可被用来防止同时访问共享资源等。</li><li>overrideIndexTriggers: 允许覆盖分支索引触发器的默认处理。</li><li>skipDefaultCheckout: 在<code>agent</code> 指令中，跳过从源代码控制中检出代码的默认情况。</li><li>skipStagesAfterUnstable: 一旦构建状态变得UNSTABLE，跳过该阶段。</li><li>checkoutToSubdirectory: 在工作空间的子目录中自动地执行源代码控制检出。</li><li>timeout: 设置流水线运行的超时时间, 在此之后，Jenkins将中止流水线。</li><li>retry: 在失败时, 重新尝试整个流水线的指定次数。</li><li>timestamps 预测所有由流水线生成的控制台输出，与该流水线发出的时间一致。</li></ul><pre><code class="language-groovy">pipeline &#123;agent anyoptions &#123;    timeout(time: 1, unit: 'HOURS') &#125;stages &#123;    stage('Example') &#123;        steps &#123;            echo 'Hello World'        &#125;    &#125;&#125;&#125;</code></pre><h2 id="parameters">parameters</h2><p>添加一些参数</p><p>💥比较特殊的是，参数只有流水线运行一次后，才可以在web图形化里看到相关的配置</p><pre><code class="language-groovy">pipeline &#123;    agent any    parameters &#123;        string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?')    &#125;    stages &#123;        stage('Example') &#123;            steps &#123;                echo &quot;Hello $&#123;params.PERSON&#125;&quot;            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="triggers">triggers</h2><p>触发器</p><ul><li>cron 计划任务</li><li>pollSCM 与 cron 类似</li><li>upstream  接受逗号分隔的工作字符串和阈值。 当字符串中的任何作业以最小阈值结束时，流水线被重新触发。</li></ul><pre><code class="language-groovy">triggers &#123; upstream(upstreamProjects: 'job1,job2', threshold: hudson.model.Result.SUCCESS) &#125;</code></pre><pre><code class="language-groovy">pipeline &#123;    agent any    triggers &#123;        cron('H */4 * * 1-5')    &#125;    stages &#123;        stage('Example') &#123;            steps &#123;                echo 'Hello World'            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="tools">tools</h2><p>获取通过自动安装或手动放置工具的环境变量。支持maven/jdk/gradle。工具的名称必须在系统设置-&gt;全局工具配置中定义。</p><p>示例:</p><pre><code class="language-groovy">pipeline &#123;    agent any    tools &#123;        maven 'apache-maven-3.0.1'     &#125;    stages &#123;        stage('Example') &#123;            steps &#123;                sh 'mvn --version'            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="input">input</h2><p>input用户在执行各个阶段的时候，由人工确认是否继续进行。</p><ul><li>message 呈现给用户的提示信息。</li><li>id 可选，默认为stage名称。</li><li>ok 默认表单上的ok文本。</li><li>submitter 可选的,以逗号分隔的用户列表或允许提交的外部组名。默认允许任何用户。</li><li>submitterParameter 环境变量的可选名称。如果存在，用<code>submitter</code> 名称设置。</li><li>parameters 提示提交者提供的一个可选的参数列表。</li></ul><p>示例：</p><pre><code class="language-groovy">pipeline &#123;    agent any    stages &#123;        stage('Example') &#123;            input &#123;                message &quot;Should we continue?&quot; // 提示信息                ok &quot;Yes, we should.&quot; // 确认按钮的标题                submitter &quot;alice,bob&quot; // 允许的提交者                parameters &#123;                    string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?')                &#125;            &#125;            steps &#123;                echo &quot;Hello, $&#123;PERSON&#125;, nice to meet you.&quot;            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="when">when</h2><p>when 指令允许流水线根据给定的条件决定是否应该执行阶段。 when 指令必须包含至少一个条件。 如果<code>when</code> 指令包含多个条件, 所有的子条件必须返回True，阶段才能执行。 这与子条件在 allOf 条件下嵌套的情况相同。</p><p>内置条件</p><ul><li><p>branch: 当正在构建的分支与模式给定的分支匹配时，执行这个阶段，这只适用于多分支流水线例如:</p><pre><code class="language-groovy">when &#123; branch 'master' &#125;</code></pre></li><li><p>environment: 当指定的环境变量是给定的值时，执行这个步骤,例如:</p><pre><code class="language-groovy">when &#123; environment name: 'DEPLOY_TO', value: 'production' &#125;</code></pre></li><li><p>expression 当指定的Groovy表达式评估为true时，执行这个阶段, 例如:</p><pre><code class="language-groovy">when &#123; expression &#123; return params.DEBUG_BUILD &#125; &#125;</code></pre></li><li><p>not 当嵌套条件是错误时，执行这个阶段,必须包含一个条件，例如:</p><pre><code class="language-groovy">when &#123; not &#123; branch 'master' &#125; &#125;</code></pre></li><li><p>allOf 当所有的嵌套条件都正确时，执行这个阶段,必须包含至少一个条件，例如:</p><pre><code class="language-groovy">when &#123; allOf &#123; branch 'master'; environment name: 'DEPLOY_TO', value: 'production' &#125; &#125;</code></pre></li><li><p>anyOf 当至少有一个嵌套条件为真时，执行这个阶段,必须包含至少一个条件，例如:</p><pre><code class="language-groovy">when &#123; anyOf &#123; branch 'master'; branch 'staging' &#125; &#125;</code></pre></li></ul><p>示例：</p><pre><code class="language-groovy">pipeline &#123;    agent any    stages &#123;        stage('Example Build') &#123;            steps &#123;                echo 'Hello World'            &#125;        &#125;        stage('Example Deploy') &#123;            when &#123;                branch 'production'            &#125;            steps &#123;                echo 'Deploying'            &#125;        &#125;    &#125;&#125;pipeline &#123;    agent any    stages &#123;        stage('Example Build') &#123;            steps &#123;                echo 'Hello World'            &#125;        &#125;        stage('Example Deploy') &#123;            when &#123;                branch 'production'                environment name: 'DEPLOY_TO', value: 'production'            &#125;            steps &#123;                echo 'Deploying'            &#125;        &#125;    &#125;&#125;pipeline &#123;    agent any    stages &#123;        stage('Example Build') &#123;            steps &#123;                echo 'Hello World'            &#125;        &#125;        stage('Example Deploy') &#123;            when &#123;                allOf &#123;                    branch 'production'                    environment name: 'DEPLOY_TO', value: 'production'                &#125;            &#125;            steps &#123;                echo 'Deploying'            &#125;        &#125;    &#125;&#125;pipeline &#123;    agent any    stages &#123;        stage('Example Build') &#123;            steps &#123;                echo 'Hello World'            &#125;        &#125;        stage('Example Deploy') &#123;            when &#123;                branch 'production'                anyOf &#123;                    environment name: 'DEPLOY_TO', value: 'production'                    environment name: 'DEPLOY_TO', value: 'staging'                &#125;            &#125;            steps &#123;                echo 'Deploying'            &#125;        &#125;    &#125;&#125;pipeline &#123;    agent any    stages &#123;        stage('Example Build') &#123;            steps &#123;                echo 'Hello World'            &#125;        &#125;        stage('Example Deploy') &#123;            when &#123;                expression &#123; BRANCH_NAME ==~ /(production|staging)/ &#125;                anyOf &#123;                    environment name: 'DEPLOY_TO', value: 'production'                    environment name: 'DEPLOY_TO', value: 'staging'                &#125;            &#125;            steps &#123;                echo 'Deploying'            &#125;        &#125;    &#125;&#125;pipeline &#123;    agent none    stages &#123;        stage('Example Build') &#123;            steps &#123;                echo 'Hello World'            &#125;        &#125;        stage('Example Deploy') &#123;            agent &#123;                label &quot;some-label&quot;            &#125;            when &#123;                beforeAgent true                branch 'production'            &#125;            steps &#123;                echo 'Deploying'            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="parallel">parallel</h2><p>声明式流水线的阶段可以在他们内部声明多隔嵌套阶段, 它们将并行执行。 注意，一个阶段必须只有一个 steps 或 <code>parallel</code>的阶段。 嵌套阶段本身不能包含 进一步的 <code>parallel</code> 阶段, 但是其他的阶段的行为与任何其他 stage<code>parallel</code>的阶段不能包含 <code>agent</code> 或 <code>tools</code>阶段, 因为他们没有相关 <code>steps</code>。</p><p>另外, 通过添加 <code>failFast true</code> 到包含<code>parallel</code>的 <code>stage</code>中， 当其中一个进程失败时，你可以强制所有的 <code>parallel</code> 阶段都被终止。</p><p>示例:</p><pre><code class="language-groovy">pipeline &#123;    agent any    stages &#123;        stage('Non-Parallel Stage') &#123;            steps &#123;                echo 'This stage will be executed first.'            &#125;        &#125;        stage('Parallel Stage') &#123;            when &#123;                branch 'master'            &#125;            failFast true  // 如果第一个stage失败，则后续stage不再执行            parallel &#123;                stage('Branch A') &#123;                    agent &#123;                        label &quot;for-branch-a&quot;                    &#125;                    steps &#123;                        echo &quot;On Branch A&quot;                    &#125;                &#125;                stage('Branch B') &#123;                    agent &#123;                        label &quot;for-branch-b&quot;                    &#125;                    steps &#123;                        echo &quot;On Branch B&quot;                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go-017测试</title>
      <link href="posts/786ba9af/"/>
      <url>posts/786ba9af/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>TDD 测试驱动开发： 先编写测试程序，然后编写开发程序</p><h2 id="命令">命令</h2><pre><code>go test -v</code></pre><p>Go 将查找当前目录下所有 <code>*_test.go</code> 文件来运行测试。</p><h2 id="逻辑">逻辑</h2><p>测试代码的逻辑是假想已有代码，通过构造测试对象并调用假想代码，如果假想代码【应该】有结果，则对结果进一步编写测试逻辑。</p><h2 id="例子">例子</h2><p>银行转账功能</p><p>测试用例</p><pre><code class="language-go">package bankimport &quot;testing&quot;// 针对 Account 结构体功能测试func TestAccount(t *testing.T) &#123;    // 假想正式代码拥有 Account 结构体    account := Account&#123;        Customer: Customer&#123;            Name:    &quot;John&quot;,            Address: &quot;Los Angeles, California&quot;,            Phone:   &quot;(213) 555 0147&quot;,        &#125;,        Number:  1001,        Balance: 0,    &#125;// 判断若已创建 Account 结构体变量 account ，则 account.Name 必然有值    if account.Name == &quot;&quot; &#123;        t.Error(&quot;can't create an Account object&quot;)    &#125;&#125;// 针对转账的功能测试func TestDeposit(t *testing.T) &#123;    account := Account&#123;        Customer: Customer&#123;            Name:    &quot;John&quot;,            Address: &quot;Los Angeles, California&quot;,            Phone:   &quot;(213) 555 0147&quot;,        &#125;,        Number:  1001,        Balance: 0,    &#125;    // 调用假想的已有 Deposit 方法，随别写入10作为测试数据。    account.Deposit(10)    // 判断若转入 10 后，则账户余额应该等于 10.    if account.Balance != 10 &#123;        t.Error(&quot;转账后余额没有增加&quot;)    &#125;        // 判断主程是否有针对转账负值的逻辑    if err := account.Deposit(-10); err == nil &#123;        t.Error(&quot;缺少针对负值的判断&quot;)    &#125;&#125;</code></pre><p>正式代码</p><pre><code class="language-go">package bankimport (    &quot;errors&quot;)// Customer ...type Customer struct &#123;    Name    string    Address string    Phone   string&#125;// Account ...type Account struct &#123;    Customer    Number  int32    Balance float64&#125;// Deposit ...func (a *Account) Deposit(amount float64) error &#123;    if amount &lt;= 0 &#123;        return errors.New(&quot;the amount to deposit should be greater than zero&quot;)    &#125;    a.Balance += amount    return nil&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go-016并发</title>
      <link href="posts/5086e20/"/>
      <url>posts/5086e20/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>go并发涉及两个概念：</p><ul><li>goroutine 轻量线程</li><li>channel  通道</li></ul><p>go的程序运行在goroutine，并通过channel来发送和接收数据。</p><p>因此，go不是通过共享内存通信，而是通过channel进行通信共享内存。</p><h2 id="goroutine">goroutine</h2><p>go的第一个goroutine是main()。你可以在一个go程序中通过<code>go 函数</code>发起更多的goroutine来执行<code>函数</code>。</p><p>💥go 关键词后面是函数，函数，函数</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;net/http&quot;    &quot;time&quot;)func checkAPI(api string)&#123;    if _, err := http.Get(api); err != nil &#123;        fmt.Printf(&quot;ERROR: %s is down!\n&quot;, api)    &#125;    fmt.Printf(&quot;SUCCESS: %s is up and running!\n&quot;, api)&#125;func main() &#123;    start := time.Now()    apis := []string&#123;        &quot;https://management.azure.com&quot;,        &quot;https://dev.azure.com&quot;,        &quot;https://api.github.com&quot;,        &quot;https://outlook.office.com/&quot;,        &quot;https://api.somewhereintheinternet.com/&quot;,        &quot;https://graph.microsoft.com&quot;,    &#125;    for _, api := range apis &#123;        go checkAPI(api)    &#125;    elapsed := time.Since(start)    fmt.Printf(&quot;Done! It took %v seconds!\n&quot;, elapsed.Seconds())&#125;</code></pre><pre><code class="language-bash">Done! It took 1.6802e-05 seconds!</code></pre><p>在上面的例子中， <code>go checkAPI(api)</code> 构建更多的 goroutine，但对于当前 main 的 goroutine 来说，<code>go checkAPI(api)</code>已执行过。因没有机制来阻止<code>main()</code>执行完，这导致<code>go checkAPI(api)</code>无法在有限的时间内执行完毕。</p><p>🤷‍♀️当然，可以通过手动添加time来阻止main()终止。</p><p>这时，就需要一种机制来告诉 main，<code>go checkAPI(api)</code>还未执行完。这个机制就是 channel.</p><h2 id="channel">channel</h2><p>当你需要将值从一个 goroutine 发送到另一个 goroutine 时，你可以使用 channel。</p><p>在上面的例子中，这个值就是你告诉 main，<code>go checkAPI(api)</code>还未执行完.</p><h3 id="写法">写法</h3><p>channel作为数据通道，需要定义可以传输的数据类型。</p><pre><code class="language-go">// 创建无缓冲channelch := make(chan 数据类型)// 创建有缓冲channelch := make(chan 数据类型, 缓冲长度)// 形参checkAPI(api string, ch chan 数据类型)// 函数形参ch只允许传入checkAPI(api string, ch chan&lt;- 数据类型)// 函数形参ch只允许传出checkAPI(api string, ch &lt;-chan 数据类型)// 数据发送ch &lt;- xxx// 数据接收xxx &lt;- ch// 函数形参只允许传出xxx// 数据接收，如果没有函数接收，则表示丢弃&lt;- ch// 关闭close(ch)</code></pre><p>✨当ch关闭后，有三种后续状况：</p><ol><li>数据发送对象将触发严重错误。</li><li>数据接收对象会一次性接收所有数据。<ol><li>再次执行数据接收，只会得到对应数据类型的默认值。</li></ol></li></ol><h3 id="无缓冲channel">无缓冲channel</h3><p>默认创建的channel是没有缓冲的。所以，有两个特点：</p><ul><li><p>数据发送对象代码必须等待数据接收对象代码执行后【才可以执行】。</p></li><li><p>数据接收对象执行后会一直等待数据，从而防止goruntine继续执行下一行代码；但一旦有数据接收，就不在阻塞所在的goruntine。</p></li></ul><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;net/http&quot;    &quot;time&quot;)func apiCheck(api string, ch chan string)&#123;    if _, err := http.Get(api); err != nil &#123;        ch &lt;- fmt.Sprintf(&quot;ERROR: %s is down!\n&quot;, api)    &#125;    ch &lt;- fmt.Sprintf(&quot;SUCCESS: %s is up and running!\n&quot;, api)&#125;func main() &#123;    start := time.Now()    apis := []string&#123;        &quot;https://management.azure.com&quot;,        &quot;https://dev.azure.com&quot;,        &quot;https://api.github.com&quot;,        &quot;https://outlook.office.com/&quot;,        &quot;https://api.somewhereintheinternet.com/&quot;,        &quot;https://graph.microsoft.com&quot;,    &#125;    ch := make(chan string)    for _, api := range apis &#123;        go apiCheck(api,ch)    &#125;    fmt.Printf(&lt;-ch)    elapsed := time.Since(start)    fmt.Printf(&quot;Done! It took %v seconds!\n&quot;, elapsed.Seconds())&#125;</code></pre><pre><code class="language-bash">➜   go run main.goSUCCESS: https://api.github.com is up and running!Done! It took 0.657230012 seconds!</code></pre><p>倒数第四行<code>fmt.Printf(&lt;-ch)</code> 作为channel数据接收的代码，会阻止程序执行下一条指令。</p><p>不过一旦<code>fmt.Printf(&lt;-ch)</code>拿到了任意数据（多次执行程序，你会发现输出并不一样），程序就会执行下一行，从而导致其它的 <code>go apiCheck(api,ch) </code>因没有新的数据输出对象而无法发送数据。</p><p>✨我们可以利用这个机制，来判断出哪个站点响应最快。</p><p>解决的方式之一是：</p><p>将<code>fmt.Printf(&lt;-ch)</code>放入for循环内，让数据输入对象和数据输出对象保持1:1。</p><p>可以看出：</p><p>✨无缓冲channel在goroutine之间的通信是同步的，因为数据发送和传出都需要等待另一方才能完成，否则就会阻塞。</p><h3 id="有缓冲channel">有缓冲channel</h3><ul><li>创建的时候，附加一个队列长度（buffer缓冲）</li><li>✨队列满之前，数据发送对象可以一直写入，无需等待数据接收对象的代码执行；但队列满后，新的数据发送对象所在的goroutine需等待</li><li>数据接收对象会一直等待队列数据，从而防止goruntine继续执行下一行代码；但一旦有数据接收，就不在阻塞所在的goruntine。</li></ul><p>验证规则2，当队列满的时候：</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;)func send(ch chan string, message string) &#123;    ch &lt;- message&#125;func main() &#123;    size := 2 // 将队列改为2    ch := make(chan string, size)    send(ch, &quot;one&quot;)    send(ch, &quot;two&quot;)    send(ch, &quot;three&quot;)    send(ch, &quot;four&quot;)    fmt.Println(&quot;All data sent to the channel ...&quot;)    for i := 0; i &lt; size; i++ &#123;        fmt.Println(&lt;-ch)    &#125;    fmt.Println(&quot;Done!&quot;)&#125;</code></pre><pre><code class="language-bash">➜   go run main.gofatal error: all goroutines are asleep - deadlock!goroutine 1 [chan send]:main.send(...)        /home/zyh/gocode/027/main.go:8main.main()        /home/zyh/gocode/027/main.go:16 +0x98exit status 2</code></pre><p>错误提示所有goroutines都处于等待。但程序只有一个goroutines，就是main()。</p><p>因此在前两个send()执行后，队列已满，第三个send()无法执行导致main()处于等待，也就无法执行后面的数据接收对象<code>fmt.Println(&lt;-ch)</code> 。</p><p>解决方法：</p><p>数据发送对象send()添加到新的goroutine，避免阻塞main()</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;)func send(ch chan string, message string) &#123;    ch &lt;- message&#125;func main() &#123;    size := 2    ch := make(chan string, size)    go send(ch, &quot;one&quot;)    go send(ch, &quot;two&quot;)    go send(ch, &quot;three&quot;)    go send(ch, &quot;four&quot;)    fmt.Println(&quot;All data sent to the channel ...&quot;)    for i := 0; i &lt; 4; i++ &#123;       fmt.Println(&lt;-ch)    &#125;    fmt.Println(&quot;Done!&quot;)&#125;</code></pre><p>网址校验例子</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;net/http&quot;    &quot;time&quot;)func apiCheck(api string, ch chan string)&#123;    if _, err := http.Get(api); err != nil &#123;        ch &lt;- fmt.Sprintf(&quot;ERROR: %s is down!\n&quot;, api)    &#125;    ch &lt;- fmt.Sprintf(&quot;SUCCESS: %s is up and running!\n&quot;, api)&#125;func main() &#123;    start := time.Now()    apis := []string&#123;        &quot;https://management.azure.com&quot;,        &quot;https://dev.azure.com&quot;,        &quot;https://api.github.com&quot;,        &quot;https://outlook.office.com/&quot;,        &quot;https://api.somewhereintheinternet.com/&quot;,        &quot;https://graph.microsoft.com&quot;,    &#125;    ch := make(chan string, 10)    for _, api := range apis &#123;        go apiCheck(api,ch)    &#125;        for i:=0;i&lt;len(apis);i++&#123;        fmt.Printf(&lt;-ch)    &#125;    elapsed := time.Since(start)    fmt.Printf(&quot;Done! It took %v seconds!\n&quot;, elapsed.Seconds())&#125;</code></pre><h3 id="两者最大不同点">两者最大不同点</h3><p>就是无缓冲的channel，数据发送和数据接收是强绑定的，任何时候都缺一不可，否则就会阻塞所在的goroutine。</p><p>而有缓冲的channel，在队列没有满之前，数据发送和数据接收是解耦的。满了之后和无缓冲规则一致。</p><h2 id="select">select</h2><p>select语句与switch类似，但它仅用来监听和channel有关的IO操作，当 IO 操作发生时，触发相应的动作。</p><p>那么既然case条件与channel有关，则case条件必然伴随着写入和读取chan类型变量</p><p>子句执行规则：</p><pre><code class="language-bash">select判断所有case条件是否与chan有关：与chan无关：直接报错与chan有关：判断case条件【是否可完成】chan的I/O操作：可完成：`随机`执行一个完成case条件的语句不可完成：有default子句：执行default子句无default子句：阻塞goruntine，直到出现一个可完成的case条件</code></pre><pre><code class="language-go">package mainimport (    &quot;fmt&quot;)func main() &#123;    ch1 := make(chan int,1)    for i := 0; i &lt; 10; i++&#123;        select &#123;        case x := &lt;- ch1:            fmt.Printf(&quot;ch1取出数据%d\n&quot;,x)        case ch1 &lt;- i:            fmt.Printf(&quot;ch1插入数据%d\n&quot;,i)        &#125;    &#125;&#125;</code></pre><pre><code class="language-bash">➜   go run main.goch1插入数据0ch1取出数据0ch1插入数据2ch1取出数据2ch1插入数据4ch1取出数据4ch1插入数据6ch1取出数据6ch1插入数据8ch1取出数据8</code></pre><p>之所以是输出偶数，原因在于：</p><p>ch1是一个队列为1的缓冲channel。</p><p>所以，当i=0时，执行插入操作，插入0；</p><p>当i=1时，因无法继续插入，只能取出，因此，取出0；</p><p>当i=2时，执行插入操作，插入2；</p><p>当i=3时，因无法继续插入，只能取出，因此，取出2；</p><h2 id="斐波那契数列例子">斐波那契数列例子</h2><pre><code class="language-GO">package mainimport (    &quot;fmt&quot;    &quot;time&quot;)var quit = make(chan bool)func fib(c chan int) &#123;    x, y := 1, 1    for &#123;        select &#123;            case c &lt;- x:                x, y = y, x+y            case &lt;-quit:                fmt.Println(&quot;Done calculating Fibonacci!&quot;) // 4            return        &#125;    &#125;&#125;func main() &#123;    start := time.Now()    command := &quot;&quot;    data := make(chan int)    go fib(data) // 1    for &#123; // 3        num := &lt;-data // 2        fmt.Println(num)        fmt.Scanf(&quot;%s&quot;, &amp;command)        if command == &quot;quit&quot; &#123;            quit &lt;- true            break        &#125;    &#125;    time.Sleep(1 * time.Second)    elapsed := time.Since(start)    fmt.Printf(&quot;Done! It took %v seconds!\n&quot;, elapsed.Seconds())&#125;</code></pre><p>通过 data 无缓冲 channel 构建一发（1号代码）一收（2号代码）的循环（3号代码）。</p><p>通过 quit 全局无缓冲 channel 构建是否退出 for（3号代码） 循环，以及退出前发送（4号代码）。</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyun☞oss</title>
      <link href="posts/b5d7d6ee/"/>
      <url>posts/b5d7d6ee/</url>
      
        <content type="html"><![CDATA[<h2 id="构建-oss-存储桶">构建 oss 存储桶</h2><ul><li>执行脚本（执行本脚本跑到阿里云的云端cli去执行）</li><li>执行前应先复制修改下面的策略模板</li></ul><p><a href="http://start.py">start.py</a></p><pre><code class="language-python">#!/usr/bin/python3.6# -*- coding: utf-8 -*-###  Usage：https://help.aliyun.com/document_detail/32027.html##  Github：https://github.com/aliyun/aliyun-oss-python-sdk##  author: zyhimport oss2, osfrom oss2.models import (LifecycleExpiration, LifecycleRule,                        BucketLifecycle,AbortMultipartUpload,                        TaggingRule, Tagging, StorageTransition,                        NoncurrentVersionStorageTransition,                        NoncurrentVersionExpiration)from oss2.models import Tagging, TaggingRule#from aliyunsdkcore.client import AcsClientfrom aliyunsdkcore.acs_exception.exceptions import ClientExceptionfrom aliyunsdkcore.acs_exception.exceptions import ServerExceptionfrom aliyunsdkram.request.v20150501.CreatePolicyRequest import CreatePolicyRequest####################################region = '我是区域ID'bucketName = '我是桶名'project = '我是标签project的值'vpcNetwork = '我是桶所在大区的ECS的内网网段'akey = skey = ###################################endpoint = 'http://oss-&#123;0&#125;.aliyuncs.com'.format(region)auth = oss2.Auth(akey,skey)bucket = oss2.Bucket(auth, endpoint, bucketName)# create bucketbucket.create_bucket()# add tagrule = TaggingRule()rule.add('project', project)tagging = Tagging(rule)bucket.put_bucket_tagging(tagging)# init dirsbucket.put_object('conf/README','我是存放配置的目录')bucket.put_object('data/README','我是存放数据的目录')bucket.put_object('hive/README','我是存放hive的目录')bucket.put_object('backup/README','我是存放备份的目录')bucket.put_object('logs/7days/README','我是存放保留7天的日志目录')bucket.put_object('logs/15days/README','我是存放保留15天的日志目录')bucket.put_object('logs/30days/README','我是存放保留30天的日志目录')bucket.put_object('logs/60days/README','我是存放保留60天的日志目录')bucket.put_object('logs/90days/README','我是存放保留90天的日志目录')bucket.put_object('logs/180days/README','我是存放永久保留的日志目录')# add lifecyclerule0 = LifecycleRule('rule0', 'tmp/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=3))rule1 = LifecycleRule('rule1', 'logs/7days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=7))rule2 = LifecycleRule('rule2', 'logs/15days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=15))rule3 = LifecycleRule('rule3', 'logs/30days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=30))rule4 = LifecycleRule('rule4', 'logs/60days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=60))rule5 = LifecycleRule('rule5', 'logs/90days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=90))rule6 = LifecycleRule('rule6', 'logs/180days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=180))rule7 = LifecycleRule('rule7', 'logs/longlasting/',                      status=LifecycleRule.ENABLED,                      storage_transitions=[StorageTransition(days=60,storage_class=oss2.BUCKET_STORAGE_CLASS_IA),                          StorageTransition(days=180,storage_class=oss2.BUCKET_STORAGE_CLASS_ARCHIVE)])lifecycle = BucketLifecycle([rule0, rule1, rule2, rule3, rule4, rule5, rule6, rule7])bucket.put_bucket_lifecycle(lifecycle)os.system(&quot;sed 's#ossBucketName#&#123;0&#125;#g' local.policy.default &gt; local/local_&#123;0&#125;.policy&quot;.format(bucketName))os.system(&quot;sed 's#ossBucketName#&#123;0&#125;#g' role.policy.default &gt; local/role_&#123;0&#125;.policy&quot;.format(bucketName))os.system(&quot;sed 's#ossBucketName#&#123;0&#125;#g' apps.policy.default &gt; local/apps_&#123;0&#125;.policy&quot;.format(bucketName))os.system(&quot;sed 's#0.0.0.0#&#123;0&#125;#g' apps.policy.default &gt; local/apps_&#123;1&#125;.policy&quot;.format(vpcNetwork, bucketName))os.system(&quot;aliyun ram CreatePolicy --PolicyName oss-&#123;0&#125;-local --PolicyDocument \&quot;`cat local/local_&#123;0&#125;.policy`\&quot;&quot;.format(bucketName))os.system(&quot;aliyun ram CreatePolicy --PolicyName oss-&#123;0&#125;-role --PolicyDocument \&quot;`cat local/role_&#123;0&#125;.policy`\&quot;&quot;.format(bucketName))os.system(&quot;aliyun ram CreatePolicy --PolicyName oss-&#123;0&#125;-apps --PolicyDocument \&quot;`cat local/apps_&#123;0&#125;.policy`\&quot;&quot;.format(bucketName))</code></pre><ul><li>远程用户策略（执行脚本前需要添加的）</li></ul><p>local.policy.default</p><p>✨自行替换策略里的1.1.1.1为公司ip或者远程ip</p><pre><code class="language-json">&#123;  &quot;Version&quot;: &quot;1&quot;,  &quot;Statement&quot;: [    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [                &quot;oss:GetObject&quot;,                &quot;oss:PutObject&quot;,                &quot;oss:DeleteObject&quot;,                &quot;oss:ListObjects&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName/logs/7days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/15days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/30days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/60days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/90days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/180days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/longlasting/*&quot;,        &quot;acs:oss:*:*:ossBucketName/conf/*&quot;,        &quot;acs:oss:*:*:ossBucketName/data/*&quot;,        &quot;acs:oss:*:*:ossBucketName/backup/*&quot;,        &quot;acs:oss:*:*:ossBucketName/hive/*&quot;,        &quot;acs:oss:*:*:ossBucketName/tmp/*&quot;,        &quot;acs:oss:*:*:ossBucketName&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;1.1.1.1&quot;          ]        &#125;      &#125;    &#125;,    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;oss:ListBuckets&quot;,        &quot;oss:GetBucketStat&quot;,        &quot;oss:GetBucketInfo&quot;,        &quot;oss:GetBucketTagging&quot;,        &quot;oss:GetBucketAcl&quot;,        &quot;oss:GetBucketLocation&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:*&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;1.1.1.1&quot;          ]        &#125;      &#125;    &#125;  ]&#125;</code></pre><ul><li>角色用户策略（执行脚本前需要添加的）</li></ul><p>角色策略不限制源，用于绑定到阿里云资源上，例如ECS</p><p>程序通过调用角色来获取临时权限</p><pre><code class="language-json">&#123;  &quot;Version&quot;: &quot;1&quot;,  &quot;Statement&quot;: [    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [                &quot;oss:GetObject&quot;,                &quot;oss:PutObject&quot;,                &quot;oss:DeleteObject&quot;,                &quot;oss:ListObjects&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName/logs/7days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/15days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/30days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/60days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/90days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/180days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/longlasting/*&quot;,        &quot;acs:oss:*:*:ossBucketName/conf/*&quot;,        &quot;acs:oss:*:*:ossBucketName/data/*&quot;,        &quot;acs:oss:*:*:ossBucketName/backup/*&quot;,        &quot;acs:oss:*:*:ossBucketName/hive/*&quot;,        &quot;acs:oss:*:*:ossBucketName/tmp/*&quot;        &quot;acs:oss:*:*:ossBucketName&quot;      ]    &#125;,    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;oss:ListBuckets&quot;,        &quot;oss:GetBucketStat&quot;,        &quot;oss:GetBucketInfo&quot;,        &quot;oss:GetBucketTagging&quot;,        &quot;oss:GetBucketAcl&quot;,        &quot;oss:GetBucketLocation&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:*&quot;      ]    &#125;  ]&#125;</code></pre><ul><li>程序用户策略</li></ul><p>用于ecs里的程序调用AK方式</p><pre><code class="language-json">&#123;  &quot;Version&quot;: &quot;1&quot;,  &quot;Statement&quot;: [    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [                &quot;oss:GetObject&quot;,                &quot;oss:PutObject&quot;,                &quot;oss:DeleteObject&quot;,                &quot;oss:ListObjects&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName/logs/7days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/15days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/30days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/60days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/90days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/180days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/longlasting/*&quot;,        &quot;acs:oss:*:*:ossBucketName/conf/*&quot;,        &quot;acs:oss:*:*:ossBucketName/data/*&quot;,        &quot;acs:oss:*:*:ossBucketName/backup/*&quot;,        &quot;acs:oss:*:*:ossBucketName/hive/*&quot;,        &quot;acs:oss:*:*:ossBucketName/tmp/*&quot;,        &quot;acs:oss:*:*:ossBucketName&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;0.0.0.0&quot;          ]        &#125;      &#125;    &#125;,    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;oss:ListBuckets&quot;,        &quot;oss:GetBucketStat&quot;,        &quot;oss:GetBucketInfo&quot;,        &quot;oss:GetBucketTagging&quot;,        &quot;oss:GetBucketAcl&quot;,        &quot;oss:GetBucketLocation&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:*&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;0.0.0.0&quot;          ]        &#125;      &#125;    &#125;  ]&#125;</code></pre><h2 id="使用">使用</h2><p>以角色授权方式+aliyun cli命令方式走起.</p><p>😔，aliyun cli 的文档和使用一言难尽=。=</p><p><strong>使用前请确保角色已经关联了对应权限以及角色已经绑定到了ECS上</strong></p><pre><code class="language-bash"># /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;</code></pre><blockquote><p>本脚本，会让任何一个会话登陆的时候就拿到角色拥有的权限</p></blockquote><p>导入角色</p><pre><code class="language-bash">##导入角色，获取权限以及一些变量source /etc/profile.d/ecs_role.sh</code></pre><p>设置端点</p><pre><code class="language-bash">## 设定 oss 的 endpoint 地址, EndpointLan VPC内使用，EndpointWan VPC外使用EndpointLan=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;EndpointWan=&quot;http://oss-$&#123;Region&#125;.aliyuncs.com&quot;</code></pre><p>进行操作</p><pre><code class="language-bash">##查询## 默认查询是递归查询，-d 只查询一层aliyun oss ls oss://test/ -d -e $&#123;EndpointLan&#125;##基本的上传或下载##上传文件 a.file 到 oss://test/ aliyun oss cp a.file oss://test/ -e $&#123;EndpointLan&#125;##基本的递归上传##上传目录 abc 下的文件到 oss://test/ 下，如果有重复内容，则需要加入 --forcealiyun oss cp abc oss://test/ --recursive -e $&#123;EndpointLan&#125;##复杂的递归上传##上传目录 abc 下的 .lzo 结尾的文件到 oss://test/ 下.##严禁在源目录里执行 --recursive 参数.##即禁止执行 aliyun oss cp . oss://test/ --recursive aliyun oss cp abc/ oss://test/ --include='*.lzo' --update --recursive -e $&#123;EndpointLan&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_output##同步目录 sync 指令变更##同步目录 abc 下的文件到 oss://test/ 下，如有重复，则忽略;同时删除目标目录下本地没有的文件aliyun oss sync abc/ oss://test/ --update --delete --force -e $&#123;EndpointLan&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_output</code></pre><h2 id="开发向-sdk">开发向 sdk</h2><p>关于php sdk访问对象存储的文档</p><p>php oss 对象 sdk<br><a href="https://packagist.org/packages/aliyuncs/oss-sdk-php?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6">https://packagist.org/packages/aliyuncs/oss-sdk-php?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6</a><br>php ram role sdk<br><a href="https://packagist.org/packages/alibabacloud/credentials?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6">https://packagist.org/packages/alibabacloud/credentials?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6</a></p>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
            <tag> oss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go-015自定义包</title>
      <link href="posts/83a93e23/"/>
      <url>posts/83a93e23/</url>
      
        <content type="html"><![CDATA[<h1>main包</h1><p>主程序需要引用<code>package main</code>，只有引用了<code>main</code>，才可以编译生成二进制程序。</p><h1>自定义包</h1><h2 id="包代码">包代码</h2><blockquote><p>&lt;$GOHOME/src/packageName&gt;/sum.go</p></blockquote><pre><code class="language-go">package calculator // 包名var logMessage = &quot;[LOG]&quot;// Version of the calculatorvar Version = &quot;1.0&quot;func internalSum(number int) int &#123;    return number - 1&#125;// Sum two integer numbersfunc Sum(number1, number2 int) int &#123;    return number1 + number2&#125;</code></pre><blockquote><p>package 定义包名</p><p>go约定：小写开头的仅用于包内部，大写开头的包内外均可以用.</p></blockquote><ul><li>只能从包内调用 <code>logMessage</code> 变量。</li><li>可以从任何位置访问 <code>Version</code> 变量。 建议你添加注释来描述此变量的用途。 （此描述适用于包的任何用户。）</li><li>只能从包内调用 <code>internalSum</code> 函数。</li><li>可以从任何位置访问 <code>Sum</code> 函数。 建议你添加注释来描述此函数的用途。</li></ul><h2 id="模块">模块</h2><p>Go通过模块引用包。</p><p>模块通常包含可提供相关功能的包。 包的模块还指定了 Go 运行你组合在一起的代码所需的上下文。 此上下文信息包括编写代码时所用的 Go 版本。</p><pre><code class="language-go">cd &lt;$GOHOME/src/packageName&gt;go mod init github.com/myuser/calculator</code></pre><blockquote><p>init 后是模块名，也是下载地址，go通过这个地址去下载。</p></blockquote><h1>包调用</h1><p>主程序：&lt;$GOHOME/src/packageName&gt;/main.go</p><p>通过模块名调用包</p><pre><code class="language-go">package mainimport (  &quot;fmt&quot;  &quot;github.com/myuser/calculator&quot;)func main() &#123;    total := calculator.Sum(3, 5) //通过模块名直接引用包下sum.go中的Sum()函数    fmt.Println(total)    fmt.Println(&quot;Version: &quot;, calculator.Version)&#125;</code></pre><h2 id="初始化主程序模块">初始化主程序模块</h2><pre><code class="language-bash">go mod init helloworld</code></pre><h2 id="修正模块文件">修正模块文件</h2><p>告知go引用本地模块 calculator 包</p><pre><code class="language-go">module helloworldgo 1.14require github.com/myuser/calculator v0.0.0replace github.com/myuser/calculator =&gt; ../calculator</code></pre><p><code>replace</code>关键词指定模块的本地路径</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞012常用模块的记录</title>
      <link href="posts/d1503f4a/"/>
      <url>posts/d1503f4a/</url>
      
        <content type="html"><![CDATA[<h2 id="log">log</h2><p>log 是内置模块，无需导入</p><pre><code class="language-go"># 直接打印log.Print(&quot;Hey, I'm a log!&quot;)2020/12/19 13:39:17 Hey, I'm a log!# 设置前缀log.SetPrefix(&quot;main():&quot;)main(): 2021/01/05 13:59:58 Hey, I'm a log!# 输出日志的同时退出程序，就如同执行os.Exit(1)log.Fatal(&quot;Hey, I'm an error log!&quot;)# 输出日志的同时执行 panic()log.Panic(&quot;Hey, I'm an error log!&quot;)# 输出日志到文件，file是打开的文件描述符log.SetOutput(file) log.Print(&quot;Hey, I'm a log!&quot;)</code></pre><h2 id="zerolog">zerolog</h2><p><a href="https://github.com/rs/zerolog">https://github.com/rs/zerolog</a></p><p>日志框架，扩展log模块。通过日志框架可以设置日志级别或者配置不同的日志输出。</p><pre><code class="language-bash">go get -u github.com/rs/zerolog/log</code></pre><pre><code class="language-go">package mainimport (    &quot;github.com/rs/zerolog&quot;    &quot;github.com/rs/zerolog/log&quot;)func main()&#123;    zerolog.TimeFieldFormat = zerolog.TimeFormatUnix    log.Print(&quot;Hey!I'm a log message&quot;)    log.Debug().        Int(&quot;EmployeeID&quot;, 1001).        Msg(&quot;Getting Employee Information&quot;)    log.Debug().Str(&quot;Name&quot;, &quot;John&quot;).Send()&#125;</code></pre><p>输出：</p><pre><code>&#123;&quot;level&quot;:&quot;debug&quot;,&quot;time&quot;:1645513637,&quot;message&quot;:&quot;Hey!I'm a log message&quot;&#125;&#123;&quot;level&quot;:&quot;debug&quot;,&quot;EmployeeID&quot;:1001,&quot;time&quot;:1645513637,&quot;message&quot;:&quot;Getting Employee Information&quot;&#125;&#123;&quot;level&quot;:&quot;debug&quot;,&quot;Name&quot;:&quot;John&quot;,&quot;time&quot;:1645513637&#125;</code></pre><h2 id="os">os</h2><p><a href="https://pkg.go.dev/os#File">os package - os - pkg.go.dev</a></p><h3 id="文件">文件</h3><p>os.OpenFile</p><pre><code>func OpenFile(name string, flag int, perm FileMode) (*File, error)</code></pre><p><code>os.OpenFile</code>根据<code>flag</code>模式来创建/打开一个文件</p><p>这里的返回值<code>*File</code>其实也是<code>io.Reader</code>，都是textual I/O这一类. 因此，<code>*File</code>也可以被用于<code>bufio</code>和<code>io</code>模块</p><p>常用的flag组合：</p><p><code>os.O_CREATE|os.O_RDWR</code> 创建并可读可写</p><p><code>os.O_CREATE|os.O_RDWR|os.O_APPEND</code>创建并可读可写可追加</p><p>⚠️需要注意的是，<code>os.O_APPEND</code>将始终如一的将指针定位到尾部，因此<code>*File.Seek</code>无法使用，所以你也无法实现计算文件行数之类的方法(因为你无法定位到首部)。</p><h3 id="全路径">全路径</h3><p>os.Executable()  返回可执行程序的全路径</p><pre><code class="language-go">func Executable() (string, error)</code></pre><p>⚠️ 但在符号链接的情况下，可能返回的是符号链接的路径或者其指向的路径</p><h3 id="命令行参数">命令行参数</h3><p>os.Args <code>[]string</code> ，其<code>os.Args[0]</code>是程序名</p><h3 id="退出程序">退出程序</h3><p>os.Exit(code int) ，以指定的code退出程序，按照通常做法，你应该以<code>0</code>表示正常退出，<code>非0</code>表示异常</p><p>⚠️它会导致<code>defer</code>不执行</p><h2 id="os-user">os/user</h2><p>获取当前用户信息</p><pre><code class="language-go">userInfo, err := user.Current()if err != nil &#123;    fmt.Println(&quot;Error: get current user&quot;)&#125; else &#123;    userName := userInfo.Username&#125;</code></pre><h2 id="os-exec">os/exec</h2><p>执行命令</p><pre><code class="language-go">cmd := appPath + &quot; &quot; + appArgsif _, err := exec.LooPath(appPath); err != nil &#123;    fmt.Printf(&quot;Error: %s not found\n&quot;, appPath)&#125; else &#123;    c := exec.Command(&quot;bash&quot;, &quot;-c&quot;, cmd)    output, err := c.CombineOutput()    if err != nil &#123;        fmt.Printf(&quot;Error: %s\n&quot;, output)    &#125; else &#123;        fmt.Printf(&quot;Success: %s\n&quot;, output)    &#125;&#125;</code></pre><h2 id="bytes">bytes</h2><p>字节对比</p><pre><code class="language-go">if bytes.Equal(aByte, bByte) &#123;    fmt.Println(&quot;两者相同&quot;)&#125;</code></pre><h2 id="path-filepath">path/filepath</h2><p>获取文件名和路径</p><pre><code class="language-go">if fullPath, err := os.Executable(); err != nil &#123;fmt.Printf(&quot;Error executing&quot;)&#125; else &#123;basePath, baseName := filepath.Split(fullPath)    &#125;</code></pre><h2 id="bufio">bufio</h2><p>实现带缓存的I/O</p><h2 id="io-和-os">io 和 os</h2><p>读写文件或者文件描述符的内容</p><p>💁另外，io/ioutil 也可以实现io和os里的一些文件相关的方法，但是他们已经迁移到了io和os</p><pre><code class="language-go">fd, err := os.OpenFile(filePath, os.O_RDWR|os.O_CREATE, 0755)defer fd.Close()if err != nil &#123;    fmt.Printf(&quot;Error(open): %s\n&quot;, filePath)&#125; else &#123;    fByte, err := io.ReadAll(fd)    if err != nil &#123;        fmt.Printf(&quot;Error(read): %s\n&quot;, err)    &#125; else &#123;        fmt.Printf(&quot;file content:\n %s\n&quot;, fByte)    &#125;&#125;</code></pre><h2 id="mysql">mysql</h2><p><a href="https://pkg.go.dev/database/sql#Rows.Next">sql package - database/sql - pkg.go.dev</a></p><pre><code class="language-go">import (    &quot;database/sql&quot;_ &quot;github.com/go-sql-driver/mysql&quot;)func main()&#123;mysqlConnect := fmt.Sprintf(&quot;%s:%s@tcp(%s)/mysql&quot;, conf.Database.Username, conf.Database.Password, conf.Database.Host)    if err != nil &#123;        errors.New(&quot;Error mysql connect&quot;)        return    &#125;    db.SetConnMaxLifetime(100*time.Second)  //最大连接周期，超过时间的连接就close    db.SetMaxOpenConns(10) //设置最大连接数    db.SetMaxIdleConns(5) //设置闲置连接    rows, err := db.Query(&quot;show full processlist&quot;)    if err != nil &#123;        errors.New(&quot;Error db Query&quot;)        return    &#125;      var Id,Time int64    var User, Host, Command, State string    var Db, Info sql.NullString    for rows.Next() &#123;        rows.Scan(&amp;Id, &amp;User, &amp;Host, &amp;Db, &amp;Command, &amp;Time, &amp;State, &amp;Info)    fmt.Printf(&quot;%d,%s,%s,%s,%s,%d,%s,\&quot;%s\&quot;\n&quot;, Id, User, Host, Db.String, Command, Time, State, Info.String&#125;</code></pre><p><code>rows.Next()</code>读取一行，并指向行尾</p><p><code>rows.Scan(dest ...interface&#123;&#125;)</code>将当前行的值逐个字段的复制给dest，dest是指针类型。dest顺序和当前行的字段顺序需要一一对应。</p><h2 id="gopkg-in-gomail-v2"><a href="http://gopkg.in/gomail.v2">gopkg.in/gomail.v2</a></h2><p><a href="https://pkg.go.dev/gopkg.in/gomail.v2">gomail package - gopkg.in/gomail.v2 - pkg.go.dev</a></p><pre><code class="language-go">func sendEmail(smtpServer string, smtpPort int, smtpUser string, smtpPassword string, receiveUser []string, subject string, content string, attach string) error &#123;    m := gomail.NewMessage()    m.SetHeader(&quot;From&quot;, smtpUser)    m.SetHeader(&quot;To&quot;, receiveUser...)    m.SetHeader(&quot;Subject&quot;, subject)    m.SetBody(&quot;text/html&quot;, content)    m.Attach(attach)    d := gomail.NewDialer(smtpServer, smtpPort, smtpUser, smtpPassword)    err := d.DialAndSend(m)    return err&#125;</code></pre><h2 id="gopkg-in-yaml-v3"><a href="http://gopkg.in/yaml.v3">gopkg.in/yaml.v3</a></h2><p><a href="https://pkg.go.dev/gopkg.in/yaml.v3">yaml package - gopkg.in/yaml.v3 - pkg.go.dev</a></p><p>配置文件示例</p><pre><code class="language-yaml">email:  smtpserver: smtp.feishu.cn  smtpport: 465  smtpuser:   smtppassword:   receiveuser:     - zhangsan@abc.com  subject:   content: database:  host:   username:   password:   timeout: </code></pre><pre><code class="language-go">//映射配置文件的结构体type Config struct &#123;    Email struct &#123;        Smtpserver string `yaml:&quot;smtpserver&quot;`        Smtpport int `yaml:&quot;smtpport&quot;`        Smtpuser string `yaml:&quot;smtpuser&quot;`        Smtppassword string `yaml:&quot;smtppassword&quot;`        Receiveuser []string `yaml:&quot;receiveuser&quot;`        Subject string `yaml:&quot;subject&quot;`        Content string `yaml:&quot;content&quot;`    &#125;    Database struct &#123;        Host string `yaml:&quot;host&quot;`        Username string `yaml:&quot;username&quot;`        Password string `yaml:&quot;password&quot;`        Timeout int64 `yaml:&quot;timeout&quot;`    &#125;&#125;//实现结构体方法 getConffunc (config *Config) getConf(confPath string) (*Config) &#123;    yamlFile, err := ioutil.ReadFile(confPath)    if err != nil &#123;        fmt.Println(&quot;Error reading config&quot;)    &#125;    err = yaml.Unmarshal(yamlFile, config)    if err != nil &#123;        fmt.Println(&quot;Error unmarshaling config&quot;)    &#125;    return config&#125;func main()&#123;    var config Config    conf := config.getConf(&quot;/to_path/config.yaml&quot;)    fmt.Println(conf.Email.Smtpserver)&#125;</code></pre><p>⚠️定义配置文件映射的结构体的时候，属性首字母要大写</p><h2 id="encoding-json">encoding/json</h2><p><a href="https://pkg.go.dev/encoding/json">https://pkg.go.dev/encoding/json</a></p><p><a href="https://json2struct.mervine.net/">https://json2struct.mervine.net/</a>    json转struct</p><h3 id="构造json对象，用来传递参数">构造json对象，用来传递参数</h3><pre><code class="language-go">import &quot;encoding/json&quot;postMap := map[string]string&#123;&quot;username&quot;: username, &quot;password&quot;: password&#125;postJson, err := json.Marshal(postMap) // postJson 已经是[]byteif err != nil &#123;    fmt.Println(&quot;Error:json.Marshal&quot;)&#125;</code></pre><h3 id="解析json对象，获取对象值">解析json对象，获取对象值</h3><pre><code class="language-go">import (    &quot;encoding/json&quot;    &quot;fmt&quot;)func main()&#123;    var bodyByte = []byte(`[        &#123;&quot;Name&quot;: &quot;Platypus&quot;, &quot;Order&quot;: &quot;Monotremata&quot;&#125;,        &#123;&quot;Name&quot;: &quot;Quoll&quot;,    &quot;Order&quot;: &quot;Dasyuromorphia&quot;&#125;    ]`)    type Animal struct &#123;        Name  string        Order string    &#125;    var animals []Animal    if err := json.Unmarshal(bodyByte, &amp;animals); err != nil &#123;        fmt.Println(err)    &#125; else &#123;        for _, val := range animals &#123;            fmt.Printf(&quot;Name:%s, Order:%s\n&quot;, val.Name, val.Order)        &#125;    &#125;&#125;</code></pre><p>输出：</p><pre><code class="language-bash">Name:Platypus, Order:MonotremataName:Quoll, Order:Dasyuromorphia</code></pre><h3 id="注意点">注意点</h3><p>以小写字母开头的结构体字段不会被编码进去</p><pre><code class="language-yaml">package mainimport (      &quot;fmt&quot;    &quot;encoding/json&quot;)type MyData struct &#123;      One int    two string&#125;func main() &#123;      in := MyData&#123;1,&quot;two&quot;&#125;    fmt.Printf(&quot;%#v\n&quot;,in) //prints main.MyData&#123;One:1, two:&quot;two&quot;&#125;    encoded,_ := json.Marshal(in)    fmt.Println(string(encoded)) //prints &#123;&quot;One&quot;:1&#125;  这里可以看出two:&quot;two&quot;没有被编码进去    var out MyData    json.Unmarshal(encoded,&amp;out)    fmt.Printf(&quot;%#v\n&quot;,out) //prints main.MyData&#123;One:1, two:&quot;&quot;&#125;&#125;</code></pre><h2 id="time">time</h2><p><a href="https://pkg.go.dev/time#Now">time package - time - pkg.go.dev</a></p><p>go采用一种很独特的方式来格式化时间，即：</p><p>2006数字代表年，如同常见的<code>%Y</code></p><p>01数字代表月，如同常见的<code>%m</code></p><p>02数字代表日</p><p>15数字代表时（24小时的下午三点）</p><p>04数字代表分</p><p>05数字代表秒</p><p>Z07数字代表时区(-07也可以)</p><p>整体记忆方法就是：01、02、15(03)、04、05、2006(06)、Z07(07)</p><pre><code class="language-go">time.Now().Format(&quot;2006.01.02 15:04:05 Z07&quot;)</code></pre><p>转时区</p><pre><code class="language-golang">package mainimport (    &quot;fmt&quot;    &quot;time&quot;)func main()&#123;    currenttime := time.Now()    time_zone, _ := time.LoadLocation(&quot;Asia/Shanghai&quot;)    fmt.Println(currenttime.In(time_zone).Format(&quot;2006-01-02-15-04-05&quot;))&#125;</code></pre><h2 id="net-http">net/http</h2><p><a href="https://pkg.go.dev/net/http">https://pkg.go.dev/net/http</a></p><pre><code class="language-go">import &quot;net/http&quot;func GetUser(url, path, username string, password string) (UserJson, error) &#123;client := &amp;http.Client&#123;&#125; // 创建客户端postMap := map[string]string&#123;&quot;username&quot;: username, &quot;password&quot;: password&#125;postJson, err := json.Marshal(postMap)if err != nil &#123;fmt.Println(&quot;Error json.Marshal&quot;)&#125;request, err := http.NewRequest(http.MethodPost, url+path, bytes.NewBuffer(postJson)) // 创建请求体if err != nil &#123;fmt.Println(&quot;Error http.NewRequest&quot;)&#125;request.Header.Set(&quot;Content-Type&quot;, &quot;application/json&quot;) // 添加类型response, err := client.Do(request)if err != nil &#123;fmt.Println(&quot;Error client do request&quot;)&#125;bodyByte, err := io.ReadAll(response.Body)defer response.Body.Close()var user UserJsonif err := json.Unmarshal(bodyByte, &amp;user); err != nil &#123;fmt.Println(&quot;Error json unmarshal response userjson&quot;)&#125;return user, err&#125;func main() &#123;    myuser, err := GetUser(acmeServer, &quot;/api/user/&quot;, authuser, authpasswd)&#125;</code></pre><h3 id></h3><p><a href="https://pkg.go.dev/github.com/ghodss/yaml@v1.0.0">https://pkg.go.dev/github.com/ghodss/yaml@v1.0.0</a></p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络☞S5700配置攻击朔源</title>
      <link href="posts/5290b79d/"/>
      <url>posts/5290b79d/</url>
      
        <content type="html"><![CDATA[<h2 id="目的">目的</h2><p>判断网络攻击来源</p><h2 id="设置管理员ip为白名单">设置管理员ip为白名单</h2><pre><code class="language-bash">acl 3666rule 1 permit ip source 10.200.15.1 0.0.0.0</code></pre><h2 id="根据IP判断">根据IP判断</h2><pre><code class="language-bash">cpu-defend policy 1auto-defend enableauto-defend attack-packet sample 5auto-defend trace-type source-mac source-ip source-portvlanauto-defend protocol arp icmp dhcp telnetauto-defend threshold 60auto-defend alarm enableauto-defend alarm threshold 60auto-defend whitelist 1 acl 3666</code></pre><h2 id="根据端口判断">根据端口判断</h2><pre><code class="language-bash">cpu-defend policy 1auto-port-defend enableauto-port-defend attack-packet sample 5auto-port-defend protocol arp-request arp-reply dhcp icmpauto-port-defend protocol arp-request threshold 30auto-port-defend protocol arp-reply threshold 30auto-port-defend protocol dhcp threshold 30auto-port-defend protocol icmp threshold 30auto-port-defend alarm enableauto-port-defend whitelist 1 acl 3666</code></pre><h2 id="应用策略">应用策略</h2><pre><code class="language-bash">cpu-defend-policy 1 global</code></pre><h2 id="查看攻击来源ip">查看攻击来源ip</h2><pre><code class="language-bash">display auto-defend attack-source</code></pre><h2 id="查看攻击来源端口">查看攻击来源端口</h2><pre><code class="language-bash">display auto-port-defend attack-source</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 交换机 </tag>
            
            <tag> S5700 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞ubuntu20.04纯命令行网卡配置</title>
      <link href="posts/e4f7ce4c/"/>
      <url>posts/e4f7ce4c/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>netplan 用于屏蔽各种网络管理器，它可以让你用一份配置，就帮你把网络给配置好，而无需你考虑网络控制器如何使用。</p><p>20.04 已经默认安装了 netplan.</p><h2 id="记录当前网卡标识名">记录当前网卡标识名</h2><pre><code class="language-bash">ip addr show # ===1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: enp0s31f6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000    link/ether d8:9e:f3:31:83:c8 brd ff:ff:ff:ff:ff:ff</code></pre><p>例如上述中的 <code>enp0s31f6</code></p><h2 id="编辑配置文件">编辑配置文件</h2><pre><code class="language-bash">cat /etc/netplan/00-installer-config.yaml# ===# This is the network config written by 'subiquity'network:  ethernets:    enp0s31f6:      addresses: [10.200.10.2/24]      dhcp4: no      optional: true      gateway4: 10.200.10.1      nameservers:        addresses: [10.200.10.1]  version: 2  renderer: networkd</code></pre><h2 id="加载配置">加载配置</h2><pre><code class="language-bash">netplan applysystemctl restart networkd-dispatcher.service</code></pre><blockquote><p>之后如果再修改 ip， 再次执行 <code>netplay apply</code> 即可</p></blockquote><h2 id="参考">参考</h2><p><a href="https://netplan.io/examples/#configuration">https://netplan.io/examples/#configuration</a></p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ubuntu </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞mysql慢sql预警</title>
      <link href="posts/ab1247fb/"/>
      <url>posts/ab1247fb/</url>
      
        <content type="html"><![CDATA[<h2 id="用途">用途</h2><p>检测数据库中执行缓慢的sql，并记录预警</p><h2 id="脚本">脚本</h2><pre><code class="language-bash">#!/bin/bash# by zyh# 检查数据库耗时过长的语句# 所需配置文件:# 1. userfile.txt 用于判断程序用户的sql是否超时，超时了杀死  ##用户名      密码    超时时间# 2. database.conf 需要show full processlist权限  ##Username=  ##Password=  ##Mysqlhostname=  ##Database=  ##Port=# bash start.sh database.conf 放到计划任务里# -------------------------------------------------------------------------------#--变量--basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`currtime=`date &quot;+%Y%m%d_%H%M%S&quot;`#Username=#Password=#Mysqlhostname=#Database=#Port=source $basedir/$1datadir=$basedir/data/$Database-$1/$currtime[[ -d $datadir ]] || mkdir -p $datadir#--main--mysql -s -r -u$Username -p$Password -h$Mysqlhostname -P$Port -e 'show full processlist' | grep -v 'Sleep' |grep -v 'system' | egrep '(Query|Execute)' | grep -i $'\tselect' &gt; $datadir/$Database.allawk -F'\t' 'BEGIN&#123;OFS=&quot;\t&quot;&#125;ARGIND==1&#123;warntime[$1]=$3;warnpwd[$1]=$2&#125; \                ARGIND==2&#123; \                        for( nametime in warntime ) &#123; \                                if( $2 == nametime &amp;&amp; $6 &gt; warntime[$2] ) &#123; \                                        print warnpwd[$2],$0;break \                                &#125; \                        &#125; \                &#125;' $basedir/userfile.txt $datadir/$Database.all &gt; $datadir/$Database.warncat $datadir/$Database.warn | while read warningpwd id warningname other;do        echo &quot;# $&#123;warningname&#125;: $id $other&quot;        echo &quot;mysql -s -r -u$warningname -p$warningpwd -h$Mysqlhostname -P$Port -e 'kill '&quot;$id&quot;'&quot;done &gt; $datadir/kill.sql[[ -s $datadir/kill.sql ]] &amp;&amp; python $&#123;basedir&#125;/sendmail.py '收件人邮箱地址' &quot;[SQL-TimeOut]-$Database&quot; &quot;($Mysqlhostname)The attachment content is timeout SQL information&quot; '抄送邮箱地址'  &quot;$datadir/kill.sql&quot; || rm -rf $datadir</code></pre><pre><code class="language-python">#!/usr/bin/python#coding:utf-8# by zyh# python sendmail.py '收件人邮箱地址'  '主题' '邮件内容' '抄送邮箱地址' '附件绝对路径'import smtplibfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartimport sysreload(sys)sys.setdefaultencoding('utf8')mail_host = 'smtp服务器地址'  # 例如 smtp.gmail.com:587mail_user = '发件人邮箱'      # 例如 it@gmail.commail_pass = '发件人密码'def send_mail(*args):    argsNum=len(args[0])    if argsNum == 4:        filename,to_list, subject, content=args[0]    elif argsNum == 5:        filename,to_list, subject, content, cc_list=args[0]    elif argsNum == 6:        filename,to_list, subject, content, cc_list, subfile=args[0]    me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;    #初始化邮件对象 msg    msg = MIMEMultipart()    msg['From'] = me    msg['to'] = to_list    emailList=to_list    # 主题    msg['Subject'] = Header(subject, 'utf-8').encode()    # 内容    msg.attach(MIMEText(content, 'plain', 'utf-8'))    # 抄送    if 'cc_list' in vars():        msg['Cc'] = cc_list        emailstring = to_list+','+cc_list        emailList=emailstring.split(&quot;,&quot;)    # 附件    if 'subfile' in vars():        att1 = MIMEText(open((&quot;%s&quot;%subfile),'rb').read(), 'base64', 'utf8')        att1[&quot;Content-Type&quot;] = 'text/plain'        att1[&quot;Content-Disposition&quot;] = 'attachment; filename='+subfile.split('/')[-1] # 这里的filename可以任意写，写什么名字，邮件中显示什么名字        msg.attach(att1)    # 发送    try:        print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)        print &quot;connect mail server suesscc&quot;        s.starttls()        s.login(mail_user,mail_pass)        print &quot;login mail server suesscc&quot;        s.sendmail(me,emailList,msg.as_string())        s.close()        return True    except Exception,e:        print &quot;%s\t%s&quot;%(to_list,str(e))        return Falseif __name__ == &quot;__main__&quot;:    if len(sys.argv) &lt; 4:        exit()    send_mail(sys.argv)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>web☞服务器承载能力计算</title>
      <link href="posts/7c44c2e8/"/>
      <url>posts/7c44c2e8/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>我们在评估一个系统需要多少个服务器来支撑的时候，一般会涉及到多个点。</p><ul><li>pv</li><li>qps</li><li>并发</li><li>平均响应时间</li></ul><p>如果你已经搜索过相关站点，你会看到一个公式：并发=qps*平均响应时间</p><p>说实话，这个公式很蛋疼，猛地一看让人一脸懵逼。这里我按照我的方式来说一下。</p><p>首先将公式变形， QPS = 工作进程数 / 平均响应时间</p><p>其次，换算两边时间单位都为秒级， QPS = （1秒 / 秒级平均响应时间）* 工作进程数</p><p>再次，将<code>并发</code>理解为系统的<code>工作进程</code>，也就是如果并发是100，那么你可以简单的理解为理想情况下100个工作进程同一时间只能处理100个请求。</p><p>那么，<code>1秒 / 秒级平均响应时间</code>就是指每个工作进程1秒能处理多少个请求。</p><p>所以，最终得到 <code>QPS</code>就是系统每秒可以处理的请求数。</p><p>如果你日PV是 300W，那么一般情况下，每天80%的请求量都集中在每天20%的时间里。因此我们可以计算出高峰时间的秒级请求量是 3000000 * 0.8  / （3600 * 24 * 0.2） = 139</p><p>因此，如果你的系统根据 <code>QPS = 工作进程数 / 平均响应时间</code> 得到的 QPS 大于 139，那么你的系统就可以顶住。</p><p>如果小于，那么你有两个选择：</p><ol><li>需要增加工作进程数，也就是增加服务器数量或者服务器里的工作进程</li><li>优化系统，降低平均响应时间</li></ol><blockquote><p>⚠️增加服务器的工作进程，并不一定会产生正向效应，因为工作进程越多，那么进程就可能频繁切换，反而导致工作进程不干活。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> web </tag>
            
            <tag> 承载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞16kubeadm集群升级</title>
      <link href="posts/7538d100/"/>
      <url>posts/7538d100/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</a></p><p>⚠️ 务必先看此页面，因为不能跨多个版本升级。需要根据当前版本一步一步升级。</p><p>分为两部分，升级核心节点和升级工作节点.</p><h1>升级控制平面节点 Control Plane</h1><p>控制平面核心组件包含：</p><ul><li>kube-apiserver</li><li>etcd</li><li>kube-scheduler</li><li>kube-controller-manager</li><li>cloud-controller-manager</li></ul><h2 id="列出想升级的kube版本">列出想升级的kube版本</h2><p>这里以1.20-1.21版本为例</p><blockquote><p>所有核心节点上执行</p></blockquote><pre><code class="language-bash"># 选择主版本号master_verions=1.21yum list --showduplicates kubeadm --disableexcludes=kubernetes | grep $&#123;master_verions&#125;===kubeadm.x86_64                       1.20.8-0                        @kuberneteskubeadm.x86_64                       1.20.0-0                        kuberneteskubeadm.x86_64                       1.20.1-0                        kuberneteskubeadm.x86_64                       1.20.2-0                        kuberneteskubeadm.x86_64                       1.20.4-0                        kuberneteskubeadm.x86_64                       1.20.5-0                        kuberneteskubeadm.x86_64                       1.20.6-0                        kuberneteskubeadm.x86_64                       1.20.7-0                        kuberneteskubeadm.x86_64                       1.20.8-0                        kubernetes</code></pre><p>–disableexcludes=kubernetes 只允许kubernetes库</p><h2 id="所有核心节点：提前下载好升级所需的镜像">所有核心节点：提前下载好升级所需的镜像</h2><blockquote><p>所有核心节点执行</p></blockquote><pre><code class="language-bash"># 选择主版本号对应的最新稳定版本full_version=1.21.7</code></pre><h3 id="列出版本所需的包">列出版本所需的包</h3><pre><code class="language-bash">kubeadm config images list --kubernetes-version=$&#123;full_version&#125;===k8s.gcr.io/kube-apiserver:v1.21.7k8s.gcr.io/kube-controller-manager:v1.21.7k8s.gcr.io/kube-scheduler:v1.21.7k8s.gcr.io/kube-proxy:v1.21.7k8s.gcr.io/pause:3.5k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns/coredns:v1.8.4</code></pre><p>⚠️本文档撰写的时候，上述命令无法列出正确的包，例如v1.22.4版本，版本发布中提到etcd已更新至3.5.0，但却无法找到，此时会输出如下信息，即列出上一个最新版本.</p><pre><code class="language-bash">could not find officially supported version of etcd for Kubernetes v1.22.4, falling back to the nearest etcd version (3.4.13-0)</code></pre><p>此时，你应该已版本发布信息里的版本为准.</p><p>ℹ️ 可以通过提前升级一个kubeadm，并执行<code>kubeadm upgrade plan</code>得到正确的包版本.</p><h3 id="构建包拉取脚本">构建包拉取脚本</h3><blockquote><p>所有核心节点执行</p></blockquote><p>根据上面的输出版本，修改下面脚本中 <code>pause</code> <code>etcd</code> <code>coredns</code> 的版本号</p><pre><code class="language-bash">cat&gt;images-pull.sh&lt;&lt;EOF#!/bin/bash# kubeadm config images listimages=(kube-apiserver:v$&#123;full_version&#125;kube-controller-manager:v$&#123;full_version&#125;kube-scheduler:v$&#123;full_version&#125;kube-proxy:v$&#123;full_version&#125;pause:3.5etcd:3.4.13-0coredns:1.8.4)for imageName in \$&#123;images[@]&#125;;do    docker pull registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;    docker tag registry.aliyuncs.com/google_containers/\$&#123;imageName&#125; k8s.gcr.io/\$&#123;imageName&#125;    docker rmi registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;doneEOF</code></pre><pre><code class="language-bash">bash images-pull.sh</code></pre><blockquote><p>如果某些包无法下载，则需自行去 dockerhub 上找，并在下载完毕后，添加 k8s 自己的 tags，例如 coredns<br>docker pull coredns/coredns:1.6.7<br>docker tag coredns/coredns:1.6.7 <a href="http://k8s.gcr.io/coredns:1.6.7">k8s.gcr.io/coredns:1.6.7</a><br>docker rmi coredns/coredns:1.6.7</p></blockquote><h2 id="第一个核心节点：安装目标版本的kubeadm">第一个核心节点：安装目标版本的kubeadm</h2><blockquote><p>在第一个要升级的核心节点上执行</p></blockquote><p>升级计划是以kubeadm的版本为基准的，例如你当前安装的 kubeadm 的版本是 x，那么之后通过 kubeadm 命令列出的升级计划就是升级到 x 的最新稳定版。</p><p>因此，我们需要将 kubeadm 升级到目标版本</p><pre><code class="language-bash">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubeadm-$&#123;full_version&#125; --disableexcludes=kubernetes</code></pre><h2 id="第一个核心节点：列出升级计划">第一个核心节点：列出升级计划</h2><blockquote><p>在第一个要升级的核心节点上执行</p></blockquote><pre><code class="language-bash">kubeadm upgrade plan</code></pre><p>会输出三部分内容，</p><p>第一部分是当前集群信息 v1.20.8和当前kubeadm版本对应的最新稳定版v1.21.7，以及当前集群版本最新稳定版v1.20.13，还有最新的v1.22.4版本</p><pre><code>[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'[preflight] Running pre-flight checks.[upgrade] Running cluster health checks[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.20.8[upgrade/versions] kubeadm version: v1.21.7I1202 16:13:13.030774    4057 version.go:254] remote version is much newer: v1.22.4; falling back to: stable-1.21[upgrade/versions] Target version: v1.21.7[upgrade/versions] Latest version in the v1.20 series: v1.20.13</code></pre><p>第二部分是升级信息，升级信息又分为两部分。</p><ul><li>升级到当前版本 v1.20.8的最新稳定版本v1.20.13</li></ul><pre><code>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT   CURRENT       TARGETkubelet     3 x v1.20.8   v1.20.13Upgrade to the latest version in the v1.20 series:COMPONENT                 CURRENT    TARGETkube-apiserver            v1.20.8    v1.20.13kube-controller-manager   v1.20.8    v1.20.13kube-scheduler            v1.20.8    v1.20.13kube-proxy                v1.20.8    v1.20.13CoreDNS                   1.7.0      v1.8.0etcd                      3.4.13-0   3.4.13-0You can now apply the upgrade by executing the following command:        kubeadm upgrade apply v1.20.13</code></pre><ul><li>升级到下一个版本v1.21.7</li></ul><pre><code>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT   CURRENT       TARGETkubelet     3 x v1.20.8   v1.21.7Upgrade to the latest stable version:COMPONENT                 CURRENT    TARGETkube-apiserver            v1.20.8    v1.21.7kube-controller-manager   v1.20.8    v1.21.7kube-scheduler            v1.20.8    v1.21.7kube-proxy                v1.20.8    v1.21.7CoreDNS                   1.7.0      v1.8.0etcd                      3.4.13-0   3.4.13-0You can now apply the upgrade by executing the following command:        kubeadm upgrade apply v1.21.7</code></pre><p>第三部分是手动更新部分</p><pre><code class="language-:">The table below shows the current state of component configs as understood by this version of kubeadm.Configs that have a &quot;yes&quot; mark in the &quot;MANUAL UPGRADE REQUIRED&quot; column require manual config upgrade orresetting to kubeadm defaults before a successful upgrade can be performed. The version to manuallyupgrade to is denoted in the &quot;PREFERRED VERSION&quot; column.API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIREDkubeproxy.config.k8s.io   v1alpha1          v1alpha1            nokubelet.config.k8s.io     v1beta1           v1beta1             no</code></pre><p>⭐️如果<code>MANUAL UPGRADE REQUIRED</code>标记是<code>yes</code>，则你需要手动进行升级。手动升级的前提是需要自行提供配置文件</p><pre><code class="language-bash">kubeadm upgrade apply --config &lt;配置文件&gt;</code></pre><p>ℹ️ kubeadm upgrade 总是会刷新证书。</p><h2 id="第一个核心节点：升级主要组件">第一个核心节点：升级主要组件</h2><blockquote><p>在第一个要升级的核心节点上执行</p></blockquote><pre><code class="language-bash">kubeadm upgrade apply v1.21.7=== 期间会有一次确认[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'[preflight] Running pre-flight checks.[upgrade] Running cluster health checks[upgrade/version] You have chosen to change the cluster version to &quot;v1.21.7&quot;[upgrade/versions] Cluster version: v1.20.8[upgrade/versions] kubeadm version: v1.21.7[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]:y=== 拉取镜像[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'=== 静态pod升级，并备份的信息[upgrade/apply] Upgrading your Static Pod-hosted control plane to version &quot;v1.21.7&quot;...Static pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3[upgrade/etcd] Upgrading to TLS for etcdStatic pod: etcd-k8s01 hash: d9b8ab2ef694da7813c41fbf36833ba1[upgrade/staticpods] Preparing for &quot;etcd&quot; upgrade[upgrade/staticpods] Current and new manifests of etcd are equal, skipping upgrade[upgrade/etcd] Waiting for etcd to become available[upgrade/staticpods] Writing new Static Pod manifests to &quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests437528341&quot;[upgrade/staticpods] Preparing for &quot;kube-apiserver&quot; upgrade[upgrade/staticpods] Renewing apiserver certificate[upgrade/staticpods] Renewing apiserver-kubelet-client certificate[upgrade/staticpods] Renewing front-proxy-client certificate[upgrade/staticpods] Renewing apiserver-etcd-client certificate[upgrade/staticpods] Moved new manifest to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; and backed up old manifest to &quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-12-02-16-21-32/kube-apiserver.yaml&quot;[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: 871c4bc8d662b466c6481ef11d3bc7ecStatic pod: kube-apiserver-k8s01 hash: c99ad4c36653e9251a653ed601ba1117[apiclient] Found 3 Pods for label selector component=kube-apiserver[upgrade/staticpods] Component &quot;kube-apiserver&quot; upgraded successfully![upgrade/staticpods] Preparing for &quot;kube-controller-manager&quot; upgrade[upgrade/staticpods] Renewing controller-manager.conf certificate[upgrade/staticpods] Moved new manifest to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; and backed up old manifest to &quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-12-02-16-21-32/kube-controller-manager.yaml&quot;[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-controller-manager-k8s01 hash: 22f0b6f7c55d4fef49a89a4f535241a0Static pod: kube-controller-manager-k8s01 hash: ecd7e36a5c07f4ccf0f769e8f0fe6dc5[apiclient] Found 3 Pods for label selector component=kube-controller-manager[upgrade/staticpods] Component &quot;kube-controller-manager&quot; upgraded successfully![upgrade/staticpods] Preparing for &quot;kube-scheduler&quot; upgrade[upgrade/staticpods] Renewing scheduler.conf certificate[upgrade/staticpods] Moved new manifest to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; and backed up old manifest to &quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-12-02-16-21-32/kube-scheduler.yaml&quot;[upgrade/staticpods] Waiting for the kubelet to restart the component[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3Static pod: kube-scheduler-k8s01 hash: 98178ef8494b07ffc6d724adb4d8a0c3Static pod: kube-scheduler-k8s01 hash: 7d2a77b067995d323e127a47f45e8f14[apiclient] Found 3 Pods for label selector component=kube-scheduler[upgrade/staticpods] Component &quot;kube-scheduler&quot; upgraded successfully![upgrade/postupgrade] Applying label node-role.kubernetes.io/control-plane='' to Nodes with label node-role.kubernetes.io/master='' (deprecated)[upgrade/postupgrade] Applying label node.kubernetes.io/exclude-from-external-load-balancers='' to control plane Nodes[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.21&quot; in namespace kube-system with the configuration for the kubelets in the cluster[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[addons] Applied essential addon: CoreDNS[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[addons] Applied essential addon: kube-proxy[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.21.7&quot;. Enjoy!=== 提醒若没有升级 kubelets 则需要继续升级[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.</code></pre><p>根据提示信息&quot; Moved new manifest to “/etc/kubernetes/manifests/kube-apiserver.yaml” and backed up old manifest to “/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-12-02-16-21-32/kube-apiserver.yaml” 可以得知备份文件路径.</p><h2 id="第一个核心节点：升级网络插件">第一个核心节点：升级网络插件</h2><blockquote><p>在拥有管理员权限的客户端操作</p></blockquote><p>这里我用的是 flannel. 所以我只是简单的重新执行一边.因为 flannel 是 DaemonSet，所以无需在其它核心节点再次执行.</p><p><a href="https://github.com/flannel-io/flannel#deploying-flannel-manually">https://github.com/flannel-io/flannel#deploying-flannel-manually</a></p><pre><code class="language-bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre><blockquote><p>修改里面的<code>data.net-conf.json</code>下的 “Network”: “10.244.0.0/16”, 变更为你自己的 pod 网段，即 cm对象 kubeadm-config 里的 networking.podSubnet 或者 kubeadm 初始化安装参数 pod-network-cidr</p></blockquote><pre><code class="language-bash">kubectl describe cm kubeadm-config -n kube-system===ClusterConfiguration:----apiServer:  extraArgs:    authorization-mode: Node,RBAC  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: k8sapi:8443controllerManager: &#123;&#125;dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: v1.20.8networking:  dnsDomain: cluster.local  podSubnet: 10.97.0.0/16  serviceSubnet: 10.96.0.0/16scheduler: &#123;&#125;</code></pre><pre><code class="language-bash">kubectl apply -f  kube-flannel.yml</code></pre><p>等待pod部署完毕</p><pre><code class="language-bash">➜   kubectl get pod -n kube-system -o wide | grep kube-flannelkube-flannel-ds-52krd             1/1     Running   0          6m25s   10.200.16.102   k8s02   &lt;none&gt;           &lt;none&gt;kube-flannel-ds-9hnrs             1/1     Running   0          4m31s   10.200.16.103   k8s03   &lt;none&gt;           &lt;none&gt;kube-flannel-ds-b5b6m             1/1     Running   0          2m47s   10.200.16.101   k8s01   &lt;none&gt;           &lt;none&gt;</code></pre><h2 id="剩余核心节点：升级-kubeadm和主要组件">剩余核心节点：升级 kubeadm和主要组件</h2><blockquote><p>剩余的核心节点</p></blockquote><pre><code class="language-bash">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装 kubeadmyum install -y kubeadm-$&#123;full_version&#125; --disableexcludes=kubernetes# 升级节点kubeadm upgrade node</code></pre><h2 id="所有核心节点：依次升级-kubelet-和-kubectl">所有核心节点：依次升级 kubelet 和 kubectl</h2><p>依次升级每一个核心节点，不要同时升级.</p><h3 id="停止调度">停止调度</h3><blockquote><p>在拥有管理员权限的客户端操作</p></blockquote><p>停止调度之前，如果有重型分布式节点，则需确保节点状态均为<code>running</code>.例如EFK或者etcd等</p><pre><code class="language-bash">kubectl get pod -n kube-system</code></pre><pre><code class="language-bash:">nodeHostname=kubectl drain $&#123;nodeHostname&#125; --ignore-daemonsets</code></pre><p>–ignore-daemonsets 忽略 daemonsets，因为 daemonsets 会在驱逐节点上重建 pod 从而导致驱逐失败。</p><p>ℹ️有时候驱逐会报错，提示如下信息：</p><pre><code class="language-bash">error: cannot delete Pods with local storage (use --delete-emptydir-data to override): kube-system/metrics-server-6bd8d94d7f-c72hn</code></pre><p>根据提示信息，判断提到的容器的本地数据是否有用，无用则直接附加<code>--delete-emptydir-data</code>即可。</p><pre><code class="language-bash">kubectl get node===NAME    STATUS                     ROLES    AGE   VERSIONk8s01   Ready,SchedulingDisabled   master   40h   v1.18.6</code></pre><h3 id="升级-kubelet、kubectl">升级 kubelet、kubectl</h3><blockquote><p>待升级核心节点执行</p></blockquote><pre><code class="language-bash"># 在待升级的核心节点上操作yum install -y kubelet-$&#123;full_version&#125; kubectl-$&#123;full_version&#125; --disableexcludes=kubernetessystemctl daemon-reloadsystemctl restart kubelet</code></pre><h3 id="恢复调度">恢复调度</h3><blockquote><p>在拥有管理员权限的客户端操作</p></blockquote><pre><code class="language-bash">kubectl uncordon $&#123;nodeHostname&#125;kubectl get nodeNAME    STATUS   ROLES                  AGE    VERSIONk8s01   Ready    control-plane,master   408d   v1.20.8k8s02   Ready    control-plane,master   408d   v1.20.8k8s03   Ready    control-plane,master   282d   v1.20.8</code></pre><h2 id="所有核心节点：开启-kube-scheduler-和-kube-controller-manager-端口">所有核心节点：开启 kube-scheduler 和 kube-controller-manager 端口</h2><p>若发现<code>kubectl get cs</code> 两个服务无法连接，则可以看下配置端口是否为0，如果是则执行下列命令</p><blockquote><p>所有核心节点执行</p></blockquote><pre><code class="language-bash">sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml</code></pre><p>🌟不一定需要执行，具体以实际环境为准</p><h2 id="检查升级结果">检查升级结果</h2><pre><code class="language-bash">➜   kubectl get nodeNAME    STATUS   ROLES                  AGE    VERSIONk8s01   Ready    control-plane,master   408d   v1.21.7k8s02   Ready    control-plane,master   408d   v1.21.7k8s03   Ready    control-plane,master   282d   v1.21.7</code></pre><pre><code class="language-bash">➜   kubectl get csWarning: v1 ComponentStatus is deprecated in v1.19+NAME                 STATUS    MESSAGE             ERRORcontroller-manager   Healthy   okscheduler            Healthy   oketcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</code></pre><pre><code class="language-bash">➜   kubectl get pod -n kube-systemNAME                              READY   STATUS    RESTARTS   AGEcoredns-85d9df8444-kd2w6          1/1     Running   0          28mcoredns-85d9df8444-m6r2f          1/1     Running   0          24metcd-k8s01                        1/1     Running   0          2m13setcd-k8s02                        1/1     Running   1          2m14setcd-k8s03                        1/1     Running   0          2m12skube-apiserver-k8s01              1/1     Running   0          2m13skube-apiserver-k8s02              1/1     Running   1          2m13skube-apiserver-k8s03              1/1     Running   0          2m15skube-controller-manager-k8s01     1/1     Running   0          105skube-controller-manager-k8s02     1/1     Running   0          106skube-controller-manager-k8s03     1/1     Running   0          105skube-flannel-ds-52krd             1/1     Running   0          44mkube-flannel-ds-9hnrs             1/1     Running   0          42mkube-flannel-ds-b5b6m             1/1     Running   0          40mkube-proxy-lmkw9                  1/1     Running   0          53mkube-proxy-nh98w                  1/1     Running   0          54mkube-proxy-wdvfd                  1/1     Running   0          53mkube-scheduler-k8s01              1/1     Running   0          107skube-scheduler-k8s02              1/1     Running   0          106skube-scheduler-k8s03              1/1     Running   0          101smetrics-server-6bd8d94d7f-wp59m   1/1     Running   0          28m</code></pre><h1>升级工作节点 node</h1><p>ℹ️应该一个一个的升级，而不是批量，避免集群压力过大.</p><p>我这里测试环境，核心节点和工作节点是重叠的，所以无需再升级工作节点。</p><pre><code class="language-bash">full_version=1.21.7nodeName=# 升级kubeadmyum install -y kubeadm-$&#123;appVersion&#125; --disableexcludes=kubernetes# 升级本地 kubelet 配置kubeadm upgrade node# 锁定nodekubectl drain $&#123;nodeName&#125; --ignore-daemonsets# 升级 kubelet 和 kubectlyum install -y kubelet-$&#123;appVersion&#125; kubectl-$&#123;appVersion&#125; --disableexcludes=kubernetes# 重启systemctl daemon-reloadsystemctl restart kubelet# 解锁nodekubectl uncordon $&#123;nodeName&#125;</code></pre><h1>升级失败</h1><h3 id="升级失败，没有回滚">升级失败，没有回滚</h3><p>可能是执行期间被意外关闭，则可以再次运行<code>kubeadm upgrade </code></p><h3 id="升级失败，自动回滚也失败">升级失败，自动回滚也失败</h3><pre><code class="language-bash">full_version=kubeadm upgrade apply --force $&#123;appVersion&#125;</code></pre><h2 id="无论如何都失败，手动恢复">无论如何都失败，手动恢复</h2><p>升级前，kubernetes 会备份 etcd 和 静态pod的内容</p><p>对应关系如下：</p><p><code>/etc/kubernetes/tmp/kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;/etcd</code>= <code>/var/lib/etcd/</code></p><p><code>/etc/kubernetes/tmp/kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code>= <code> /etc/kubernetes/manifests/</code></p><p>将等号左边的路径内容覆盖到等号右边，以便于静态pod重建</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> kubeadm </tag>
            
            <tag> 升级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞15证书管理</title>
      <link href="posts/55da993d/"/>
      <url>posts/55da993d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>k8s默认的CA证书有效期是10年。默认签发的服务证书有效期是1年。</p><p>k8s支持自动轮换证书。</p><p>k8s支持轮换时候的证书有效期。不过最长是10年。（我这边设置为100年，没效果）</p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</a></p><h2 id="检查当前控制面节点里证书">检查当前控制面节点里证书</h2><pre><code class="language-bash">kubeadm alpha certs check-expiration</code></pre><p>你可以通过此命令查看各类证书的有效期</p><h2 id="触发自动轮换证书的行为">触发自动轮换证书的行为</h2><p>当你通过kubeadm upgrade node命令进行升级或者通过control plane进行升级的时候，k8s会自动轮换证书。</p><h2 id="通过命令手动触发证书轮换">通过命令手动触发证书轮换</h2><p><code>kubeadm alpha certs renew all</code>你可以通过此命令进行所有证书轮换</p><p>⭐️请注意，如果你运行了高可用集群，则需要在所有的control-plane节点上运行此命令，也就是说主节点上运行。</p><h2 id="为kubelet配置证书自动轮转">为kubelet配置证书自动轮转</h2><p><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/">https://kubernetes.io/docs/tasks/tls/certificate-rotation/</a></p><p>⭐️1.18和1.19有差异。1.18是已经默认开启的，不过默认的轮转的证书有效期是1年</p><p>使用自动轮转，需要开启两个服务的配置：</p><ul><li><p>开启kubelet启动标记：<code>--rotate-certificates=true</code></p><p>⭐️开启确认命令：<code>kubectl describe cm/kubelet-config-1.18 -n kube-system | grep 'rotateCertificates'</code></p><p>⭐️动态的修改kubelet <a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#reconfiguring-the-kubelet-on-a-running-node-in-your-cluster">https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#reconfiguring-the-kubelet-on-a-running-node-in-your-cluster</a></p><pre><code class="language-bash"># 请先安装 jq 命令. 他是 epel 里的# 开启代理监听kubectl proxy --port=8001 # 导出kubelet配置NODE_NAME=&quot;节点名&quot;; curl -sSL &quot;http://localhost:8001/api/v1/nodes/$&#123;NODE_NAME&#125;/proxy/configz&quot; | jq '.kubeletconfig|.kind=&quot;KubeletConfiguration&quot;|.apiVersion=&quot;kubelet.config.k8s.io/v1beta1&quot;' &gt; kubelet_configz_$&#123;NODE_NAME&#125;# 编辑完配置，重新导入，并确认新配置对象名kubectl -n kube-system create configmap node-config-new --from-file=kubelet=kubelet_configz_$&#123;NODE_NAME&#125; --append-hash -o yaml# 调整每一个node对象，添加新的配置对象kubectl edit node $&#123;NODE_NAME&#125;===configSource:    configMap:        name: CONFIG_MAP_NAME # 将这个名字替换成新的配置对象名        namespace: kube-system        kubeletConfigKey: kubelet# 检查配置生效kubectl get node $&#123;NODE_NAME&#125; -o json | jq '.status.config'</code></pre></li><li><p>调整默认证书有效期</p><p>开启<code>kube-controller-manager</code>启动标记：<code>--cluster-signing-duration</code>，这个标记默认是1年，你可以改成10年 <code>87600h0m0s</code></p><p>修改位置：<code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code></p><p>修改生效：kube-controller-manager pod 将自动重建</p><p>⚠️如果是1.18，则启动标记是<code>--experimental-cluster-signing-duration</code></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> 证书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞14认证授权</title>
      <link href="posts/a5ed67a6/"/>
      <url>posts/a5ed67a6/</url>
      
        <content type="html"><![CDATA[<h2 id="基础">基础</h2><h3 id="流程介绍">流程介绍</h3><p>这里是一个认证的一个基本流程：</p><p><img src="/posts/a5ed67a6/image-20201016114248549.png" alt="image-20201016114248549"></p><p>即： kubectl =&gt; 用户认证 =&gt; 用户授权 =&gt; 入口控制 -&gt; 资源对象</p><p>用户认证：检查递交信息包含的证书，用户名。确定是否是正常用户。</p><p>用户授权：给用户授权权限策略，授权方式有多种：ABAC mode， RBAC Mode， and Webhook mode。确定是否有权限。</p><p>入口控制器：具有特殊功能的过滤器，他们会把请求拦截下来，如果请求违反了过滤器的配置，则请求会被拒绝。确定是否有更精细的粒度控制。</p><blockquote><p>以上三阶段都是通过插件实现的。当某个阶段的某个插件授权通过后，就不会再需要此阶段的其它插件进行校验。</p></blockquote><h2 id="用户认证">用户认证</h2><h3 id="令牌认证">令牌认证</h3><p>kubectl 通过提交令牌给用户认证</p><h3 id="TSL认证">TSL认证</h3><p>kubectl 和 Api server 双向认证</p><h3 id="username-password认证">username/password认证</h3><p>一般不用这种方式</p><h2 id="RBAC">RBAC</h2><p><a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/">https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/</a></p><p>RBAC授权是目前比较常见的方式。通过角色（规则体），用户（实体），用户绑定角色的方式，赋予用户权限。</p><p>通过kubeadm安装的集群，默认就是RBAC授权机制。</p><p>涉及到的基本对象：</p><ul><li>角色：包含权限的对象，分为role和clusterrole。<ul><li>role只能用于某个命名空间。clusterrole可以用于整个集群。</li></ul></li><li>主题：被授权的对象，可以是 user，group，serviceaccount</li><li>绑定：用于将角色和主题绑定在一起的对象，分为rolebinding和clusterrolebinding</li></ul><h3 id="角色">角色</h3><p>角色分两种，一种是命名空间级别的 Role，一种是集群级别的 ClusterRole。</p><p>用于授权对xxx资源有xxx操作权限。</p><pre><code class="language-yaml"># role## https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/role-v1/## 拥有 dev 命名空间里 &quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot; 资源的增删改查权限。apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  name: developer  namespace: devrules:- apiGroups: [&quot;&quot;, &quot;apps&quot;]  resources: [&quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;]  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] # 也可以使用['*']</code></pre><pre><code class="language-yaml"># clusterrole## https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/cluster-role-v1/apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: filebeat  labels:    app: filebeatrules:- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group  resources:  - namespaces  - pods  - nodes  verbs:  - get  - watch  - list</code></pre><h3 id="用户">用户</h3><p>从使用者角度来看的时候，k8s把用户分为<code>普通用户</code>和<code>服务用户</code>。普通用户对外（也就是上图里的Human），服务用户对内（也就是上图里的Pod）。</p><p>💥请记住，k8s并没有普通用户的<strong>实体对象</strong>，也就是说通过<code>kubectl get user</code>是没有这个对象的。因此，只要你递交的用户拥有k8s集群内部CA所签发的证书，那么这个用户就会被rbac子系统认为是有效的。</p><h3 id="普通用户">普通用户</h3><h4 id="创建普通用户所需的证书">创建普通用户所需的证书</h4><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user">https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user</a></p><ol><li><p>创建私钥key，通过私钥发起证书请求，生成请求文件csr</p><p>创建私钥会让你填写一些属性，这里有两个属性特别重要，分别是属性CN（填写用户名）和属性O（填写用户组）</p><p>例如创建用户<code>zyh</code>，群组<code>it</code></p><pre><code>openssl genrsa -out zyh.key 2048openssl req -new -key zyh.key -out zyh.csr -subj &quot;/CN=zyh/O=it&quot;</code></pre></li><li><p>构建k8s的 CertificateSigningRequest 证书请求对象</p><ul><li>先将 csr 文件进行base64编码</li></ul><pre><code class="language-bash">RequestStr=`cat zyh.csr | base64 | tr -d &quot;\n&quot;`</code></pre><ul><li>将编码后的内容写入 CertificateSigningRequest 对象的 request 字段</li></ul><pre><code class="language-yaml">cat &lt;&lt;EOF | kubectl apply -f -apiVersion: certificates.k8s.io/v1kind: CertificateSigningRequestmetadata:  name: zyhspec:  groups:  - system:authenticated  request: $&#123;RequestStr&#125;  signerName: kubernetes.io/kube-apiserver-client  usages:  - client authEOF</code></pre><p>⭐️usages字段必须是<code>client auth</code></p><p>⭐️需要注意的是，证书签发默认只有1年有效期</p><p>🌟确保当前 csr 没有重名申请</p></li><li><p>批准证书请求，并获取证书</p><p>批准CertificateSigningRequest对象的证书请求：</p><pre><code class="language-bash">kubectl get csrkubectl certificate approve zyh</code></pre><p>获取批准后的颁发的证书：</p><pre><code class="language-bash">kubectl get csr zyh -o jsonpath='&#123;.status.certificate&#125;' | base64 --decode &gt; zyh.crt</code></pre><p>⭐️检查证书有效期：​<code>openssl x509 -in zyh.crt -noout -dates</code></p><p>至此，一个普通用户就创建完毕了。但是它没有任何权限，需要随后声明绑定对象，将角色和用户关联。</p></li></ol><h3 id="服务用户">服务用户</h3><h4 id="流程">流程</h4><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#service-account-automation">https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#service-account-automation</a></p><p>服务用户对象，即ServiceAccount。</p><ul><li><p>每一个namespace的时候都会自动创建一个默认ServiceAccount和默认secret，并且默认ServiceAccount关联默认secret。</p></li><li><p>secret中含有sa认证所需的证书和Token。</p></li><li><p>当Pod创建好之后，secret会将证书和Token挂载到<code>/var/run/secrets/kubernetes.io/serviceaccount/</code>路径下。Pod通过挂载的认证文件合法化。</p></li><li><p>通过RABC机制对sa进行授权，从而使Pod拥有了获取诸多资源的权限。</p></li><li><p>在没有显式指定sa的时候，Pod会调用默认sa。</p></li></ul><p>例如，下面的SA对象通过<code>imagePullSecrets</code>附加了额外的secret授权信息<code>myregistrykey</code>，使得Pod可以拉取镜像。</p><pre><code class="language-yaml">apiVersion: v1kind: ServiceAccountmetadata:  name: default  namespace: defaultsecrets:- name: default-token-uudge # 这是默认服务账户的TokenimagePullSecrets:- name: myregistrykey  # 这是Secret对象的key，存储着拉取镜像的用户密码</code></pre><h4 id="创建服务用户">创建服务用户</h4><pre><code class="language-yaml">apiVersion: v1kind: ServiceAccountmetadata:  name: admin  namespace: kube-system  labels:    app: filebeat</code></pre><blockquote><p>创建完毕后，对应的 secret.namespace 会生成一个新的对象：&lt;sa_name&gt;-token-$RANDOM，用来进行资源对象的认证，请注意这里是认证，不是授权。也就是说通过了认证但没权限访问资源。</p></blockquote><h3 id="用户绑定角色">用户绑定角色</h3><h4 id="普通用户绑定">普通用户绑定</h4><p>绑定创建。位于 dev 命名空间的 rolebinding 对象</p><pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: developer-rolebinding  namespace: devsubjects:- kind: User  name: zyh  apiGroup: &quot;&quot;roleRef:  kind: Role  name: developer  apiGroup: rbac.authorization.k8s.io  # 留空字符串也可以，则使用当前的apiGroup</code></pre><p>通过上述创建过程，你可以发现用户创建和用户授权两个阶段是分离的。也就是说，你可以创建多个用户，然后用同一个角色绑定多个用户。</p><h4 id="服务用户绑定">服务用户绑定</h4><pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: filebeatsubjects:- kind: ServiceAccount  name: filebeat  namespace: kube-systemroleRef:  kind: ClusterRole  name: filebeat  apiGroup: rbac.authorization.k8s.io</code></pre><h2 id="用户调用">用户调用</h2><h3 id="普通用户-2">普通用户</h3><ol><li><p>添加用户证书到kubectl配置</p><pre><code class="language-bash">kubectl config set-credentials zyh --client-key=zyh.key --client-certificate=zyh.crt --embed-certs=true</code></pre></li><li><p>设置用户上下文，方便进行用户切换</p><pre><code class="language-bash">kubectl config set-context zyh --cluster=kubernetes --user=zyh</code></pre><p>–cluster 指定要访问集群的名称</p></li><li><p>通过用户上下文进行用户切换</p><pre><code class="language-bash">kubectl config use-context zyh</code></pre></li></ol><p>用户的有效期，取决于你证书的有效期。</p><ol start="4"><li><p>测试权限</p><pre><code class="language-bash">kubectl get pod -n dev===No resources found in developer namespace.kubectl get svc -n dev===Error from server (Forbidden): services is forbidden: User &quot;zyh&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; in the namespace &quot;developer&quot;</code></pre></li></ol><h3 id="服务用户-2">服务用户</h3><p>Pod通过spec.serviceAccountName显式指定一个SA</p><pre><code class="language-yaml">apiVersion: apps/v1kind: DaemonSetmetadata:  name: k8s-pod-logs  namespace: kube-systemspec:  selector:    matchLabels:      project: k8s-pod-logs      app: filebeat  template:    metadata:      labels:        project: k8s-pod-logs        app: filebeat    spec:      serviceAccountName: filebeat</code></pre><h2 id="对象总结">对象总结</h2><p>role 和 rolebinding 是 namespace 级别对象，clusterrole 和 clusterrolebinding 是集群级别对象。</p><ol><li>授权某个用户单命名空间或者集群权限。</li></ol><ul><li><p>通过 rolebinding 将 user 和 role 关联起来，实现对一个 user 添加 namespace 级别的 role 权限。</p></li><li><p>通过 clusterrolebinding 将 user 和 clusterrole 关联起来，实现对一个 user 添加是集群级别的 clusterrole 权限。</p></li></ul><ol start="2"><li>授权某个用户多个命名空间的权限。</li></ol><ul><li><p>通过 rolebinding 将 user 和 clusterrole 关联起来，实现对一个 user 添加 namespace 级别的 clusterrole 权限。</p><p>因为 clusterrole 是集群级别，而 rolebinding 是命名空间级别，通过不同namespace下的 rolebinding 将 user 关联到一个 clusterrole 下，可以将 clusterrole 的权限约束到 rolebinding 所在的命名空间。</p></li></ul><blockquote><p>当你把 user 替换为 group 的时候，将会授权某个组</p><p>当你把 user 替换为 serviceaccount 的时候，将会授权调用 spec.serviceAccountName 的 pod</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> authorization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞010特殊控制流(defer panic recover)</title>
      <link href="posts/2dab3d00/"/>
      <url>posts/2dab3d00/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>三个奇特的关键字<code>defer</code>/<code>recover</code>/<code>Panic</code></p><h1>defer</h1><p><code>defer</code>用于当前函数作用域结束后，执行 defer 后定义的代码。也就是说 defer 可以推迟语句的执行。</p><h2 id="规则">规则</h2><ol><li>一个函数体内可以包含多个 defer，多个 defer 会暂存在队列中，并且多个 defer 的执行顺序是逆序的，即先进后出。</li><li>defer 进暂存队列之前，其推迟的代码中与主函数同作用域下的变量若已赋值，则变量需带值进队列；</li></ol><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;    for i := 1; i &lt;= 4; i++ &#123;        defer fmt.Println(&quot;deferred&quot;, -i)        fmt.Println(&quot;regular&quot;, i)    &#125;&#125;</code></pre><pre><code class="language-bash">#输出regular 1regular 2regular 3regular 4deferred -4deferred -3deferred -2deferred -1</code></pre><h2 id="推迟函数">推迟函数</h2><pre><code class="language-go">defer funcA() // 推迟一个已有的函数funcAdefer func(&lt;args&gt;)&#123;&#125;(&lt;vars&gt;) // 推迟一个全新的函数，vars是传入变量，args是形参。</code></pre><p>args和vars是可选的。但 <code>defer func()&#123; xxx &#125;()</code> 格式不可变。</p><h2 id="特殊例子">特殊例子</h2><ul><li>情况1：<code>defer</code>中的变量与函数返回值变量一致，导致返回结果超出预期</li></ul><pre><code class="language-go">func test01()(p int)&#123;    defer func()&#123;        p++    &#125;()    return 0&#125;func main()&#123;    f := test01()    fmt.Printf(&quot;f是：%d\n&quot;,f)&#125;</code></pre><pre><code class="language-bash">#输出f是：1</code></pre><p>上述例子中，</p><ol><li>defer 推迟 p++</li><li>函数 test01 执行完毕后，return 0，传递给返回变量p;</li><li>执行 p++</li><li>p = 1</li></ol><ul><li>情况2:<code>defer</code>与当前函数构成闭包。</li></ul><pre><code class="language-go">package mainimport &quot;fmt&quot;func test01(x int)(p int)&#123;    defer func(y int)&#123;        y++        p=x+y    &#125;(x)    x++    return 0&#125;func main()&#123;    f := test01(2)    fmt.Println(f)&#125;</code></pre><p>上述例子中，:</p><ol><li>执行test01(2);</li><li>推迟 func(y int) { y++; p=x+y} (2)</li><li>test01(2) 执行完，return 0，传递给返回变量 p。此时 p = 0，x = 3;</li><li>因 defer 调用了 x，所以 test01 虽然执行完，但 x 无法释放。</li><li>执行 func(y int) { y++; p=3+y} (2) 得 y=3;p=3+y</li><li>p = 6</li></ol><h2 id="常用方式">常用方式</h2><p><code>defer</code> 函数的一个典型用例是在使用完文件后将其关闭.</p><pre><code class="language-go">package mainimport (    &quot;io&quot;    &quot;os&quot;    &quot;fmt&quot;)func main() &#123;    newfile, error := os.Create(&quot;learnGo.txt&quot;)    if error != nil &#123;        fmt.Println(&quot;Error: Could not create file.&quot;)        return    &#125;    defer newfile.Close()    if _, error = io.WriteString(newfile, &quot;Learning Go!&quot;); error != nil &#123;        fmt.Println(&quot;Error: Could not write to file.&quot;)        return    &#125;    newfile.Sync()&#125;</code></pre><h1>panic</h1><p>调用内置 <code>panic()</code> 函数可以停止 Go 程序中的正常控制流，但不会影响到<code>panic()</code>调用前由<code>defer</code>扔到堆栈的延迟函数代码。</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞011错误处理</title>
      <link href="posts/336ec0ac/"/>
      <url>posts/336ec0ac/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Go语言检测和报告错误情况的常用方法：</p><p>函数返回的多个值，并按照惯例将最后一个值定位错误。if条件判断这个错误值，如果为nil，则没有错误。</p><h2 id="例子">例子</h2><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;time&quot;    &quot;errors&quot;//    &quot;os&quot;)type Employee struct &#123;    ID        int    FirstName string    LastName  string    Address   string&#125;var ErrNotFound = errors.New(&quot;Employee not found&quot;)func main() &#123;    employee, err := getInformation(1001)    if err != nil &#123;        fmt.Printf(&quot;%v&quot;, err)        // Something is wrong. Do something.    &#125; else &#123;        fmt.Print(employee)    &#125;&#125;func getInformation(id int) (*Employee, error) &#123;    for tries := 0; tries &lt; 3; tries++ &#123;        employee, err := apiCallEmployee(1000)        if err == nil &#123;            return employee, nil        &#125;        fmt.Printf(&quot;Server is not responding, retrying...\n%v\n&quot;, err)        time.Sleep(time.Second * 2)    &#125;    return nil, fmt.Errorf(&quot;server has failed to respond to get the employee infotmation&quot;)&#125;func apiCallEmployee(id int) (*Employee, error) &#123;    if id != 1001 &#123;        return nil, ErrNotFound    &#125;    employee := Employee&#123;LastName: &quot;Doe&quot;, FirstName: &quot;John&quot;&#125;    return &amp;employee, nil&#125;</code></pre><h2 id="常见的一些写法">常见的一些写法</h2><pre><code class="language-go">fmt.Errorf(&quot;Got an error when getting the employee information: %v&quot;, err)errors.New('string')</code></pre><p>这两种写法的区别，就是fmt.Errorf()支持格式化，errors.New()可以用来创建错误变量从而重复利用。</p><pre><code class="language-go">var ErrNotFound = errors.New(&quot;Employee not found!&quot;)</code></pre><p>ErrNotFound变量中Err前缀是错误变量常用的惯例写法。</p><pre><code class="language-go">if errors.Is(err, ErrNotFound) &#123;    fmt.Printf(&quot;Not Fount: %v\n&quot;, err)&#125; else &#123;    fmt.Print(employee)&#125;</code></pre><p>返回 true/false 用来判断比较错误变量。其中 ErrNotFound 是提前定义好的错误，err是需要比对的错误</p><h2 id="推荐的错误用法">推荐的错误用法</h2><ul><li>始终检查是否存在错误，即使预期不存在。 然后正确处理它们，以免向最终用户公开不必要的信息。</li><li>在错误消息中包含一个前缀，以便了解错误的来源。 例如，可以包含包和函数的名称。</li><li>创建尽可能多的可重用错误变量。</li><li>了解使用返回错误和 panic 之间的差异。 不能执行其他操作时再使用 panic。 例如，如果某个依赖项未准备就绪，则程序运行无意义（除非你想要运行默认行为）。</li><li>在记录错误时记录尽可能多的详细信息（我们将在下一部分介绍记录方法），并打印出最终用户能够理解的错误。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞009接口</title>
      <link href="posts/77a46e91/"/>
      <url>posts/77a46e91/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>接口和结构体一样，也是一种自定义数据类型，它是抽象类型。</p><p>接口内只是定义了实现这个接口的具体类型所需要的方法名，并不处理方法的逻辑。</p><p>✨这就如同你制作一艘飞船（具体类型），需要一个蓝图（接口）来告诉你飞船需要实现哪些功能（方法）。</p><p>✨只不过飞船（具体类型）的这些功能（方法）如何制作，蓝图（接口）不管，由你自己去实现。</p><p>所以，当一个数据类型实现了接口里的所有方法定义，就意味着这个数据类型实现了接口。</p><p>总结：若多个数据类型都拥有同样的功能，且功能逻辑一致。为了方便的调用这个功能，我们就可以将功能里的方法定义为一个接口，并将<code>功能(变量 数据类型)</code>替换为<code>功能(变量 接口类型)</code>。使用时，<code>功能(变量 接口类型)</code>可以直接传入<code>数据类型</code>变量。</p><p>✨变量 接口类型 == main.变量 数据类型</p><p>接口类型是引用类型，本质是一个指针。</p><h2 id="什么时候用">什么时候用</h2><p>你写了一个程序（功能），它通过 type-c 接口接收设备（例如结构）数据（结构属性）并判断这些数据是否异常。</p><p>但是拥有 type-c 接口的设备种类很多，每一个设备都是一个结构对象，所以你不可能自己声明一堆设备结构类型并为每一个设备写一个判断程序，因此你的程序只接收一个 type-c 接口类型作为形参。即：只要你的设备有type-c接口（结构已实现 type-c 接口的方法），我就允许你传入。</p><h2 id="用法">用法</h2><h3 id="如何定义接口？">如何定义接口？</h3><p>定义接口，接口包含方法名。</p><blockquote><p>方法名必须唯一，且不能为空。</p></blockquote><pre><code class="language-go">type &lt;inter_name&gt; interface &#123;    &lt;method_name1&gt; [return_type]    &lt;method_name2&gt; [return_type]    ...&#125;</code></pre><h3 id="如何满足接口？">如何满足接口？</h3><p>定义结构</p><pre><code class="language-go">type &lt;struct_name&gt; struct &#123;    var var_name1 var_type&#125;</code></pre><p>实现接口方法</p><pre><code class="language-go">// 实现方法1，例如充电func (self &lt;struct_name&gt;) &lt;method_name1&gt;() [return_type]&#123;    ...&#125;// 实现方法2，例如数据传输func (self &lt;struct_name&gt;) &lt;method_name2&gt;() [return_type]&#123;    ...&#125;</code></pre><blockquote><p>接口可以嵌套</p></blockquote><h3 id="例子1">例子1</h3><pre><code class="language-go">package mainimport &quot;fmt&quot;// typec 接口type typeC interface &#123;    chongdian()    hdmi()&#125;// 结构type diannao struct &#123;    PinPai string    TypeC int&#125;func (self diannao) chongdian() &#123;    if self.TypeC == 1 &#123;        fmt.Printf(&quot;%s电脑可以充电\n&quot;, self.PinPai)    &#125;&#125;func (self diannao) hdmi() &#123;    if self.TypeC == 2&#123;        fmt.Printf(&quot;%s电脑既可以充电，也可以HDMI\n&quot;, self.PinPai)    &#125;&#125;type shouji struct &#123;    PinPai string    TypeC int&#125;func (self shouji) chongdian() &#123;    if self.TypeC == 1 &#123;        fmt.Printf(&quot;%s手机可以充电\n&quot;, self.PinPai)    &#125;&#125;func (self shouji) hdmi() &#123;    if self.TypeC == 2&#123;        fmt.Printf(&quot;%s手机既可以充电，也可以HDMI\n&quot;, self.PinPai)    &#125;&#125;func TypeCInfo(j typeC)&#123;    j.chongdian()    j.hdmi()&#125;func main()&#123;    d1 := diannao&#123;&quot;sanxing&quot;, 2&#125;    s1 := shouji&#123;&quot;huawei&quot;, 1&#125;    TypeCInfo(d1)    TypeCInfo(s1)&#125;</code></pre><p>输出：</p><pre><code>sanxing电脑既可以充电，也可以HDMIhuawei手机可以充电</code></pre><h3 id="例子2">例子2</h3><p>自定义输出</p><pre><code class="language-go">package mainimport &quot;fmt&quot;type Person struct &#123;    Name, Country string&#125;func (p Person) String() string &#123;    return fmt.Sprintf(&quot;%v is from %v&quot;, p.Name, p.Country)&#125;func main() &#123;    rs := Person&#123;&quot;John Doe&quot;, &quot;USA&quot;&#125;    ab := Person&#123;&quot;Mark Collins&quot;, &quot;United Kingdom&quot;&#125;    fmt.Printf(&quot;%s\n%s\n&quot;, rs, ab)&#125;</code></pre><pre><code class="language-bash">John Doe is from USAMark Collins is from United Kingdom</code></pre><p>从 fmt.Printf(“%s\n%s\n”, rs, ab) 以及输出结果可以得知，fmt.Printf()（通用功能）接收一个未知接口，这个未知接口定义了 String() 方法。</p><p>Person 结构实现了 String() 方法，因此可以直接传入 fmt.Printf()</p><p>所以，我们可以通过自定义 String() 方法，来输出自定义字符串</p><p>经查证，这个未知接口的代码是：</p><pre><code class="language-go">type Stringer interface &#123;    String() string&#125;</code></pre><h3 id="列子3">列子3</h3><p>通过重写io.Copy()，只输出 respone 数据中想要的部分.</p><pre><code class="language-go">package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;    &quot;io&quot;    &quot;net/http&quot;    &quot;os&quot;)type GitHubResponse []struct &#123; // 通过解析 respone json 数据，定义新的 respone 结构    FullName string `json:&quot;full_name&quot;`&#125;type customWriter struct&#123;&#125;func (w customWriter) Write(p []byte) (n int, err error) &#123; // 重写 io.Copy第一个接口参数的 write 方法    var resp GitHubResponse    json.Unmarshal(p, &amp;resp)    for _, r := range resp &#123;        fmt.Println(r.FullName)    &#125;    return len(p), nil&#125;func main() &#123;    resp, err := http.Get(&quot;https://api.github.com/users/microsoft/repos?page=15&amp;per_page=5&quot;)    if err != nil &#123;        fmt.Println(&quot;Error:&quot;, err)        os.Exit(1)    &#125;    writer := customWriter&#123;&#125;    io.Copy(writer, resp.Body) // 传递实现了接口的 customWriter 数据类型变量&#125;</code></pre><h3 id="例子4">例子4</h3><p>通过浏览器访问，输出数据</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;log&quot;    &quot;net/http&quot;)type dollars float32func (d dollars) String() string &#123;    return fmt.Sprintf(&quot;$%.2f&quot;, d)&#125;type database map[string]dollarsfunc (db database) ServeHTTP(w http.ResponseWriter, req *http.Request) &#123;    for item, price := range db &#123;        fmt.Fprintf(w, &quot;%s: %s\n&quot;, item, price)    &#125;&#125;func main() &#123;    db := database&#123;&quot;Go T-Shirt&quot;: 25, &quot;Go Jacket&quot;: 55&#125;    log.Fatal(http.ListenAndServe(&quot;localhost:8000&quot;, db))&#125;</code></pre><h2 id="思考？">思考？</h2><p>当某个功能函数A的形参argsA是一个接口类型的时候，</p><p>情况一：如果满足接口的数据类型B，在实现方法的时候，方法接收者（数据类型本身）是<code>值</code>，则可以直接将数据类型B的变量传递给功能函数A，也可以将<code>&amp;B</code>传递给A，也可以将接口变量（已指向&amp;B）传递给A；又因为是<code>值传递</code>，所以方法内针对接收者的修改，并不影响方法外的接收者本身。</p><p>情况二：如果满足接口的数据类型B，在实现方法的时候，方法接收者（数据类型本身）是<code>指针</code>，则只能将<code>&amp;B</code>传递给A，或者将接口变量（已指向&amp;B）传递给A；又因为是<code>指针传递</code>，所以方法内针对接收者的修改，会影响到方法外的接收者本身。</p><p>最后，关于情况一，其实go是强类型，只不过指针可以传入的原因是go解析器底层帮你修正了代码。</p><p>例如，sort.Sort(data Interface) ，它接收<code>sort.Interface</code>类型，因此只要确保排序的数据对象实现了<code>sort.Interface</code>定义的<code>Len(),Less(),Swap()</code>方法，那么直接传入你需要排序的数据对象即可。</p><p>那么，为什么TypeCInfo(j typeC) 明明接收一个interface，却可以接收一个满足interface的数据类型。</p><p>因为golang在底层帮你改了，实际的代码是：</p><pre><code class="language-golang">    var myd1 typeC    d1 := diannao&#123;&quot;sanxing&quot;, 2&#125;    s1 := shouji&#123;&quot;huawei&quot;, 1&#125;    myd1 = &amp;d1    TypeCInfo(myd1)    myd1 = &amp;s1    TypeCInfo(myd1)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞007结构</title>
      <link href="posts/a705a91a/"/>
      <url>posts/a705a91a/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>结构是多种类型的集合，它本身就是类型的一种，因此需要用<code>type</code>关键字来声明</p><p>鉴于上述的性质，常用来描述一个具有多种属性的类。</p><p>例如，人类，书本等。</p><h2 id="写法和使用">写法和使用</h2><p>当定义好一个struct后，就可以通过var声明struct类型了</p><pre><code class="language-go">package mainimport &quot;fmt&quot;type Person struct &#123;    name string    age int&#125;type Book struct &#123;    title string    author string&#125;func readBook(b *Book) &#123;    fmt.Printf(&quot;他们读取的书籍：%s\n&quot;, (*b).title)&#125;func main()&#123;    p1 := Person&#123;&quot;zhangsan&quot;, 20&#125;    p2 := Person&#123;name:&quot;lisi&quot;, age:10&#125;    b := Book&#123;title:&quot;魔兽世界&quot;, author:&quot;wangwu&quot;&#125;    fmt.Println(p1.name, p2.name)    readBook(&amp;b)&#125;</code></pre><pre><code class="language-bash">zhangsan lisi他们读取的书籍：魔兽世界</code></pre><h2 id="结构和数据格式之间的编解码">结构和数据格式之间的编解码</h2><p>定义结构的时候，添加数据格式映射关系</p><pre><code class="language-go">package mainimport (    &quot;encoding/json&quot;    &quot;fmt&quot;)type Person struct &#123;    ID        int    FirstName string `json:&quot;name&quot;`    LastName  string    Address   string `json:&quot;address,omitempty&quot;`&#125;type Employee struct &#123;    Person    ManagerID int&#125;type Contractor struct &#123;    Person    CompanyID int&#125;func main() &#123;    employees := []Employee&#123;        Employee&#123;            Person: Person&#123;                LastName: &quot;Doe&quot;, FirstName: &quot;John&quot;,            &#125;,        &#125;,        Employee&#123;            Person: Person&#123;                LastName: &quot;Campbell&quot;, FirstName: &quot;David&quot;,            &#125;,        &#125;,    &#125;    data, _ := json.Marshal(employees)    fmt.Printf(&quot;%s\n&quot;, data)    var decoded []Employee    json.Unmarshal(data, &amp;decoded)    fmt.Printf(&quot;%v&quot;, decoded)&#125;</code></pre><pre><code class="language-bash">[&#123;&quot;ID&quot;:0,&quot;name&quot;:&quot;John&quot;,&quot;LastName&quot;:&quot;Doe&quot;,&quot;ManagerID&quot;:0&#125;,&#123;&quot;ID&quot;:0,&quot;name&quot;:&quot;David&quot;,&quot;LastName&quot;:&quot;Campbell&quot;,&quot;ManagerID&quot;:0&#125;][&#123;&#123;0 John Doe &#125; 0&#125; &#123;&#123;0 David Campbell &#125; 0&#125;]</code></pre><p>在上面的代码中，有一段若改为下面代码，则报错</p><pre><code class="language-go">        Employee&#123;            Person: Person&#123;                LastName: &quot;Doe&quot;, FirstName: &quot;John&quot;, &quot;Beijing&quot;            &#125;,        &#125;,</code></pre><p>因为不可将初始化值方式和直接赋值方式混用。正确的写法如下：</p><pre><code class="language-go">        Employee&#123;            Person: Person&#123;                LastName: &quot;Doe&quot;, FirstName: &quot;John&quot;, Address: &quot;Beijing&quot;            &#125;,        &#125;,</code></pre><p>✨结构对象中没有初始化的属性，则会自动附加默认值。</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞008方法</title>
      <link href="posts/f8eb4b1b/"/>
      <url>posts/f8eb4b1b/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>一个函数是没有<code>接收方</code>的。但当一个函数有了<code>接收方</code>后，则称之为<code>方法</code>。</p><p>在python中，一个类（接收方）定义方法，直接在类里写方法即可，只不过有一个<code>self</code>指向了它本身。这样，一个类对象就通过方法实现了行为。</p><p>在go中，没有类概念，但当自定义的<code>结构</code>作为<code>方法</code>的<code>接收方</code>后，<code>结构</code>就既有了属性，又有了行为，从而实现了面向对象编程中的部分功能。</p><p>✨并非只有自定义结构可以声明方法，其它类型也可以。</p><h2 id="定义">定义</h2><pre><code class="language-go">func (&lt;变量&gt; &lt;接收方类型&gt;) 方法名(形参) 方法返回类型&#123;    ...&#125;</code></pre><p>上面可以看出，方法和函数定义的区别，就是<code>函数名</code>前面添加了一个<code>(变量 接收方类型)</code>变量。</p><p>这里的<code>变量</code>仅用于方法内<code>接收方</code>调用自身属性所用。</p><p>💥接收方类型必须先创建，即结构必须先创建。</p><p>💥方法的定义不能跨包，也就是说必须和接收方在一个包内。</p><p>✨方法可以绑定到多个结构类型上。</p><h2 id="例子">例子</h2><pre><code class="language-go">package mainimport &quot;fmt&quot;// 定义所属者人类type person struct &#123;    name string&#125;// 定义人类的方法 change func (self person) change()&#123;    self.name = &quot;凡尔赛.&quot;+self.name    fmt.Printf(&quot;你的新名字可能变成了：%s\n&quot;, self.name)&#125;// 主函数func main()&#123;    // 声明人类变量 p    p := person&#123;name:&quot;张三&quot;&#125;    // 调用 change 方法    p.change()    fmt.Printf(&quot;你的新名字变更失败，它依然是：%s\n&quot;, p.name)&#125;</code></pre><p>上面的例子，有一个现象，就是<code>change()</code>并不能变更<code>person.name</code></p><p>因为方法的<code>接收方person</code>是<code>值</code>类型，所以<code>p.change()</code>其实是将p复制了一份传递给了方法的<code>self</code>变量。因p和self的<code>内存地址</code>不一样，所以修改<code>self.name</code>不会改变<code>p.name</code>。</p><p>如何改变?</p><ol><li>将方法内修改后的值通过 return 直接返回</li></ol><pre><code class="language-go">package mainimport &quot;fmt&quot;// 定义所属者人类type person struct &#123;    name string&#125;// 定义人类的方法 change func (self person) change() string&#123;    self.name = &quot;凡尔赛.&quot;+self.name    fmt.Printf(&quot;你的新名字可能变成了：%s\n&quot;, self.name)    return self.name&#125;// 主函数func main()&#123;    // 声明人类变量 p    p := person&#123;name:&quot;张三&quot;&#125;    // 调用 change 方法    p.name = p.change()    fmt.Printf(&quot;你的新名字是：%s\n&quot;, p.name)&#125;</code></pre><h2 id="指针接收方">指针接收方</h2><p>将方法的接收者变更为指针接收方。</p><pre><code class="language-go">func (self *person) change()&#123;    self.name = &quot;凡尔赛.&quot;+self.name    fmt.Printf(&quot;你的新名字变成了：%s\n&quot;, self.name)&#125;func main()&#123;    p := person&#123;name:&quot;张三&quot;&#125;    (&amp;p).change()    fmt.Printf(&quot;你的新名字是：%s\n&quot;, p.name)&#125;</code></pre><p>什么时候会用到方法中的指针？</p><ol><li>参数太大，避免复制</li></ol><p>💥根据go的约定，要么结构的所有方法接收方都是指针类型，要么都是值类型，不要混用。</p><h2 id="嵌套方法">嵌套方法</h2><p>结构可以嵌套另一个结构，外层结构可以直接调用嵌套结构的方法</p><pre><code>package mainimport &quot;fmt&quot;type triangle struct &#123;    size int&#125;type coloredTriangle struct &#123;    triangle    color string&#125;func (t *triangle) perimeter() int &#123;    return t.size * 3&#125;func (t *triangle) doubleSize() &#123;    t.size *= 2&#125;func main() &#123;    t := coloredTriangle&#123;        triangle&#123;3&#125;,        &quot;blue&quot;,    &#125;    t.doubleSize()    fmt.Println(&quot;NewSize:&quot;, t.size)    fmt.Println(&quot;Perimeter:&quot;, t.perimeter())&#125;</code></pre><pre><code class="language-bash">OldSize: 3NewSize: 6Perimeter: 18</code></pre><p>coloredTriangle 之所以可以调用 triangle 的方法 doubleSize 和 perimeter，原因在于 go 编译器自动创建了两个包装方法（包装器）。</p><pre><code class="language-go">func (t *coloredTriangle) doubleSize() &#123;    t.triangle.size = t.triangle.doubleSize()&#125;func (t *coloredTriangle) perimeter() int &#123;    return t.triangle.perimeter()&#125;</code></pre><h2 id="重载方法">重载方法</h2><p>重载方法：表现上就是相同的方法名，不同的接收方。结构对象通过接收方来区分调用的方法。</p><p>通过重载方法，可以在不修改主代码的情况下，改变主代码的输出。</p><p>以上面的例子来说，就是主动去写包装方法，覆盖掉嵌套时候自动创建的包装方法。</p><pre><code class="language-go">package mainimport &quot;fmt&quot;type triangle struct &#123;    size int&#125;type coloredTriangle struct &#123;    triangle    color string&#125;func (t *triangle) perimeter() int &#123;    return t.size * 3&#125;// 重载方法perimeterfunc (t *coloredTriangle) perimeter() int &#123;    return t.size * 3 * 2&#125;func main() &#123;    t := coloredTriangle&#123;        triangle&#123;3&#125;,        &quot;blue&quot;,    &#125;    fmt.Println(&quot;Size:&quot;, t.size)    fmt.Println(&quot;Color Perimeter:&quot;, t.perimeter()) // 调用外结构的重载方法。    fmt.Println(&quot;NoColor Perimeter:&quot;, t.triangle.perimeter()) // 显式的调用嵌入结构的方法。&#125;</code></pre><pre><code class="language-bash">Size: 3Color Perimeter: 18NoColor Perimeter: 9</code></pre><h2 id="封装方法">封装方法</h2><p>封装可以让方法或者属性变成私有，即仅可用于包内部。通过包的封装方法，可以仅提供部分方法供调用包的程序所用。</p><p>go的封装方法仅用于程序包之间。</p><p>将方法或者属性名的首字母大写即为公开，小写即为私有。</p><pre><code class="language-bash">➜   tree.├── geometry│   ├── geometry.go│   └── go.mod # go mod init geometry├── go.mod # go mod init 022└── main.go➜   cat go.modmodule 022go 1.17require geometry v0.0.0replace geometry =&gt; ./geometry</code></pre><pre><code class="language-go">//geometry.gopackage geometrytype Triangle struct &#123;    size int&#125;func (t *Triangle) doubleSize() &#123;    t.size *= 2&#125;func (t *Triangle) SetSize(size int) &#123;    t.size = size&#125;func (t *Triangle) Perimeter() int &#123;    t.doubleSize()    return t.size * 3&#125;</code></pre><pre><code class="language-go">//main.gopackage mainimport (    &quot;fmt&quot;    &quot;geometry&quot;)func main()&#123;    t := geometry.Triangle&#123;&#125;    t.SetSize(3)    fmt.Println(&quot;Perimeter&quot;, t.Perimeter())    //fmt.Print(t.size)&#125;</code></pre><pre><code class="language-bash">➜   go run main.goPerimeter 18</code></pre><p>如果调用geometry私有属性size，则会报错</p><pre><code class="language-go"># command-line-arguments./main.go:12:18: t.size undefined (cannot refer to unexported field or method size)</code></pre><h2 id="其它类型的方法">其它类型的方法</h2><p>默认情况下，无法基于基础类型声明方法，例如无法直接给string类型声明方法。</p><p>但是可以基于基础类型创建新的类型，然后针对新的类型声明方法</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;strings&quot;)type stringupper stringfunc(s stringupper) Upper() string &#123;    return strings.ToUpper(string(s)) // s 是 stringupper 类型，因此需要转成 string 类型，才可以被 ToUpper 方法调用&#125;func main()&#123;    s := stringupper(&quot;this is string&quot;) // stringupper 类型拥有 string() 方法，因此直接()传递字符串.    fmt.Println(s)    fmt.Println(s.Upper())&#125;</code></pre><pre><code class="language-bash">➜   go run 023.gothis is stringTHIS IS STRING</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞006函数-闭包</title>
      <link href="posts/318675c1/"/>
      <url>posts/318675c1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>构建闭包原则：</p><ol><li>函数A中还有一个函数B，并且函数A返回了函数B</li><li>函数B调用了函数A的形参</li><li>main函数里调用函数A</li></ol><h2 id="基本原理">基本原理</h2><p>正常情况下，函数被调用执行完毕后，函数的形参将被释放回收。</p><p>但当函数是一个闭包后，这里假设是函数A。</p><p>此时<code>C=函数A</code>, 而函数A返还函数B，因此<code>C=函数B</code>，函数A已被调用完毕，按理说函数A的形参A应该被回收。</p><p>因为<code>C还存在</code>，故而函数B中调用的函数A的形参A无法被回收，因此，形参A就可以一直存活下来。</p><p>那么，每当你执行一次C，函数B就执行一次，形参A因为无法被回收，也会一直保持最新的数据。</p><h2 id="举例">举例</h2><pre><code class="language-go">package mainimport &quot;fmt&quot;// 返回func(y int) int 函数func fca(x int) func(y int) int&#123;    return func(y int) int &#123;        x+=y        return x    &#125;&#125;func fcb(startNum,step,time int) &#123;    fmt.Printf(&quot;初始数(startNum)：%d, 步长(step)%d, 迈步次数(time)：%d\n&quot;,startNum, step, time)    nextNum := fca(startNum)    fmt.Printf(&quot;nextNum被赋值，即nextNum等函数func(y int) int&#123;&#125;, 此时func中引用了形参x:1\n&quot;)    for i:=1;i&lt;=time;i++ &#123;        fmt.Printf(&quot;执行nextNum(step) &#123;x+=y&#125;, 返回x:%d\n&quot;,nextNum(step))        if i == 3 &#123;            fmt.Printf(&quot;重新赋值nextNum，导致nextNum释放，进而释放变量x，x重新被赋值为startNum。\n&quot;)            nextNum = fca(startNum)        &#125;    &#125;&#125;func main()&#123;    fcb(1,2,5)&#125;===初始数(startNum)：1, 步长(step)2, 迈步次数(time)：5nextNum被赋值，即nextNum等函数func(y int) int&#123;&#125;, 此时func中引用了形参x:1执行nextNum(step) &#123;x+=y&#125;, 返回x:3执行nextNum(step) &#123;x+=y&#125;, 返回x:5执行nextNum(step) &#123;x+=y&#125;, 返回x:7重新赋值nextNum，导致nextNum释放，进而释放变量x，x重新被赋值为startNum。执行nextNum(step) &#123;x+=y&#125;, 返回x:3执行nextNum(step) &#123;x+=y&#125;, 返回x:5</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞005函数</title>
      <link href="posts/98f86e16/"/>
      <url>posts/98f86e16/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>如果有参数，则需提供参数类型</p><p>如果有返回值，则需提供返回值类型</p><h2 id="结构">结构</h2><pre><code class="language-go">func &lt;func_name&gt; (&lt;args&gt; args_type) &lt;rtn_value_type&gt; &#123;    ...    return rtn_value&#125;</code></pre><h2 id="函数参数">函数参数</h2><p>函数参数是形参，调用函数并传递给函数的参数是实参，形参和实参类型要<code>一致</code></p><p>函数参数（形参）仅在函数体内生效</p><p>在函数没有被调用的时候，函数参数（形参）不会分配内存单元</p><p>当函数参数（形参）接收了一个实参<code>内存值</code>的时候，函数体内的<code>函数参数</code>将和<code>实参</code>没有任何关联</p><p>当函数参数（形参）接收了一个实参<code>内存地址</code>的时候，函数体内的<code>&amp;函数参数</code>将受到<code>实参</code>影响</p><h3 id="参数个数">参数个数</h3><p>参数可以有多个，只需要在形参类型前添加<code>...</code></p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main()&#123;    print(1,2,3,4,5)&#125;func print (a ...int) &#123;    for _,v := range a&#123;        fmt.Println(v)    &#125;&#125;</code></pre><blockquote><p><code>_</code> 表示接收无用的值</p></blockquote><h2 id="用例">用例</h2><pre><code class="language-go">package mainimport &quot;fmt&quot;func fcLen(yp *string) int &#123;    fmt.Printf(&quot;传递的形参是：%p,%s&quot;,yp, *yp) // %p标识指针变量    return len(*yp)&#125;func main()&#123;    a := &quot;abc&quot;    b := fcLen(&amp;a)    fmt.Println(b)                                            &#125;</code></pre><h2 id="递归函数">递归函数</h2><p>递归函数在嵌套的时候，需要一个退出条件，从而防止死循环</p><p>例如</p><pre><code class="language-go">func jiecheng(i int)int &#123;    if i &gt; 1 &#123;        result = i * jiecheng(i-1)        return result    &#125;    return 1&#125;</code></pre><h2 id="特殊函数">特殊函数</h2><p>init() 包内的init()在包被引用的时候，会自动执行。另外引用包的时候，如果通过<code>_ &quot;package_name&quot;</code>引用，表示仅引用包里的init()函数</p><h2 id="内置函数">内置函数</h2><p>make()用于chan、map以及切片创建</p><p>new(<type>)用于创建一个指向type内存地址的指针</type></p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞004控制流</title>
      <link href="posts/8a6b5455/"/>
      <url>posts/8a6b5455/</url>
      
        <content type="html"><![CDATA[<h1>if</h1><pre><code class="language-go">package mainimport &quot;fmt&quot;func givemeanumber() int &#123;    return -1&#125;func main() &#123;    if num := givemeanumber(); num &lt; 0 &#123;        fmt.Println(num, &quot;is negative&quot;)    &#125; else if num &lt; 10 &#123;        fmt.Println(num, &quot;has only one digit&quot;)    &#125; else &#123;        fmt.Println(num, &quot;has multiple digits&quot;)    &#125;&#125;</code></pre><p>在上述代码中，num 变量是在 if 中声明的，因此它的作用域仅限于 if 中。这种写法在 go 中很常见。</p><h1>switch</h1><p>switch 旨在避免维护含有多个 if 的语句。</p><ul><li>满足变量</li></ul><p>☠case 后不可跟条件表达式，只能跟值</p><pre><code class="language-go">switch 变量/常量 &#123;    case 值1:        ...    case 值2, 值3:        ...    default:        ...&#125;</code></pre><p>在上述代码中，若case后面的值若匹配变量，就执行对应代码，否则执行default代码。default语句不是必须的。</p><p>🤷‍♂️需要注意的是，switch 关键词后的变量类型和case后的值类型要保持一致。如果不一致，则编译将失败。</p><ul><li>满足条件</li></ul><p>☠case 后跟条件表达式，switch 后不可有变量</p><pre><code class="language-go">switch &#123;    case 条件1:        ...    case 条件2:        ...    default:        ...&#125;</code></pre><p>在上述代码中，若条件为真，就执行对应代码，否则执行default代码</p><ul><li>满足函数返回值</li></ul><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;time&quot;)func main() &#123;    switch time.Now().Weekday().String() &#123;    case &quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;:        fmt.Println(&quot;It's time to learn some Go.&quot;)    default:        fmt.Println(&quot;It's weekend, time to rest!&quot;)    &#125;    fmt.Println(time.Now().Weekday().String())&#125;</code></pre><h2 id="switch：fallthrough">switch：fallthrough</h2><p><code>fallthrough</code>关键词</p><p>当某个case上下文中包含<code>fallthrough</code>的话，则会直接执行紧邻的下一条<code>case</code>且不判断条件。</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;)func main() &#123;    switch num := 15; &#123;    case num &lt; 50:        fmt.Printf(&quot;%d is less than 50\n&quot;, num)        fallthrough    case num &gt; 100:        fmt.Printf(&quot;%d is greater than 100\n&quot;, num)        fallthrough    case num &lt; 200:        fmt.Printf(&quot;%d is less than 200&quot;, num)    &#125;&#125;</code></pre><pre><code class="language-bash">15 is less than 5015 is greater than 10015 is less than 200</code></pre><h1>for</h1><pre><code class="language-go">func main()&#123;    sum := 0    for i := 0; i &lt; 100; i++ &#123;        sum += 1    &#125;    fmt.Printf(&quot;result: %d\n&quot;, sum)&#125;</code></pre><p>go 没有 while 关键词。不过，可以用 for 去模拟。</p><p>for 后可以直接跟条件表达式，表达式为 true 的时候，循环就不会结束；</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;math/rand&quot;    &quot;time&quot;)func main() &#123;    var num int64    rand.Seed(time.Now().Unix())    for num != 5 &#123;        num = rand.Int63n(15)        fmt.Println(num)    &#125;&#125;</code></pre><p>若 for 后面没有任何表达式，则循环永远不会结束，除非循环体中包含 break。</p><p>另外，循环体中遇到 continue 将终止当前迭代，直接进入下次循环。</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞003运算符</title>
      <link href="posts/1a034363/"/>
      <url>posts/1a034363/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>常见的运算符就不说了，学过数学的都会。</p><h2 id="逻辑运算符">逻辑运算符</h2><p><code>A &amp;&amp; B</code> A和B都为真，则真</p><p><code>A || B</code> A和B任意为真，则为真</p><p><code>!A</code> A为真，则为假</p><p>当上述条件为真时，则执行下一步代码</p><h2 id="位运算符">位运算符</h2><p>将整数转为二进制后进行运算</p><p>二进制数：0为假，1为真</p><p>例如：</p><pre><code class="language-go">a,b := 2,4</code></pre><p>若<code>c := a &amp; b</code>按位与，按照<code>位数</code>分别进行<code>与</code>运算</p><pre><code>a： 0 1 0b： 1 0 0c： 0 0 0</code></pre><p>因此，最终 c = 0</p><p>若<code>c := a | b</code>按位或，按照<code>位数</code>分别进行<code>或</code>运算</p><pre><code>a： 0 1 0b： 1 0 0c： 1 1 0</code></pre><p>因此，最终 c = 6</p><p>若<code>c := a ^ b</code>按位异或，按照<code>位数</code>分别进行<code>异或</code>运算</p><p>异或：相同为0，不同为1</p><pre><code>a： 0 1 0b： 1 0 0c： 1 1 0</code></pre><p>因此，最终 c = 6</p><p>若<code>c := a &amp;^ b</code>，</p><p>则c等于将a中ab都为1的位写为0后的新a</p><pre><code>a： 0 1 0b： 1 0 0c： 0 1 0  # a中没有ab都为1的位，因此c的值依然是a</code></pre><p>若<code>c := a&lt;&lt;b</code>左移运算，即<code>a*2的b次方</code>。c等于32</p><p>若<code>c := a&gt;&gt;b</code>右移运算，即<code>a/2的b次方</code>，c等于0</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞002常量</title>
      <link href="posts/8c3e952e/"/>
      <url>posts/8c3e952e/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>常量只有一个元数据，就是内存地址存储的数据。它是一个静态值。</p><p>常量声明的同时必须赋值。</p><p>常量不可用<code>:=</code>声明并赋值。</p><p>在 Go 中，常量名称通常以混合大小写字母或全部大写字母书写。</p><p>常量定义后可以不用。</p><h2 id="声明和赋值">声明和赋值</h2><p>常量关键词是const。</p><blockquote><p>不能省略 const 关键词，省略了就没法判断是常量还是变量了</p></blockquote><h3 id="单类型">单类型</h3><pre><code class="language-go">const &lt;常量1&gt;,&lt;常量2&gt; &lt;常量类型&gt; = &lt;常量1的值&gt;, &lt;常量2的值&gt;</code></pre><h3 id="多类型">多类型</h3><p>根据变量值，自动判断变量类型</p><pre><code class="language-go">const &lt;常量1&gt;, &lt;常量2&gt; = &lt;常量1的值&gt;, &lt;常量2的值&gt;</code></pre><h3 id="单类型和多类型混合">单类型和多类型混合</h3><pre><code class="language-go">const (    &lt;常量1&gt;,&lt;常量11&gt; &lt;类型1&gt; = &lt;常量1的值&gt;, &lt;常量11的值&gt;    &lt;常量2&gt; &lt;类型2&gt; = &lt;常量2的值&gt;)</code></pre><p>上述写法中，如果常量2和常量1一样，则可以直接写常量2名，类型2和常量2的值可以省略。</p><pre><code class="language-go">const (    &lt;常量1&gt;,&lt;常量11&gt; &lt;类型1&gt; = &lt;常量1的值&gt;, &lt;常量11的值&gt;    &lt;常量2&gt;,&lt;常量22&gt;)</code></pre><p>在这种省略写法中，需要注意的是，前两列需要对齐一致。</p><h2 id="特殊常量iota">特殊常量iota</h2><p>有一个内置的特殊常量iota，每声明一个常量，它会+1，初始值是0</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞001变量</title>
      <link href="posts/759b68d8/"/>
      <url>posts/759b68d8/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>变量涉及到类型、值以及内存地址。</p><h2 id="声明类型">声明类型</h2><ul><li>基本类型：数字、字符串和布尔值</li><li>聚合类型：数组和结构</li><li>引用类型：指针、切片、映射、函数和通道</li><li>接口类型：接口</li></ul><p>部分变量类型及对应的零值:</p><p>int，整型，默认是0</p><p>string，字符串，默认是空</p><p>​💥字符串的表示用双引号和反引号，不可用单引号，单引号在golang里表示<code>rune</code> ，类似于其它语言的字节类型</p><p>​字符串算是只读切片，因此无法直接修改，若真修改，先转成byte</p><p>bool，布尔，默认是false</p><p>chan，管道，默认是nil</p><p>[数组大小]数组值类型 数组，默认是nil</p><p>[]数组值类型 切片，默认是nil</p><p>map   字典，默认是nil</p><p>byte，字节，默认是0，本质上是一个整型</p><h2 id="变量类型">变量类型</h2><p>如果变量值对应的是类型数据，则这类变量称之为<code>值类型</code>变量，也就是<code>普通</code>变量</p><p>如果变量值对应的是<code>另一个变量的内存地址</code>，则这类变量称之为<code>引用类型</code>变量，也叫<code>指针</code>变量</p><p>💁 变量的内存地址里不能存储变量自己的内存地址</p><h3 id="值变量">值变量</h3><p>方法1：</p><p>仅定义一种变量类型。</p><p>✨若变量的值没有提供，则系统会根据变量类型提供零值。</p><pre><code class="language-go">var &lt;变量1&gt;,&lt;变量2&gt; &lt;变量类型&gt; = &lt;变量1的值&gt;, &lt;变量2的值&gt;package mainimport &quot;fmt&quot;func main()&#123;    var a,b int = 1,2    fmt.Println(a,b)    var c,d string = &quot;abc&quot;,&quot;def&quot;    fmt.Println(c,d)    var e,f string = &quot;123&quot;,&quot;456&quot;    fmt.Println(e,f)&#125;1 2abc def123 456</code></pre><p>方法2:（不推荐）</p><p>可以同时定义多种类型</p><p>根据变量值，自动判断变量类型</p><pre><code class="language-go">var &lt;变量1&gt;, &lt;变量2&gt; = &lt;变量1的值&gt;, &lt;变量2的值&gt;</code></pre><p>方法3:（函数内使用）</p><p>可以同时定义多种类型</p><p>根据变量值，自动判断变量类型</p><p>省略 var 声明关键词</p><ol><li>只能用于函数体内。💥</li><li>仅可以用于新变量。💥</li><li>多个变量单行同时声明并赋值的时候，如果某一个变量是结构体字段，则同时待声明的变量都需要提前声明。</li></ol><pre><code class="language-go">&lt;变量1&gt;, &lt;变量2&gt; := &lt;变量1的值&gt;,&lt;变量2的值&gt;package mainimport &quot;fmt&quot;func main()&#123;    n,m := &quot;nnn&quot;, 12345    fmt.Println(n,m)&#125;nnn 12345</code></pre><p>方法4：</p><p>因式分解，常用来在函数体外声明全局变量</p><p>当然，也可以在函数体内定义局部变量</p><pre><code class="language-go">var (    &lt;变量1&gt; &lt;类型1&gt; = &lt;变量1的值&gt;    &lt;变量2&gt; &lt;类型2&gt; = &lt;变量2的值&gt;)</code></pre><p>方法5：</p><p>内置函数new()</p><pre><code class="language-go">pvar := new(T)package mainimport &quot;fmt&quot;func main()&#123;    var e string = &quot;abcdef&quot;    fmt.Println(&quot;e:&quot;+e)    f := new(string)    f = &amp;e    fmt.Println(&quot;f:&quot;+*f)&#125;e:abcdeff:abcdef</code></pre><p>✨直接返回指针</p><p>方法6：</p><p>内置函数make(T, args)</p><pre><code class="language-go">var := make(T, args)</code></pre><p>💥仅用于切片slice、映射map、通道channel</p><h3 id="引用变量">引用变量</h3><p>变量的值存储的是另一个变量的<code>内存地址</code>。因此，通过指针变量你可以修改另一个变量的值。</p><p>在 Go 中，有两个运算符可用于处理指针：</p><ul><li><code>&amp;</code> 运算符调用变量对象的内存地址。</li><li><code>*</code> 运算符表示取消引用指针或声明一个指针。<ul><li><code>*指针变量</code>取消引用指针，本质上是指针变量存储的内存地址所对应的变量对象。</li><li><code>*变量类型</code>声明指针。</li></ul></li></ul><p>例如:</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;    firstName := &quot;John&quot;    updateName(&amp;firstName) // 3. 普通变量前加&amp;，表示获取普通变量的内存地址    fmt.Println(firstName)&#125;func updateName(name *string) &#123; // 1. 形参变量类型前加*,表示这个形参是一个指针变量    *name = &quot;David&quot; // 2. 指针变量前加*，等同于指针变量所指向的普通变量&#125;</code></pre><h3 id="变量类型转换">变量类型转换</h3><p>Go 中隐式强制转换不起作用。</p><ol><li>浮点和整数之间</li></ol><p><code>type_name(expression)</code></p><ol start="2"><li>字符串和数字之间</li></ol><blockquote><p>import “strconv”</p></blockquote><p><code>strconv.Atoi(expression)</code>  字符串到数字</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;strconv&quot;)func main() &#123;    i, _ := strconv.Atoi(&quot;-42&quot;)    s := strconv.Itoa(-42)    fmt.Println(i, s)&#125;</code></pre><h2 id="普通变量和指针变量的使用区别">普通变量和指针变量的使用区别</h2><p>通过老的普通变量赋值生成的新普通变量，两者之间因为内存地址不一样，因此没有任何关联</p><p>例如：</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;var var01 int = 123var var02 int = var01fmt.Printf(&quot;var01的值是:%d\nvar02的值是:%d\n&quot;, var01, var02)fmt.Printf(&quot;var01的内存地址是:%d\nvar02的内存地址是:%d\n&quot;, &amp;var01, &amp;var02)var01 = 456    fmt.Printf(&quot;var01的新值是:%d\nvar02的值不变:%d\n&quot;, var01, var02) &#125;===var01的值是:123var02的值是:123var01的内存地址是:824634941440var02的内存地址是:824634941448var01的新值是:456var02的值不变:123</code></pre><p>通过老的普通变量赋值生成的新指针变量，两者之间是引用关系</p><p>当普通变量被赋予新值后，对应的指针变量也可以输出新值，因为普通变量的内存地址始终没变</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;var var01 int = 123var var02 *int = &amp;var01fmt.Printf(&quot;var01的值是:%d\nvar01的内存地址是:%d\n&quot;, var01, var02)fmt.Printf(&quot;var01的的值是（通过*var02获取）:%d\n&quot;, *var02)&#125;===var01的值是:123var01的内存地址（通过var02获取）:824633802928var01的值是（通过*var02获取）:123var01的新值是:456var01的内存地址（通过var02获取）:824633802928var01的新值是（通过*var02获取）:456</code></pre><h2 id="作用范围">作用范围</h2><p>函数内定义的变量，生命周期仅在函数被调用期间</p><p>​需要注意的是：if 和 for 这些控制结构中声明的变量的作用域只在相应的代码块内。即一般情况下，局部变量的作用域可以通过代码块（用大括号括起来的部分）判断。</p><p>函数外定义的变量，可以在本级以及下一级嵌套里使用</p><p>当两种变量名一致时，函数内变量优先级更高</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;      x := 1    fmt.Println(x)     //prints 1    &#123;        fmt.Println(x) //prints 1        x := 2        fmt.Println(x) //prints 2    &#125;    fmt.Println(x)     //prints 1 (bad if you need 2)&#125;</code></pre><h2 id="数组">数组</h2><h3 id="写法">写法</h3><p>一维数组：<code>var &lt;var_name&gt; [数组大小]&lt;type&gt;</code></p><p>二维数组：<code>var &lt;var_name&gt; [一维数组大小][二维数组大小]&lt;type&gt;</code></p><p>💥数组一旦定义则【长度不可变】，但你可以修改已有的元素。</p><p>🤷‍♂️另外，通过var声明的数组会填充默认值。</p><p>[] 填入<code>...</code>代表不确定数组大小由系统根据初始化的值数量自动计算大小</p><ul><li>[…]int{99: -1} 表示初始化一个100长度的数组，末位是-1，其余是0</li></ul><p>不管是一维数组，还是二维数组，只能存在一种类型。</p><p>最后，函数参数是数组的时候，默认是值传递。</p><h3 id="例子">例子</h3><pre><code class="language-go">package mainimport &quot;fmt&quot;func main()&#123;    array01 := [3]int&#123;1,2,3&#125;    array02 := [3]int&#123;4,5,6&#125;    array03 := [][3]int&#123;&#125;                    array03 = append(array03, array01)    array03 = append(array03, array02)    for _,x := range(array03)&#123;        for _,y := range(x)&#123;            fmt.Println(y)        &#125;    &#125;&#125;</code></pre><p><code>array03</code>定义了一个不限行，限3列的数组，这里不限行<code>[]</code>的写法叫<code>切片</code></p><h2 id="切片">切片</h2><p>切片包含三个组件数据：</p><ul><li><strong>指向基础数组中切片可以访问的第一个元素的指针</strong>。此元素不一定是数组的第一个元素 <code>array[0]</code>。</li><li><strong>切片的长度</strong>。切片中元素数目。</li><li><strong>切片的容量</strong>。切片开头（切片可访问的第一个元素）与基础数组结束之间的元素数目。</li></ul><p>切片是一个引用变量，因此切片变量存储的是内存地址，这个内存地址就是基础数组。</p><p>在一个基础数组上产出的切片，因基础数组不变，所以内存开销都很低。</p><p><img src="/posts/759b68d8/image-20220219152117217.png" alt="image-20220219152117217"></p><p><img src="/posts/759b68d8/image-20220219161029354.png" alt="image-20220219161029354"></p><h3 id="写法-2">写法</h3><p>基本写法</p><pre><code class="language-go">s1 := []int&#123;1,2,3,&#125; // 初始化一个内置3元素，容量3的切片s1</code></pre><p>写法2：</p><pre><code class="language-go">s1 := make([]int, &lt;len&gt;, &lt;cap&gt;) // &lt;len&gt;是定义切片内元素的长度，&lt;cap&gt;是定义切片容纳元素的上限数</code></pre><p>长度决定了切片的初始元素个数，容量决定了切片底层数组的元素个数</p><h3 id="常用函数">常用函数</h3><p>这两个函数是仅可以用于切片的，不可用于数组的。</p><ul><li>追加元素到切片</li></ul><p><code>新切片 := append(&lt;目标切片&gt;, &lt;填充元素&gt;)</code>，用来扩展目标切片并切出一个新切片。新切片是否在&lt;目标切片&gt;基础上操作取决于&lt;目标切片&gt;容量是否充足。</p><p>&lt;填充元素&gt; 如果是一个切片，则需要在其后面追加<code>...</code>。</p><p>就如同<code>新切片 := append(&lt;目标切片&gt;, &lt;填充切片&gt;...)</code></p><ul><li>完全复制一个切片</li></ul><p><code>copy(&lt;目标切片&gt;, &lt;需复制切片&gt;)</code>，用来将切片复制到目标切片，目标切片有多长，就填充多长。【目标切片是全新的，它的基础数组也是全新的】</p><ul><li>从切片里删除元素</li></ul><p>切片里没有内置删除功能.</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;    letters := []string&#123;&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;&#125;    remove := 2    if remove &lt; len(letters) &#123;        fmt.Println(&quot;Before&quot;, letters, &quot;Remove &quot;, letters[remove])        letters = append(letters[:remove], letters[remove+1:]...)        fmt.Println(&quot;After&quot;, letters)    &#125;&#125;</code></pre><h3 id="切片截取">切片截取</h3><pre><code class="language-go">s[:3] //忽略首位数字写法：截取0-2构建新切片s[3:] //忽略末尾数字写法：截取3-最后，构建新切片</code></pre><h3 id="遍历输出">遍历输出</h3><p>用<code>range</code>关键字</p><p>如果是数组或者切片，它返回(元素序号，元素值)</p><p>如果是集合，它返回(key, value)</p><pre><code class="language-go">for _, i := range &lt;切片&gt;&#123;    fmt.Println(i)&#125;</code></pre><h2 id="映射">映射</h2><p>map 类似于 pythhon 的字典，由kv对组成，它是无序的，因此，在使用 range 返回的时候，无法决定返回顺序。</p><h3 id="写法-3">写法</h3><pre><code class="language-go">// 通过 var 声明一个map，会自动附加默认值。var m1 map[&lt;key_type&gt;]value_type// 声明的时候m1 := map[&lt;key_type&gt;]value_type &#123;    &quot;key_type&quot;:&quot;value_type&quot;&#125;</code></pre><p>😒nil map不可通过<code>m1[key]=value</code>加集合元素。</p><p>所以，如果单纯的声明一个空映射（就是什么都没有，nil都没有），可以使用make()</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;    m := make(map[string]int)    m[&quot;a&quot;]=1    fmt.Println(m[&quot;a&quot;])&#125;</code></pre><p>例子：</p><pre><code class="language-go">m1 := map[string]int&#123;&quot;河南&quot;:1, &quot;郑州&quot;:2&#125;fmt.Println(m1[&quot;河南&quot;])fmt.Println(m1[&quot;郑州&quot;])===12</code></pre><h3 id="删除">删除</h3><p>delete(<map>, <key>) 从map中删除对应的kv</key></map></p><h3 id="技巧">技巧</h3><p>下面这个例子，通过判断map取值返回的第二个值<code>ok</code>，来判断接下来的逻辑行为。</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;      x := map[string]string&#123;&quot;one&quot;:&quot;a&quot;,&quot;two&quot;:&quot;&quot;,&quot;three&quot;:&quot;c&quot;&#125;    if _,ok := x[&quot;two&quot;]; !ok &#123;        fmt.Println(&quot;no entry&quot;)    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞09存储资源</title>
      <link href="posts/1080c6b6/"/>
      <url>posts/1080c6b6/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><p>在使用云服务的时候，我们创建磁盘，需要在存储服务中提交一个请求，里面包含了磁盘类型，磁盘大小.之后，我们再把这个申请的磁盘挂载到所需的计算资源上即可.</p><p>而在k8s中. 当使用存储的时候，我们会涉及到以下主要概念</p><p>构建存储的概念：</p><ul><li>volume 声明存储服务，例如公有云的云盘（aws-ebs，阿里云-云盘），也可以是nfs/cephfs。</li><li>pv 声明volume的信息，否则，k8s就不知道如何使用 volume。</li></ul><p>使用存储的概念：</p><ul><li><p>pvc 通过PV声明【期望】的存储。例如期望得到的磁盘大小、访问模式。</p></li><li><p>spec.template.spec.volumes  通过 PVC 或者 CM 创建 pod 所需的卷。如同你在aws中你将EBS绑定到EC2。</p></li><li><p>spec.template.spec.containers.volumeMounts  声明 pod 如何挂载 volumes。如同你登陆到aws的EC2中，并使用mount命令进行实际挂载。</p></li></ul><p>你可能会发现，<code>podtemplate.spec.volumes</code>和<code>podtemplate.spec.containers.volumeMounts</code>之间缺少了一个格式化文件系统的过程，这是因为k8s已经在你通过pv资源帮你创建好了文件系统。当然你也可以通过将pv的模式改为块设备，不过这样一来，程序就需要确认是否可以识别块设备了。我想一般的应用程序是用不上这个类型的。<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode</a></p><p>最后，pod 与 pv 与 pvc 之间是一对一强依赖关系。三者之间，在A资源对象【被B调用】的过程中，任何一个删除A资源对象的操作都不会立即执行，而是在【B调用方】生命周期结束之后才会执行。</p><h1>volume卷</h1><p>支持的存储类型</p><p><a href="https://kubernetes.io/docs/concepts/storage/volumes/#volume-types">https://kubernetes.io/docs/concepts/storage/volumes/#volume-types</a></p><p>在使用某一个存储服务类型之前，详细的阅读存储服务的限制很有必要。</p><p>存储类型支持的模式:</p><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a></p><ol><li><p><strong>ReadWriteOnce</strong>(单节点挂载读写，单节点内多pod读写)</p></li><li><p><strong>ReadOnlyMany</strong>(多节点挂载只读，多pod只读)</p></li><li><p><strong>ReadWriteMany</strong>(多节点挂载读写，多pod读写)</p></li><li><p><strong>ReadWriteOncePod</strong>(单Pod读写)</p></li></ol><p>⚠️ReadWriteOncePod 是新增，仅支持 csi + k8s 1.22+</p><h2 id="NFS">NFS</h2><p>使用NFS之前，需要先搭建NFS服务端。</p><p>例如云服务已有的NFS服务，或者自行搭建。</p><p>下面是k8s官方给出的自行搭建：</p><p><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs">https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs</a></p><h2 id="本地">本地</h2><p>local volume 特点：</p><ul><li>当前只支持静态PV。</li><li>显式的添加节点亲和性，系统通过PV的节点亲和性将Pod调度到本地卷所在节点上。</li><li>建立一个动态PV，并通过设置模式延迟绑定直到Pod调度到本地卷所在节点上。</li></ul><p>一个例子：</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name: example-pvspec:  capacity:    storage: 100Gi  volumeMode: Filesystem  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Delete  storageClassName: local-storage  local:    path: /mnt/disks/ssd1  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:          - example-node    --- apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: local-storageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumer</code></pre><h1>PV持久卷</h1><h2 id="状态">状态</h2><pre><code class="language-bash">kubectl get pv -n &lt;ns&gt;</code></pre><ul><li>Available（可用）：表示可用状态，还未被任何 PVC 绑定</li><li>Bound（已绑定）：表示 PV 已经被 PVC 绑定</li><li>Released（已释放）：PVC 被删除，但是数据资源还在</li><li>Failed（失败）： 表示该 PV 的自动回收失败</li></ul><h2 id="类型">类型</h2><p>PV分为静态和动态。</p><p>静态pv: 需要先创建一定数量的pv， 然后才能通过pvc对应申请。若pvc无法匹配任意静态pv，则pvc会一直等待下去。</p><p>动态pv: 需要先构建供应商.主流的云商存储基本都有供应商， 区别在于是内置了，还是需要自行外部创建.然后再通过pvc去申请.</p><p>内置供应商列表: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></p><p>外置供应商部署: <a href="https://github.com/kubernetes-retired/external-storage">https://github.com/kubernetes-retired/external-storage</a></p><h2 id="回收策略">回收策略</h2><p><code>spec.persistentVolumeReclaimPolicy</code> 定义 PVC 被删除的时候，集群如何处理PV。</p><p><strong>Retain</strong>  不处理PV。但 PV 默认不能直接再次复用，因为里面还有之前Pod的数据。</p><ul><li><p>复用PV：</p><ul><li><p>在静态PV下，PV的名字是固定的，因此删除PVC之后，通过删除<code>pv.spec.claimRef.uid</code>解除PV的<code>Released</code>状态，就可以采用原有的PVC配置分配到。</p></li><li><p>在动态PV下，PV的名字是动态生成的， 因此删除PVC之后， 当PVC重新申请资源的时候， PV默认将会分配到一个新的资源路径。</p></li></ul></li></ul><p><strong>Delete</strong> 同时删除PVC和Volume。只不过Volume是否删除取决于每一个StorageClass中parameters的定义。</p><ul><li>当StorageClass是nfs-client时，开启 <code>parameters.archiveOnDelete=true</code> ，就可以定义不删除而是在nfs中将其数据目录重命名归档。</li><li>当StorageClass是微软/aws/阿里云的云盘的时候，一般默认是删除云盘。从而避免产生大量闲置云盘。</li></ul><p><strong>Recycle</strong> 删除PV。</p><ul><li>当前只有nfs和hostPath支持.</li><li>已废弃。建议通过动态PV+Delete去控制。</li></ul><h2 id="静态PV">静态PV</h2><p>静态PV，必须由管理员用户先创建好pv.就如同你想构建一个云服务器，但需要先创建好一个云盘。</p><p>一个静态pv的例子， 这里以 nfs 为例:</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name:  nas-nginx-pv  # pv 名字，会被 pvc 引用spec:  claimRef:    name: nas-nginx-pvc    namespace: default  capacity:    storage: 10Gi  # 定义本持久卷大小  accessModes:  - ReadWriteMany   # 定义本持久卷访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /volume1/k8s  # 定义 nfs 共享路径    server: 10.200.10.4        # 定义 nfs 服务器地址</code></pre><ul><li><p>claimRef 显式的指定允许绑定的PVC。</p></li><li><p>accessModes 访问模式。</p></li><li><p>persistentVolumeReclaimPolicy 详见<strong>PV回收策略。</strong></p></li></ul><h3 id="静态PV对应的PVC">静态PV对应的PVC</h3><pre><code class="language-yaml">kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-nginx-pvc  namespace: defaultspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 10Gi  volumeName: nas-nginx-pv</code></pre><ul><li>spec.volumeName 显式的指定要绑定的pv</li></ul><h2 id="动态PV">动态PV</h2><p>动态PV目的在于用户可以自行通过PVC申请资源，无需提前通过管理员创建PV。</p><p>动态PV的构建需要一个新的资源对象StroageClass，它通过<code>provisioner</code>指定卷插件，从而对接不同的储存。卷插件其实就是一个存储类型的客户端。</p><p>卷插件都会拥有自己的一些特定参数，从而满足向卷供应商提供创建卷时所需要的信息。</p><p>当前支持的volume类型以及对应的卷插件参数：</p><p><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></p><p>⚠️ k8s并不提供所有的卷插件。</p><h3 id="基于nfs构建StroageClass">基于nfs构建StroageClass</h3><p>以网络存储nfs为例。nfs属于外置供应商，因此没有内置卷插件。因此需要先创建。创建文档：</p><p><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/blob/master/charts/nfs-subdir-external-provisioner/README.md">https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner/blob/master/charts/nfs-subdir-external-provisioner/README.md</a></p><p>⚠️ 需要注意的是，nfs-client正常运行条件：</p><ul><li>所有节点安装nfs-utils。<code>yum install nfs-utils -y</code></li></ul><p>安装命令：</p><ul><li>国外源</li></ul><pre><code class="language-yaml">#############################################国外源###################################################helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/helm repo updatehelm search repo --max-col-width 200 | grep nfshelm install nfs-client nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \         --set storageClass.name=nfs-client \         --set nfs.server=10.200.10.4 \         --set nfs.path=/volume1/k8s \         --set storageClass.reclaimPolicy=Delete \         --set storageClass.archiveOnDelete=true \         --set storageClass.allowVolumeExpansion=true \         --set storageClass.defaultClass=true</code></pre><ul><li>阿里源(版本可能陈旧)</li></ul><pre><code class="language-bash">#############################################阿里源###################################################helm repo add apphub https://apphub.aliyuncs.comhelm repo updatehelm install nfs-client apphub/nfs-client-provisioner \--set storageClass.name=nfs-client \--set nfs.server=10.200.10.4 \--set nfs.path=/volume1/k8s \--set storageClass.reclaimPolicy=Delete \--set storageClass.archiveOnDelete=true \--set storageClass.allowVolumeExpansion=true \--set storageClass.defaultClass=true</code></pre><ul><li><p>parameters.archiveOnDelete 当回收策略在被执行的时候（<a href="#persistentVolumeReclaimPolicy">persistentVolumeReclaimPolicy</a>），pv删除的同时申请的存储资源是否要归档。false就是不归档直接删除。这里归档的意思就是将申请的存储资源的物理路径前加一个archived前缀。最终格式：<code>archived-$&#123;NS_NAME&#125;-$&#123;PVC_NAME&#125;-$&#123;PV_NAME&#125;</code></p></li><li><p>storageClass.allowVolumeExpansion 开启卷扩展。这个选项可以让你动态的调整 pvc 请求的对象大小。</p><blockquote><p>其实这个参数没用，因为k8s目前不支持nfs动态扩展。</p></blockquote></li></ul><p>nfs-client会自动帮你创建一个 StroageClass。</p><pre><code class="language-yaml">kubectl get StorageClass/nfs-client -o yamlallowVolumeExpansion: trueapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  annotations:    meta.helm.sh/release-name: nfs-client    meta.helm.sh/release-namespace: default    storageclass.kubernetes.io/is-default-class: &quot;true&quot;  creationTimestamp: &quot;2021-12-27T06:25:24Z&quot;  labels:    app: nfs-subdir-external-provisioner    app.kubernetes.io/managed-by: Helm    chart: nfs-subdir-external-provisioner-4.0.14    heritage: Helm    release: nfs-client  ......  name: nfs-clientparameters:  archiveOnDelete: &quot;true&quot;provisioner: cluster.local/nfs-client-nfs-subdir-external-provisionerreclaimPolicy: DeletevolumeBindingMode: Immediate</code></pre><h3 id="动态PV对应的PVC">动态PV对应的PVC</h3><pre><code class="language-yaml">kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-vsftpd-pvcspec:  storageClassName: nfs-client  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi</code></pre><p>在这里，我们通过 <code>spec.storageClassName</code>显式指定StroageClass。</p><h3 id="Pod中的PVC模板">Pod中的PVC模板</h3><p>PVC模板可以动态的创建PVC，然后PVC再通过StorageClass动态创建PV。</p><p>💥 volumeClaimTemplates 不支持 Deployment.spec</p><pre><code class="language-yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: app-demospec:...    spec:      containers:...        volumeMounts:        - name: appdata          mountPath: /app/data  volumeClaimTemplates:  - metadata:      name: appdata  # appdata 即 volumeMounts.name    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      resources:        requests:          storage: 1Gi</code></pre><h3 id="在线扩展（仅支持动态pv）">在线扩展（仅支持动态pv）</h3><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims</a> 文档中列举了支持扩展的存储类。</p><p>💔NFS是不被支持的。</p><p>另外，还需要满足下列条件：</p><ol><li><p>feature-gates的<code>ExpandInUsePersistentVolumes</code>被开启。它在1.15版本以后默认开启。</p></li><li><p>StorageClass的<code>allowVolumeExpansion</code>被开启。</p></li><li><p>文件系统是XFS， Ext3， or Ext4。这个需要StorageClass的支持。一般来说volume属于块设备类型的资源才支持，比如aws的ebs。</p><blockquote><p>你可以在这里确认StorageClass是否支持fstype属性。<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters">https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters</a></p></blockquote></li></ol><p>满足上述条件后，你就可以通过修改pvc提升存储大小了。即使 pvc 正在被使用。</p><h3 id="局限性">局限性</h3><p>当一个 pod 同时申请多个 volume 的时候，可能会出现 volume-A 申请成功，volume-B 因物理容量不够申请失败的问题，此时 pod 将卡住。这种情况下，需要手动介入去清理。</p><h3 id="PVC申请失败">PVC申请失败</h3><p>需要我们手动的时候，都是资源申请失败或者不小心删除了PVC之类的。而不管是什么，我们第一目的是数据不丢。</p><p>因此，我们在清理故障对象资源的时候，应该遵循下列步骤：</p><ol><li>将pv的回收策略<code>persistentVolumeReclaimPolicy</code>定义为<code>Retain</code></li><li>删除pvc，这个时候 pv 对象将会保留，但是 pv 状态会变成 Released，这个状态下 pv 无法再被使用</li><li>删除pv对象字段<code>spec.claimRef.uid</code>，从而将pv与pvc解绑，从而使 pv 状态变为 Available</li><li>将pvc对象字段<code>volumeName</code>设置为pv的名字，重建pvc</li><li>恢复pv的回收策略。</li></ol><h1>Pod使用PVC</h1><p>上述例子中的 nfs 卷类型为例。</p><p>Deployment配置：</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: vsftpd  namespace: it  labels:    app: vsftpdspec:  serviceName: vsftpd  revisionHistoryLimit: 10  replicas: 1  selector:    matchLabels:      app: vsftpd  template:    metadata:      labels:        app: vsftpd    spec:      hostNetwork: true      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io/hostname                operator: In                values:                - k8s01      containers:      - name: vsftp        image: fauria/vsftpd        ports:        - containerPort: 20        - containerPort: 21        env:        - name: LOG_STDOUT          value: STDOUT        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: vsftpd-vol          subPath: vsftpd/data          mountPath: /home/vsftpd        - name: vsftpd-vc          subPath: vsftpd.conf          mountPath: /etc/vsftpd/vsftpd.conf          readOnly: true        - name: vsftpd-vc          subPath: virtual_users.txt          mountPath: /etc/vsftpd/virtual_users.txt          readOnly: true        - name: vsftpd-vc          subPath: admin          mountPath: /etc/vsftpd/virtual/admin          readOnly: true      volumes:      - name: vsftpd-vol        persistentVolumeClaim:          claimName: vsftpd-pvc      - name: vsftpd-vc        configMap:          name: vsftpd-cm</code></pre><ol><li><p><code>spec.template.spec.volumes</code>：定义Pod所用的卷</p></li><li><p><code>spec.template.spec.containers.volumeMounts</code>：定义 <code>nas-vsftpd-vol</code>如何挂载</p><p>这里将 nfs 共享目录<code>&lt;nfs_root&gt;/vsftpd/data</code>挂载到容器里的 <code>/home/vsftpd</code>路径。subPath不存在的时候，nfs会自动创建。</p></li><li><p>configMap 参见临时卷</p></li></ol><p>⚠️ subPath 方式的挂载无法接收数据更新。</p><h1>临时卷</h1><p><a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#types-of-ephemeral-volumes">https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#types-of-ephemeral-volumes</a></p><p>k8s用来进行临时的数据存储或者临时调用数据。例如缓存/会话/密码/配置一类的。</p><p>ℹ️ 他们无需人为的提前创建PVC去绑定，而是直接在Pod中去调用。</p><p>临时卷均通过节点的kubelet自动管理，实现临时卷的PVC将随着POD的删除而自动删除。</p><ul><li><p>emptyDir</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: k8s.gcr.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /cache      name: cache-volume  volumes:  - name: cache-volume    emptyDir: &#123;&#125;</code></pre><p>emptyDir支持构建一个内存层缓存，也就是 tmpfs。你可以通过<code>emptyDir.medium: &quot;Memory&quot;</code>来开启。</p></li><li><p>configMap、secret</p><p>配置卷和密码卷，很重要的类型。当然，这只是ConfigMap和secret对象的其中一种使用方式。</p><p>⚠️ 需要先创建ConfigMap和secret对象，才能在Pod中引用他们。</p></li><li><p>Downward API</p><p>常用来给容器提供Pod信息。</p></li><li><p>generic ephemeral volumes 这一种是更加灵活的emptyDir，可以是网络存储，如果插件支持，则各种持久卷的特性它都有可能支持。从1.19开始支持，1.19也是alpha，默认不开启。</p><p>具体使用看https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes</p></li></ul><h1>Projected Volumes</h1><p>可实现将volumes的所有列表项挂载在Pod同一个位置上。</p><p>详细使用见【k8s☞10应用配置与密码与信息提供】</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装库☞vsftpd</title>
      <link href="posts/894f9595/"/>
      <url>posts/894f9595/</url>
      
        <content type="html"><![CDATA[<h2 id="配置文件">配置文件</h2><pre><code class="language-yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: nas-it-local-dep  namespace: it  labels:    app: nas-it-localspec:  serviceName: nas-it-local-vsftpd  revisionHistoryLimit: 10  replicas: 1  selector:    matchLabels:      app: nas-it-local  template:    metadata:      labels:        app: nas-it-local    spec:      hostNetwork: true      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io/hostname                operator: In                values:                - k8s01  # 这里绑定 pod 所在节点      containers:      - name: vsftp        image: fauria/vsftpd        ports:        - containerPort: 20        - containerPort: 21        env:        - name: LOG_STDOUT          value: STDOUT        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: nas-it-local-vol          subPath: nas.it.local/data          mountPath: /home/vsftpd        - name: nas-it-local-vc          subPath: vsftpd.conf          mountPath: /etc/vsftpd/vsftpd.conf          readOnly: true        - name: nas-it-local-vc          subPath: virtual_users.txt          mountPath: /etc/vsftpd/virtual_users.txt          readOnly: true        - name: nas-it-local-vc          subPath: admin          mountPath: /etc/vsftpd/virtual/admin          readOnly: true        - name: nas-it-local-vc          subPath: yuangong          mountPath: /etc/vsftpd/virtual/yuangong          readOnly: true      volumes:      - name: nas-it-local-vol        persistentVolumeClaim:          claimName: nas-it-local-pvc      - name: nas-it-local-vc        configMap:          name: nas-it-local-cm---kind: ConfigMapapiVersion: v1metadata:  name: nas-it-local-cm  namespace: itdata:  vsftpd.conf: |    # Run in the foreground to keep the container running:    background=NO    # Allow anonymous FTP? (Beware - allowed by default if you comment this out).    anonymous_enable=NO    # Uncomment this to allow local users to log in.    local_enable=YES    ## Enable virtual users    guest_enable=YES    ## Virtual users will use the same permissions as anonymous    virtual_use_local_privs=YES    # Uncomment this to enable any form of FTP write command.    write_enable=YES    ## PAM file name    pam_service_name=vsftpd_virtual    ## Home Directory for virtual users    user_sub_token=$USER    local_root=/home/vsftpd/$USER    user_config_dir=/etc/vsftpd/virtual    # You may specify an explicit list of local users to chroot() to their home    # directory. If chroot_local_user is YES, then this list becomes a list of    # users to NOT chroot().    chroot_local_user=YES    # Workaround chroot check.    # See https://www.benscobie.com/fixing-500-oops-vsftpd-refusing-to-run-with-writable-root-inside-chroot/    # and http://serverfault.com/questions/362619/why-is-the-chroot-local-user-of-vsftpd-insecure    allow_writeable_chroot=YES    ## Hide ids from user    hide_ids=YES    ## Enable logging    xferlog_enable=YES    xferlog_file=/var/log/vsftpd/vsftpd.log    ## Enable active mode    port_enable=YES    connect_from_port_20=YES    ftp_data_port=20    ## Disable seccomp filter sanboxing    seccomp_sandbox=NO    ### Variables set at container runtime    pasv_address=10.200.16.10    pasv_max_port=21110    pasv_min_port=21100    pasv_addr_resolve=NO    pasv_enable=YES    file_open_mode=0666    local_umask=077    xferlog_std_format=NO    reverse_lookup_enable=YES    pasv_promiscuous=NO    port_promiscuous=NO  virtual_users.txt: |    admin    admin    yuangong    yuangong  admin: |    local_root=/home/vsftpd    anon_world_readable_only=NO    write_enable=YES    anon_mkdir_write_enable=YES    anon_upload_enable=YES    anon_other_write_enable=YES  yuangong: |    local_root=/home/vsftpd/yuangong    anon_world_readable_only=NO    write_enable=NO    anon_mkdir_write_enable=NO    anon_upload_enable=NO    anon_other_write_enable=NO---apiVersion: v1kind: PersistentVolumemetadata:  name:  nas-it-local-pv  # 定义 pv 名字, 会被 pvc 引用  namespace: itspec:  claimRef:    name: nas-it-local-pvc    namespace: it  capacity:    storage: 100Gi  # 定义大小  accessModes:  - ReadWriteMany   # 定义访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /mnt/data001/nfs-k8s   # 定义 nfs 共享路径    server: 10.200.16.250        # 定义 nfs 服务器地址---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-it-local-pvc  namespace: itspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi  volumeName: nas-it-local-pv</code></pre><ul><li>这是一个<strong>主动模式</strong>的 ftp. 鉴于 k8s 特殊的网络环境.以及端口的繁琐配置. 所以采用主动模式, 可以让我们更舒服一些.</li><li>pv和pvc方面根据你自己的环境自行调整.</li><li>你需要自行将pod强制绑定到某个物理节点的ip上. 避免因pod重构时漂移到其它ip. 配置文件里绑定的是k8s01</li></ul><h2 id="用户创建">用户创建</h2><p>上述配置会创建admin和yuangong两个用户。</p><p>admin用户拥有所有权。根目录是/home/vsftpd</p><p>yuangong用户只有下载权限。根目录是/home/vsftpd/yuangong</p><h3 id="添加新用户">添加新用户</h3><ul><li><p>添加用户名和密码到 virtual_users.txt 配置</p></li><li><p>添加用户配置到cm对象中</p><pre><code class="language-yaml">  我是用户名: |    local_root=/home/vsftpd/&lt;我是用户名&gt;    anon_world_readable_only=NO    write_enable=YES    anon_mkdir_write_enable=YES    anon_upload_enable=YES    anon_other_write_enable=YES</code></pre></li><li><p>创建用户目录 /home/vsftpd/&lt;我是用户名&gt;</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> vsftpd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞monit</title>
      <link href="posts/ae41ae33/"/>
      <url>posts/ae41ae33/</url>
      
        <content type="html"><![CDATA[<h2 id="安装">安装</h2><pre><code class="language-bash">yum install monit -y</code></pre><h2 id="主配置">主配置</h2><pre><code class="language-bash"># vim /etc/monitrcset daemon  10 # 调整周期</code></pre><p>⭐️官方配置文档: <a href="https://mmonit.com/monit/documentation/monit.html">https://mmonit.com/monit/documentation/monit.html</a></p><h2 id="程序保活配置">程序保活配置</h2><p>默认存放位置: /etc/monit.d/</p><p>这里以 pidfile 检测方式为例</p><p>nginx的配置:</p><pre><code class="language-bash">check process nginx with pidfile /var/run/nginx.pid  start program = &quot;/usr/sbin/nginx&quot;  stop program = &quot;/usr/bin/killall nginx&quot;if changed pid then alert</code></pre><p>上述nginx配置的意思:</p><ol><li><p>检测方式: pidfile</p></li><li><p>开关程序路径</p></li><li><p>发现pid改变,则触发动作: 预警</p></li></ol><p>tomcat的配置:</p><pre><code class="language-bash">check process tomcat-8080 with pidfile /opt/tomcat/tomcat-8080/bin/tomcat.pid  start program = &quot;/usr/bin/bash /opt/tomcat/tomcat-8080/bin/startup.sh&quot;    as uid &quot;root&quot; and gid &quot;root&quot;  stop program = &quot;/usr/bin/ps aux | /usr/bin/grep '/opt/tomcat/tomcat-8080/bin/bootstrap.jar' | /usr/bin/awk '&#123;print &quot;kill -9 &quot;$2&#125;' | /usr/bin/bash&quot;    as uid &quot;root&quot; and gid &quot;root&quot;if failed port 8080 then alert</code></pre><p>上述tomcat配置的意思:</p><ol><li>检测方式: pidfile</li><li>开关程序路径, 并且执行的时候需要以 root 权限执行</li><li>发现 8080 不通, 则触发动作: 预警</li></ol><p>🌟需要注意的是, 所有的文件路径都需要是绝对路径, 包括命令.</p><h2 id="启动-关闭">启动/关闭</h2><pre><code class="language-bash">systemctl start/stop/reload monit</code></pre><p>🌟当monit启动之后, 它就会加载保活配置, 并进行检测.</p><h2 id="日志">日志</h2><pre><code>/var/log/monit.log</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> monit </category>
          
      </categories>
      
      
        <tags>
            
            <tag> monit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞13-1 ingress-nginx的各种用法</title>
      <link href="posts/5d39569c/"/>
      <url>posts/5d39569c/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p>官方文档</p><p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/</a></p><h2 id="关键须知">关键须知</h2><ol><li>ingress 的注解可能会在当前所有的 path 里生效。例如在 ingress-nginx 中，下列两个就会遵循这个规则：</li></ol><ul><li><p><a href="http://inginx.ingress.kubernetes.io/rewrite-target">inginx.ingress.kubernetes.io/rewrite-target</a></p></li><li><p><a href="http://nginx.ingress.kubernetes.io/configuration-snippet">nginx.ingress.kubernetes.io/configuration-snippet</a></p><p>💥这里列举的注解并非是所有遵循这个规则的注解，仅是一个例子。</p></li></ul><p>​   ✨如果你只想针对某个 path 来添加，则需要重新创建一个新的ingress对象，并保持 ingress 对象中的 host 一致。</p><p>​             💥ingress对象名不可以相同。</p><p>​       ✨不同的ingress对象，但拥有相同的host和不同的path，则两个ingress对象规则会合并。</p><ol start="2"><li>两个ingress的host和path不能完全一样，一样的时候，第二个ingress应用的时候会被拒绝，并提示已定义。</li></ol><pre><code class="language-bash">Error from server (BadRequest): error when creating &quot;1-ingress-err.yaml&quot;: admission webhook &quot;validate.nginx.ingress.kubernetes.io&quot; denied the request: host &quot;ingress.xxx.com&quot; and path &quot;/foo&quot; is already defined in ingress default/test-ingress</code></pre><ol start="3"><li>如果你发现某个注解没有在 ingress-controller 里的 nginx.conf 里生效，则看下注解的key是否写的正确。</li></ol><p>👙可能的最佳方式：</p><p>一个域名一个yaml，一个yaml里添加多个 ingress，一个ingress配置通用，其它ingress对应特殊的path（额外注解），并确保所有的ingress保持相同的host和不同的name</p><h2 id="基本配置示例1">基本配置示例1</h2><h3 id="后端服务">后端服务</h3><p><a href="http://xn--registry-yy6e.cn-hangzhou.aliyuncs.com/yilong/ingress-test:web1">✨registry.cn-hangzhou.aliyuncs.com/yilong/ingress-test:web1</a>  可以接收 /foo 和 /bar ，并输出 web1: /foo 和 web1: /bar</p><p>0-dep-web1service.yaml</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: test-web1  labels:    app: test-web1spec:  replicas: 1  selector:    matchLabels:      app: test-web1  template:    metadata:      labels:        app: test-web1    spec:      containers:      - name: test-web1        imagePullPolicy: IfNotPresent        image: registry.cn-hangzhou.aliyuncs.com/yilong/ingress-test:web1        ports:        - containerPort: 8080---apiVersion: v1kind: Servicemetadata:  name: web1-servicespec:  type: ClusterIP  selector:    app: test-web1  ports:    - port: 8080      targetPort: 8080</code></pre><h3 id="ingress-配置">ingress 配置</h3><p>1-ingress-web1service.yaml</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: test-ingress  namespace: defaultspec:  rules:  - host: foo.bar.com    http:      paths:      - path: /foo        backend:          service:            name: web1-service            port:              number: 8080        pathType: ImplementationSpecific      - path: /bar        backend:          service:            name: web1-service            port:              number: 8080        pathType: ImplementationSpecific</code></pre><h3 id="访问">访问</h3><pre><code class="language-bash">curl -H &quot;host:foo.bar.com&quot; http://39.103.219.253/fooweb1: /foocurl -H &quot;host:foo.bar.com&quot; http://39.103.219.253/barweb1: /bar</code></pre><h2 id="基本重定向">基本重定向</h2><p>使用<code>nginx.ingress.kubernetes.io/rewrite-target</code>注解支持基本的Rewrite配置。</p><p>下列配置实现效果： <a href="http://foo.bar.com/aoo/xxx">http://foo.bar.com/aoo/xxx</a> =&gt; <a href="http://foo.bar.com/xxx">http://foo.bar.com/xxx</a></p><p>💥这个基本重定向的 rewrite 会写在当前 ingress 对象里的所有 path 下，等同于</p><pre><code class="language-bash">location /foo &#123;rewrite &quot;(?i)/foo&quot; /$1 break;&#125;location /bar &#123;rewrite &quot;(?i)/bar&quot; /$1 break;&#125;location /aoo/(.*) &#123;rewrite &quot;(?i)/aoo/(.*)&quot; /$1 break;&#125;</code></pre><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: test-ingress  namespace: default  annotations:    #URL重定向。    nginx.ingress.kubernetes.io/rewrite-target: /$1spec:  rules:  - host: foo.bar.com    http:      paths:      - path: /foo        backend:          service:            name: web1-service            port:              number: 8080        pathType: ImplementationSpecific      - path: /bar        backend:          service:            name: web1-service            port:              number: 8080        pathType: ImplementationSpecific      - path: /aoo/(.*)        backend:          service:            name: web1-service            port:              number: 8080        pathType: ImplementationSpecific</code></pre><h2 id="复杂重定向">复杂重定向</h2><ul><li><code>nginx.ingress.kubernetes.io/server-snippet</code>：添加配置到nginx的server配置上下文。</li><li><code>nginx.ingress.kubernetes.io/configuration-snippet</code>：添加配置到nginx的【所有】location上下文。</li></ul><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: gray-release  annotations:    nginx.ingress.kubernetes.io/server-snippet: |      rewrite ^/v4/(.*)/card/query http://foo.bar.com/v5/#!/card/query permanent;    nginx.ingress.kubernetes.io/configuration-snippet: |      rewrite ^/v6/(.*)/card/query http://foo.bar.com/v7/#!/card/query permanent;</code></pre><p>对应的nginx配置如下（configuration-snippet的配置根据location来动态添加）</p><pre><code class="language-conf">## start server foo.bar.com    server &#123;        server_name foo.bar.com ;        listen 80;        listen [::]:80;        set $proxy_upstream_name &quot;-&quot;;        ### server-snippet配置。        rewrite ^/v4/(.*)/card/query http://foo.bar.com/v5/#!/card/query permanent;        ...        location / &#123;          ### configuration-snippet配置。          rewrite ^/v6/(.*)/card/query http://foo.bar.com/v7/#!/card/query permanent;          ...      &#125;    &#125;    ## end server foo.bar.com</code></pre><h2 id="基本重定向和复杂重定向混合使用">基本重定向和复杂重定向混合使用</h2><p>实现需求：</p><pre><code class="language-bash">➜   curl ingress.pengwin.com/fooweb1: /foo➜   curl ingress.pengwin.com/barweb1: /bar➜   curl ingress.pengwin.com/aoo/fooweb1: /foo➜   curl ingress.pengwin.com/aoo/barweb1: /bar</code></pre><pre><code class="language-yaml"># 满足 curl ingress.pengwin.com/foo 和 curl ingress.pengwin.com/barapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: ingress-pengwin-com-01  annotations:    nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;     # 启用正则，否则 path 无法支持正则spec:  tls:  - hosts:      - ingress.pengwin.com    secretName: pengwin-com-tls  rules:  - host: ingress.pengwin.com    http:      paths:      - path: /(foo|bar)        backend:          service:            name: web1-service            port:              number: 8080        pathType: Prefix---# 满足  curl ingress.pengwin.com/aoo/foo 和 curl ingress.pengwin.com/aoo/bar# 仅针对 path: /aoo/ 启用特殊的重定向规则.apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: ingress-pengwin-com-02  annotations:    nginx.ingress.kubernetes.io/configuration-snippet: |        rewrite &quot;(?i)/aoo/(.*)&quot; /$1 break; spec:  tls:  - hosts:      - ingress.pengwin.com    secretName: pengwin-com-tls  rules:  - host: ingress.pengwin.com    http:      paths:      - path: /aoo/        backend:          service:            name: web1-service            port:              number: 8080        pathType: Prefix</code></pre><h2 id="SSL配置">SSL配置</h2><p>假设你需要创建一个foo.bar.com域名的私有证书</p><pre><code class="language-bash">openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout foo.bar.com.key -out foo.bar.com.crt -subj &quot;/CN=foo.bar.com/O=foo.bar.com&quot;</code></pre><p>通过k8s的secret对象导入证书</p><p>命令方式：</p><pre><code class="language-bash">kubectl create secret tls foo-bar-com --key foo.bar.com.key --cert foo.bar.com.crt</code></pre><p>声明方式：</p><pre><code class="language-yaml">apiVersion: v1kind: Secretmetadata:  name: foo-bar-com  namespace: defaultdata:  tls.crt: base64 encoded cert # 这里需要填写 base64 编码后的 cert 内容. tls.crt 不可更名  tls.key: base64 encoded key  # 这里需要填写 base64 编码后的 key 内容. tls.key 不可更名type: kubernetes.io/tls</code></pre><p>ingress调用证书</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: foo-bar-com-foo  namespace: defaultspec:  tls:  - hosts:    - foo.bar.com    secretName: foo-bar-com  rules:  - host: foo.bar.com    http:      paths:      - path: /foo        backend:          service:             name: web1-service            port:               number: 80        pathType: ImplementationSpecific</code></pre><p>ingress设置tls之后，会自动创建443的配置</p><pre><code class="language-bash">➜   kubectl get ingressNAME           CLASS   HOSTS                 ADDRESS          PORTS     AGEfoo-bar-com-foo   nginx   foo.bar.com   xxx.xxx.xxx.xxx   80, 443   10m</code></pre><h2 id="双向SSL配置">双向SSL配置</h2><p>创建ca、server、client的私钥和证书</p><p>✨命令严格执行</p><p>✨server 请求文件 /CN=<a href="http://foo.bar.com">foo.bar.com</a> 替换成自有域名</p><pre><code class="language-bash">#caopenssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 3560 -nodes -subj '/CN=Fern CA'#serveropenssl req -new -newkey rsa:4096 -keyout server.key -out server.csr -nodes -subj '/CN=foo.bar.com'openssl x509 -req -sha256 -days 3650 -in server.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out server.crt#clientopenssl req -new -newkey rsa:4096 -keyout client.key -out client.csr -nodes -subj '/CN=Fern'openssl x509 -req -sha256 -days 3650 -in client.csr -CA ca.crt -CAkey ca.key -set_serial 02 -out client.crt</code></pre><p>创建ca、server的secret对象</p><pre><code class="language-bash">kubectl create secret generic ca-secret --from-file=ca.crt=ca.crtkubectl create secret generic tls-secret --from-file=tls.crt=server.crt --from-file=tls.key=server.key</code></pre><p>ingress</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  annotations:    nginx.ingress.kubernetes.io/auth-tls-verify-client: &quot;on&quot;    nginx.ingress.kubernetes.io/auth-tls-secret: &quot;default/ca-secret&quot;    nginx.ingress.kubernetes.io/auth-tls-verify-depth: &quot;1&quot;    nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: &quot;true&quot;  name: nginx-test  namespace: defaultspec:  rules:  - host: foo.bar.com    http:      paths:      - backend:          service:            name: web1-service            port:               number: 80        path: /        pathType: ImplementationSpecific  tls:  - hosts:    - foo.bar.com    secretName: tls-secret</code></pre><p>测试</p><pre><code class="language-bash"> curl --cacert ./ca.crt --cert ./client.crt --key ./client.key https://foo.bar.com</code></pre><h2 id="ingress转发到https服务">ingress转发到https服务</h2><p>通过添加注解：<a href="http://nginx.ingress.kubernetes.io/backend-protocol:">nginx.ingress.kubernetes.io/backend-protocol:</a> “HTTPS”</p><h2 id="追加多个域名到server-name">追加多个域名到server_name</h2><p><a href="http://nginx.ingress.kubernetes.io/server-alias:">nginx.ingress.kubernetes.io/server-alias:</a> ‘~^\d+.pengwin.com$’</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: ingress-pengwin-com-foo  annotations:    nginx.ingress.kubernetes.io/server-alias: '~^\d+\.bar\.com$'spec:  tls:  - hosts:    - foo.bar.com    secretName: foo-bar-com  rules:  - host: foo.bar.com    http:      paths:      - path: /foo        backend:          service:            name: web1-service            port:              number: 8080        pathType: ImplementationSpecific</code></pre><h2 id>-------------------------------------------------------------------------------------</h2><h2 id="基本配置示例2">基本配置示例2</h2><h3 id="后端服务-2">后端服务</h3><p>0-dep-oldnginx.yaml</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: old-nginxspec:  replicas: 2  selector:    matchLabels:      run: old-nginx  template:    metadata:      labels:        run: old-nginx    spec:      containers:      - image: registry.cn-hangzhou.aliyuncs.com/acs-sample/old-nginx        imagePullPolicy: Always        name: old-nginx        ports:        - containerPort: 80          protocol: TCP      restartPolicy: Always---apiVersion: v1kind: Servicemetadata:  name: old-nginxspec:  ports:  - port: 80    protocol: TCP    targetPort: 80  selector:    run: old-nginx  sessionAffinity: None  type: NodePort</code></pre><h3 id="ingress-配置-2">ingress 配置</h3><p>1-ingress-oldnginx.yaml</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: gray-releasespec:  rules:  - host: www.example.com    http:      paths:      # 新版本服务。      - path: /        backend:          service:            name: old-nginx            port:              number: 80        pathType: ImplementationSpecific</code></pre><h3 id="测试">测试</h3><pre><code class="language-bash">➜ kubectl get ingress➜ curl -H &quot;host:www.example.com&quot; http://&lt;ingress_ip&gt;old</code></pre><h2 id="蓝绿发布">蓝绿发布</h2><p>蓝绿发布的基本逻辑：</p><ol><li>构建新版本的yaml配置清单。<ul><li>通过 label 来区分新旧程序。例如这里通过：run: old-nginx 和 run: new-nginx 来区分新旧版本</li><li>通过 <a href="http://service.metadata.name">service.metadata.name</a> 来区分新旧程序的服务。例如这里是： old-nginx 和 new-nginx</li></ul></li></ol><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: new-nginxspec:  replicas: 1  selector:    matchLabels:      run: new-nginx  template:    metadata:      labels:        run: new-nginx    spec:      containers:      - image: registry.cn-hangzhou.aliyuncs.com/acs-sample/new-nginx        imagePullPolicy: Always        name: new-nginx        ports:        - containerPort: 80          protocol: TCP      restartPolicy: Always---apiVersion: v1kind: Servicemetadata:  name: new-nginxspec:  ports:  - port: 80    protocol: TCP    targetPort: 80  selector:    run: new-nginx  sessionAffinity: None  type: NodePort</code></pre><ol start="2"><li>更新旧版本的ingress</li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ingress </tag>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞13高级版负载均衡ingress控制器</title>
      <link href="posts/7627a41d/"/>
      <url>posts/7627a41d/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><p>svc的NodePort解决了4层的对外服务提供。但7层的功能svc无法解决。例如https。</p><p>ingress controller 可以将多个域名转发到service，提供负载均衡/SSL管理/虚拟命名主机/path路由。</p><p>ingress controller 负责实现功能，不过路由配置规则由ingress对象实现。</p><p>ingress-controller 的实现有很多，具体可以查看页面，k8s官方维护了<code>aws</code>，<code>gce</code>，<code>nginx</code>。</p><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers/">https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers/</a></p><p>例如：</p><ul><li>nginx ingress 基于 nginx 的 ingress 控制器</li><li>istio ingress 基于 istio 的 ingress 控制器</li><li>traefik ingress 基于 traefik 的 ingress 控制器</li></ul><h1>特点</h1><ul><li><p>ingress 提供路由规则给 ingress controller。ingress 和后端业务位于同一个 namespace 中。另外，configMap 也可以给 ingress controller 提供配置。</p></li><li><p>ingress controller 可以单独位于一个ns，它会自动去解析 Ingress 对象。</p><ul><li>需要 ingress 对象通过 ingressClass对象关联 ingress controller。</li></ul></li><li><p>ingress 针对的主要是 http 和 https. 这些之外的端口暴漏，如果仅仅是4层，则通过 Service.Type=NodePort 或者 Service.Type=LoadBalancer 即可。</p></li></ul><h1>流量路径</h1><p>internet =&gt; Ingress =&gt; Service</p><p><img src="/posts/7627a41d/image-20201023110559218.png" alt="image-20201023110559218"></p><h1>组件</h1><p>之后的信息以 ingress-nginx 为基准。</p><h2 id="Ingress-controller">Ingress controller</h2><p>部署文档：</p><p><a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md</a></p><p>或者</p><p><a href="https://kubernetes.github.io/ingress-nginx/deploy/">https://kubernetes.github.io/ingress-nginx/deploy/</a></p><h3 id="通过helm来安装ingress-nginx">通过helm来安装ingress-nginx</h3><p>当前 ingress-nginx 的仓库地址是: <a href="https://hub.helm.sh/charts/ingress-nginx/ingress-nginx">https://hub.helm.sh/charts/ingress-nginx/ingress-nginx</a></p><pre><code class="language-bash">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginxhelm repo update</code></pre><h3 id="配置ingress-nginx">配置ingress-nginx</h3><pre><code class="language-bash">## 查看 repo:ingress-nginx 下 chart：ingress-nginx 的所有版本➜   helm search repo ingress-nginx/ingress-nginx  -lNAME                            CHART VERSION   APP VERSION     DESCRIPTIONingress-nginx/ingress-nginx     4.0.19          1.1.3           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.18          1.1.2           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.17          1.1.1           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.16          1.1.1           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.15          1.1.1           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.13          1.1.0           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.12          1.1.0           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.11          1.1.0           Ingress controller for Kubernetes using NGINX a...ingress-nginx/ingress-nginx     4.0.10          1.1.0           Ingress controller for Kubernetes using NGINX a......</code></pre><p>✨需要注意的是，CHART VERSION 和 APP VERSION 都不是 nginx 版本。</p><p>CHART VERSION 是这个图表的版本，APP VERSION 是 ingress-nginx 的版本。</p><p>对应关系看：</p><p><a href="https://github.com/kubernetes/ingress-nginx#support-versions-table">https://github.com/kubernetes/ingress-nginx#support-versions-table</a></p><p>✨建议安装倒数第二个chart版本。</p><pre><code class="language-bash">➜   version=4.0.18➜   # helm fetch ingress-nginx/ingress-nginx ➜   helm show values ingress-nginx/ingress-nginx --version=$&#123;version&#125; &gt; values.yaml</code></pre><p>helm 通过 values.yaml 去修改 template。</p><p>修改<code>values.yaml</code>，参考下列信息：</p><ul><li><p>command 的示例配置是之前的一种控制器关闭时防止丢失链接的方法。非必须添加。其中<code>sleep 5</code>是必须的，但原因不明。没有它，控制器关闭的时候会丢失连接。</p><ul><li>不过现在官方已经加入了  /wait-shutdown 命令来解决这个问题。</li></ul></li><li><p>ingressClass 单 ingress 可以不用指定。如果创建多个 ingress-nginx，则在一个集群中名称不可以相同。</p><ul><li>ingress 对象通过这个名称来将自身规则写入到对应的 ingress-controller。</li></ul></li><li><p>metrics 开启 prometheus 指标。示例配置里，监控指标服务类型是 clusterip，如果你需要集群外访问，则需要设置为nodeport</p></li></ul><pre><code class="language-yaml">controller:  ......  #lifecycle:    #preStop:      #exec:        #command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 5; /usr/local/openresty/nginx/sbin/nginx -c /etc/nginx/nginx.conf -s quit; while pgrep -x nginx; do sleep 1; done&quot;]  ingressClass: nginx  metrics:    port: 10254    # if this port is changed, change healthz-port: in extraArgs: accordingly    enabled: true    service:      annotations: #&#123;&#125;        prometheus.io/scrape: &quot;true&quot;        prometheus.io/port: &quot;10254&quot;</code></pre><p>✨应该保存好 values.yaml，便于之后优雅的0停机升级。</p><h3 id="安装ingress-nginx">安装ingress-nginx</h3><pre><code class="language-bash">➜   helm install ingress-nginx-001 ingress-nginx/ingress-nginx --version $&#123;version&#125; -n ingress-nginx --create-namespace -f values.yaml</code></pre><pre><code class="language-bash">kubectl get all -n ingress-nginx===NAME                                            READY   STATUS    RESTARTS   AGEpod/ingress-nginx-controller-5886685d54-hf6l6   1/1     Running   0          68mNAME                                         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGEservice/ingress-nginx-controller             LoadBalancer   10.96.210.139   &lt;pending&gt;     80:32489/TCP,443:30936/TCP   68mservice/ingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;        443/TCP                      68mNAME                                       READY   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/ingress-nginx-controller   1/1     1            1           68mNAME                                                  DESIRED   CURRENT   READY   AGEreplicaset.apps/ingress-nginx-controller-5886685d54   1         1         1       68m</code></pre><p>我们可以看到 ingress-nginx 控制器通过<code>deployment.apps/ingress-nginx-controller </code>创建了一个 <code>pod/ingress-nginx-controller-5886685d54-hf6l6</code>。这个 pod 其实就是一个 nginx. 它通过 ingress 定义的路由规则，来动态的变更nginx配置，从而正确的将流量转发给后端应用的service对象.</p><pre><code class="language-bash">kubectl describe pod/ingress-nginx exec -it pod/ingress-nginx-controller-5886685d54-hf6l6===    Requests:      cpu:      100m      memory:   90Mi</code></pre><p>当前配置默认没有做limit限制. 所以需要关注ingress-nginx创建的pod所在物理节点的性能是否可以满足. 建议强制绑定到多个node节点（通过污点）， 这批节点专门跑用来进行ingress控制器.</p><p>通过登陆pod，可以查看转化后的nginx配置.</p><pre><code class="language-bash">kubectl --namespace ingress-nginx exec -it pod/ingress-nginx-controller-5886685d54-hf6l6 -- /bin/bash==cat /etc/nginx/nginx.conf</code></pre><h2 id="ingress">ingress</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></p><p>client =&gt; <a href="http://one.foo.com/one">http://one.foo.com/one</a> =&gt; Ingress = &gt; Service(one):8001 =&gt; pod</p><p>client =&gt; http://*.foo.com/other =&gt; Ingress  =&gt; Service(other):8002 =&gt; pod</p><p>ingress编写路由规则，并由ingress-controller进行转化。</p><p>下面是一个示例：</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: minimal-ingress  namespace: default#  annotations:#    nginx.ingress.kubernetes.io/rewrite-target: /spec:  ingressClassName: nginx  rules:  - host: one.foo.com    http:      paths:      - path: /one        pathType: Prefix        backend:          service:            name: one            port:              number: 8001  - host: &quot;*.foo.com&quot;    http:      paths:      - pathType: Prefix        path: &quot;/other&quot;        backend:          service:            name: other            port:              number: 8002</code></pre><p>🌟请注意, <code>networking.k8s.io/v1</code>必须是v1.19+才可以使用, 如果你不是,则应该使用<code>networking.k8s.io/v1beta1</code></p><p>❤<code>spec.ingressClassName</code> 用来指定 ingressClass 对象。</p><p>为了方便理解 ingress 的配置项，我参考 nginx 配置，来进行了一些横向对比：</p><p>nginx.servername:  这里是 spec.rules[].host</p><p>nginx.location: 这里是 spec.rules.http.paths[].path</p><p>nginx.upstream: 这里是 spec.rules.http.paths[].backend</p><h3 id="关于-spec-rules-host">关于 spec.rules[].host</h3><p>这里有一些注意事项. host 支持统配匹配. 不过 <code>*</code>不能跨级匹配. 例如,</p><p>*.abc.com 可以匹配 <a href="http://one.abc.com">one.abc.com</a> 但是不能匹配 <a href="http://two.one.abc.com">two.one.abc.com</a>.</p><h3 id="关于-spec-rules-http-paths-path">关于 spec.rules.http.paths[].path</h3><p>这里有一些注意事项.如上例子所述. pathType 定义了 path 的类型. 它有三个类型:</p><ul><li><p>ImplementationSpecific: 根据选用的 ingress class 来定性.这是默认类型.</p></li><li><p>Exact: 严格匹配. 不能有一点不同.</p></li><li><p>Prefix: 基于&quot;/&quot;分段进行前缀匹配. 例如,</p><ul><li>/aaa/bbb/ 可以匹配任何 /aaa/bbb/ 开头的, 或者 /aaa/bbb.  因此 /aaa/bbbb 是不能匹配的.</li><li>/aaa/ 可以匹配任何 /aaa/ 开头的, 或者 /aaa. 因此, /aaaa 是不能匹配的.</li><li>/ 可以匹配任何 / 开头的. 因此, 匹配所有.</li></ul><p>💥请注意, Prefix 类型下.你写的匹配字符串首尾都需要加<code>/</code>, 如果你尾部没有加<code>/</code>, 那么 ingress 在匹配的时候, 也会帮你加上<code>/</code>.</p></li></ul><p>当Exact和Prefix混用的时候, 如果一个请求同时匹配了这两个类型下的path, 那么Exact 优先级更高.</p><h3 id="构建TLS">构建TLS</h3><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress/#tls">https://kubernetes.io/zh/docs/concepts/services-networking/ingress/#tls</a></p><p><s>默认情况下，如果为 Ingress 启用了 TLS，ingress-controller 将使用 308 永久重定向响应将 HTTP 客户端重定向到 HTTPS 端口 443。</s></p><p>💥<code>ingress.spec.tls.hosts</code>必须完全包含<code>ingress.spec.rules.hosts</code></p><h4 id="手动证书">手动证书</h4><ol><li><h4 id="创建-Secret-对象-导入证书文件">创建 Secret 对象, 导入证书文件</h4><pre><code class="language-yaml">apiVersion: v1kind: Secretmetadata:  name: foo-com-tls  namespace: defaultdata:  tls.crt: base64 encoded cert # 这里需要填写 base64 编码后的 cert 内容. tls.crt 不可更名  tls.key: base64 encoded key  # 这里需要填写 base64 编码后的 key 内容. tls.key 不可更名type: kubernetes.io/tls</code></pre><p>可以通过<code>cat 证书文件|base64</code>获取编码后的内容</p></li><li><h4 id="在Ingress对象中调用-Secret">在Ingress对象中调用 Secret</h4><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: tls-example-ingressspec:  tls:  - hosts:      - https-example.foo.com    secretName: foo-com-tls  rules:  - host: https-example.foo.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: service1            port:              number: 80</code></pre><p>…backend.service.name svc对象名</p><p>…backend.service.port svc对象端口</p></li></ol><p>不同的Ingress控制器对于TLS这块也有一定差异.具体以控制器本身说明为准.</p><h4 id="自动证书">自动证书</h4><p>构建cert-manager证书管理器。具体安装参见证书管理器文档。</p><h1>问题点</h1><h2 id="本地裸机解决svc-lb类型的问题">本地裸机解决svc lb类型的问题</h2><p>我们可以看到通过helm默认安装的 ingress crotroller 的 Service 对象是LoadBalancer类型. 这种类型我们用于云端环境. 而我这里测试环境是裸机,因此你可以看到这里它一直无法获取到<code>EXTERNAL-IP</code>.</p><p>这意味着，我们无法将流量通过<code>service/ingress-nginx-controller</code>传递到<code>pod/ingress-nginx-controller-5886685d54-hf6l6</code>.🙄</p><p>如何解决这个问题.k8s官方文档这里提供了多种裸机安装下的解决方案：</p><p><a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/</a></p><p>这里通过 <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb</a> 纯软件方案来解决 LB 类型的 <code>EXTERNAL-IP</code>获取问题. 其它方案并不是采用的 LB 类型, 因此这里不再叙说.</p><p>⚠️metallb 方案在当前并不是一个成熟的方案,当然它应该是可用的😄 所以用于本地测试环境是可以的. 线上环境建议直接用云服务的LB即可.</p><h3 id="安装metallb模拟LB">安装metallb模拟LB</h3><p><a href="https://metallb.universe.tf/installation/#installation-with-helm">https://metallb.universe.tf/installation/#installation-with-helm</a></p><p>metallb 模拟了云环境中的负载均衡器功能.</p><p>🌟If you’re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode.</p><p>如果你是ipvs 且 k8s 版本在1.14.2以上, 那么你需要启动严格arp模式.</p><pre><code class="language-bash">kubectl edit configmap -n kube-system kube-proxy===ipvs:  strictARP: true</code></pre><p>开始安装</p><pre><code class="language-bash">helm repo add metallb https://metallb.github.io/metallbhelm fetch metallb/metallb</code></pre><p>修改 metalla/values.yaml, 添加地址池，用来给svc下发ip。请记住, 这些地址必须是没有被其它资源占用的.</p><pre><code class="language-bash">configInline:  address-pools:   - name: default     protocol: layer2     addresses:     - 10.200.16.11 - 10.200.16.19</code></pre><p>通过 helm 安装</p><pre><code class="language-bash">kubectl create ns metallb-systemhelm install metallb --namespace metallb-system -f metallb/values.yaml metallb/metallb# 下面这个命令仅在你初始安装的时候需要运行kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=&quot;$(openssl rand -base64 128)&quot;</code></pre><p>下面,让我们再看看 ingress crotroller 的 svc 对象信息.</p><pre><code class="language-bash">kubectl get svc -n ingress-nginx===NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                      AGEingress-nginx-controller             LoadBalancer   10.96.210.139   10.200.16.11   80:32489/TCP,443:30936/TCP   88mingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;         443/TCP                      88m</code></pre><p>终于<code>svc/ingress-nginx-controller</code>拿到了一个<code>EXTERNAL-IP</code>, 现在你就可以将域名解析到<code>EXTERNAL-IP</code>, 并通过这个域名来访问你的应用了.(而且它必须是能通过80访问的.)😄</p><p>假设已经设置编写一个 ingress，则应用后如下所示：</p><pre><code class="language-bash">kubectl get ingress -n it===NAME                   CLASS    HOSTS          ADDRESS        PORTS   AGE test-it-local-ingress   &lt;none&gt;   test.it.local   10.200.16.11   80      106m</code></pre><p>可以看到 ingress 对象中也显示了 metallb 的下发的ip.</p><p>你可以在任意一个节点上通过 ipvs 规则来看到转发情况.</p><pre><code class="language-bash">ipvsadm -L -n===TCP  10.200.16.11:80 rr  -&gt; 10.97.2.57:80                Masq    1      0          0TCP  10.200.16.11:443 rr  -&gt; 10.97.2.57:443               Masq    1      0          0</code></pre><p>这里 10.97.2.57 是 ingress 控制器的 pod ip.</p><blockquote><p>metallb升级：<a href="https://metallb.universe.tf/installation/#upgrade">https://metallb.universe.tf/installation/#upgrade</a></p></blockquote><h2 id="暴漏非80和非443端口">暴漏非80和非443端口</h2><p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/">https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/</a></p><p>默认你会发现ingress无法暴漏80和443之外的端口.这是因为默认ingress-nginx并没有开启4层转发.</p><h3 id="开启ingress-nginx中pod的4层转发">开启ingress-nginx中pod的4层转发</h3><p>检查 deployment: ingress-nginx 中 pod 模板是否开启了下列参数</p><pre><code class="language-bash">deploymentName=`kubectl get all -n ingress-nginx | grep deployment | awk '&#123;print $1&#125;'`kubectl edit $&#123;deploymentName&#125; -n ingress-nginx===- args:    - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services    - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</code></pre><h3 id="创建暴漏端口的配置文件">创建暴漏端口的配置文件</h3><pre><code class="language-yaml">apiVersion: v1kind: ConfigMapmetadata:  name: tcp-services  namespace: ingress-nginxdata:  8080: &quot;it/test-it-local-svc:8080&quot;</code></pre><p>这里8080: &quot;it/test-it-local-svc:8080&quot;<code>第一个8080指的是ingress-nginx的pod对象里需要监听的端口, </code>it/test-it-local-svc:8080` 指的是ns:it下的svc对象test-it-local-svc的8080端口</p><h3 id="添加暴漏端口到ingress-nginx中svc对象">添加暴漏端口到ingress-nginx中svc对象</h3><pre><code class="language-bash">kubectl get all -n ingress-nginx | grep service | awk '&#123;print $1&#125;' kubectl edit service/ingress-nginx-controller -n ingress-nginx===  - name: test-8080    port: 8080    protocol: TCP    targetPort: 8080</code></pre><p>三步过后, 不出意外, 你将可以通过ingress-nginx的pod对象中的nginx.conf文件看到tcp的转发配置.</p><pre><code class="language-bash">        # TCP services        server &#123;                preread_by_lua_block &#123;                        ngx.var.proxy_upstream_name=&quot;tcp-it-test-it-local-svc-8080&quot;;                &#125;                listen                  8080;                proxy_timeout           600s;                proxy_pass              upstream_balancer;        &#125;</code></pre><h2 id="ingress无法创建，报找不到控制器的svc">ingress无法创建，报找不到控制器的svc</h2><pre><code>Error from server (InternalError): error when creating &quot;cms-ingress.yaml&quot;: Internal error occurred: failed calling webhook &quot;validate.nginx.ingress.kubernetes.io&quot;: Post &quot;https://ingress-nginx-controller-admission-nginx.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s&quot;: service &quot;ingress-nginx-controller-admission-nginx&quot; not found</code></pre><p>上述问题，常见于通过helm重建ingress-nginx之后，这是因为<code>ValidatingWebhookConfiguration</code>陈旧配置没有被删除，以至于ingress找到了错误的配置.</p><pre><code class="language-bash">➜   kubectl get ValidatingWebhookConfigurationNAME                                      WEBHOOKS   AGEingress-nginx-admission                   1          46mingress-nginx-admission-nginx             1          29dingress-nginx-doc-it-local-admission      1          448d</code></pre><p>如上所示，我这里有两个陈旧的配置<code>ingress-nginx-admission-nginx </code>和<code>ingress-nginx-doc-it-local-admission</code>，删除即可.</p><h1>优雅更新ingress控制器</h1><h2 id="启动临时控制器">启动临时控制器</h2><p>临时控制器用于临时接收现有流量，创建临时控制器，并且版本和配置与原控制器一致。</p><pre><code class="language-bash">kubectl create ns ingress-nginx-temp# 原控制器版本oldVersion=# 查看原控制器的 valueshelm get values ingress-nginx# 通过原控制器的 values.yaml 创建原控制器版本的临时版本helm install ingress-nginx-temp ingress-nginx/ingress-nginx --version $&#123;oldVersion&#125; -n ingress-nginx-temp -f values-$&#123;oldVersion&#125;.yaml</code></pre><h2 id="切换DNS，指向临时控制器">切换DNS，指向临时控制器</h2><p>✨通过观察原控制器日志，等待原控制器流量结束。</p><h2 id="导出新控制器配置">导出新控制器配置</h2><pre><code class="language-bash"># 新版本newVersion=# 导出新版本的 ingress-nginx 的 values.yaml helm show values ingress-nginx/ingress-nginx --version=$&#123;newVersion&#125; &gt; values-$&#123;newVersion&#125;.yaml </code></pre><h2 id="比对新旧配置">比对新旧配置</h2><p>将旧的配置转移到新配置中，并确认新配置中有没有额外的参数</p><h2 id="更新原控制器">更新原控制器</h2><pre><code class="language-bash"># values.yaml 更新后的配置helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx --version $&#123;newVersion&#125; -n ingress-nginx -f values-$&#123;newVersion&#125;.yaml</code></pre><p>查看结果</p><pre><code class="language-bash">kubectl get deployment nginx-ingress-controller -n ingress-nginx -o yaml</code></pre><h2 id="切换DNS，指向原控制器">切换DNS，指向原控制器</h2><p>✨通过观察临时控制器日志，等待临时控制器流量结束</p><h2 id="删除临时控制器">删除临时控制器</h2><pre><code class="language-bash">helm delete --purge nginx-ingress-temp --namespace ingress-nginx-temphelm delete ns ingress-nginx-temp</code></pre><h1>release 卸载</h1><pre><code class="language-bash">helm uninstall ingress-nginx</code></pre><h1>release 版本</h1><pre><code class="language-bash">helm history ingress-nginx</code></pre><h1>release 回滚</h1><pre><code class="language-bash">helm rollback ingress-nginx &lt;history_num&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ingress </tag>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞12-2有状态服务StatefulSet</title>
      <link href="posts/4bf29cf0/"/>
      <url>posts/4bf29cf0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>当我们构建一个有状态的应用的时候, 例如 mysql / ftp / 包含session的管理界面等. 我们可能会有下列预期:</p><ul><li>稳定的、唯一的网络标识符。</li><li>稳定的、持久的存储。</li><li>有序的、优雅的部署和缩放。</li><li>有序的、优雅的删除和终止。</li><li>有序的、自动的滚动更新。</li></ul><p>StatefulSet 就是干这个的。不过在实际的项目中，其实我们还是很少会去直接通过 StatefulSet 来部署我们的有状态服务的，除非你自己能够完全能够 hold 住，对于一些特定的服务，我们可能会使用更加高级的 Operator 来部署，比如 etcd-operator、prometheus-operator 等等，这些应用都能够很好的来管理有状态的服务，而不是单纯的使用一个 StatefulSet 来部署一个 Pod 就行，因为对于有状态的应用最重要的还是数据恢复、故障转移等等。</p><h2 id="例子">例子</h2><pre><code class="language-yaml:">apiVersion: v1kind: Servicemetadata:  name: nginx  labels:    app: nginxspec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata:  name: webspec:  serviceName: &quot;nginx&quot;  replicas: 2  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: k8s.gcr.io/nginx-slim:0.8        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates:  - metadata:      name: www    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      resources:        requests:          storage: 1Gi</code></pre><ul><li>svc对象必须先创建</li><li>StatefulSet.spec.serviceName 设定 svc 名称</li><li>StatefulSet.spec.volumeClaimTemplates PVC模板，可以自动创建一个PVC.<ul><li>如果没有空闲的PV或者没有StorageClass，则需要先提供。</li></ul></li></ul><h2 id="特点">特点</h2><ul><li>Pod 标识：拥有唯一的有序索引和稳定的网络身份。pod 被部署的时候其名字是按照<code>&lt;statefulset.name&gt;-&lt;index.num&gt;</code>创建的，例如(web-0, web-1)。这个名字就是他们的hostname，且不管 pod 如何调度，都永不变。</li><li>Service 必须是 headless。StatefulSet 使用 headless 结合Pod标识来构建 Pod 子域名。其Pod子域名格式：<code>$(statefulset.name)-$(index.name).$(svc.name).$(ns.name).svc.cluster.local</code></li><li>pod 不管怎么删, pod与pvc之间的关系都不会混乱.</li><li>pod 之间可以通过固定的Pod子域名互相访问.</li><li>启动 pod 的时候, 按照索引, 从 0 开始. 一个一个启动. 不会同时启动多个. (这是默认行为)</li><li>删除 pod 的时候, 按照索引倒序删除, 从最后一个开始, 一个一个删除. 不会同时删除多个. (这是默认行为)</li><li>删除 pod 的时候, pvc 和 pv 并不会删除。且PVC的名称也是有序的。$(<a href="http://VCT.name">VCT.name</a>)-$(<a href="http://statefulset.name">statefulset.name</a>)-${index.num}</li></ul><blockquote><p>你可以通过设置statefulset管理策略, 来改变上述默认行为</p><p><a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy">https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy</a></p><p><code>StatefulSet.spec.podManagementPolicy</code>设定为：Parallel 。就可以并发的创建Pod和删除Pod。默认这个值是：OrderedReady</p></blockquote><h2 id="扩展-缩放">扩展/缩放</h2><p>扩展: <code>kubectl scale sts web --replicas=5</code></p><p>缩放: <code>kubectl patch sts web -p '&#123;&quot;spec&quot;:&#123;&quot;replicas&quot;:3&#125;&#125;'</code></p><p>当然你也可以通过修改配置文件里 replicas 的值来进行变更.</p><h2 id="更新">更新</h2><p>有状态应用的更新和无状态应用的更新有些差异.</p><p>有状态应用的更新策略(spec.updateStrategy)是 RollingUpdate 和 OnDelete. 其中 RollingUpdate 是默认策略, 这个是自动滚动更新. 而OnDelete的意思是只有你手动删除了pod才会更新.</p><p>更新是针对pod模板的. 例如:</p><pre><code class="language-bash">kubectl patch statefulset web --type='json' -p='[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;gcr.io/google_containers/nginx-slim:0.8&quot;&#125;]'</code></pre><blockquote><p>这里更新了nginx的镜像.</p></blockquote><p>更新遵循以下原则:</p><ul><li><p>所有的 pod 需要是就绪状态.</p></li><li><p>更新采用索引倒序进行(即从索引最后一个号码开始更新), 并且是一个一个的更新.</p></li><li><p>当更新失败的时候, 已经收到更新的pod将保持更新后的版本, 没有开始更新的pod将恢复到更新前的版本.</p></li></ul><p>🌟需要注意的是, 如果你的更新文件(二进制文件故障或者配置文件)有问题从而导致更新失败, 你可能需要强制回滚. 以下是说明信息:</p><p>即你恢复了更新前的模板,却发现statefulset依然不正常, 则你需要<strong>手动删除</strong>所有由错误模板产出的pod.</p><p>在这之后, statefulset将会采用更新前的模板重建pod.</p><p>(<a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#forced-rollback">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#forced-rollback</a>)</p><h2 id="阶段更新-staging-an-update">阶段更新(staging an update)</h2><p>有时候我们可能并不想一次更新所有, 此时可以进行阶段更新.</p><p>阶段更新的意思就是通过在索引上设置一个点. 当pod的索引大于等于这个点的时候, 才会更新. (默认点是索引0，即所有Pod都更新)</p><pre><code class="language-bash">kubectl patch statefulset web -p '&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:3&#125;&#125;&#125;&#125;'</code></pre><p>这个意思是设置索引3为阶段分割点.</p><p>如果你想回到默认更新, 则只需要调整分割点, 重新执行上述命令即可.</p><h2 id="删除">删除</h2><p>删除分为联级删除和非联级删除.</p><p>联级删除就是 statefulset 和 pod 都删除. 这是默认行为.</p><p>非联级删除, 只会删除 statefulset. 你可以通过删除的时候附加<code>--cascade=false</code>开启它.</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> StatefulSet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞12-1控制器DaemonSet和Job和CronJob</title>
      <link href="posts/63d8d3d4/"/>
      <url>posts/63d8d3d4/</url>
      
        <content type="html"><![CDATA[<h2 id="DaemonSet">DaemonSet</h2><h3 id="基础">基础</h3><p>DaemonSet 确保符合规则的 k8s node 上都存在一份 pod。常用来构建节点常驻性的app. 例如监控节点的app， 或者采集节点日志的app，以及网络app等。</p><h3 id="特点">特点</h3><ul><li><p>调度在所有符合条件的node上。</p></li><li><p>当DaemonSet创建或删除， 则指定节点上的 pod 都会创建或删除。</p></li><li><p>当一个符合DaemonSet规则的节点添加到集群或从集群中删除， 则 pod 会自动被添加/删除。</p></li><li><p>当DaemonSet被删除， 则指定节点上的 pod 都会被删除.</p></li><li><p>pod模板中的 RestartPolicy 必须是默认值， 也就是 Always.</p></li><li><p>创建后.spec.selector 不可修改.</p></li></ul><h3 id="例子">例子</h3><pre><code class="language-yaml">apiVersion: apps/v1kind: DaemonSetmetadata:  name: fluentd-elasticsearch  namespace: kube-system  labels:    k8s-app: fluentd-loggingspec:  selector:    matchLabels:      name: fluentd-elasticsearch  template:    metadata:      labels:        name: fluentd-elasticsearch    spec:      tolerations:      # 添加容忍标签，允许将 Pod 调度到附加了 NoSchedule 的 master 主机      - key: node-role.kubernetes.io/master        effect: NoSchedule      containers:      - name: fluentd-elasticsearch        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2        resources:          limits:            memory: 200Mi          requests:            cpu: 100m            memory: 200Mi        volumeMounts:        - name: varlog          mountPath: /var/log        - name: varlibdockercontainers          mountPath: /var/lib/docker/containers          readOnly: true      terminationGracePeriodSeconds: 30      volumes:      - name: varlog        hostPath:          path: /var/log      - name: varlibdockercontainers        hostPath:          path: /var/lib/docker/containers</code></pre><p>这是一个日志采集pod. 它将k8s集群节点的 /var/log 和 /var/lib/docker/containers 挂载到 pod 中. 这样， elasticsearch 就可以获取到k8s集群所有节点的信息了.</p><h3 id="调度">调度</h3><p>默认情况下，DaemonSets的Pod会调度到每一个Node上。</p><p>与调度选择有关的知识，分三类：</p><ul><li>节点污点和Pod容忍度</li><li>节点标签选择器</li><li>节点亲和性</li></ul><h3 id="节点污点和容忍度">节点污点和容忍度</h3><p>DaemonSet会给Pod自动附加容忍度，从而可以让Pod一直在节点上运行。 (截至到v1.19)。以下是自动附加的容忍度：</p><pre><code class="language-bash">node.kubernetes.io/not-readynode.kubernetes.io/unreachablenode.kubernetes.io/disk-pressurenode.kubernetes.io/memory-pressurenode.kubernetes.io/unschedulablenode.kubernetes.io/network-unavailable</code></pre><p>上述情况基本保证了DaemonSet的pod不会因各种意外情况导致pod被驱逐.</p><blockquote><p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</a></p><p>k8s通过污点来剔除某些故障节点或构建一些专属节点，防止Pod调度到存在污点的节点上。</p><p>k8s通过容忍度来确保某些pod可以忽略污点，从而调度到存在污点的节点上。</p><p>在正常情况下. 没有任何容忍度的pod不会被调度到存在污点的node上. (当出现节点故障时候， 默认会附加一个容忍度， 容忍度失效300秒)</p></blockquote><h3 id="通过节点标签-nodeSelector-实现部分调度">通过节点标签(nodeSelector)实现部分调度</h3><p>通过下列命令获知节点标签:</p><pre><code class="language-bash">kubectl describe node k8s01Labels:             beta.kubernetes.io/arch=amd64                    beta.kubernetes.io/os=linux                    kubernetes.io/arch=amd64                    kubernetes.io/hostname=k8s01                    kubernetes.io/os=linux                    node-role.kubernetes.io/control-plane=                    node-role.kubernetes.io/master=                    node.kubernetes.io/exclude-from-external-load-balancers=</code></pre><p><code>.spec.template.spec.nodeSelector</code>可以让你仅在匹配的node上创建pod.</p><p>例如: 仅调度到拥有ssd磁盘标签的节点</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  nodeSelector:    disktype: ssd</code></pre><h3 id="通过节点亲和性-nodeAffinity-实现部分调度">通过节点亲和性(nodeAffinity)实现部分调度</h3><p><code>.spec.affinity.nodeAffinity</code> ， 它有以下类型: (每一个类型由两部分意思组成)</p><p>requiredDuringScheduling<strong>IgnoredDuringExecution</strong><br>表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署后运行期间，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。</p><p>preferredDuringScheduling<strong>IgnoredDuringExecution</strong><br>表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: with-node-affinityspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: kubernetes.io/e2e-az-name            operator: In            values:            - e2e-az1            - e2e-az2      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: another-node-label-key            operator: In            values:            - another-node-label-value  containers:  - name: with-node-affinity    image: gcr.io/google_containers/pause:2.0</code></pre><p>这个配置的意思是pod【必须被调度】到拥有标签【<a href="http://kubernetes.io/e2e-az-name=e2e-az1%E6%88%96kubernetes.io/e2e-az-name=e2e-az2%E3%80%91">kubernetes.io/e2e-az-name=e2e-az1或kubernetes.io/e2e-az-name=e2e-az2】</a> 的节点。与此同时， 还将在上述条件的基础上， <strong>优先调度</strong>到额外拥有标签为another-node-label-key=another-node-label-value的节点上。</p><h3 id="访问">访问</h3><p>你可以通过构建一个 Headless Service 并通过 endpoint  来访问各个 pod.</p><h2 id="Job">Job</h2><pre><code class="language-yaml"># job-para-demo.yamlapiVersion: batch/v1kind: Jobmetadata:  name: job-para-testspec:  activeDeadlineSeconds: 100  # Job 对象运行的最长时间。超过这个时间，所有 job 创建的 pod 都会被删除。且 job 状态变成 DeadlineExceeded。  backoffLimit: 6 # 失败后的重试次数。 Job 控制器重新创建 Pod 的间隔是呈指数增加的，即下一次重新创建 Pod 的动作会分别发生在 10s、20s、40s… 后。  parallelism: 2  # 并行运行的 pod 数  completions: 8  # 本次 Job 需要执行8次  template:    spec:      restartPolicy: Never      containers:      - name: test-job        image: busybox        command: [&quot;echo&quot;, &quot;test paralle job!&quot;]</code></pre><h2 id="CronJob">CronJob</h2><pre><code class="language-yaml"># cronjob-demo.yamlapiVersion: batch/v1kind: CronJobmetadata:  name: cronjob-demospec:  schedule: &quot;*/1 * * * *&quot;  successfulJobsHistoryLimit: 3  # 默认保留成功的历史job数量  failedJobsHistoryLimit: 1      # 默认保留失败的历史job数量  jobTemplate:    spec:      template:        spec:          restartPolicy: OnFailure          containers:          - name: hello            image: busybox            args:            - &quot;bin/sh&quot;            - &quot;-c&quot;            - &quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i; done&quot;</code></pre><pre><code class="language-bash">➜  ~ kubectl delete cronjob cronjob-democronjob &quot;cronjob-demo&quot; deleted</code></pre><p>👙需要注意的是这将会终止正在创建的 Job，但是运行中的 Job 将不会被终止，不会删除 Job 或 它们的 Pod。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> DaemonSet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞11-2namespace资源约束LimitRange</title>
      <link href="posts/ed4c29d5/"/>
      <url>posts/ed4c29d5/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p><a href="https://kubernetes.io/docs/concepts/policy/limit-range/">https://kubernetes.io/docs/concepts/policy/limit-range/</a></p><p>namespace 资源约束(LimitRange)提供默认的Pod资源约束，并防止命名空间内的Pod资源配置超出管理员允许的预期范围。若超出则不允许创建，从而让我们可以更方便的管理资源.</p><h2 id="特点">特点</h2><ul><li>提供 pod 的默认 request 约束.</li><li>提供 pod 的默认 limit 约束.</li><li>提供默认的 pvc.</li><li>LimitRange配置策略的构建和变更仅影响之后的pod创建。已创建Pod不受影响。</li></ul><h3 id="写法">写法</h3><pre><code class="language-yaml">apiVersion: v1kind: LimitRangemetadata:  name: cpu-mem-storage-min-max-defaultspec:  limits:  - type: Container    max:      cpu: &quot;1000m&quot;      memory: &quot;1G&quot;    min:      cpu: &quot;50m&quot;      memory: &quot;50M&quot;    default:      cpu: &quot;250m&quot;      memory: &quot;250M&quot;    defaultRequest:      cpu: &quot;50m&quot;      memory: &quot;50M&quot;  - type: PersistentVolumeClaim    max:      storage: 30G    min:      storage: 8G</code></pre><pre><code class="language-bash">kubectl apply -f limitrange-default.yaml --namespace=default</code></pre><p>上述的意思基本遵循：</p><ul><li>不设置Pod约束，则Pod约束limit级走default，Pod约束request级走defaultRequest</li><li>设置Pod约束，则Pod约束不能低于min，不能大于max，且request级不能大于limit级.</li><li>在继承上述条件的基础下，若Pod只设置了limit约束，则Pod的request约束直接等同于Pod的limit约束</li></ul><p>上述例子为 default 命名空间加了一个资源策略.效果如下:</p><ul><li><p>当pod没有任何限制的时候, pod 规则如下遵循 default 和 defaultRequest 的属性</p><ul><li><p>limit: cpu=250m mem=250M</p></li><li><p>request: cpu=50m mem=50M</p></li><li><p>8G &lt; pvc &lt; 30G</p><p>会全部走limitrange的默认值</p></li></ul></li><li><p>当pod仅设置了request: cpu=100m mem=150M的时候, pod规则如下</p><ul><li>limit: cpu=250m mem=250M</li><li>request: cpu=100m mem=150M</li><li>8G &lt; pvc &lt; 30G</li></ul></li><li><p>当pod仅设置了limit，且limit值不超过namespace的max的时候, pod规则如下</p><blockquote><p>假设这里pod的limit是 cpu=1000m mem=1G</p></blockquote><ul><li>limit: cpu=1000m mem=1G</li><li>request: cpu=1000m mem=1G</li><li>8G &lt; pvc &lt; 30G</li></ul><p>即仅当设置limit级的Pod约束时，request级Pod约束等同于limit级</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> 资源约束 </tag>
            
            <tag> namespace </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞11-1pod资源约束</title>
      <link href="posts/64fa9c84/"/>
      <url>posts/64fa9c84/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a></p><p>资源约束在我们平时使用的时候主要是cpu和内存层面, 以及本地临时存储(emptyDir)</p><p>k8s资源约束分为两种: request(软约束) 和 limits(硬约束)</p><p>k8s通过request(软约束)将Pod调度到有资源的Node上，确保了pod中的容器至少可以使用这么多资源. 不过当节点如果没有其它容器,则此容器可以突破request限制.</p><p>k8s通过limits(硬约束) 将调度完毕的Pod所使用的资源限制在limits之内。如果容器请求的内存大于了limits,则会收到oom错误。</p><blockquote><p>如果只设置了limits，则k8s会将request的值自动设置为和limits一致。</p></blockquote><h2 id="写法">写法</h2><ul><li><code>spec.containers[].resources.limits.cpu </code>cpu限制</li><li><code>spec.containers[].resources.limits.memory</code> 内存限制</li><li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code>   不常用, 可以忽略</li><li><code>spec.containers[].resources.limits.ephemeral-storage</code> empty临时存储限制</li><li><code>spec.containers[].resources.requests.cpu</code></li><li><code>spec.containers[].resources.requests.memory</code></li><li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt; </code> 不常用, 可以忽略</li><li><code>spec.containers[].resources.requests.ephemeral-storage</code></li></ul><h2 id="单位">单位</h2><p>k8s将一个超线程称为一个vcpu. 1vcpu=1000m. 我们在定义资源限制时, 应该始终用 m 作为单位.假设你限制0.5个vcpu,则填写500m.</p><p>k8s的内存和临时存储单位和平时我们所用的没什么区别. 你只需要记住 K/M/G/T/P/E 这些即可. 例如, 100M就是100兆</p><p>一个例子:</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: frontendspec:  containers:  - name: app    image: images.my-company.example/app:v4    resources:      requests:        memory: &quot;64M&quot;        cpu: &quot;250m&quot;        ephemeral-storage: &quot;2Gi&quot;      limits:        memory: &quot;128M&quot;        cpu: &quot;500m&quot;        ephemeral-storage: &quot;4Gi&quot;  - name: log-aggregator    image: images.my-company.example/log-aggregator:v6    resources:      requests:        memory: &quot;64M&quot;        cpu: &quot;250m&quot;        ephemeral-storage: &quot;2Gi&quot;      limits:        memory: &quot;128M&quot;        cpu: &quot;500m&quot;        ephemeral-storage: &quot;4Gi&quot;</code></pre><h2 id="资源限制如何运作">资源限制如何运作</h2><p>k8s会通过kubelet将pod定义的资源限制传递给容器.</p><p>如果你容器使用的是docker.</p><p>cpu软限制将对标docker的–cpu-shares. 而cpu硬限制会告诉容器每100ms可以使用的CPU时间总量是  limits.cpu * 100.</p><blockquote><p>关于docker的–cpu-shares, 可以参考https://docs.docker.com/engine/reference/run/#cpu-share-constraint</p><p>总的来说, --cpu-shares 会让容器按照所设定的分值比例去使用cpu.不过, 在多核心节点上, 这个规则又不是很适用. 按照官方的说法, 当多核心cpu的时候,它的规则是:</p><p>On a multi-core system, the shares of CPU time are distributed over all CPU cores. Even if a container is limited to less than 100% of CPU time, it can use 100% of each individual CPU core.</p><p>For example, consider a system with more than three cores. If you start one container <code>&#123;C0&#125;</code> with <code>-c=512</code> running one process, and another container <code>&#123;C1&#125;</code> with <code>-c=1024</code> running two processes, this can result in the following division of CPU shares:</p><pre><code>PID    containerCPUCPU share100    &#123;C0&#125;0100% of CPU0101    &#123;C1&#125;1100% of CPU1102    &#123;C1&#125;2100% of CPU2</code></pre><p>这里三个容器,都是单核心程序</p></blockquote><p>内存的限制没有什么特别需要注意的.</p><h2 id="容器中的可见资源">容器中的可见资源</h2><p>默认情况下,不管你如何设置资源限制,容器里的可见资源都等于节点资源.也就是说你在容器里查看/proc/meminfo显示的资源并不是你设置的.这个问题会带来很多麻烦.</p><p>因为很多程序的参数都是根据可见资源来设定的.例如nginx的auto, 或者jvm里的内存参数.</p><blockquote><p>从 Java 8u191开始, jvm 已经默认实现了容器感知( -XX:+UseContainerSupport). 因此无需安装下面的 lxcfs 方案.</p><p>并且, 在容器中建议只设置如下内存参数:</p><p>-XX:MaxRAMPercentage  最大堆内存=物理内存百分比, 建议为容器限制的50%-75% . 毕竟还需要预留其它内存.</p></blockquote><p>而社区常见的作法是用<code>lxcfs</code> 来提升资源的可见性.<code>lxcfs</code> 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器, 它也可以支持Docker容器.</p><p>LXCFS通过用户态文件系统, 在容器中提供下列 <code>procfs</code> 的文件.</p><pre><code class="language-bash">/proc/cpuinfo/proc/diskstats/proc/meminfo/proc/stat/proc/swaps/proc/uptime</code></pre><p>与我们资源限制有关的, 主要是 cpuinfo 和 meminfo.</p><p>目前社区的做法如下:</p><ol><li><p>所有节点安装 fuse-libs 包.</p><pre><code class="language-bash">yum install -y fuse-libs</code></pre></li><li><p>安装部署lxcfs</p><pre><code class="language-bash">git clone https://github.com/denverdino/lxcfs-admission-webhook.gitcd lxcfs-admission-webhookvim deployment/lxcfs-daemonset.yaml=== 修正配置里的 apiVersion=== 当前git里的代码是陈旧的...代码里的版本在 1.18 已经被废弃). 具体归属于什么版本, 请参考k8s官方api文档 === https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#daemonset-v1-appsapiVersion:apps/v1  kubectl apply -f deployment/lxcfs-daemonset.yamlkubectl get pod#等待 lxcfs pod 状态成 running#如果你发现 pod 始终出错，那么执行下列命令：kubectl delete -f deployment/lxcfs-daemonset.yamlrm -rf /var/lib/lxcfskubectl apply -f deployment/lxcfs-daemonset.yamldeployment/install.shkubectl get secrets,pods,svc,mutatingwebhookconfigurations=== 查看各个对象状态</code></pre></li><li><p>给相关namespace注入lxcfs，例如default</p><pre><code class="language-bash">kubectl label namespace default lxcfs-admission-webhook=enabled</code></pre></li><li><p>重启添加了资源限制的pod, 此时 /proc/cpuinfo 和 /proc/meminfo 将会正确显示.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> pod </tag>
            
            <tag> 资源约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞10应用配置与密码与信息提供</title>
      <link href="posts/4b0e4a5/"/>
      <url>posts/4b0e4a5/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>在Pod中应用配置和密码，我们可以创建配置对象ConfigMap和密码对象Secret。</p><p>而这两种对象的使用有多种方式：</p><ul><li>存储卷方式</li><li>环境变量方式</li><li>命令行方式</li></ul><h1>非加密ConfigMap</h1><p><a href="https://kubernetes.io/docs/concepts/configuration/configmap/">https://kubernetes.io/docs/concepts/configuration/configmap/</a></p><p>ConfigMap 常用在两种情况中. 第一种是提供环境变量.第二种是提供配置文件</p><h2 id="写法">写法</h2><pre><code class="language-yaml">kind: ConfigMapapiVersion: v1metadata:  name: cm-demo  namespace: defaultdata:  data.1: hello  data.2: world  data.conf: |    property.1=value-1    property.2=value-2    property.3=value-3#immutable: true</code></pre><pre><code class="language-yaml">immutable: true 表示这是一个不可变更的CM，一旦CM创建，则这个属性不可更改。意味着如果你要更改，则需要删除CM和调用CM的Pod。</code></pre><p>上述例子中， 包含两种写法</p><ul><li>环境变量</li></ul><pre><code class="language-yaml">  data.1: hello  data.2: world</code></pre><ul><li>配置文件</li></ul><pre><code class="language-yaml">  data.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>data.conf 是文件名， 管道符 | 下面是文件内容. ✨需要注意的是， data.conf 内容依然要遵循 yaml 的缩进规则</p><h3 id="命令式写法">命令式写法</h3><pre><code class="language-bash">kubectl create cm cm-demo --from-file=data.conf</code></pre><h2 id="使用">使用</h2><ul><li>方法1</li></ul><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: busyboxspec:  replicas: 1  selector:    matchLabels:      run: busybox  template:    metadata:      labels:        run: busybox     spec:      containers:      - name: busybox        image: busybox        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $&#123;DATA1&#125; $&#123;DATA2&#125;&quot;]        env:        - name: DATA1          valueFrom:            configMapKeyRef:              name: cm-demo  # 注意，这是ConfigMap对象名              key: data.1    # 这是ConfigMap里的key        - name: DATA2          valueFrom:            configMapKeyRef:              name: cm-demo              key: data.2        volumeMounts:        - name: cm-demo-vol          mountPath: &quot;/etc&quot;  # cm 对象获取的 key 内容文件的根路径。          readOnly: true      volumes:      - name: cm-demo-vol        configMap:          name: cm-demo          items:             # items 就是获取 cm 对象下的所有 key          - key: &quot;data.conf&quot; # 获取名叫 data.conf 的 key            path: &quot;data.conf&quot; # 将 data.conf 这个 key 的内容写入到 /etc/data.conf 这个路径文件里</code></pre><p>通过上面的写法，可以在容器内找到 /etc/data.conf 。</p><p>✨当你更新 cm 对象的时候，cm 对象会把更新内容同步到容器中。更新速度并不是立马就能响应，也是轮询更新。</p><p>💥注意：这种方式会清空 mountPath 路径。因为挂载点是目录。</p><ul><li>方法2</li></ul><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: busyboxspec:  replicas: 1  selector:    matchLabels:      run: busybox  template:    metadata:      labels:        run: busybox     spec:      containers:      - name: busybox        image: busybox        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $&#123;DATA1&#125; $&#123;DATA2&#125;&quot;]        env:        - name: DATA1          valueFrom:            configMapKeyRef:              name: cm-demo  # 注意，这是ConfigMap对象名              key: data.1    # 这是ConfigMap里的key        - name: DATA2          valueFrom:            configMapKeyRef:              name: cm-demo              key: data.2        volumeMounts:        - name: cm-demo-vol          subPath: data.conf                   mountPath: &quot;/etc/data.conf&quot;          readOnly: true      volumes:      - name: cm-demo-vol        configMap:          name: cm-demo</code></pre><p>✨这种通过 subPath 获取 cm 的 key data.conf，并针对性的挂载到指定文件 /etc/data.conf 。可以避免挂载点是目录的时候被覆盖。</p><p>💥这种方式，cm对象修改无法同步到容器中。</p><ul><li><p>方法3</p><p>通过在 spec.containers.env 中定义. 例子中， DATA1 是环境变量名， DATA1的值通过valueFrom定义.最终，你可以在容器中使用环境变量DATA1和DATA2</p><p>通过在 spec.containers.envFrom 中定义一组环境变量，这种方式要求 cm 里的 key 必须都符合 C_IDENTIFIER 规范.</p></li></ul><pre><code class="language-yaml">        env:        - name: DATA1          valueFrom:            configMapKeyRef:              name: cm-demo  # 注意，这是ConfigMap对象名              key: data.1    # 这是ConfigMap里的key        - name: DATA2          valueFrom:            configMapKeyRef:              name: cm-demo              key: data.2        envFrom:        - configMapRef:            name: cm-envs</code></pre><ul><li>💥这种方式下ConfigMap更新后，环境变量不会更新。</li></ul><h1>加密Secret</h1><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a></p><p>secret对象里的值需要写入加密后的.</p><h2 id="写法-2">写法</h2><p>我们定义一个用户密码对，分别是username和password</p><p>secret对象要求值必须进行base64编码加密(当type为Opaque的时候).</p><pre><code class="language-bash">[root@k8s00 test-yaml]# echo -n &quot;admin&quot; | base64YWRtaW4=[root@k8s00 test-yaml]# echo -n &quot;admin321&quot; | base64YWRtaW4zMjE=</code></pre><pre><code class="language-yaml"># 创建 secret 对象 mysecretapiVersion: v1kind: Secretmetadata:  name: mysecret  namespace: defaulttype: Opaquedata:  username: YWRtaW4=  password: MWYyZDFlMmU2N2Rm</code></pre><p>👀<code>type</code>用来定义不同的类型，比较常用的有:</p><ul><li><code>Opaque</code> 普通加密字符串</li><li><code>kubernetes.io/tls</code> tls证书</li><li><code>kubernetes.io/dockerconfigjson</code> docker仓库账户密码</li><li><code>kubernetes.io/service-account-token</code> 服务账户授权token</li></ul><h2 id="使用-2">使用</h2><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    command:    - sleep    - &quot;3600&quot;    env:      - name: SECRET_USERNAME        valueFrom:          secretKeyRef:            name: mysecret            key: username      - name: SECRET_PASSWORD        valueFrom:          secretKeyRef:            name: mysecret            key: password    volumeMounts:    - name: mysecret-vol      mountPath: &quot;/etc/foo&quot;      readOnly: true  volumes:  - name: mysecret-vol    secret:      secretName: mysecret</code></pre><p>✨secret 也支持 immutable: true ，即不可变的 secret。</p><ul><li>使用2</li></ul><p>通过在 spec.containers.envFrom.secretRef 中定义一组环境变量，这种方式要求 secret 里的 key 必须都符合 C_IDENTIFIER 规范.</p><pre><code class="language-yaml">kubectl explain deployment.spec.template.spec.containers.envFrom.secretRef</code></pre><h2 id="immutable-true">immutable: true</h2><p>有以下好处</p><ul><li>避免对象被意外修改</li><li>降低 kube-apiserver 的压力，因为不可变，所以 kubelet 就无需 watch kube-apiserver.</li></ul><h3 id="临时卷">临时卷</h3><ul><li>通过在 spec.volumes.secret 中定义临时卷 mysecret-vol。</li><li>剩余用法和 spec.volumes.configMap 一样。</li></ul><h3 id="环境变量">环境变量</h3><ul><li>通过在 spec.containers.env 中定义。</li></ul><h3 id="测试">测试</h3><p>在通过上述配置创建好资源后，我们进入pod，进行测试.</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl exec -it mypod -- /bin/sh===可以看到两个加密信息生成了两个软连接，并指向了隐藏文件/ # ls -l /etc/foototal 0lrwxrwxrwx    1 root     root            15 Sep 17 07:18 password -&gt; ..data/passwordlrwxrwxrwx    1 root     root            15 Sep 17 07:18 username -&gt; ..data/username===环境变量/ # echo $&#123;SECRET_USERNAME&#125;admin===文件方式/ # cat /etc/foo/usernameadmin </code></pre><h1>限制</h1><ul><li>资源和Pod限制在同一命名空间</li><li>对象数据大小不能超过1MB</li><li>当pod引用不存在的对象时，pod无法启动</li></ul><h1>其它</h1><ol><li><p>命令方式创建</p><p>kubectl create configmap/secret <name> xxx</name></p><p>这里xxx可以用两种方式:</p><ul><li>–from-literal=<key>=<value> 指定kv</value></key></li><li>–from-file=&lt;文件/目录&gt; 当为目录的时候，会递归将目录里的文件都写入对象中</li></ul></li><li><p>在Pod中隐藏配置或者密码文件</p><p>你可以将 <code>secret.data.&lt;key&gt;</code> 写成 <code>secret.data.&lt;.key&gt;</code> 来隐藏它.</p></li></ol><h1>信息提供Downward API</h1><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></p><p>Downward API用来向Pod中运行的容器公开提供容器和Pod的信息。</p><p>例子1:</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: oom-sims  namespace: default  labels:    app: oom-simsspec:  replicas: 1  selector:    matchLabels:      app: oom-sims  template:    metadata:      labels:        app: oom-sims    spec:      volumes:        - name: dumplog          emptyDir: &#123;&#125;      containers:        - name: oom-sims-container          image: cloudbeer/oom-sims:1.0          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;java -Xms256m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/dumplog/$&#123;RANDOM&#125;.dump -jar /app/oom-sims-1.0-SNAPSHOT.jar 1000&quot;]          resources:            requests:              memory: &quot;2Gi&quot;              cpu: &quot;500m&quot;            limits:              memory: &quot;2Gi&quot;              cpu: &quot;500m&quot;          volumeMounts:          - name: dumplog            mountPath: /dumplog        - name: dumplogupload          image: aaa103439/dumplogupload:latest          env:          - name: POD_NAME            valueFrom:              fieldRef:                fieldPath: metadata.name          - name: NAMESPACE            valueFrom:              fieldRef:                fieldPath: metadata.namespace          - name: NODE_IP            valueFrom:              fieldRef:                fieldPath: status.hostIP          - name: OSS_ENDPOINT            value: &quot;oss-cn-beijing-internal.aliyuncs.com&quot;          - name: OSS_ACCESSID            value: &quot;&quot; #           - name: OSS_ACCESSSECRET            value: &quot;&quot;          - name: OSS_BUCKET            value: &quot;juaiit&quot;          - name: OSS_DUMPER_ROOT            value: &quot;logs/dump/$(NAMESPACE)&quot;          - name: APP_NAME # 应用名称，这个名称会附加在文件名后面            value: $(POD_NAME)_$(NODE_IP)          - name: DUMPER_ROOT # 会监视这个文件夹下面的文件，-XX:HeapDumpPath = DUMPER_ROOT/xxx.DUMPER_SUFFIX            value: &quot;/dumplog&quot;           - name: DUMPER_SUFFIX # dump 文件后缀            value: &quot;.dump&quot;          volumeMounts:          - name: dumplog            mountPath: /dumplog</code></pre><p>例子2: 将资源限制数据挂载到容器/etc/podinfo</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: kubernetes-downwardapi-volume-example-2spec:  containers:    - name: client-container      image: k8s.gcr.io/busybox:1.24      command: [&quot;sh&quot;， &quot;-c&quot;]      args:      - while true; do          echo -en '\n';          if [[ -e /etc/podinfo/cpu_limit ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_limit; fi;          if [[ -e /etc/podinfo/cpu_request ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_request; fi;          if [[ -e /etc/podinfo/mem_limit ]]; then            echo -en '\n'; cat /etc/podinfo/mem_limit; fi;          if [[ -e /etc/podinfo/mem_request ]]; then            echo -en '\n'; cat /etc/podinfo/mem_request; fi;          sleep 5;        done;      resources:        requests:          memory: &quot;32Mi&quot;          cpu: &quot;125m&quot;        limits:          memory: &quot;64Mi&quot;          cpu: &quot;250m&quot;      volumeMounts:        - name: podinfo          mountPath: /etc/podinfo  volumes:    - name: podinfo      downwardAPI:        items:          - path: &quot;cpu_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.cpu              divisor: 1m          - path: &quot;cpu_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.cpu              divisor: 1m          - path: &quot;mem_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.memory              divisor: 1Mi          - path: &quot;mem_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.memory              divisor: 1Mi</code></pre><h1>projected volume使用方式</h1><p>kubernetes 提供了一种卷类型 projected volume。可以将多个volume挂载到一个位置上。</p><p>写法：</p><pre><code class="language-yaml:">        volumeMounts:        - name: all-in-one          mountPath: /etc/allinfo          readonly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: user      - secret:          name: pass      - configmap:          name: content      - downwardAPI:          items:            - path: &quot;labels&quot;              fieldRef:                fieldPath: metadata.labels            - path: &quot;annotations&quot;              fieldRef:                fieldPath: metadata.annotations</code></pre><blockquote><p>user， pass， content， labels， annotations 将会以文件形式存在于容器挂载点 /etc/allinfo 内。</p></blockquote><p>创建Secret对象： secret: user 和 secret: pass</p><pre><code class="language-bash">echo -n &quot;admin&quot; &gt; ./username.txtkubectl create secret generic user --from-file=./username.txtecho -n &quot;1f2d1e2e67df&quot; &gt; ./username.txtkubectl create secret generic pass --from-file=./password.txt</code></pre><p>创建 ConfigMap对象: content</p><pre><code class="language-bash">echo -n &quot;userinfo&quot; &gt; ./content.txtkubectl create configmap content --from-file=./content.txt</code></pre><p>DownwardAPI 临时卷内容直接配置里写入</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> ConfigMap </tag>
            
            <tag> Secret </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞08负载均衡服务service</title>
      <link href="posts/d3b80b5f/"/>
      <url>posts/d3b80b5f/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>在实际业务中,因业务压力问题,经常会有多个后端服务副本,它们共同承担请求.我们使用云服务的时候,可以购买阿里云的slb或者aws的elb/alb等负载均衡器向这些后端副本分发流量. 并通过这些负载均衡向公/内网发布后端程序。</p><p>k8s设计了一个service对象来实现这一目的.</p><h2 id="k8sIP">k8sIP</h2><p>在提Service对象之前,需要先知道k8s中存在的三种IP.即 NodeIP, ClusterIP, PodIP</p><p>NodeIP 就是物理节点ip, 这个没得说, 玩家自己定义</p><p>ClusterIP 是k8s的一个虚拟ip, 本身没有任何实体, 也就是VIP</p><p>PodIP 是容器共享的一个网络命名空间对应的ip, 一个pod里的容器共用</p><h2 id="与kube-proxy">与kube-proxy</h2><p>kube-proxy 是实现 svc 的重要组件，kube-proxy 通过代理方式转发流量到Pod。其代理方式分三种：namespace 方式，iptables 方式，ipvs方式。基本用的都是 ipvs 方式。</p><p>ipvs方式的前置要求是 ipvs 组件，如果 kube-proxy 没有检测到 ipvs 组件则会回退到 iptables 方式，如果依然不合适，则继续回退到 namespace。</p><h2 id="类型">类型</h2><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/">https://kubernetes.io/zh/docs/concepts/services-networking/service/</a></p><p><a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/">https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/</a></p><p>Service对象通过spec.type来设定类型, 共计4个类型: ClusterIP, NodePort, LoadBalancer, ExternalName. 这四个类型可以分为两类</p><h3 id="ClusterIP">ClusterIP</h3><p>ClusterIP(默认类型): 反向代理集群内的pod. 供集群内其它服务访问. 流量过程是: 集群内部其它服务=&gt;svc_name=&gt;ClusterIP:端口=&gt;Pod</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  name: myservicespec:  selector:    app: myapp  type: ClusterIP  ports:  - name: myapp-http    protocol: TCP    port: 80    targetPort: 8080  - name: myapp-https    protocol: TCP    port: 443    targetPort: 8081</code></pre><blockquote><p>spec.selector：选择器，选择目标pod的标签</p><p>spec.ports：</p><ul><li><p>name 是端口名，由小写字母、数字、<code>-</code>组成</p></li><li><p>port 是Service暴露端口</p></li><li><p>targetPort是pod暴露端口. 默认情况下, targetPort将等于port</p></li></ul></blockquote><p>ℹ️若spec.type没有定义，则默认是<code>ClusterIP</code></p><h3 id="ExternalName">ExternalName</h3><p>ExternalName: 构建一个CNAME解析(service对象-CNAME-其它域名).  我能想到的主要是让集群内部服务可以访问到集群外部服务.</p><p>例如:</p><pre><code class="language-yaml">kind: ServiceapiVersion: v1metadata:  name: my-service  namespace: prodspec:  type: ExternalName  externalName: my.database.example.com</code></pre><blockquote><p>集群内部访问my-service.prod的时候, <a href="http://xn--k8sdnsmy-vq8m536awioci6an95by58d3far6b.database.example.com">将通过k8s的dns服务返回my.database.example.com</a></p></blockquote><h3 id="NodePort">NodePort</h3><p>NodePort: 反向代理集群内的pod(使用NAT在每一个集群node上的相同端口上公开Service, 是【ClusterIP类型的超集】). 供集群外服务访问. 流量过程是: 集群外=&gt;任意节点ip:端口=&gt;svc_name=&gt;ClusterIP:端口=&gt;Pod</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  name: myservicespec:  selector:    app: myapp  type: NodePort  ports:  - protocol: TCP    nodePort: 31000    port: 80    targetPort: 80    name: myapp-http</code></pre><blockquote><p>集群外部此时可以访问任意物理节点ip:31000, 此时可以访问到集群内部app=myapp的pod.</p><p>这里nodePort是物理节点暴露端口, port是Service暴露端口, targetPort是pod暴露端口.</p><p>nodePort 可以不定义, 默认会自动从30000-32767随机分配。如果你想修改，则需要指定 apiserver 组件的<code>--service-node-port-range</code>。</p></blockquote><h3 id="LoadBalancer">LoadBalancer</h3><p>LoadBalancer: 对接云商的负载均衡服务, 给Service分配一个固定IP. 方便云服务商的LB服务绑定. 【是NodePort类型的超集。 流量过程是: 集群外=&gt;云服务LB=&gt;任意物理节点ip:端口=&gt;svc_name=&gt;ClusterIP:端口=&gt;Pod</p><p>这种类型建议直接参考官方文档: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer</a></p><h3 id="Headless-Service">Headless Service</h3><p>正常情况下，dns服务会将svc的域名解析为clusterIP（vip），然后通过代理方式将到达vip的流量转发到endpoints列表。</p><p>而Headless类型下，svc没有vip，也没有转发规则，而是通过A记录解析将<code>svc_name.ns_name</code>直接解析为<code>endpoints</code>列表，即直达pod。</p><p>不同点：</p><ul><li>配置上，设置<code>spec.clusterIP: None</code>。</li><li>解析上，A记录方式解析为endpoints列表的所有ip，而不是ClusterIP。</li></ul><p>常见于分布式应用部署场景，例如StatefulSet 控制器使用 Headless 结合Pod标识构建 Pod 唯一子域名，具体如何使用参考 StatefulSet，这里暂不考虑。</p><h2 id="粘性会话">粘性会话</h2><p>有些时候,我们需要会话黏性,你可以通过service.spec.sessionAffinity=ClientIP来设置.并同时可以通过service.spec.sessionAffinityConfig.clientIP.timeoutSeconds来设置会话保持时间,它默认是3小时.</p><h2 id="服务发现">服务发现</h2><p>Pod 可以通过两种方式发现svc。</p><h3 id="DNS">DNS</h3><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#dns">https://kubernetes.io/zh/docs/concepts/services-networking/service/#dns</a></p><p>Pod可以发现集群任何位置的svc</p><p>不同ns下，你可以通过<code>svc_name.ns_name</code>访问 svc</p><p>相同ns下，可只通过<code>svc_name</code>访问 svc</p><h3 id="环境变量">环境变量</h3><p>Pod仅可以发现相同ns下的svc</p><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables">https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables</a></p><p>当service创建的时候,kubelet会生成一批环境变量。例如，svc 名称是 myservice，ClusterIP是10.0.0.11，暴漏的端口是6379，则生成以下环境变量：</p><pre><code class="language-bash">MYSERVICE_SERVICE_HOST=10.0.0.11MYSERVICE_SERVICE_PORT=6379MYSERVICE_PORT=tcp://10.0.0.11:6379MYSERVICE_PORT_6379_TCP=tcp://10.0.0.11:6379MYSERVICE_PORT_6379_TCP_PROTO=tcpMYSERVICE_PORT_6379_TCP_PORT=6379MYSERVICE_PORT_6379_TCP_ADDR=10.0.0.11</code></pre><p>ℹ️ 则不管你使用哪种方式，都应该提前通过<code>init</code>容器来校验svc对象已成功创建，例如通过until死循环来判断解析是否成功。</p><h2 id="其它暴露方式">其它暴露方式</h2><p>除了 svc 暴露服务，还可以通过 ingress 暴露服务，它是充当集群入口，可以在单ip下暴露多服务。这里暂不讨论。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞07自动扩缩</title>
      <link href="posts/f6e328b5/"/>
      <url>posts/f6e328b5/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><p>k8s的pod缩放功能,和aws的auto scaling功能是一回事。虽然可能没有aws的auto scaling功能强大。</p><p>k8s的pod自动缩放功能称之为HPA(Horizontal Pod Autoscaler)，它可以基于设定的cpu阈值，来自动调整控制器中的pod数量。</p><h1>手动缩放</h1><p>就是指定一个副本数量</p><pre><code class="language-yaml">kubectl scale deployment.v1.apps/nginx-dep --replicas=10</code></pre><h1>自动缩放</h1><p>自动缩放基于hpa对象资源，hpa需要监控服务，毕竟没有监控指标，就无法根据指标进行自动缩放。</p><p>流程图如下：</p><p><img src="/posts/f6e328b5/image-20220507115550891.png" alt="image-20220507115550891"></p><h2 id="添加监控服务-metrics-server">添加监控服务 metrics-server</h2><p><a href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a></p><p>metrics 服务器可以通过资源度量值 API 对外提供度量数据，Horizontal Pod Autoscaler 正是根据此 API 来获取度量数据.如果没有此服务,HPA将无法工作.</p><pre><code class="language-bash">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</code></pre><p>👙默认 metrics-server 的 deployment 无法直接使用, 我们需要添加几个参数,  来禁止 ca 认证和开通 dns</p><p>在 deployment.spec.template.spec.containers 中新加入下列配置:</p><pre><code class="language-yaml">        command:          - /metrics-server          - --kubelet-insecure-tls          - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname</code></pre><pre><code class="language-bash">kubectl apply -f components.yaml</code></pre><h2 id="校验">校验</h2><p>可以看到CPU和内存的指标，就表示工作正常了。</p><pre><code class="language-bash">➜  ~ kubectl top pods -n kube-systemNAME                                  CPU(cores)   MEMORY(bytes)   coredns-78fcd69978-dt4lx              1m           29Micoredns-78fcd69978-nzb7m              1m           31Mietcd-k8s01                            25m          118Mietcd-k8s02                            38m          385Mietcd-k8s03                            27m          325Mik8s-logs-4mx8d                        1m           46Mik8s-logs-csqq4                        1m           77Mik8s-logs-ctntv                        1m           73Mik8s-pod-logs-6zgbf                    2m           157Mik8s-pod-logs-pf669                    1m           131Mik8s-pod-logs-qnscr                    1m           150Mikube-apiserver-k8s01                  62m          615Mikube-apiserver-k8s02                  87m          677Mikube-apiserver-k8s03                  80m          503Mikube-controller-manager-k8s01         2m           29Mikube-controller-manager-k8s02         15m          130Mikube-controller-manager-k8s03         3m           30Mikube-flannel-ds-8gk87                 1m           23Mikube-flannel-ds-9hnrs                 2m           28Mikube-flannel-ds-rqdm4                 2m           24Mikube-proxy-9lp9q                      1m           31Mikube-proxy-rwt48                      4m           28Mikube-proxy-xlxt6                      8m           31Mikube-scheduler-k8s01                  4m           36Mikube-scheduler-k8s02                  3m           31Mikube-scheduler-k8s03                  3m           33Mikube-state-metrics-74c958c8bc-nlmcx   2m           19Mimetrics-server-6bd8d94d7f-shs7n       3m           35Misnapshot-controller-0                 1m           20Mi</code></pre><h2 id="创建测试用例">创建测试用例</h2><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</a></p><p>hpa-test.yaml</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: php-apachespec:  selector:    matchLabels:      run: php-apache  replicas: 1  template:    metadata:      labels:        run: php-apache    spec:      containers:      - name: php-apache        image: k8s.gcr.io/hpa-example        ports:        - containerPort: 80        resources:          limits:            cpu: 500m          requests:            cpu: 200m---apiVersion: v1kind: Servicemetadata:  name: php-apache  labels:    run: php-apachespec:  ports:  - port: 80  selector:    run: php-apache</code></pre><blockquote><p>Service 类型通过<code>spec.selector</code>选择pod</p></blockquote><h2 id="基于CPU指标开启">基于CPU指标开启</h2><p>命令方式:</p><pre><code class="language-bash">kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=5</code></pre><p>声明方式:</p><pre><code class="language-yaml">apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: php-apachespec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: php-apache  minReplicas: 1  maxReplicas: 5  targetCPUUtilizationPercentage: 50</code></pre><blockquote><p>这里的意思是, cpu 阈值50%, 最小pod数1, 最大pod数5. hpa会将所有pod的平均cpu利用率维持在50%, 并且pod数量在1~5的范围内波动</p></blockquote><p>⚠️只允许拥有一个规则</p><p>查看当前cpu使用率</p><pre><code class="language-bash">➜   kubectl get hpa --watchNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          57m</code></pre><p>⚠️如果<code>TARGETS</code>显示<code>unknown</code>没有获取到Deployment的资源利用率，则说明Deployment中没有添加对应的资源限制。</p><p>通过busybox以死循环方式访问web服务，快速增加测试服务的cpu使用率</p><pre><code class="language-bash">kubectl run -it --rm load-generator --image=busybox /bin/shwhile true; do wget -q -O- http://php-apache; done=== 会输出大量OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!</code></pre><p>在经过一段时间等待后(不会超过1分钟, 默认监控抓取数据间隔时间是1分钟),我们可以看到 pod 数量发生变化</p><pre><code class="language-bash">➜   kubectl get hpa --watchNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          60mNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          60mNAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   250%/50%   1         5         1          61mNAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   250%/50%   1         5         1          61m➜   kubectl get deployment/php-apacheNAME         READY   UP-TO-DATE   AVAILABLE   AGEphp-apache   5/5     5            5           68m</code></pre><p>现在,关闭客户端请求,等待1分钟以上.再次看hpa, 这时候cpu利用率已经下降</p><pre><code class="language-bash">➜   kubectl get hpaNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   11%/50%   1         5         5          64m</code></pre><p>再看pod数量,它应该已经开始缩减。</p><blockquote><p>缩减并不是在cpu使用率下降之后就立即执行,而是内部有一个算法.它避免因立即执行从而导致资源出现反复波动.</p></blockquote><pre><code class="language-bash">➜   kubectl get deployment php-apacheNAME         READY   UP-TO-DATE   AVAILABLE   AGEphp-apache   5/5     5            5           70m➜   kubectl describe deployment/php-apacheName:                   php-apacheNamespace:              defaultCreationTimestamp:      Wed, 16 Sep 2020 10:22:32 +0800Labels:                 &lt;none&gt;Annotations:            deployment.kubernetes.io/revision: 1Selector:               run=php-apacheReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailableStrategyType:           RollingUpdateMinReadySeconds:        0RollingUpdateStrategy:  25% max unavailable, 25% max surgePod Template:  Labels:  run=php-apache  Containers:   php-apache:    Image:      k8s.gcr.io/hpa-example    Port:       80/TCP    Host Port:  0/TCP    Limits:      cpu:  500m    Requests:      cpu:        200m    Environment:  &lt;none&gt;    Mounts:       &lt;none&gt;  Volumes:        &lt;none&gt;Conditions:  Type           Status  Reason  ----           ------  ------  Progressing    True    NewReplicaSetAvailable  Available      True    MinimumReplicasAvailableOldReplicaSets:  &lt;none&gt;NewReplicaSet:   php-apache-5c4f475bf5 (1/1 replicas created)Events:  Type    Reason             Age    From                   Message  ----    ------             ----   ----                   -------  Normal  ScalingReplicaSet  14m    deployment-controller  Scaled up replica set php-apache-5c4f475bf5 to 4  Normal  ScalingReplicaSet  14m    deployment-controller  Scaled up replica set php-apache-5c4f475bf5 to 5  Normal  ScalingReplicaSet  6m50s  deployment-controller  Scaled down replica set php-apache-5c4f475bf5 to 4  Normal  ScalingReplicaSet  4m49s  deployment-controller  Scaled down replica set php-apache-5c4f475bf5 to 1</code></pre><hr><h1>常用的也就上面的基于CPU指标</h1><h2 id="基于其它非资源类型的指标">基于其它非资源类型的指标</h2><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics</a></p><p>当前hpa的api版本是autoscaling/v1，它只能抓取cpu指标。</p><p>如果你想抓取其它资源指标，例如内存指标(当前也只支持内存)，那么需要将版本变更为autoscaling/v2beta2。</p><p>✨资源类型的指标名在集群内是统一的，不可变的。</p><p>另外，v2 版本还支持非资源类型指标，例如Pods和Object类型。</p><p>✨非资源类型指标名是特定于集群的，不是不可变的，且指标数据依赖于额外的监控系统。</p><p>最后，autoscaling/v2beta2版本的配置有了一些变化,且可能随时会有新的变化.</p><p><a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec">https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/horizontal-pod-autoscaler-v2/#HorizontalPodAutoscalerSpec</a></p><pre><code class="language-yaml">apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata:  name: php-apachespec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: php-apache  minReplicas: 1  maxReplicas: 10  metrics:  - type: Resource    resource:      name: cpu      target:        type: AverageUtilization        averageUtilization: 50  - type: Pods    pods:      metric:        name: packets-per-second      target:        type: AverageValue                  # 当前Pods类型的指标只支持 AverageValue 数值类型        averageValue: 1k  - type: Object    object:      metric:        name: requests-per-second      describedObject:        apiVersion: networking.k8s.io/v1        kind: Ingress        name: main-route      target:        type: Value        value: 10k</code></pre><p>上面的例子意思是：</p><ol><li>保持每个pod的cpu使用率不超过50%，并且每秒能承受1000个数据包。</li><li>所有位于ingress后面的pod，每秒能处理10000个请求。</li></ol><p>💥上面的例子仅表示 type: Object 中从 Ingress 中获取指标 requests-per-second。但不代表 Ingress 有提供指标API。💥</p><blockquote><p>请注意：</p><ol><li>指标字段targetCPUUtilizationPercentage被metrics数组代替，metrics数组包含了多种度量指标。</li><li>指标分为：直接资源类 Resource，Pod类 Pods，第三类 Object，非k8s对象类 External，容器资源类 ContainerResource</li><li>指标分类：<ol><li>name 指定指标标识名</li><li>target.type 指定指标【值】类型，例如<code>AverageValue</code>表示平均值，即每一个pod都需要满足此指标值</li></ol></li></ol></blockquote><p>⚠️关于<code>Resource</code>类型，要么是cpu，要么是memory，两者应二选一。</p><h2 id="基于标签的k8s对象指标">基于标签的k8s对象指标</h2><pre><code class="language-yaml">- type: Object  object:    metric:      name: `http_requests`      selector: `verb=GET`    target:      type: AverageValue      averageValue: 30</code></pre><blockquote><p>从监控系统里获取标签verb=GET的值</p></blockquote><p>上述例子的意思是，确保每一个pod都能满足标签为verb=GET的http请求数量。</p><h2 id="基于非k8s对象指标">基于非k8s对象指标</h2><pre><code class="language-yaml">- type: External  external:    metric:      name: queue_messages_ready      selector: &quot;queue=worker_tasks&quot;    target:      type: AverageValue      averageValue: 30</code></pre><p>当类型变为External的时候，就可以指定监控系统里存储的其它监控指标。</p><p>例如，监控系统从消息队列服务里拿到了消息数量的指标，并附加queue=worker_tasks。那么就可以和上述例子一样，根据消息数量指标进行缩放。</p><h1>度量</h1><p>hpa和度量指标API里的数据均使用k8s中称为量纲的特殊整数表示。</p><p>量纲的意思就是：</p><ul><li>数字太小就加上后缀单位m，例如1.5写成1500m。</li><li>数字刚好就是整数显示。</li><li>数字大就加上后缀单位，比如k，M。</li><li>总之没有小数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞06无状态服务deployment</title>
      <link href="posts/18e4fa73/"/>
      <url>posts/18e4fa73/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><p>Deployment控制器用来管理无状态应用，通过Deployment控制器创建ReplicaSet，ReplicaSet控制Pod副本集。</p><p>任何时候，都不应该直接去创建ReplicaSet控制器，应始终使用Deployment控制器。</p><h1>例子</h1><p>API语法：<a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/">https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/</a></p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      imagePullSecrets:      - name: apps-ro-token      containers:      - name: nginx        image: nginx:v1.20.1        command:        - nginx        args:        - -g        - &quot;daemon off;&quot;        ports:        - containerPort: 80</code></pre><blockquote><p><code>spec.selector</code>：Deployment通过它控制Pod，与Pod<code>spec.template.metadata.labels</code>一致。</p><p><code>metadata</code>： 这里会定义name，labels，namespace，annotations。</p></blockquote><p>⚠️Deployment的<code>spec.selector</code>选择的标签应该是全局唯一，这样可以避免多个Deployment出现冲突出现不可预知的问题。</p><p>ℹ️ 其中 annotations （注解）很特殊，k8s通过注解中的<code>key</code>来确认要关联的额外服务信息。</p><p>例如在阿里云的k8s服务中，你可以添加镜像快照注解信息，从而让 pod 无需从镜像仓库下载启动，而是直接通过快照加速启动。</p><h1>更新</h1><p>👙Deployment的每次更新都会创建一个新的ReplicaSet对象。Deployment通过保留新旧的ReplicaSet，来实现回滚功能。</p><p>Deployment的默认策略大致如下</p><p>更新规则：</p><ul><li>先扩展一部分新版本</li><li>删除一部分老版本</li><li>重复上述行为,直至所有老版本被替换</li></ul><p>回滚规则：</p><ul><li>回滚到任何一个之前的版本</li></ul><blockquote><p>Deployment只会针对<code>spec.template</code>更新Pod</p></blockquote><h2 id="初始版本">初始版本</h2><p>创建一个nginx的deployment</p><pre><code class="language-bash">kubectl apply -f nginx-deployment-1.14.2.yaml===apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  minReadySeconds: 3  revisionHistoryLimit: 10  strategy:    type: RollingUpdate    rollingUpdate:      maxUnavailable: 30%      maxSurge: 30%  template:    metadata:      labels:        app: nginx        version: v1.14.2      annotations:        kubernetes.io/change-cause: &quot;version: v1.14.2&quot;    spec:      containers:      - name: nginx        image: nginx:1.14.2        lifecycle:          postStart:            exec:              command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo This is v1.14.2 &gt; /usr/share/nginx/html/index.html&quot;]        command:        - nginx        args:        - -g        - &quot;daemon off;&quot;        ports:        - containerPort: 80</code></pre><p><a href="http://spec.template.metadata.annotations.kubernetes.io/change-cause">spec.template.metadata.annotations.kubernetes.io/change-cause</a> 通过注解添加变更说明</p><h2 id="更新策略">更新策略</h2><p>deployment 的 .spec 中可以添加一些策略.</p><pre><code class="language-bash">spec:    replicas: 3  minReadySeconds: 3  # pod 就绪时间，在此时间之前，deployment 认为 pod 还没有准备好. 默认0  revisionHistoryLimit: 10 # 最大版本保留次数. 默认10   strategy:    type: RollingUpdate # 定义变更策略. 除了 RollingUpdate，还可以是Recreate.Recreate指的是先删除所有pod,再创建.    rollingUpdate:      maxUnavailable: 30% # 变更期间，不可用的副本数。可以是具体数字      maxSurge: 30% # 变更期间，可以超出replicas定义的副本数。可以是具体数字</code></pre><blockquote><p>maxUnavailable 和 maxSurge 设置相等即可. 即开多少个新的同时，就关多少个老的</p></blockquote><h3 id="通过命令添加-删除变更说明">通过命令添加/删除变更说明</h3><ul><li>添加</li></ul><pre><code class="language-bash">kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=&quot;first&quot;</code></pre><ul><li>删除(命令最后一个减号)</li></ul><pre><code class="language-bash">kubectl annotate deployment/nginx-deployment &lt;注解key&gt;-</code></pre><ul><li>执行kubectl命令的同时将命令写入<code>CHANGE-CAUSE</code></li></ul><pre><code class="language-bash">kubectl xxx --record</code></pre><h2 id="版本历史">版本历史</h2><pre><code class="language-bash">➜   kubectl rollout history deployment/nginx-deploymentdeployment.apps/nginx-deploymentREVISION  CHANGE-CAUSE1         version: v1.14.2</code></pre><h2 id="暂停发布">暂停发布</h2><p>此操作不会影响pod的运行，仅仅是阻断deployment实施更新。</p><p>如果更新之前就暂停，则你可以多次更新，这些更新在【恢复发布】之前不会被应用，也不会记录在【历史版本】中。</p><p>如果更新期间执行暂停，则更新会处于等待状态，已更新的不会回滚。</p><pre><code class="language-yaml">kubectl rollout pause deployment/nginx-deployment</code></pre><h2 id="更新版本">更新版本</h2><p>发布版本1.20.1</p><pre><code>sed 's@1.14.2@1.20.1@g' nginx-deployment-1.14.2.yaml &gt; nginx-deployment-1.20.1.yamlkubectl apply -f nginx-deployment-1.20.1.yaml</code></pre><h2 id="恢复发布">恢复发布</h2><pre><code class="language-yaml">kubectl rollout resume deployment/nginx-deployment</code></pre><p>启用恢复后，则暂停期间启用的所有更新将开始发挥作用，并且只会将更新的最终版本写入【历史版本】</p><pre><code class="language-bash">➜   kubectl rollout history deployment/nginx-deploymentdeployment.apps/nginx-deploymentREVISION  CHANGE-CAUSE1         version: v1.14.22         version: v1.20.1</code></pre><p>更新过程和结果查看，可运行：</p><pre><code class="language-bash">kubectl rollout status deployment/nginx-deployment</code></pre><p>更深层的rs状态查看，可运行：</p><pre><code class="language-bash">kubectl get rs -l app=nginx -l version=v1.20.1kubectl get pod -l app=nginx -l version=v1.20.1</code></pre><blockquote><p>-l 是调用标签过滤</p></blockquote><p>你可以通过追加版本号来看具体的版本内容</p><pre><code class="language-bash">kubectl rollout history deployment/nginx-deployment --revision=&lt;num&gt;</code></pre><h2 id="回滚版本">回滚版本</h2><p>我们选择将nginx版本回滚到第一个版本。被回滚的版本会从版本库里移动到最新位置。</p><pre><code class="language-bash">kubectl rollout undo deployment/nginx-deployment --to-revision=1➜   kubectl rollout history deployment/nginx-deploymentdeployment.apps/nginx-deploymentREVISION  CHANGE-CAUSE2         version: v1.20.13         version: v1.14.2</code></pre><blockquote><p>如果不指定版本，则回滚到上一个版本。</p></blockquote><h2 id="小结">小结</h2><pre><code class="language-bash">kubectl rollout history deployment/&lt;&gt; # 查看对象历史kubectl rollout undo deployment/&lt;&gt; --to-revision=&lt;&gt; # 对象版本回退kubectl rollout status deployment/&lt;&gt;# 展示执行状态kubectl rollout pause deployment/&lt;&gt; # 暂停此次版本操作行为kubectl rollout resume deployment/&lt;&gt; # 恢复此次版本操作行为kubectl set image # 修改镜像配置kubectl annotate # 添加一个注释</code></pre><p>更新过程：</p><ul><li>创建新的replicaset</li><li>将新的replicaset扩展</li><li>将旧的replicaset缩容</li><li>新的replicaset对应的pod已就绪或可用</li></ul><p>另外，可以通过判断<code>kubectl rollout status deployment/&lt;&gt;</code>命令执行结果，$? 状态为 0，则说明 deployment 处于 complete。</p><h1>金丝雀发布</h1><p>金丝雀发布的逻辑：</p><ul><li>开新Pod，不关老Pod，即确保可用的Pod数不低于预期数。</li><li>待新 Pod 创建后，立即暂停发布。</li><li>新 Pod Ready 后，部分流量经过新 Pod。</li><li>若新 Pod 通过验证，则恢复发布。</li><li>若新 Pod 没有通过验证，则恢复发布并立即执行回滚操作。</li></ul><p>操作步骤：</p><ol><li>调整新版本Deployment更新策略</li></ol><ul><li><p><code>minReadySeconds</code>设置为3，添加执行【暂停发布命令】的缓冲时间，避免新Pod被Deployment认为可用的同时删除旧Pod。</p></li><li><p><code>maxUnavailable</code>设置为0，避免更新中，可用Pod低于预期值。</p></li><li><p><code>maxSurge</code>的值就是验证新版本的Pod（”金丝雀“）数量。</p></li></ul><ol start="2"><li>启动更新，并立即暂停发布</li></ol><pre><code class="language-bash">kubectl apply -f new-nginx-deployment.yaml &amp;&amp; kubectl rollout pause deployment/nginx-deployment</code></pre><ol start="3"><li>若新 Pod 没有通过验证，则恢复发布并立即回滚上一个版本。</li></ol><pre><code class="language-bash">kubectl rollout resume deployment/nginx-deployment &amp;&amp; kubectl rollout undo deployment/nginx-deployment</code></pre><blockquote><p>这个过程不会重建老版本Pod，只会通过新版本rs删除新版本Pod</p></blockquote><ol start="4"><li>若新 Pod 通过验证，则恢复发布</li></ol><pre><code class="language-bash">kubectl rollout resume deployment/nginx-deployment</code></pre><p>整个更新过程，若顺利，效果如下所示：</p><p><img src="/posts/18e4fa73/image-20211215163355711.png" alt="image-20211215163355711"></p><h2 id="注意点">注意点</h2><ol><li>deployment只会保留不一致的历史版本。即，假设按照默认参数，deployment会保留10个历史版本，则如果你第三次的配置清单和第一次一致，则第一次的就会被删除，此时，只会有2和3版本。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go-100问题</title>
      <link href="posts/717b7a86/"/>
      <url>posts/717b7a86/</url>
      
        <content type="html"><![CDATA[<h2 id="类型断言">类型断言</h2><p>函数通过 interface{} 作为形参类型来接收任意变量。</p><p>当一个变量类型是 interface{} 的时候，则无法作为其它类型传入形参。</p><p>此时需要进行类型断言。也就是说显式的说明这个变量应该是什么。</p><pre><code class="language-go">// 假设 a 是 interface&#123;&#125; 但你知道其实是 string// 通过下面的方式转换a.(string)</code></pre><h2 id="同一个包内，不能存在重名函数">同一个包内，不能存在重名函数</h2><p>略</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞常用命令</title>
      <link href="posts/e6ae9474/"/>
      <url>posts/e6ae9474/</url>
      
        <content type="html"><![CDATA[<h2 id="查询库表占用空间">查询库表占用空间</h2><pre><code class="language-mysql">SELECT    table_schema AS '数据库',    table_name AS '表名',    table_rows AS '记录数',    TRUNCATE (data_length / 1024 / 1024 / 1024, 3) AS '数据容量(GB)',    TRUNCATE (index_length / 1024 / 1024 / 1024 , 3) AS '索引容量(GB)',TRUNCATE (DATA_FREE / 1024 / 1024 / 1024, 3) AS '已占用空间但未使用(GB)'FROM    information_schema. TABLESWHERE    table_schema in ('new_data','megable_main','megable_active')ORDER BY    data_length DESC;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞05容器生命周期-回调和探针</title>
      <link href="posts/9aa66899/"/>
      <url>posts/9aa66899/</url>
      
        <content type="html"><![CDATA[<h2 id="容器生命周期-回调">容器生命周期-回调</h2><p>💥这里说的是容器，容器，容器，不是Pod，Pod，Pod</p><p>💥这里说的是容器，容器，容器，不是Pod，Pod，Pod</p><p>💥这里说的是容器，容器，容器，不是Pod，Pod，Pod</p><p>容器状态包含3个：</p><ul><li><p>waiting，一般对应Pod的pending阶段</p></li><li><p>running，容器运行OK</p></li><li><p>terminated，容器退出</p></li></ul><p>我们在实际工作中，可能会遇到需要在容器生命周期中按预定计划执行某个动作:</p><ul><li>容器程序的准备工作，以及容器结束之前的处理工作。</li><li>容器开始之前下载一些包，或者容器结束之前上传日志等。</li><li>容器内程序优雅的关闭。</li></ul><p>k8s 给我们提供了两个回调用于处理这些工作，分别是 postStart和 preStop.</p><p><a href="https://kubernetes.io/zh/docs/concepts/containers/container-lifecycle-hooks/">https://kubernetes.io/zh/docs/concepts/containers/container-lifecycle-hooks/</a></p><h3 id="执行方式">执行方式</h3><p>回调函数的执行方式有三种：</p><pre><code class="language-bash">$ kubectl explain pod.spec.containers.lifecycle.postStartFIELDS:   exec &lt;Object&gt;     One and only one of the following should be specified. Exec specifies the     action to take.   httpGet      &lt;Object&gt;     HTTPGet specifies the http request to perform.   tcpSocket    &lt;Object&gt;     TCPSocket specifies an action involving a TCP port. TCP hooks not yet     supported</code></pre><p>👙通常主要使用的就是 exec  和 httpGet</p><h3 id="postStart">postStart</h3><ol><li>k8s在在容器创建后立即【发送】postStart，但【不保证】在 ENTRYPOINT 之前运行</li><li>如果它卡住，【容器】无法达到 running 状态。</li></ol><p>示例：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: poststart-testspec:  containers:  - name: poststart-test    image: nginx    lifecycle:      postStart:        exec:          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo This is postStart-test &gt; /usr/share/nginx/html/index.html&quot;]</code></pre><pre><code class="language-bash">[root@k8s00 ~]# kubectl apply -f poststart-test.yamlpod/poststart-test created[root@k8s00 ~]# kubectl get pod -o wideNAME             READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATESpoststart-test   0/1     ContainerCreating   0          7s    &lt;none&gt;   k8s02   &lt;none&gt;           &lt;none&gt;[root@k8s00 ~]# kubectl get pod -o wideNAME             READY   STATUS    RESTARTS   AGE   IP          NODE    NOMINATED NODE   READINESS GATESpoststart-test   1/1     Running   0          10s   10.97.2.6   k8s02   &lt;none&gt;           &lt;none2[root@k8s00 ~]# curl 10.97.2.6This is postStart-test</code></pre><h3 id="preStop">preStop</h3><ol><li>k8s它将在【容器】结束前立即【发送】preStop，但执行时间受限于【Pod】终止宽限期。</li><li>应该设置足够长的终止宽限期<code>terminationGracePeriodSeconds</code>，这个期限应该大于preStop执行时间+kubelet通知【container runtime】发关闭信号给【容器】+【容器】关闭时间。</li><li>它执行期间，Pod依然处于svc的端点列表内。</li></ol><blockquote><p>【容器】的结束需要被k8s知悉，任何k8s不知悉的结束动作，不会触发preStop。例如容器因执行完自动结束，这种状态k8s无法得知。</p></blockquote><p>示例1：</p><p>终止前写入数据</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: prestop-testspec:  containers:  - name: prestop-test    image: nginx    volumeMounts:    - name: prestop-test-tmp      mountPath: /usr/share    lifecycle:      preStop:        exec:          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo This is preStop &gt; /usr/share/message&quot;]  volumes:  - name: prestop-test-tmp    hostPath:      path: /tmp</code></pre><pre><code class="language-bash">[root@k8s00 ~]# kubectl delete pod prestop-test# 这里 prestop-test 被调度到 k8s02[root@k8s02 ~]# cat /tmp/messageThis is preStop</code></pre><p>示例2：</p><p>终止前优雅关闭nginx</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: hook-demo2spec:  containers:  - name: hook-demo2    image: nginx    lifecycle:      preStop:        exec:          command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]  # 优雅退出</code></pre><h2 id="💥分歧💥">💥分歧💥</h2><p><strong>说明：</strong> Kubernetes 只有在 Pod 手动结束或因控制器调度的时候才会发送 preStop 事件， 但在 Pod（Completed）时 preStop 的事件处理逻辑不会被触发。这个限制在 <a href="https://github.com/kubernetes/kubernetes/issues/55807">issue #55087</a> 中被追踪。</p><p>因此，💥Job类不应该使用preStop来执行结束逻辑💥，而是应该将逻辑放在容器内执行。k8s无法提前获悉容器退出信号，因此无法在容器退出前发送preStop。</p><h2 id="容器生命周期-探针">容器生命周期-探针</h2><blockquote><p><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes">https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes</a></p><p><a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle-1">https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle-1</a></p></blockquote><p>k8s通过探测器判断何时重启容器。</p><p>探测器种类：启动探测器startupProbe、存活探测器livenessProbe，就绪探测器readinessProbe。</p><p>每种探测器探测方式：命令方式，http方式，tcp方式。</p><ul><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#execaction-v1-core">ExecAction</a>： 在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。</li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#tcpsocketaction-v1-core">TCPSocketAction</a>： 对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。</li><li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#httpgetaction-v1-core">HTTPGetAction</a>： 对容器的 IP 地址上指定端口和路径执行 HTTP Get 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。</li></ul><p>例如命令方式：</p><pre><code class="language-yaml">    livenessProbe:      exec:        command:        - cat        - /tmp/healthy      initialDelaySeconds: 5      periodSeconds: 5</code></pre><p>优先级：启动探测器&gt;存活探测器&gt;就绪探测器</p><p>👙默认行为：任何没有显式指定的探针，自动设为<code>success</code>。</p><p>kubelet 使用启动探测器可以知道应用程序容器什么时候启动了。 如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查， 确保这些存活、就绪探测器不会影响应用程序的启动。 这可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。</p><p>若启动探针存在，则其它两类探针将延后运行。行为上，探针失败则重启【容器】.。</p><p>若存活探针存在，行为上，探针失败重启【容器】。</p><p>若就绪探针存在，行为上，探针成功Pod加入svc端点列表。默认为<code>failure</code>，因此，成功之前svc会将Pod从端点列表中移除。</p><blockquote><p>首先，就绪探针比较符合常见需求，即探针失败，则不会提供服务。但它不会重启容器，因此无法自动解决问题。</p><p>其次，需要存活探针来判断程序的基本条件从而确保容器重启。</p><p>最后，启动探针可以判断环境，环境不满足直接重启。</p></blockquote><p>三种探测器，均在 pod.spec.containers 下配置</p><pre><code class="language-bash">➜  ~ kubectl explain pod.spec.containers | grep Probe   livenessProbe        &lt;Object&gt;   readinessProbe       &lt;Object&gt;   startupProbe &lt;Object&gt;</code></pre><h3 id="探测器的探测周期配置">探测器的探测周期配置</h3><ul><li><code>initialDelaySeconds</code>：容器启动后要等待多少秒后存活和就绪探测器开始第一次探测，默认是 0 秒，最小值是 0。</li><li><code>periodSeconds</code>：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。</li><li><code>timeoutSeconds</code>：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。</li><li><code>successThreshold</code>：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。 存活和启动探测的这个值必须是 1。最小值是 1。</li><li><code>failureThreshold</code>：当探测失败时，Kubernetes 的重试次数。 存活探测情况下的放弃就意味着重新启动容器。 就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。</li></ul><h3 id="启动探测器-startupProbe">启动探测器 startupProbe</h3><p>💥目的在于设置一个容器启动宽容期，适合慢速启动程序的初期检测。只要启动探测器在超时时间以内成功一次，后续的健康状态将交给存活探测器。</p><p>💥如果启动探测器在超时时间内没有成功，则容器直接被杀死，而pod按照 restartPolicy 策略执行。</p><p>例如：</p><pre><code class="language-yaml">ports:- name: liveness-port  containerPort: 8080  hostPort: 8080startupProbe:  httpGet:    path: /healthz    port: liveness-port  failureThreshold: 30  # 错误次数阈值  periodSeconds: 10  # 检测间隔周期  # 启动超时时间=failureThreshold*periodSeconds</code></pre><p>上述例子意思是：kubelet 检测总时间超过 300 秒后，如果还未检测成功，认为启动失败。</p><h3 id="存活探测器-livenessProbe">存活探测器 livenessProbe</h3><p>💥目的在于确认容器是否健康，但在不健康的时候重启容器。它将在容器整个生命周期中运行。</p><p>💥它与就绪探测器最大区别就是，kubelet检测失败后会重启容器。</p><p>例子如下：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  labels:    test: liveness  name: liveness-httpspec:  containers:  - name: liveness    image: k8s.gcr.io/liveness    args:    - /server    livenessProbe:      httpGet:        path: /healthz        port: 8080        httpHeaders:        - name: Custom-Header          value: Awesome      initialDelaySeconds: 3      periodSeconds: 3</code></pre><blockquote><p>探测器会认为 200&lt;=x&lt;400 的状态码为正常。</p></blockquote><h3 id="就绪探测器-readinessProbe">就绪探测器 readinessProbe</h3><p>👙生产应用应该有的配置。</p><p>💥目的在于仅确认容器是否健康，但是并不重启容器；它将在容器整个生命周期中运行。</p><p>💥容器刚启动的时候，就绪探针会告知kubelet是否把当前pod的endpoint地址加入svc 。</p><p>💥容器运行中如果就绪探针失败，就绪探针负责告知kubelet将pod endpoint从svc中移除</p><p>适用于下列场景：</p><ol><li>容器启动后，加载数据多，加载完之前无法提供服务</li></ol><p>例如：</p><pre><code class="language-yaml:">apiVersion: v1kind: Podmetadata:  name: goproxy  labels:    app: goproxyspec:  containers:  - name: goproxy    image: k8s.gcr.io/goproxy:0.1    ports:    - containerPort: 8080    readinessProbe:      tcpSocket:        port: 8080      initialDelaySeconds: 5      periodSeconds: 10</code></pre><p>上述例子意思是：kubelet 等待 initialDelaySeconds 后开始第一次检测，每次检测间隔10秒，检测成功，认为可以提供服务</p><h2 id="最佳实践">最佳实践</h2><ol><li><p>存活和就绪探针的健康状态接口应该是两个互相独立的接口</p></li><li><p>存活探针的健康状态接口应该是直接返回http code，不应该有任何逻辑</p></li><li><p>就绪探针的健康状态接口应该提供就绪所需要的逻辑。但是不应该添加重新构建就绪的逻辑。</p><p>例如：处理http请求的程序需要一个数据库连接的就绪状态，那么就应该在就绪探针的健康状态接口里添加数据库连接的检查逻辑。</p></li><li><p>存活探针可以用来判定程序的最简单功能是否正常，如果最简单都不正常，则无需让程序启动。</p></li><li><p>就绪探针可以用来判定程序复杂一些的逻辑是否正常。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞预警</title>
      <link href="posts/4ebbd3a8/"/>
      <url>posts/4ebbd3a8/</url>
      
        <content type="html"><![CDATA[<p>定义预警分为几个步骤</p><ol><li>定义预警媒介</li><li>定义预警用户</li><li>定义预警动作</li></ol><p>触发器达到阈值，调用预警动作，预警动作调用预警用户，预警用户调用预警媒介</p><h2 id="预警媒介">预警媒介</h2><p>配置-报警媒介类型，这里有两个要素</p><h3 id="报警媒介类型">报警媒介类型</h3><p>类型有很多种，比如邮件，自定义脚本，这里我们选自定义脚本，来定义一个企业微信预警</p><p><img src="/posts/4ebbd3a8/image-20200902144015196.png" alt="image-20200902144015196"></p><p>脚本需要接收三个参数：</p><p>{ALERT.SENDTO}：收件人</p><p>{ALERT.SUBJECT}：主题</p><p>{ALERT.MESSAGE}：内容</p><p>我们一般只用{ALERT.SENDTO}和{ALERT.MESSAGE}即可</p><h3 id="信息模板">信息模板</h3><p>定义信息模板，一般我们定义告警模板/恢复模板/自动注册模板</p><p>告警模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144408513.png" alt="image-20200902144408513"></p><pre><code>【[骷髅]】=》&#123;TRIGGER.SEVERITY&#125; 事件 : &#123;EVENT.ID&#125;【问题时间】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125; 【问题名字】: &#123;EVENT.NAME&#125;【问题主机】: &#123;HOST.NAME&#125;【问题级别】: &#123;EVENT.SEVERITY&#125;【阈值信息】: &#123;TRIGGER.NAME&#125;【阈值项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;【Operational data】: &#123;EVENT.OPDATA&#125;&#123;TRIGGER.URL&#125;</code></pre><p>恢复模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144504865.png" alt="image-20200902144504865"></p><pre><code>【[OK]】=》&#123;TRIGGER.SEVERITY&#125; 事件 : &#123;EVENT.ID&#125;【问题时间】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125; 【问题恢复时间】: &#123;EVENT.RECOVERY.DATE&#125;  &#123;EVENT.RECOVERY.TIME&#125;【问题名字】: &#123;EVENT.NAME&#125;【问题主机】: &#123;HOST.NAME&#125;【问题级别】: &#123;EVENT.SEVERITY&#125;【阈值信息】: &#123;TRIGGER.NAME&#125;【阈值项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;【Operational data】: &#123;EVENT.OPDATA&#125;&#123;TRIGGER.URL&#125;</code></pre><p>自动注册模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144535568.png" alt="image-20200902144535568"></p><pre><code>自动注册: 主机名: &#123;HOST.HOST&#125;主机ip: &#123;HOST.IP&#125;代理端口: &#123;HOST.PORT&#125;</code></pre><h3 id="放置发送信息的脚本">放置发送信息的脚本</h3><p>具体位置以zb server的安装和配置为基准.</p><pre><code class="language-python">#!/usr/bin/env python3&quot;&quot;&quot;@author: zyh@contact: aaa103439@hotmail.com@software: vscode@file: sendchat.py@time: 2020/02/05@use: pip3 install requests configparser&quot;&quot;&quot;import sys, os, requests, pathlib, json, configparserimport loggingimport logging.handlersfrom datetime import datetimeclass PySendchat():    def __init__(self, corpid, agentid, secret, touser, content):        self.corpid=corpid        self.agentid=agentid        self.secret=secret        self.touser=touser        self.content=content        LOG_PATHDIR=os.path.dirname(os.path.abspath(__file__))        LOG_FILENAME = '&#123;0&#125;/sendchat.log'.format(LOG_PATHDIR)        self.my_logger = logging.getLogger('SendChat')        self.my_logger.setLevel(logging.INFO)        # Add the log message handler to the logger        handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=102400000, backupCount=5)        # create formatter and add it to the handlers        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')        handler.setFormatter(formatter)        self.my_logger.addHandler(handler)    def gettoken(self):        self.my_logger.info('-----------------------------------------------------------------------------------------')        pwd=os.path.dirname(os.path.abspath(__file__))        tokenfile='&#123;0&#125;/wechat.&#123;1&#125;'.format(pwd,self.agentid)        if pathlib.Path(tokenfile).exists():            tokenfilectime=os.path.getctime(tokenfile)            currenttime=datetime.now().timestamp()            dtime=currenttime-tokenfilectime            self.my_logger.info('&#123;0&#125; lived &#123;1&#125;s.'.format(tokenfile, dtime))            if dtime &gt;= 7200:                try:                    os.remove(tokenfile)                    self.my_logger.info('Token file &#123;0&#125;: delete success'.format(tokenfile))                except Exception as e:                    self.my_logger.error('Token file:&#123;0&#125; delete error.Reason:&#123;1&#125;'.format(tokenfile,e))                    exit        # check token file        try:            tokensize = os.path.getsize(tokenfile)        except Exception as e:            self.my_logger.info('Token file is not exist.Reason:&#123;0&#125;'.format(e))            tokensize = 0        # get token from token file        if tokensize != 0:            with open(tokenfile, 'rb') as fd:                token = fd.read() # get token success                self.my_logger.info('Get token from token file.')            jsonObject = json.loads(token.decode(encoding='utf8'))            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token        # get token from weixin api        else:            try:                self.my_logger.info('New Token Create.')                f = requests.get('https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&#123;0&#125;&amp;corpsecret=&#123;1&#125;'.format(self.corpid, self.secret))                token = f.content                self.my_logger.info('Get token from weixin api.')                jsonObject = json.loads(token.decode(encoding='utf8'))                errcode=int(jsonObject.get(&quot;errcode&quot;))                if errcode != 0:                    errmsg=jsonObject.get(&quot;errmsg&quot;)                    self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(errmsg))                    exit()            except Exception as e:                self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(e))                exit()            try:                self.my_logger.info('Write token to &#123;0&#125;.'.format(tokenfile))                with open(tokenfile, 'wb') as fd:                    fd.write(token)            except Exception as e:                self.my_logger.error('Write &#123;0&#125; error!Reason:&#123;1&#125;'.format(tokenfile,e))                exit()            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token    def sendmsg(self):        accessToken = self.gettoken()        self.my_logger.info('Token:&#123;0&#125;'.format(accessToken))        sendMapDirectroy = &#123;&#125;        sendMapDirectroy[&quot;agentid&quot;] = self.agentid        sendMapDirectroy[&quot;touser&quot;] = self.touser        sendMapDirectroy[&quot;msgtype&quot;] = &quot;text&quot;        sendMapDirectroy[&quot;safe&quot;] = &quot;0&quot;        contentDirectory = &#123;&#125;        sendMapDirectroy[&quot;text&quot;] = contentDirectory        contentDirectory[&quot;content&quot;] = self.content        bodyStr = json.dumps(sendMapDirectroy, ensure_ascii=False).encode(encoding=&quot;utf-8&quot;)        try:            f = requests.post(url=&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s&quot; % accessToken,                          data=bodyStr, timeout=5)            self.my_logger.info(f.content)        except Exception as e:            self.my_logger.error('Send chat network error!Reason:&#123;0&#125;'.format(e))if __name__ == '__main__':    appname = sys.argv[1]    content = sys.argv[3]    # read conf.ini    conf = configparser.ConfigParser()    conf_path = os.path.dirname(os.path.abspath(__file__))    conf_ini = &quot;&#123;0&#125;/conf.ini&quot;.format(conf_path)    if pathlib.Path(conf_ini).exists():        conf.read(conf_ini)        corpid = conf.get(&quot;wechat&quot;, &quot;corpid&quot;)        appinfo = conf.get(&quot;app&quot;, appname)        agentid = appinfo.split(':')[0]        secret = appinfo.split(':')[1]        groupname = conf.get(&quot;group&quot;, appname)        touser = groupname.split(':')[0]        chatobj = PySendchat(corpid, agentid, secret, touser, content)    else:        print('conf.ini error')        exit()    try:        chatobj.sendmsg()    except:        chatobj.my_logger.error(&quot;Send chat failure!&quot;)</code></pre><pre><code class="language-ini"># conf.ini[wechat]corpid = [app]it = &lt;app_agent_id&gt;:&lt;app_secret&gt;[group]it = usera|userb</code></pre><p>eg:</p><p>python3 <a href="http://wechat.py">wechat.py</a> it ‘’ &lt;预警内容&gt;</p><p>因 zabbix 调用脚本并非 root 用户，所以给脚本附加 x 权限，避免权限问题。</p><pre><code class="language-bash">chmod a+x wechat.py# 因脚本会生成其它文件# 所以还需要给脚本所在的目录添加其它用户的写入权限chmod 757 alertscripts</code></pre><h2 id="预警用户">预警用户</h2><p>管理-用户-添加用户，这里有三要素</p><h3 id="用户">用户</h3><p>你无需登陆这个用户，所以密码可以尽量的复杂</p><p><img src="/posts/4ebbd3a8/image-20200902150131147.png" alt="image-20200902150131147"></p><h3 id="报警媒介">报警媒介</h3><p><img src="/posts/4ebbd3a8/image-20200902150145022.png" alt="image-20200902150145022"></p><blockquote><p>我们定义这个用户被调用的时候，将会发送信息给it组，至于it组包含哪些人员，则由脚本定义。</p></blockquote><h3 id="权限">权限</h3><p><img src="/posts/4ebbd3a8/image-20200902150153011.png" alt="image-20200902150153011"></p><h2 id="预警动作">预警动作</h2><p>配置-动作-左上角（触发器动作Trigger actions），这里我们需要定义两个要素</p><ol><li>什么条件下执行动作</li><li>动作的实际行为</li></ol><h3 id="条件">条件</h3><p>需要注意的是计算方式</p><blockquote><p>问题没有被制止的意思：没有人为的关闭预警</p></blockquote><p><img src="/posts/4ebbd3a8/image-20200902150806009.png" alt="image-20200902150806009"></p><h3 id="操作">操作</h3><p><img src="/posts/4ebbd3a8/image-20200902151037597.png" alt="image-20200902151037597"></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
            <tag> wechat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞容器搭建</title>
      <link href="posts/25f36846/"/>
      <url>posts/25f36846/</url>
      
        <content type="html"><![CDATA[<h2 id="创建网络">创建网络</h2><pre><code class="language-bash">docker network create -d bridge zbnet</code></pre><h2 id="mysql">mysql</h2><pre><code class="language-bash">mkdir -p /export/docker-data-mysql/&#123;config,logs,data&#125;docker run --name mysql57 \-p 3306:3306 \--network zbnet \--restart=always \--mount 'type=bind,src=/export/docker-data-mysql/config,dst=/etc/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/logs,dst=/var/log/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/data,dst=/var/lib/mysql' \-e MYSQL_ROOT_PASSWORD=123456 \-d mysql:5.7</code></pre><h2 id="zabbix-server">zabbix-server</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-server-mysql">https://hub.docker.com/r/zabbix/zabbix-server-mysql</a></p></blockquote><p>安装 centos + server + mysql 版本</p><p>默认，环境变量 <code>MYSQL_USER</code> and <code>MYSQL_PASSWORD</code> are <code>zabbix</code>, <code>zabbix</code>.</p><pre><code>mkdir -p /export/docker-data-zbserver/alertscriptsdocker volume create zabbixServerEtcdocker run --name=zabbix_server \-p 10051:10051 \--network zbnet \--restart=always \--mount 'type=volume,src=zabbixServerEtc,dst=/etc/zabbix' \--mount 'type=bind,src=/export/docker-data-zbserver/alertscripts,dst=/usr/lib/zabbix/alertscripts' \-e DB_SERVER_HOST=&quot;mysql57&quot; \-e MYSQL_DATABASE=&quot;zabbixserver&quot; \-e MYSQL_USER=&quot;zabbix&quot; \-e MYSQL_PASSWORD=&quot;zabbix&quot; \-e MYSQL_ROOT_PASSWORD=&quot;123456&quot; \-d zabbix/zabbix-server-mysql:centos-latest</code></pre><ol><li><p>默认安装完之后，容器内缺少一些开发环境，这会导致我们的告警脚本执行失败，比如我是用 python3 写的微信告警，而环境里没有。因此需要额外加装。</p><pre><code class="language-bash">docker logs -f zabbix_server # 可以看到如下错误203:20200831:030724.713 Failed to execute command &quot;/usr/lib/zabbix/alertscripts/wechat.py 'it' '自动注册: xxx-use-001-10-240-128-100' '主机名: xxx-use-001-10-240-128-100主机ip: xxx代理端口: 10050'&quot;: env: 'python3': No such file or directory</code></pre><pre><code class="language-bash"># 安装 python3 和模块docker exec -it -u root zabbix_server yum install python3docker exec -it -u root zabbix_server pip3 install requests configparser</code></pre></li><li><p>alertscripts 目录需要有 other 可执行权限</p><pre><code class="language-bash">chmod o+w /export/docker-data-zbserver/alertscripts</code></pre><p>因为 zabbix_server 在执行告警脚本的时候，用的是普通用户</p></li></ol><h2 id="zabbix-server-web">zabbix-server-web</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-web-nginx-mysql">https://hub.docker.com/r/zabbix/zabbix-web-nginx-mysql</a></p></blockquote><pre><code class="language-bash"># 安装字体yum install google-noto-sans-simplified-chinese-fonts.noarch -ydocker run --name zabbix_web \-p 80:8080 \-p 443:8443 \--restart=always \--network zbnet \--mount 'type=bind,src=/usr/share/fonts/google-noto/NotoSansSC-Regular.otf,dst=/usr/share/zabbix/assets/fonts/DejaVuSans.ttf' \-e DB_SERVER_HOST=&quot;mysql57&quot; \-e MYSQL_DATABASE=&quot;zabbixserver&quot; \-e MYSQL_USER=&quot;zabbix&quot; \-e MYSQL_PASSWORD=&quot;zabbix&quot; \-e MYSQL_ROOT_PASSWORD=&quot;123456&quot; \-e ZBX_SERVER_HOST=&quot;zabbix_server&quot; \-e PHP_TZ=&quot;Asia/Shanghai&quot; \-d zabbix/zabbix-web-nginx-mysql:latest</code></pre><blockquote><p><a href="http://ip">http://ip</a> 即可</p><p>初始账户密码是 Admin/zabbix</p></blockquote><p>到这里，你就可以访问zabbix了，不过这时候你会发现zabbix提示agent不可达</p><p><img src="/posts/25f36846/image-20200901172913999.png" alt="image-20200901172913999"></p><p>其原因是我们还没创建agent</p><p>需要注意的是，因为默认zabbix server的主机配置项监听的是 127.0.0.1:10050, 并且主机名配置的是</p><p><code>zabbix server</code>. 这和我们本文档有一些冲突，所以需要修改一些配置。</p><ol start="2"><li>修改主机配置中的 hostname 为 agent 容器中的环境变量 ZBX_HOSTNAME。这里我们为 zabbix_server</li><li>修改主机配置中的监听地址为 zabbix_server，且监听方式为 dns</li></ol><p>最终修改完如下：</p><p><img src="/posts/25f36846/image-20200901175159400.png" alt="image-20200901175159400"></p><h3 id="节点自动注册规则">节点自动注册规则</h3><p>需配合agent配置，实现节点agent启动后，自动配置server端。</p><p><img src="/posts/25f36846/image-20220406143143263.png" alt="image-20220406143143263"></p><h2 id="zabbix-agent">zabbix-agent</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-agent">https://hub.docker.com/r/zabbix/zabbix-agent</a></p></blockquote><pre><code class="language-bash">docker volume create zbAgentEtcdocker run --name zabbix_agent \--network=container:zabbix_server \--mount 'type=volume,src=zbAgentEtc,dst=/etc/zabbix/zabbix_agentd.d' \-e ZBX_DEBUGLEVEL=&quot;3&quot; \-e ZBX_HOSTNAME=&quot;zabbix_server&quot; \-e ZBX_SERVER_HOST=&quot;zabbix_server&quot; \-d zabbix/zabbix-agent:latest</code></pre><p>这里 zabbix_agent 采用网络模式为容器模式，并加入到 zabbix_server 容器中，以便于 zabbix_server 可以找到 zabbix_agent</p><p>其它配置变量详见上面官方文档</p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞离线迁移脚本</title>
      <link href="posts/c83ca575/"/>
      <url>posts/c83ca575/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">databases=&quot;&quot;mysqltonew()&#123;    olddatabasehost=    olddatabaseport=    olddatabase=$1    oldUserName=    oldpassword=    newdatabasehost=    newolddatabaseport=    newdatabase=$&#123;olddatabase&#125;    newUserName=    newpassword=    # utf8 / utf8mb4    read -p &quot;character [utf8/utf8mb4/latin1]:&quot; Character    # 导出老库    mysqldump -u$&#123;oldUserName&#125; -p$&#123;oldpassword&#125; -h$&#123;olddatabasehost&#125; -P$&#123;olddatabaseport&#125; --default-character-set=$&#123;Character&#125; --single-transaction $&#123;olddatabase&#125; &gt; $&#123;olddatabase&#125;.sql    # 创建新库    mysql -u$&#123;newUserName&#125; -p$&#123;newpassword&#125; -h$&#123;newdatabasehost&#125; -P$&#123;newolddatabaseport&#125; -e &quot;CREATE DATABASE $&#123;newdatabase&#125; DEFAULT CHARSET $&#123;Character&#125; COLLATE $&#123;Character&#125;_general_ci;&quot;    # 导入新库    mysql -u$&#123;newUserName&#125; -p$&#123;newpassword&#125; -h$&#123;newdatabasehost&#125; -P$&#123;newolddatabaseport&#125; --default-character-set=$&#123;Character&#125; $&#123;newdatabase&#125; &lt; $&#123;olddatabase&#125;.sql&#125;for i in $&#123;databases&#125;;do        echo &quot;start $i&quot;        mysqltonew $idone</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
            <tag> mysqldump </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞03Pod</title>
      <link href="posts/94518966/"/>
      <url>posts/94518966/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>pod 可以说是 k8s 的基础单元. 我觉得可以类比云环境的ecs/ec2这一类的基本计算单元.而 pod 上运行的容器， 可以类比为ecs/ec2上的app程序.</p><p>你总能在k8s的各类资源中找到云环境对应的资源影子. 如果你用过GCP，你会更有这种感觉.</p><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/">https://kubernetes.io/docs/concepts/workloads/pods/</a></p><h2 id="Pod与容器">Pod与容器</h2><p>一个pod可以拥有多个容器。</p><p>pod 内包含多个容器，所以多个容器共享以下资源。</p><ul><li><p>PID命名空间: pod内的进程能互相看到PID</p></li><li><p>网络命名空间: pod中的多个容器共享一个ip (唯一)</p></li><li><p>IPC命名空间: pod中的多个容器之间可以互相通信</p></li><li><p>UTS命名空间: pod中的多个容器共享一个主机名 (唯一)</p></li><li><p>存储卷: pod多个容器可以共同访问pod定义的存储卷</p></li></ul><p>另外，Pod可以包含一个init的特殊容器，它始终首先运行。</p><p>若pod只有一个容器，那么pod就是一个包装器</p><p>若pod有多个容器，则一般主容器提供服务；边车/挂斗/附属容器提供额外功能，例如刷新主容器的文件，收集日志。</p><p>ℹ️上述的功能的实现基于pod内的容器共享网络命名空间和存储空间.</p><p><img src="https://d33wubrfki0l68.cloudfront.net/aecab1f649bc640ebef1f05581bfcc91a48038c4/728d6/images/docs/pod.svg" alt="example pod diagram"></p><blockquote><p>如果你玩过星际争霸，那么应该知道一个人族建筑物，总是会有一个附属建筑物，它很小，但提供了主建筑物所需的科技。</p></blockquote><p>因此，除非你两个容器必须放在一起，否则你应该用多个单容器pod.</p><h2 id="Pod与负载控制器">Pod与负载控制器</h2><p>生产环境中，pod 一般不单独使用，因为单独使用意味着没有高可用，且难以管理。k8s建议 pod 要始终和负载控制器一起使用。负载控制器可以批量创建pod。</p><p>k8s将负载控制器主要分为三种:</p><ul><li>Deployment 无状态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li><li>StatefulSet 有状态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a></li><li>DaemonSet 守护态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a></li></ul><p>还有CronJob、Jobs任务类型的.</p><p>负载控制器需要依托于镜像模板创建 pod 和依托于缩放规则控制 pod 数量。</p><p>镜像模板即 pod 模板(pod template).</p><h2 id="负载控制器-Pod模板">负载控制器 - Pod模板</h2><p>一个构建nginx的例子</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80</code></pre><h3 id="如何寻找最合适的kind所属的apiVersion">如何寻找最合适的kind所属的apiVersion</h3><p>在这里可能有人不知道如何选择apiVersion。你可以通过kubectl api-versions来找到kind所属的apiGroup，然后再通过</p><p>kubectl get --raw “/apis” 的输出找 preferredVersion。</p><pre><code class="language-bash">➜   kubectl api-resources | grep deploymentdeployments                       deploy       apps/v1                                true         Deployment➜   kubectl get --raw &quot;/apis&quot;|jq '.groups[]|select(.name==&quot;apps&quot;)'&#123;  &quot;name&quot;: &quot;apps&quot;，  &quot;versions&quot;: [    &#123;      &quot;groupVersion&quot;: &quot;apps/v1&quot;，      &quot;version&quot;: &quot;v1&quot;    &#125;  ]，  &quot;preferredVersion&quot;: &#123;    &quot;groupVersion&quot;: &quot;apps/v1&quot;，    &quot;version&quot;: &quot;v1&quot;  &#125;&#125;</code></pre><p>如上命令所示，在我的k8s版本中kind: Deployment的最合适apiVersion是apps/v1</p><h2 id="pod-存储">pod 存储</h2><p>这是一个大问题. 如果你想真正的使用k8s的pod资源， 那么需要先看这一部分的内容.</p><p>简单来说， 存储资源主要分网络和本地两大类.</p><ul><li><p>本地 用于临时或者特殊环境. 就如同云服务中的【存储类节点】里的那种本地盘， 它不可靠. 因为pod本身默认是不强制绑定某个节点的，因此如果你pod异常了，那么它有可能重建的时候漂移到其它节点.此时你如果用本地盘，那么数据将丢失。</p></li><li><p>网络 可以对接的有云服务厂家的存储资源，也可以对接自建的nfs这一类网络存储。</p></li></ul><p>细致的说明， 参考官方文档https://kubernetes.io/docs/concepts/storage/</p><h2 id="pod-网络">pod 网络</h2><p>鉴于前面提到的pod类同于ecs/ec2. 因此. pod中的容器就如同ecs/ec2里的app一样，都有相同的ip， 端口范围， 主机名.</p><p>k8s的网络基于各种插件.每一种插件的实现详情见官网. <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model">https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model</a></p><p>如果你是本地搭建， 那么常用的插件是Flannel. 如果你是在云服务上搭建，那么建议使用云服务已有的k8s服务.</p><p>如果你必须在云服务上自己搭建，那么aws/azure/gcp都有对应的网络插件.它可以让你在k8s中结合使用云服务的网络组件.<br>当然你依然可以用 flannel 网络插件。</p><h2 id="静态pod">静态pod</h2><p>特点：</p><ol><li>永远运行在固定节点</li><li>由所在节点的kubelet管理，但只负责保活，即pod崩溃重生</li><li>kubelet会让apiserver创建一个镜像pod，便于可以通过kubectl查询到静态pod</li></ol><p>配置：</p><ol><li>存放在 /etc/kubernetes/manifests 当采用kubeadm安装的时候，一般位于此目录。具体需要去看kubelet配置。</li><li>配置本身可以按照标准pod方式来创建</li></ol><p>检测：</p><ol><li>kubelet会定期检测配置目录加载配置创建/重建pod</li></ol><blockquote><p>当你通过kubeadm创建的时候，那么k8s的几个重要组件均会以静态pod的方式在master节点上创建，你可以在/etc/kubernetes/manifests/这里找到他们的配置</p></blockquote><pre><code class="language-bash">➜   ll /etc/kubernetes/manifests/total 16-rw------- 1 root root 1848 Aug 25 16:28 etcd.yaml-rw------- 1 root root 2709 Aug 25 16:28 kube-apiserver.yaml-rw------- 1 root root 2564 Aug 25 16:33 kube-controller-manager.yaml-rw------- 1 root root 1120 Aug 25 16:33 kube-scheduler.yaml</code></pre><h2 id="容器生命周期">容器生命周期</h2><ul><li>waiting，一般对应Pod的pending阶段</li><li>running，容器运行OK</li><li>terminated，容器退出</li></ul><h2 id="Pod-生命周期">Pod-生命周期</h2><p>Pod启动、终止中涉及到的各个组件。</p><p><img src="/posts/94518966/image-20220506125719802.png" alt="image-20220506125719802"></p><p>pod生命周期状态包含5个状态：</p><ul><li>pending，调度时间和拉取容器镜像时间，容器均处于waiting状态</li><li>running，pod里所有容器都已经创建，且至少一个容器处于启动、重启、running状态</li><li>failed，pod里的容器全退出，有部分容器以非0状态进入terminated状态</li><li>succeeded，pod里的容器全部以0状态进入terminated状态，并且不会再重启</li><li>unknown，pod所在节点和主节点之间失联，不过这种状态会因k8s的策略转为failed状态</li></ul><p>pod是通过uid来鉴别，而不是pod名，pod被替换时名称可以不变。</p><h3 id="重启策略">重启策略</h3><p><a href="https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle">https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle</a></p><p>pod重启的启示间隔时间是10s，成指数上涨，但不超过5分钟。一旦重启成功且运行10分钟，则重置为10s。</p><p>pod.spec中定义：restartPolicy：Always、Nerver、OnFailure</p><p>从实际使用来说：</p><ul><li>Job重启策略通常是：OnFailure或者Nerver</li><li>ReplicaSet、DaemonSet、Deployment重启策略通常是：Always</li></ul><h3 id="运行状况">运行状况</h3><p>通过<code>kubectl describe pod/&lt;pod_name&gt;</code>查看<code>Conditions</code>字段条件</p><pre><code class="language-bash">Conditions:  Type              Status  Initialized       True  Ready             False  ContainersReady   False  PodScheduled      True</code></pre><blockquote><p>Ready为True，即表示【应该】被加入到svc的端点列表中。</p><p>但极端情况下有可能因为其它服务没准备好，导致及时Pod的Ready为True，svc也无法正常的转发流量到Pod。</p><p>这种行为，可能在滚动更新的时候，会导致丢数据。</p></blockquote><p>针对上述问题，kubernetes允许在上面4个默认状态的基础上自定义就绪状态类型。</p><p><a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate">https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate</a></p><h3 id="终止流程">终止流程</h3><p><a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination">https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination</a></p><ol><li><p>Apiserver拿到删除请求、Apiserver更新pod状态，转为<code>Terminating</code>。默认Pod有30 秒优雅终止时间。</p></li><li><p>kubelet推送preStop事件到容器中执行，如果设置了preStop的话</p></li><li><p>kubelet通过<code>container runtime</code>发送TERM信号(kill-14)给每个容器中pid为1的进程号，并同时将pod从svc端点中剥离。</p><ol><li>如果容器在30秒内没有停止成功，则kubelet会发送SIGKILL信号（kill -9）给容器，强行杀掉。</li></ol></li><li><p>容器关闭状态转为Terminated，Apiserver将Pod删除。</p></li></ol><blockquote><ul><li>2和3是并行的，并且执行时间取决于Pod终止宽限期<code>terminationGracePeriodSeconds</code></li><li>kubectl 添加<code>--grace-period=0 --force</code>可以立即删除Pod</li></ul></blockquote><p>ℹ️失败的pod状态会根据<code>kube-controller-manager</code> 参数 <code>terminated-pod-gc-threshold</code>阈值设置保存一定数量。默认这个值是<code>12500</code>，不清楚为何设置这么高。<a href="https://github.com/kubernetes/kubernetes/pull/79047%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E8%A2%AB%E9%A9%B3%E5%9B%9E%E7%9A%84%E4%BF%AE%E6%AD%A3%EF%BC%8C%E5%AE%83%E6%8F%90%E8%AE%AE%E8%AE%BE%E7%BD%AE%E4%B8%BA">https://github.com/kubernetes/kubernetes/pull/79047这是一个被驳回的修正，它提议设置为</a><code>500</code>。</p><h2 id="容器Terminated">容器Terminated</h2><p>通过下方命令可以查找Terminated的原因。</p><pre><code class="language-bash">kubectl get pod -o go-template='&#123;&#123;range.status.containerStatuses&#125;&#125;&#123;&#123;"Container Name: "&#125;&#125;&#123;&#123;.name&#125;&#125;&#123;&#123;"\r\nLastState: "&#125;&#125;&#123;&#123;.lastState&#125;&#125;&#123;&#123;end&#125;&#125;'  &lt;pod_name:simmemleak&gt;simmemleakContainer Name: simmemleakLastState: map[terminated:map[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]]</code></pre><h2 id="Downward-API">Downward-API</h2><p>👙需要注意的是，<code>Downward API</code> 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息。而如果你想要获取 Pod 容器运行后才会出现的信息，比如，容器进程的 PID，那就肯定不能使用 <code>Downward API</code> 了，而应该考虑在 Pod 里定义一个 sidecar 容器来获取了。</p><p>其支持的字段有：</p><pre><code class="language-bash">1. 使用 fieldRef 可以声明使用:spec.nodeName - 宿主机名字status.hostIP - 宿主机IPmetadata.name - Pod的名字metadata.namespace - Pod的Namespacestatus.podIP - Pod的IPspec.serviceAccountName - Pod的Service Account的名字metadata.uid - Pod的UIDmetadata.labels['&lt;KEY&gt;'] - 指定&lt;KEY&gt;的Label值metadata.annotations['&lt;KEY&gt;'] - 指定&lt;KEY&gt;的Annotation值metadata.labels - Pod的所有Labelmetadata.annotations - Pod的所有Annotation2. 使用 resourceFieldRef 可以声明使用:容器的 CPU limit容器的 CPU request容器的 memory limit容器的 memory request</code></pre><h3 id="环境变量方式">环境变量方式</h3><p>通过Downward API 获取Pod信息并存入到环境变量中，然后在容器里打印出来。</p><pre><code class="language-yaml"># env-pod.yamlapiVersion: v1kind: Podmetadata:  name: env-pod  namespace: kube-systemspec:  containers:  - name: env-pod    image: busybox    command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot;]    env:    - name: POD_NAME      valueFrom:        fieldRef:          fieldPath: metadata.name    - name: POD_NAMESPACE      valueFrom:        fieldRef:          fieldPath: metadata.namespace    - name: POD_IP      valueFrom:        fieldRef:          fieldPath: status.podIP</code></pre><h3 id="volume卷方式">volume卷方式</h3><p>将Pod的metadata.labels和metadata.annotations以 labels 和 annotations 文件方式挂载到容器里的 /etc/podinfo 目录下。</p><pre><code class="language-yaml"># volume-pod.yamlapiVersion: v1kind: Podmetadata:  name: volume-pod  namespace: kube-system  labels:    k8s-app: test-volume    node-env: test  annotations:    own: youdianzhishi    build: testspec:  volumes:  - name: podinfo    downwardAPI:      items:      - path: labels        fieldRef:          fieldPath: metadata.labels      - path: annotations        fieldRef:          fieldPath: metadata.annotations  containers:  - name: volume-pod    image: busybox    args:    - sleep    - &quot;3600&quot;    volumeMounts:    - name: podinfo      mountPath: /etc/podinfo</code></pre><pre><code class="language-bash">➜  ~ kubectl exec -it volume-pod /bin/sh -n kube-system/ # ls /etc/podinfo/..2019_11_13_09_57_15.990445016/  annotations..data/                           labels/ # cat /etc/podinfo/labelsk8s-app=&quot;test-volume&quot;/ # cat /etc/podinfo/annotationsbuild=&quot;test&quot;kubectl.kubernetes.io/last-applied-configuration=&quot;&#123;\&quot;apiVersion\&quot;:\&quot;v1\&quot;,\&quot;kind\&quot;:\&quot;Pod\&quot;,\&quot;metadata\&quot;:&#123;\&quot;annotations\&quot;:&#123;\&quot;build\&quot;:\&quot;test\&quot;,\&quot;own\&quot;:\&quot;youdianzhishi\&quot;&#125;,\&quot;labels\&quot;:&#123;\&quot;k8s-app\&quot;:\&quot;test-volume\&quot;,\&quot;node-env\&quot;:\&quot;test\&quot;&#125;,\&quot;name\&quot;:\&quot;volume-pod\&quot;,\&quot;namespace\&quot;:\&quot;kube-system\&quot;&#125;,\&quot;spec\&quot;:&#123;\&quot;containers\&quot;:[&#123;\&quot;args\&quot;:[\&quot;sleep\&quot;,\&quot;3600\&quot;],\&quot;image\&quot;:\&quot;busybox\&quot;,\&quot;name\&quot;:\&quot;volume-pod\&quot;,\&quot;volumeMounts\&quot;:[&#123;\&quot;mountPath\&quot;:\&quot;/etc/podinfo\&quot;,\&quot;name\&quot;:\&quot;podinfo\&quot;&#125;]&#125;],\&quot;volumes\&quot;:[&#123;\&quot;downwardAPI\&quot;:&#123;\&quot;items\&quot;:[&#123;\&quot;fieldRef\&quot;:&#123;\&quot;fieldPath\&quot;:\&quot;metadata.labels\&quot;&#125;,\&quot;path\&quot;:\&quot;labels\&quot;&#125;,&#123;\&quot;fieldRef\&quot;:&#123;\&quot;fieldPath\&quot;:\&quot;metadata.annotations\&quot;&#125;,\&quot;path\&quot;:\&quot;annotations\&quot;&#125;]&#125;,\&quot;name\&quot;:\&quot;podinfo\&quot;&#125;]&#125;&#125;\n&quot;kubernetes.io/config.seen=&quot;2019-11-13T17:57:15.320894744+08:00&quot;kubernetes.io/config.source=&quot;api&quot;</code></pre><h2 id="注意点">注意点</h2><ol><li>pod.spec.containers.ports 这里只是一个容器端口的信息公告，哪怕不设定，容器内程序的端口也会暴露。也就是说，如果你容器是nginx，其端口是80。而哪怕你这里设置的 pod.spec.conntainers.port.containerPort 是8080，则实际暴露的依然是80。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞04Pod-init容器 - 容器 - k8s - pod</title>
      <link href="posts/3ec72f7e/"/>
      <url>posts/3ec72f7e/</url>
      
        <content type="html"><![CDATA[<h2 id="使用场景">使用场景</h2><ul><li>Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。 例如，没有必要仅为了在安装过程中使用类似 <code>sed</code>、<code>awk</code>、<code>python</code> 或 <code>dig</code> 这样的工具而去 <code>FROM</code> 一个镜像来生成一个新的镜像。</li><li>Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。</li><li>应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。</li><li>Init 容器能以不同于 Pod 内应用容器的文件系统视图运行。因此，Init 容器可以访问应用容器不能访问的 <a href="https://kubernetes.io/zh/docs/concepts/configuration/secret/">Secret</a> 的权限。</li><li>由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器 提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。 一旦前置条件满足，Pod 内的所有的应用容器会并行启动。</li></ul><h2 id="例子">例子</h2><p>等待其他服务就绪的例子</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: myapp-pod  labels:    app: myappspec:  initContainers:  - name: init-myservice    image: busybox:1.28    command: ['sh', '-c', &quot;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&quot;]  - name: init-mydb    image: busybox:1.28    command: ['sh', '-c', &quot;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&quot;]  containers:  - name: myapp-container    image: busybox:1.28    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']</code></pre><p>通过添加init容器（init-myservice和init-mydb），等待svc对象创建，延迟启动应用容器（myapp-container）。</p><p>需要对服务进行预处理的例子</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: init-demospec:  volumes:  - name: workdir    emptyDir: &#123;&#125;  initContainers:  - name: install    image: busybox    command:    - wget    - &quot;-O&quot;    - &quot;/work-dir/index.html&quot;    - http://www.baidu.com  # https    volumeMounts:    - name: workdir      mountPath: &quot;/work-dir&quot;  containers:  - name: web    image: nginx    ports:    - containerPort: 80    volumeMounts:    - name: workdir      mountPath: /usr/share/nginx/html</code></pre><p>这个例子，通过init容器（install）来提前拉取代码到 workdir 本地临时卷，然后再将 workdir 本地临时卷挂载到 nginx 容器的 /usr/share/nginx/html。</p><h2 id="启动顺序">启动顺序</h2><p>init容器按照Pod.spec.initContainers中定义的顺序启动。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞kubectl多集群管理</title>
      <link href="posts/c15432fa/"/>
      <url>posts/c15432fa/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://github.com/sunny0826/kubecm">sunny0826/kubecm: Easier management of kubeconfig. (github.com)</a></p><h2 id="下载、安装">下载、安装</h2><pre><code class="language-bash">wget -O kubecm_release.tgz https://github.com/sunny0826/kubecm/releases/download/v0.15.3/kubecm_0.15.3_Linux_x86_64.tar.gztar xf kubecm_release.tgz &amp;&amp; rm -rf kubecm_release.tgzmv kubecm /usr/local/bin/</code></pre><h2 id="基本使用">基本使用</h2><blockquote><p><a href="https://kubecm.cloud/#/en-us/cli/kubecm_add">add (kubecm.cloud)</a></p></blockquote><p>添加配置 zyh.yaml</p><pre><code class="language-yaml">apiVersion: v1clusters:- cluster:    server: https://x.x.x.x:6443    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSU      name: kubernetescontexts:- context:    cluster: kubernetes                                                                                         user: &quot;kubernetes-admin&quot;  name: aaa103439current-context: kubernetes-adminkind: Configpreferences: &#123;&#125;users:- name: &quot;kubernetes-admin&quot;  user:    client-certificate-data: LS0tLS1CRUdJTiBDRV        client-key-data: LS0tLS1CRUdJTiBSU0EgUFJ</code></pre><p>💥<code>contexts.[].name</code>不可与其它集群配置重复.</p><pre><code class="language-bash">kubecm add -f zyh.yaml # 合并 zyh.yaml 到 ~/.kube/config===选择 true</code></pre><p>切换配置</p><pre><code class="language-bash">kubecm switch qa</code></pre><p>删除配置</p><pre><code class="language-bash">kubecm delete qa</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞02-1切换使用runtime-containerd</title>
      <link href="posts/c3c9f0f8/"/>
      <url>posts/c3c9f0f8/</url>
      
        <content type="html"><![CDATA[<h1>基础</h1><blockquote><p>需要注意的是，替换runtime，需要重新下载镜像。这会严重的加大Pod的恢复时间。</p><p>特别是在国内，一些镜像因为某些不可描述的原因，可能会无法下载成功。</p><p>另外，虽然镜像一样，但是containerd无法去找到docker本地的镜像缓存。至少我还没找到解决方式。。。</p></blockquote><p>配置必要系统环境</p><pre><code class="language-bash">cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.confoverlaybr_netfilterEOFsudo modprobe overlaysudo modprobe br_netfilter# Setup required sysctl params, these persist across reboots.cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.confnet.bridge.bridge-nf-call-iptables  = 1net.ipv4.ip_forward                 = 1net.bridge.bridge-nf-call-ip6tables = 1EOF# Apply sysctl params without rebootsudo sysctl --system</code></pre><h1>安装</h1><p>containerd可以从docker源中获取</p><pre><code class="language-bash"># add repoyum-config-manager \    --add-repo \    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# installyum install containerd.io# modify containerd configsudo mkdir -p /etc/containerdcontainerd config default | sudo tee /etc/containerd/config.toml</code></pre><p>修正配置/etc/containerd/config.toml，更改存储根路径</p><pre><code class="language-yaml">root=&quot;&quot;</code></pre><p>修正配置/etc/containerd/config.toml，使用 systemd cgroup driver</p><pre><code class="language-bash">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]  ...  [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]    SystemdCgroup = true</code></pre><p>修正配置/etc/containerd/config.toml，使用国内163镜像源</p><pre><code class="language-yaml">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]          endpoint = [&quot;http://hub-mirror.c.163.com&quot;,&quot;https://registry-1.docker.io&quot;]</code></pre><h1>更改runtime</h1><p>当使用kubeadm安装集群的时候：</p><ul><li>kubeadm 会将kubelet的配置写入 /var/lib/kubelet/config.yaml和cm对象</li><li>kubeadm 会将启动参数以环境变量的方式写入/var/lib/kubelet/kubeadm-flags.env</li></ul><p>配置选项可以参考https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration</p><p>启动参数可以参考 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/</a></p><p>我们需要将kubelet的runtime变更为containerd</p><pre><code class="language-bash"># 手动追加runtime参数到kubelet配置：/var/lib/kubelet/kubeadm-flags.env--cgroup-driver=systemd --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock</code></pre><h1>安装客户端工具crictl</h1><p><a href="https://kubernetes.io/zh/docs/tasks/debug-application-cluster/crictl/">https://kubernetes.io/zh/docs/tasks/debug-application-cluster/crictl/</a></p><pre><code class="language-bash">wget 'https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.22.0/crictl-v1.22.0-linux-amd64.tar.gz'tar xf crictl-*.tar.gzcp crictl /usr/local/bin/</code></pre><h1>配置客户端工具</h1><pre><code class="language-bash">vim /etc/crictl.yaml</code></pre><pre><code class="language-yaml">runtime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 10debug: true</code></pre><p>endpoint的地址根据选用的runtime指定，不出意外应该是下面三个：</p><ul><li>unix:///var/run/dockershim.sock</li><li>unix:///run/containerd/containerd.sock</li><li>unix:///run/crio/crio.sock</li></ul><h1>查看当前k8s所用的镜像版本</h1><pre><code class="language-bash">docker images | grep k8s.gcr.iok8s.gcr.io/kube-apiserver:v1.22.4k8s.gcr.io/kube-controller-manager:v1.22.4k8s.gcr.io/kube-scheduler:v1.22.4k8s.gcr.io/kube-proxy:v1.22.4k8s.gcr.io/pause:3.5k8s.gcr.io/etcd:3.5.0-0k8s.gcr.io/coredns/coredns:v1.8.4</code></pre><p>✨当然，不仅仅有k8s自身镜像，还有其它部署的镜像。</p><h1>拉取镜像到containerd本地库</h1><p>根据上面的指令填充</p><pre><code class="language-bash">full_version=1.22.4cat&gt;crictl-pull-images.sh&lt;&lt;EOF#!/bin/bash# kubeadm config images listimages=(kube-apiserver:v$&#123;full_version&#125;kube-controller-manager:v$&#123;full_version&#125;kube-scheduler:v$&#123;full_version&#125;kube-proxy:v$&#123;full_version&#125;pause:3.5etcd:3.5.0-0coredns:1.8.4)for imageName in \$&#123;images[@]&#125;;do    crictl pull registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;    ctr --namespace k8s.io images tag registry.aliyuncs.com/google_containers/\$&#123;imageName&#125; k8s.gcr.io/\$&#123;imageName&#125;doneEOF</code></pre><p>保存先不执行脚本。</p><blockquote><p>ctr -n <a href="http://k8s.io">k8s.io</a> 指定位于k8s.io命名空间的镜像</p></blockquote><h1>逐个更新</h1><p>💥千万不能docker和containerd同时启动</p><pre><code class="language-bash"># nodeName nodeName=k8s01# 驱逐nodekubectl drain $&#123;nodeName&#125; --ignore-daemonsets# 校验是否只有 daemonsets podkubectl get all --all-namespaces -o wide| grep $&#123;nodeName&#125;# 关闭kubeletsystemctl stop kubelet# 关闭runtimesystemctl stop docker &amp;&amp; systemctl disable docker</code></pre><p>确保节点上异常的信息，在本文档实施过程中，出现 docker 和 kubelet 关闭的情况下，容器进行依然运行占用端口，导致containerd启动的时候无法正常运行Pod.</p><pre><code class="language-bash"># 检查是否有异常端口ss -tnalp</code></pre><pre><code class="language-bash"># 启动 containerdsystemctl start containerd &amp;&amp; systemctl enable containerd &amp;&amp; systemctl stop kubelet# 执行拉取容器镜像的脚本bash crictl-pull-images.sh# 启动 kubeletsystemctl restart kubelet# 恢复 nodekubectl uncordon $&#123;nodeName&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞02安装</title>
      <link href="posts/9602dad/"/>
      <url>posts/9602dad/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/">使用 kubeadm 引导集群 | Kubernetes</a></p><h2 id="结构">结构</h2><p><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/">高可用拓扑选项 | Kubernetes</a></p><p>我们选堆叠结构</p><p><img src="https://d33wubrfki0l68.cloudfront.net/d1411cded83856552f37911eb4522d9887ca4e83/b94b2/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg" alt="堆叠的 etcd 拓扑"></p><p>不过因为我环境里没有 LB 组件，因此选用 haproxy 来代替，又因为要为 haproxy 做高可用，因此为 haproxy 再外套一层 keepalived。</p><p>如果在阿里云构建，可以认为 LB 是健壮的。</p><h2 id="机器">机器</h2><p>机器系统都是 centos7,</p><p>三个节点，均为控制平面节点，同时也是Node节点，并利用 keepalived + haproxy 进行 <code>控制平面组件：apiserver</code> 高可用</p><table><thead><tr><th>hostname</th><th>ip</th><th>type</th></tr></thead><tbody><tr><td>k8sapi</td><td>10.200.16.100</td><td>keepalived vip</td></tr><tr><td>k8s01</td><td>10.200.16.101</td><td>master keepalived(主) haproxy</td></tr><tr><td>k8s02</td><td>10.200.16.102</td><td>master keepalived(备) haproxy</td></tr><tr><td>k8s03</td><td>10.200.16.103</td><td>master</td></tr></tbody></table><p>请务必确保内网可以通过表格里的 <code>hostname</code> 解析到对应的 <code>ip</code></p><p>请务必将系统的 hostname 改为上述表里的 hostname</p><p>数据走向：</p><p>client-&gt;keepalived(vip:8443)-&gt;haproxy(vip:8443)-&gt; all:6443</p><h2 id="所有节点">所有节点</h2><h3 id="时间同步">时间同步</h3><pre><code class="language-bash">yum install chrony -ysed -i  '1a server cn.pool.ntp.org prefer iburst' /etc/chrony.confsystemctl restart chronydsystemctl enable chronydchronyc activity</code></pre><h3 id="系统配置">系统配置</h3><pre><code class="language-bash"># 加载模块modprobe overlaymodprobe br_netfilter# 添加配置cat &gt; /etc/sysctl.d/99-kubernetes-cri.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-iptables  = 1net.ipv4.ip_forward                 = 1net.bridge.bridge-nf-call-ip6tables = 1EOF# 配置生效sysctl --system# 清空防火墙systemctl stop firewalld.service iptables.servicesystemctl disable firewalld.servicesystemctl disable iptables.service;iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X# 关闭selinuxsetenforce 0sed -i 's@^\(SELINUX=\).*@\1disabled@' /etc/sysconfig/selinuxsed -i 's@^\(SELINUX=\).*@\1disabled@' /etc/selinux/config# 关闭 swap，kubelet 1.18 要求.如果你fstab也有，请一并注释swapoff -a# 安装 ipvsyum install ipvsadm -yipvsadm --clearcat&gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt; 'EOF'ipvs_mods_dir=&quot;/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs&quot;for i in $(ls $ipvs_mods_dir | grep -o &quot;^[^.]*&quot;); do    /sbin/modinfo -F filename $i  &amp;&gt; /dev/null    if [ $? -eq 0 ]; then        /sbin/modprobe $i    fidoneEOFchmod +x /etc/sysconfig/modules/ipvs.modules/etc/sysconfig/modules/ipvs.modules</code></pre><p>一份额外的内核参数优化</p><p>/etc/sysctl.d/99-sysctl.conf</p><pre><code class="language-bash">  ### kernel  # 关闭内核组合快捷键  kernel.sysrq = 0  # 内核消息队列  kernel.msgmnb = 65536  kernel.msgmax = 65536  # 定义 core 文件名  kernel.core_uses_pid = 1  # 定义 core 文件存放路径  kernel.core_pattern = /corefile/core-%e-%p-%t&quot;  # 系统级别上限， 即整个系统所有进程单位时间可打开的文件描述符数量  fs.file-max = 6553500  ### tcp/ip  # 开启转发  net.ipv4.ip_forward = 1  # 保持反向路由回溯是关闭的，默认也是关闭  net.ipv4.conf.default.rp_filter = 0  net.ipv4.conf.default.accept_source_route = 0  #  net.ipv4.tcp_window_scaling = 1  # 针对外网访问, 开启有选择应答，便于客户端仅发送丢失报文，从而提高网络接收性能，但会增加CPU消耗  net.ipv4.tcp_sack = 1  # 三次握手请求频次  net.ipv4.tcp_syn_retries = 5  # 放弃回应一个TCP请求之前，需要尝试多少次  net.ipv4.tcp_retries1 = 3  # 三次握手应答频次  net.ipv4.tcp_synack_retries = 2  # 三次握手完毕， 没有数据沟通的情况下， 空连接存活时间  net.ipv4.tcp_keepalive_time = 60  # 探测消息发送次数  net.ipv4.tcp_keepalive_probes = 3  # 探测消息发送间隔时间  net.ipv4.tcp_keepalive_intvl = 15  net.ipv4.tcp_retries2 = 5  net.ipv4.tcp_fin_timeout = 5  # 尽量缓存syn，然而服务器压力过大的时候，并没有啥软用  net.ipv4.tcp_syncookies = 1  # 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目.放大10倍  net.core.netdev_max_backlog = 10240  # 对于还未获得对方确认的连接请求，可保存在队列中的最大数目.放大20倍  net.ipv4.tcp_max_syn_backlog = 10240  # 定义了系统中每一个端口最大的监听队列的长度.放大20倍  net.core.somaxconn=10240  # 开启时间戳  net.ipv4.tcp_timestamps=1  # 仅当服务器作为客户端的时候有效，必须在开启时间戳的前提下  net.ipv4.tcp_tw_reuse = 1  #最大timewait数  net.ipv4.tcp_max_tw_buckets = 20000  net.ipv4.ip_local_port_range = 1024 65500  # 系统处理不属于任何进程的TCP链接  net.ipv4.tcp_orphan_retries = 3  net.ipv4.tcp_max_orphans = 327680  # 开启 iptables 后，链路追踪上限和超时时间, 若没有使用 iptables，则无效  net.netfilter.nf_conntrack_max = 6553500  net.netfilter.nf_conntrack_tcp_timeout_established = 150  # 下列参数如果设置不当，有可能导致系统进不去  #net.ipv4.tcp_mem = 94500000 915000000 927000000  #net.ipv4.tcp_rmem = 4096 87380 4194304  #net.ipv4.tcp_wmem = 4096 16384 4194304  #net.core.wmem_default = 8388608  #net.core.rmem_default = 8388608  #net.core.rmem_max = 16777216  #net.core.wmem_max = 16777216  #kernel.shmmax = 68719476736  #kernel.shmall = 4294967296</code></pre><h3 id="安装容器">安装容器</h3><pre><code class="language-bash">yum install -y yum-utils \  device-mapper-persistent-data \  lvm2yum-config-manager \    --add-repo \    https://download.docker.com/linux/centos/docker-ce.repo</code></pre><h4 id="选择版本">选择版本</h4><p>一定要选择所安装的 k8s 版本兼容的最新容器版本</p><blockquote><p><a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker">https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker</a></p></blockquote><pre><code class="language-bash">yum list docker-ce --showduplicates | sort -r===docker-ce.x86_64            3:20.10.7-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.6-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.5-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.4-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.3-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.2-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.1-3.el7                    docker-ce-stable#################### 选择好所要安装的版本 ####################docker_version=20.10.7  # 例如选择20.10.7</code></pre><h4 id="安装并启动">安装并启动</h4><pre><code class="language-bash"># 安装兼容k8s的docker版本yum install -y docker-ce-$&#123;docker_version&#125; docker-ce-cli-$&#123;docker_version&#125;#sed -i '/ExecStart=/a ExecStartPort=/usr/sbin/iptables -P FORWARD ACCEPT' /usr/lib/systemd/system/docker.service;mkdir -p /etc/docker;cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;data-root&quot;: &quot;/export/docker-data-root&quot;,  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;100m&quot;  &#125;,  &quot;storage-driver&quot;: &quot;overlay2&quot;,  &quot;storage-opts&quot;: [    &quot;overlay2.override_kernel_check=true&quot;  ]&#125;EOFsystemctl daemon-reloadsystemctl restart dockersystemctl enable docker</code></pre><h3 id="安装-kubelet-kubeadm-kubectl">安装 kubelet kubeadm kubectl</h3><blockquote><p>阿里巴巴镜像点：</p><p><a href="https://developer.aliyun.com/mirror/kubernetes">https://developer.aliyun.com/mirror/kubernetes</a></p><p>google 官方安装文档 <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p></blockquote><ol><li>google 官方源</li></ol><pre><code class="language-bash">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearchenabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOF</code></pre><ol start="2"><li>阿里源</li></ol><pre><code class="language-bash">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF</code></pre><ol start="3"><li>安装</li></ol><pre><code class="language-bash"># 确定 kube* 版本yum list kube* --showduplicates --disableexcludes=kubernetes# 选择要安装的版本full_verions=1.20.8-0# 安装 kube 管理组件和 kubeletyum install -y kubelet-$&#123;full_verions&#125; kubeadm-$&#123;full_verions&#125; kubectl-$&#123;full_verions&#125; --disableexcludes=kubernetessystemctl enable --now kubelet</code></pre><h2 id="高可用组件节点">高可用组件节点</h2><h3 id="安装负载均衡组件-haproxy-和-高可用组件-keepalived">安装负载均衡组件 haproxy 和 高可用组件 keepalived</h3><blockquote><p>k8s01，k8s02 执行</p></blockquote><ul><li>安装</li></ul><pre><code class="language-bash">yum install haproxy keepalived -y</code></pre><ul><li>haproxy配置</li></ul><pre><code class="language-bash">cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.defaultcat&gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal    log /dev/log local0    log /dev/log local1 notice    daemondefaults    mode                    http    log                     global    option                  httplog    option                  dontlognull    option http-server-close    option forwardfor       except 127.0.0.0/8    option                  redispatch    retries                 1    timeout http-request    10s    timeout queue           20s    timeout connect         5s    timeout client          20s    timeout server          20s    timeout http-keep-alive 10s    timeout check           10sfrontend apiserver    bind *:8443    mode tcp    option tcplog    default_backend apiserverbackend apiserver    option httpchk GET /healthz    http-check expect status 200    mode tcp    option ssl-hello-chk    balance     roundrobin        server k8sapivip 10.200.16.101:6443 check        server k8sapivip 10.200.16.102:6443 check        server k8sapivip 10.200.16.103:6443 checkEOF</code></pre><ul><li>keepalived 主配置</li></ul><blockquote><p>两节点配置不一样</p></blockquote><pre><code class="language-bash">cp /etc/keepalived/keepalived.conf  /etc/keepalived/keepalived.conf.default############## 放在 k8s01 ################cat&gt;/etc/keepalived/keepalived.conf&lt;&lt;EOFglobal_defs &#123;    router_id LVS_DEVEL&#125;vrrp_script check_apiserver &#123;  script &quot;/etc/keepalived/check_apiserver.sh&quot;  interval 3  weight -2  fall 10  rise 2&#125;vrrp_instance VI_1 &#123;    state MASTER  #     interface ens192  # 物理网卡名    virtual_router_id 51    priority 101  #    authentication &#123;        auth_type PASS        auth_pass 42    &#125;    virtual_ipaddress &#123;        10.200.16.100    &#125;    track_script &#123;        check_apiserver    &#125;&#125;EOF############## 放在 k8s02 ################cat&gt;/etc/keepalived/keepalived.conf&lt;&lt;EOFglobal_defs &#123;    router_id LVS_DEVEL&#125;vrrp_script check_apiserver &#123;  script &quot;/etc/keepalived/check_apiserver.sh&quot;  interval 3  weight -2  fall 10  rise 2&#125;vrrp_instance VI_1 &#123;    state BACKUP  #     interface ens192  # 物理网卡名    virtual_router_id 51    priority 100  #    authentication &#123;        auth_type PASS        auth_pass 42    &#125;    virtual_ipaddress &#123;        10.200.16.100    &#125;    track_script &#123;        check_apiserver    &#125;&#125;EOF</code></pre><ul><li>keepalived 检测脚本</li></ul><blockquote><p>k8s01和k8s02均需要</p></blockquote><pre><code class="language-bash">cat&gt;/etc/keepalived/check_apiserver.sh&lt;&lt;'EOF'#!/bin/sherrorExit() &#123;    echo &quot;*** $*&quot; 1&gt;&amp;2    exit 1&#125;APISERVER_VIP=10.200.16.100APISERVER_DEST_PORT=6443curl --silent --max-time 2 --insecure https://localhost:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://localhost:$&#123;APISERVER_DEST_PORT&#125;/&quot;if ip addr | grep -q $&#123;APISERVER_VIP&#125;; then    curl --silent --max-time 2 --insecure https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/&quot;fiEOFchmod a+x /etc/keepalived/check_apiserver.sh</code></pre><ul><li>启动keepalived和haproxy</li></ul><pre><code class="language-bash">systemctl start keepalived haproxysystemctl enable keepalived haproxy</code></pre><h2 id="控制平面节点">控制平面节点</h2><h3 id="拉取组件镜像">拉取组件镜像</h3><blockquote><p>k8s01，k8s02，k8s03 执行</p></blockquote><blockquote><p>鉴于网络问题，所以国内一般无法直接运行初始化命令，因此最好先自行安装好包</p></blockquote><pre><code class="language-bash"># 查看对应版本 k8s 所需包yum list --showduplicates kubeadm --disableexcludes=kubernetes | grep $&#123;master_verions&#125;===kubeadm.x86_64                       1.20.8-0                        @kuberneteskubeadm.x86_64                       1.20.0-0                        kuberneteskubeadm.x86_64                       1.20.1-0                        kuberneteskubeadm.x86_64                       1.20.2-0                        kuberneteskubeadm.x86_64                       1.20.4-0                        kuberneteskubeadm.x86_64                       1.20.5-0                        kuberneteskubeadm.x86_64                       1.20.6-0                        kuberneteskubeadm.x86_64                       1.20.7-0                        kuberneteskubeadm.x86_64                       1.20.8-0                        kubernetes</code></pre><pre><code class="language-bash">full_version=1.20.8kubeadm config images list --kubernetes-version=$&#123;full_version&#125;===k8s.gcr.io/kube-apiserver:v1.20.8k8s.gcr.io/kube-controller-manager:v1.20.8k8s.gcr.io/kube-scheduler:v1.20.8k8s.gcr.io/kube-proxy:v1.20.8k8s.gcr.io/pause:3.2k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns:1.7.0</code></pre><p>根据上面的输出版本，修改下面脚本中 <code>pause</code> <code>etcd</code> <code>coredns</code> 的版本号</p><pre><code class="language-bash">cat&gt;images-pull.sh&lt;&lt;EOF#!/bin/bash# kubeadm config images listimages=(kube-apiserver:v$&#123;full_version&#125;kube-controller-manager:v$&#123;full_version&#125;kube-scheduler:v$&#123;full_version&#125;kube-proxy:v$&#123;full_version&#125;pause:3.2     # 修改我etcd:3.4.13-0 # 修改我coredns:1.7.0 # 修改我)for imageName in \$&#123;images[@]&#125;;do    docker pull registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;    docker tag registry.aliyuncs.com/google_containers/\$&#123;imageName&#125; k8s.gcr.io/\$&#123;imageName&#125;    docker rmi registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;doneEOF</code></pre><pre><code class="language-bash">bash images-pull.sh</code></pre><h3 id="第一个控制平面节点安装（k8s01）">第一个控制平面节点安装（k8s01）</h3><p>(info: <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/%EF%BC%89">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/）</a></p><h4 id="采取配置文件安装模式">采取配置文件安装模式</h4><pre><code class="language-bash">kubeadm config print init-defaults &gt; kubeadm.yaml.default</code></pre><p>配置文件详细参数介绍</p><p><a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration">https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration</a></p><p>根据生成的默认配置，来动态的调整，因为 k8s 版本变更很快，所以下面的配置不一定是正确的</p><pre><code class="language-bash">cat &gt; kubeadm.yaml &lt;&lt; EOF# kubeadm.yaml 将默认的配置进行修改apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.8272827282sksksu   # 改我，改成你自己随意的字符串  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 10.200.16.101  # 改我，改成第一个控制平面节点的物理ip  bindPort: 6443nodeRegistration:  criSocket: /var/run/dockershim.sock  # 如果是containerd，则改为/run/containerd/containerd.sock  name: k8s01 # 改我，理论上它会自动获取配置所在节点的 hostname  taints:  - effect: NoSchedule    key: node-role.kubernetes.io/master---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;controllerManagerExtraArgs:  address: 0.0.0.0  # 开放kube-controller-manager，便于之后prometheus监控scheduler: &#123;&#125;schedulerExtraArgs:  address: 0.0.0.0  # 开放kube-scheduller，便于之后prometheus监控controlPlaneEndpoint: &quot;k8sapi:8443&quot; # 新加我，k8sapi 是 apiserver 的负载均衡器的地址dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcd# 默认镜像地址是 k8s.gcr.io，国内基本没法访问。这里改为阿里云的，当然还是建议提前下载好镜像imageRepository: registry.aliyuncs.com/google_containers kind: ClusterConfigurationkubernetesVersion: v$&#123;full_version&#125; # 改我，改成你所需安装的版本networking:  dnsDomain: cluster.local  serviceSubnet: 10.96.0.0/16 # 改我，定义 svc 的网段  podSubnet: 10.97.0.0/16 # 改我，定义 pod 的网段  --- # 追加 KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvsEOF--- # 追加 KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemd</code></pre><pre><code class="language-bash"># 进行初始化安装kubeadm init --config kubeadm.yaml</code></pre><p>init调用配置流程：</p><p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/#workflow-when-using-kubeadm-init">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/kubelet-integration/#workflow-when-using-kubeadm-init</a></p><h4 id="配置通过-kubectl-访问集群">配置通过 kubectl 访问集群</h4><pre><code class="language-bash">mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# kubectl 命令补全source &lt;(kubectl completion bash)# You should now deploy a pod network to the cluster.# Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:# https://kubernetes.io/docs/concepts/cluster-administration/addons/</code></pre><pre><code class="language-bash"># 添加用户上下文到配置里，方便切换用户kubectl config set-context admin --cluster=kubernetes --user=kubernetes-admin</code></pre><h4 id="调整-kube-controller-manager-和-scheduler-的配置">调整 kube-controller-manager 和 scheduler 的配置</h4><pre><code class="language-bash"># 检查状态kubectl get cs# 你可能会发现,出现服务连接拒绝问题controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refusedscheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused# 原因是这两个服务配置默认端口是0。至于为啥就不晓得了# 你需要注释掉两个配置里的端口（- --port=0），恢复为默认端口sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml# 重启 kubeletsystemctl restart kubelet# 再次检查kubectl get cs===NAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</code></pre><blockquote><p><code>kube-controller-manager</code> 参数 <code>terminated-pod-gc-threshold</code>用于设置保留多少失败的Pod状态，默认是12500。这么高不清楚是否有性能影响或者资源溢出风险。</p></blockquote><h4 id="配置网络插件">配置网络插件</h4><p>网络插件常用的有两种</p><p>第一种是 flannel，涉及到更多的 iptables 规则</p><p>第二种是 calico，涉及到更多的路由规则</p><ul><li>flannel (本文档选用的插件)</li></ul><blockquote><p><a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a> 代码托管地址</p></blockquote><pre><code class="language-bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改里面的 &quot;Network&quot;: &quot;10.244.0.0/16&quot;, 变更为你自己的 podSubnet 网段，即kubeadm初始化阶段的 --pod-network-cidrkubectl apply -f  kube-flannel.yml</code></pre><p>flannel 将会在每一个节点上创建一个 VTEP 设备<code>flannel.1</code>，并将podSubnet下发给所有的<code>flannel.1</code></p><ul><li>calico</li></ul><blockquote><p><a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart">https://docs.projectcalico.org/getting-started/kubernetes/quickstart</a> 安装文档</p></blockquote><pre><code class="language-bash">kubectl apply -f https://docs.projectcalico.org/manifests/tigera-operator.yamlwget https://docs.projectcalico.org/manifests/custom-resources.yaml# custom-resources.yaml 修改里面的网络为 pod 网段kubectl apply -f custom-resources.yaml</code></pre><blockquote><p>注意，当你使用了 calico 后， 会生成一些 cni 的配置，这些配置会导致你返回 flannel 的时候出现问题。例如无法创建 cni</p><p>你可以使用 find / -name ‘*calico*’ 找到所有信息，然后都删除</p></blockquote><p>默认kubeadm安装完后，禁止调度pod到master上。你可以通过下面的命令，关闭所有master节点的禁止调度</p><pre><code class="language-bash"> kubectl taint nodes --all node-role.kubernetes.io/master-</code></pre><h4 id="配置web控制台dashboard">配置web控制台dashboard</h4><blockquote><p><a href="https://github.com/kubernetes/dashboard">https://github.com/kubernetes/dashboard</a> 代码托管地址</p></blockquote><pre><code class="language-bash">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml &amp;&amp; mv recommended.yaml kubernetes-dashboard.yaml</code></pre><ul><li>如果你部署了 metallb，或者有云服务的 lb，那么只需要修改 kubernetes-dashboard.yaml 配置中的 svc 对象类型为  LoadBalancer</li></ul><pre><code class="language-yaml">kind: ServiceapiVersion: v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443  type: LoadBalancer   # 这里新加一条  selector:    k8s-app: kubernetes-dashboard</code></pre><p>之后通过 lb 地址访问即可</p><ul><li>如果你没有配置 metallb，也没有部署在云服务中（即没有云LB），那么需要修改 svc 类型为 NodePort，并且将 pod 的部署到固定的节点上，例如 k8s01</li></ul><pre><code class="language-bash"># 这里我们要修改一些东西# #---spec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort   # 这里新加一条#---# 还需要修改一下部署的位置，如果你集群中已经加入了多个节点，则会导致 pod 分发到其它节点上。这里我们强制分发到 master 上. 找到 kind: Deployment 配置，并修改两个 pod 的分发位置为 nodeName: &lt;master 节点主机名&gt;#---    spec:      nodeName: k8s01    # 这里新加一条      containers:        - name: kubernetes-dashboard          image: kubernetesui/dashboard:v2.0.3              spec:      nodeName: k8s01    # 这里新加一条      containers:        - name: dashboard-metrics-scraper          image: kubernetesui/metrics-scraper:v1.0.4          #---# 然后创建kubectl create -f kubernetes-dashboard.yaml</code></pre><p>之后通过 <a href="https://k8s01:30001">https://k8s01:30001</a> 访问</p><ul><li>获取访问web服务的token</li></ul><p>dashboard本身是一个pod，如果你想让pod去访问其它的k8s资源，则需要给pod创建一个服务账户(serviceaccount).</p><p>构建一个服务账户admin-user，并通过ClusterRoleBinding授权admin级别的ClusterRole对象</p><pre><code class="language-bash"># 添加访问 token# 官方文档： https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md# 通过下面内容创建 kube-db-auth.yamlcat &gt; kube-db-auth.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata:  name: admin-user  namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: admin-userroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: admin-user  namespace: kubernetes-dashboardEOFkubectl apply -f kube-db-auth.yaml# 通过下面命令拿到 tokenkubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '&#123;print $1&#125;')  </code></pre><p><img src="/posts/9602dad/image-20210711163654410.png" alt="image-20210711163654410"></p><h3 id="获取其他节点加入集群的命令（k8s01）">获取其他节点加入集群的命令（k8s01）</h3><h4 id="新加控制平面">新加控制平面</h4><pre><code class="language-bash"># 命令格式kubeadm join k8sapi:8443 \--token xxx \--discovery-token-ca-cert-hash xxx \--control-plane --certificate-key xxx</code></pre><h4 id="新加Node节点">新加Node节点</h4><pre><code class="language-bash"># 命令格式kubeadm join k8sapi:8443 \--token xxx \--discovery-token-ca-cert-hash xxx </code></pre><p>加入集群的命令是有24小时有效期，过期之后需要重建命令。</p><h4 id="命令重建">命令重建</h4><p>添加Node节点命令</p><pre><code class="language-bash"># 添加Node节点的命令add_node_command=`kubeadm token create --print-join-command`echo $&#123;add_node_command&#125;</code></pre><p>添加控制平面节点命令</p><pre><code class="language-bash"># 创建 --certificate-keyjoin_certificate_key=`kubeadm init phase upload-certs --upload-certs|tail -1`# 命令组合echo &quot;$&#123;add_node_command&#125; --control-plane --certificate-key $&#123;join_certificate_key&#125;&quot;</code></pre><h3 id="k8s02和k8s03作为控制平面添加到集群">k8s02和k8s03作为控制平面添加到集群</h3><pre><code class="language-bash">kubeadm join k8sapi:8443 \--token h77qhb.cjj8ig4t2v15dncm \--discovery-token-ca-cert-hash sha256:b7138bb76d19d3d7baf832e46559c400f4fe544d8f0ee81a87f \--control-plane --certificate-key fd996c7c0c2047c9c10a377f25a332bf4b5b00ca# sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml# 重启 kubeletsystemctl restart kubelet# 根据提示，执行相关命令，一般都是下面的命令mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 等待一分钟，查看所有 pod 是否正常kubectl get pod -n kube-system===NAME                            READY   STATUS    RESTARTS   AGEcoredns-74ff55c5b-9gkgk         1/1     Running   0          2d13hcoredns-74ff55c5b-tndb5         1/1     Running   0          15detcd-k8s01                      1/1     Running   0          15detcd-k8s02                      1/1     Running   0          7m12setcd-k8s03                      1/1     Running   0          15dkube-apiserver-k8s01            1/1     Running   0          26hkube-apiserver-k8s02            1/1     Running   0          7m11skube-apiserver-k8s03            1/1     Running   0          26hkube-controller-manager-k8s01   1/1     Running   0          15dkube-controller-manager-k8s02   1/1     Running   0          82skube-controller-manager-k8s03   1/1     Running   1          15dkube-flannel-ds-42hng           1/1     Running   0          15dkube-flannel-ds-b42qj           1/1     Running   0          15dkube-flannel-ds-ss8w5           1/1     Running   0          15dkube-proxy-2xxpn                1/1     Running   0          15dkube-proxy-pg7j7                1/1     Running   0          15dkube-proxy-txh2t                1/1     Running   0          15dkube-scheduler-k8s01            1/1     Running   0          15dkube-scheduler-k8s02            1/1     Running   0          82skube-scheduler-k8s03            1/1     Running   0          15d</code></pre><h4 id="问题点：etcd已有节点信息">问题点：etcd已有节点信息</h4><p>如果etcd中曾经有k8s02和k8s03的节点信息，则你需要先从etcd中删除，否则加入的时候，会卡在检测etcd处，并最终报错.</p><p>删除etcd信息方式：</p><pre><code class="language-bash"># 输出 etcd 节点 idETCD=`docker ps|grep etcd|grep -v POD|awk '&#123;print $1&#125;'`docker exec \  -it $&#123;ETCD&#125; \  etcdctl \  --endpoints https://127.0.0.1:2379 \  --cacert /etc/kubernetes/pki/etcd/ca.crt \  --cert /etc/kubernetes/pki/etcd/peer.crt \  --key /etc/kubernetes/pki/etcd/peer.key \  member list===2f401672c9a538f1, started, k8s01, https://10.200.16.101:2380, https://10.200.16.101:2379, falsed6d9ca2a70f6638e, started, k8s02, https://10.200.16.102:2380, https://10.200.16.102:2379, falseee0e9340a5cfb4d7, started, k8s03, https://10.200.16.103:2380, https://10.200.16.103:2379, false# 假设这里我要删除 k8s03ETCD=`docker ps|grep etcd|grep -v POD|awk '&#123;print $1&#125;'`docker exec \  -it $&#123;ETCD&#125; \  etcdctl \  --endpoints https://127.0.0.1:2379 \  --cacert /etc/kubernetes/pki/etcd/ca.crt \  --cert /etc/kubernetes/pki/etcd/peer.crt \  --key /etc/kubernetes/pki/etcd/peer.key \  member remove ee0e9340a5cfb4d7</code></pre><p>如果是containerd，命令如下</p><pre><code class="language-bash">crictl exec -it &lt;etcd_id&gt; etcdctl --endpoints https://127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/peer.crt --key /etc/kubernetes/pki/etcd/peer.key get --prefix /coreos.com/</code></pre><h2 id="Node节点">Node节点</h2><blockquote><p>加入命令，位于 k8s01初始化命令尾部，worker加入的时候，不需要添加 --control-plane --certificate-key</p></blockquote><pre><code class="language-bash">kubeadm join k8sapi:8443 \--token h77qhb.cjj8ig4t2v15dncm \--discovery-token-ca-cert-hash sha256:b7138bb76d19d3d7baf832e46559c400f4fe544d8f0ee81a87f</code></pre><h2 id="其他内容">其他内容</h2><h3 id="容器中显示正确的可见资源">容器中显示正确的可见资源</h3><p>默认情况下,不管你如何设置资源限制,容器里的可见资源都等于节点资源.也就是说你在容器里查看/proc/meminfo显示的资源并不是你设置的.这个问题会带来很多麻烦.</p><p>因为很多程序的参数都是根据可见资源来设定的.例如nginx的auto, 或者jvm里的内存参数.</p><blockquote><p>从 Java 8u191开始, jvm 已经默认实现了容器感知( -XX:+UseContainerSupport). 因此无需安装下面的 lxcfs 方案.</p><p>并且, 在容器中建议只设置如下内存参数:</p><p>-XX:MaxRAMPercentage  最大堆内存=物理内存百分比, 建议为容器限制的50%-75% . 毕竟还需要预留其它内存.</p></blockquote><p>而社区常见的作法是用<code>lxcfs</code> 来提升资源的可见性.<code>lxcfs</code> 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器, 它也可以支持Docker容器.</p><p>LXCFS通过用户态文件系统, 在容器中提供下列 <code>procfs</code> 的文件.</p><pre><code class="language-bash">/proc/cpuinfo/proc/diskstats/proc/meminfo/proc/stat/proc/swaps/proc/uptime</code></pre><p>与我们资源限制有关的, 主要是 cpuinfo 和 meminfo.</p><p>目前社区的做法如下:</p><ol><li><p>所有节点安装 fuse-libs 包.</p><pre><code class="language-bash">yum install -y fuse-libs</code></pre></li><li><p>集群部署 lxcfs 的 deployment 资源对象</p><pre><code class="language-bash">git clone https://github.com/denverdino/lxcfs-admission-webhook.gitcd lxcfs-admission-webhookvim deployment/lxcfs-daemonset.yaml=== 修正配置里的 apiVersion=== 当前git里的代码是陈旧的...代码里的版本在 1.18 已经被废弃). 具体归属于什么版本, 请参考k8s官方api文档 === https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#daemonset-v1-appsapiVersion:apps/v1  kubectl apply -f deployment/lxcfs-daemonset.yamlkubectl get pod#等待 lxcfs pod 状态成 running#如果你发现 pod 始终出错，那么执行下列命令：kubectl delete -f deployment/lxcfs-daemonset.yamlrm -rf /var/lib/lxcfskubectl apply -f deployment/lxcfs-daemonset.yamldeployment/install.shkubectl get secrets,pods,svc,mutatingwebhookconfigurations</code></pre></li><li><p>给相关namespace注入lxcfs，例如default</p><pre><code class="language-bash">kubectl label namespace default lxcfs-admission-webhook=enabled</code></pre></li><li><p>重启添加了资源限制的pod, 此时 /proc/cpuinfo 和 /proc/meminfo 将会正确显示.</p></li></ol><h3 id="集群指标监控服务">集群指标监控服务</h3><p>添加 metrics-server  <a href="https://github.com/kubernetes-sigs/metrics-server#configuration">https://github.com/kubernetes-sigs/metrics-server#configuration</a></p><pre><code class="language-bash">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml# 修改metrics-server容器参数部分，添加额外的启动参数(arg)args:  - --kubelet-preferred-address-types=InternalIP  - --kubelet-insecure-tlskubectl apply -f components.yamlkubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;</code></pre><blockquote><p>kubectl top 指令需要指标才能输出</p></blockquote><hr><h2 id="kubectl-命令文档">kubectl 命令文档</h2><blockquote><p><a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands</a></p></blockquote><p>一些常用的命令</p><ol><li>kubectl explain</li></ol><blockquote><p>非常有用的命令，帮助你写配置文件</p></blockquote><p>输出配置某个字段的详细说明,例如 deployment.metadata</p><blockquote><p>其中标注有 -required- 的字段是必选字段</p></blockquote><pre><code class="language-bash">kubectl explain deployment.metadata</code></pre><ol start="2"><li>-o yaml --dry-run</li></ol><p>输出create命令的yaml配置</p><pre><code class="language-bash">kubectl create serviceaccount mysvcaccount -o yaml --dry-run</code></pre><ol start="3"><li>常用的工具容器</li></ol><pre><code class="language-bash">kubectl run cirros-$RANDOM -it --rm --restart=Never --image=cirros -- /bin/shkubectl run dnstools-$RANDOM -it --rm --restart=Never --image=infoblox/dnstools:latest </code></pre><h3 id="kubectl-调用的配置">kubectl 调用的配置</h3><pre><code class="language-yaml">➜   kubectl config viewapiVersion: v1clusters:- cluster:    certificate-authority-data: DATA+OMITTED    server: https://&lt;api_server&gt;:8443  name: kubernetes-qa- cluster:    certificate-authority-data: DATA+OMITTED    server: https://&lt;api_server&gt;:6443  name: kubernetes-prod  contexts:- context:    cluster: kubernetes-prod    user: kubernetes-admin  name: prod- context:    cluster: kubernetes-qa    user: user-kfktf786bk  name: qa  current-context: qa           ## 当前使用的配置kind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin  user:    client-certificate-data: REDACTED    client-key-data: REDACTED- name: user-kfktf786bk  user:    client-certificate-data: REDACTED    client-key-data: REDACTED</code></pre><h2 id="删除与清理">删除与清理</h2><pre><code class="language-bash"># 从集群里删除某个节点# master execkubectl drain &lt;NODE_ID&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;NODE_ID&gt;# worker execkubeadm resetiptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -Xipvsadm --clear;ip link set cni0 down &amp;&amp; ip link delete cni0;ip link set flannel.1 down &amp;&amp; ip link delete flannel.1;ip link set kube-ipvs0 down &amp;&amp; ip link delete kube-ipvs0;ip link set dummy0 down &amp;&amp; ip link delete dummy0;rm -rf /var/lib/cni/rm -rf $HOME/.kube;</code></pre><h2 id="节点一致性测试">节点一致性测试</h2><blockquote><p><a href="https://kubernetes.io/docs/setup/node-conformance/">https://kubernetes.io/docs/setup/node-conformance/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s-安全上下文</title>
      <link href="posts/87e37b21/"/>
      <url>posts/87e37b21/</url>
      
        <content type="html"><![CDATA[<h2 id="安全上下文">安全上下文</h2><p>应用级别：</p><ul><li><code>Container-level Security Context</code></li><li><code>Pod-level Security Context</code></li></ul><h2 id="设置">设置</h2><ul><li>访问权限控制</li><li>privileged</li><li>linux capabilities</li></ul><h2 id="Security-Context">Security Context</h2><p><img src="https://www.qikqiak.com/k3s/assets/img/security/security-context-list.png" alt="Security Context List"></p><h3 id="Pod-级别的安全上下文">Pod 级别的安全上下文</h3><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: security-context-pod-demospec:  volumes:  - name: sec-ctx-vol    emptyDir: &#123;&#125;  securityContext:    runAsUser: 1000              runAsGroup: 3000             fsGroup: 2000              containers:  - name: sec-ctx-demo    image: busybox    command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 60m&quot;]    volumeMounts:    - name: sec-ctx-vol      mountPath: /pod/demo    securityContext:      allowPrivilegeEscalation: false</code></pre><p>简要解释</p><pre><code class="language-yaml">runAsUser: 1000          # pod内所有容器的entrypoint启动的进程UID为1000runAsGroup: 3000         # pod内所有容器的entrypoint启动的进程GID为3000fsGroup: 2000            # pod内挂载的卷里的文件以及新创建的文件GID为2000</code></pre><h3 id="容器级别的安全上下文">容器级别的安全上下文</h3><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: security-context-container-demospec:  securityContext:    runAsUser: 1000  containers:  - name: sec-ctx-demo    image: busybox    command: [ &quot;sh&quot;, &quot;-c&quot;, &quot;sleep 60m&quot; ]    securityContext:      runAsUser: 2000      allowPrivilegeEscalation: false</code></pre><h2 id="linux-capabilities">linux capabilities</h2><p>linux capabilites 可以给某个程序附加其它用户以程序拥有者身份去执行的权限。</p><p>当一个执行程序被添加了SUID标识后，则其它用户去执行这个程序的时候，就可以临时代入到程序的拥有者，从而以程序拥有者的权限来执行程序。</p><p>以 /bin/passwd 命令为例，它的拥有者是root，且拥有SUID标识，因此普通用户可以代入到root来修改自身密码。</p><p>但这样，代入期间普通用户权限比较大，有安全风险，因此有了 linux capabilities。</p><p>linux capabilities 将权限细分为很多能力，从而避免超出执行程序时所需要的权限。</p><h3 id="linux与capabilities">linux与capabilities</h3><pre><code class="language-bash"># 查看➜   getcap /bin/ping/bin/ping = cap_net_raw+ep# 删除➜   setcap cap_net_raw-ep /bin/ping# 添加➜   getcap cap_net_raw+ep /bin/ping</code></pre><h3 id="docker与capabilities">docker与capabilities</h3><p>我们说容器本质上就是一个进程，所以理论上容器就会和进程一样会有一些默认的开放权限，默认情况下 Docker/Containerd 会删除必须的 <code>capabilities</code> 之外的所有 <code>capabilities</code>，因为在容器中我们经常会以 root 用户来运行，使用 <code>capabilities</code> 现在后，容器中的使用的 root 用户权限就比我们平时在宿主机上使用的 root 用户权限要少很多了，这样即使出现了安全漏洞，也很难破坏或者获取宿主机的 root 权限，所以 Docker/Containerd 支持 <code>Capabilities</code> 对于容器的安全性来说是非常有必要的。</p><p>不过我们在运行容器的时候可以通过指定 <code>--privileded</code> 参数来开启容器的超级权限，这个参数一定要慎用，因为他会获取系统 root 用户所有能力赋值给容器，并且会扫描宿主机的所有设备文件挂载到容器内部，所以是非常危险的操作。</p><p>但是如果你确实需要一些特殊的权限，可以通过 <code>--cap-add</code> 和 <code>--cap-drop</code> 这两个参数来动态调整，可以最大限度地保证容器的使用安全。</p><p>举例：</p><pre><code class="language-bash">--cap-add和--cap-drop 这两参数都支持ALL值，比如如果你想让某个容器拥有除了MKNOD之外的所有内核权限，那么可以执行下面的命令： ➜   sudo docker run --cap-add=ALL --cap-drop=MKNOD ...</code></pre><img src="https://www.qikqiak.com/k3s/assets/img/security/docker-capabilites.png" alt="docker capabilities" style="zoom:67%;"><p>默认，docker 关闭下列 capabilities</p><p><img src="https://www.qikqiak.com/k3s/assets/img/security/docker-drop-capabilites.png" alt="docker drop capabilities"></p><h3 id="k8s与capabilities">k8s与capabilities</h3><pre><code class="language-yaml"># cpb-demo.yamlapiVersion: v1kind: Podmetadata:  name: cpb-demospec:  containers:  - name: cpb    image: busybox    args:    - sleep    - &quot;3600&quot;    securityContext:      capabilities:        add: # 添加        - NET_ADMIN        drop:  # 删除        - KILL</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞记录点</title>
      <link href="posts/e1096625/"/>
      <url>posts/e1096625/</url>
      
        <content type="html"><![CDATA[<h3 id="1-pv和pvc的绑定关系是终生的-也就是说你删除了当前pvc-pv依然不能被其它pvc所绑定">1. pv和pvc的绑定关系是终生的. 也就是说你删除了当前pvc, pv依然不能被其它pvc所绑定.</h3><p>删除pvc后, pv的状态将从bound转为Released.</p><p>如果你想让pv从新绑定到原来的pvc上, 则需要两个步骤:</p><ol><li>先创建原来的pvc.</li><li>删除当前pv并重新创建, 或者修改当前pv的配置, 删除spec.claimRef信息.</li></ol><h3 id="2-pod无法正常启动，报错：“mounting-“-var-lib-lxcfs-proc-loadavg-”">2. pod无法正常启动，报错：“mounting \“/var/lib/lxcfs/proc/loadavg\”</h3><p>to rootfs \&quot;/export/docker-data-root/overlay2/710d09a6715d88a01b417ba1a669dab69b67c3d57e576c1ae6d79aa03e1b294a/merged”</p><p>根据信息可知问题点应该是 lxcfs 问题。</p><p>查看故障pod所在节点的 lxcfs pod 日志，得到信息</p><p>“fuse: mountpoint is not empty<br>fuse: if you are sure this is safe, use the ‘nonempty’ mount option”</p><p>因此删除节点 lxcfs 目录，并重建 lxcfs pod</p><pre><code class="language-bash">rm -rf  /var/lib/lxcfs/ &amp;&amp; kubectl delete pod/lxcfs-hthgq</code></pre><h3 id="3-变更pv回收策略">3. 变更pv回收策略</h3><pre><code class="language-bash">kubectl patch pv &lt;your-pv-name&gt; -p '&#123;&quot;spec&quot;:&#123;&quot;persistentVolumeReclaimPolicy&quot;:&quot;Retain&quot;&#125;&#125;'</code></pre><h3 id="4-kubernetes-qos">4. kubernetes qos</h3><p>kubernetes 有三种策略，分别是Guaranteed和Burstable和BestEffort</p><p>Guaranteed：任何容器的cpu请求值和限制值必须一样。内存同样如此</p><p>Burstable：最少有一个容器的cpu请求值和限制值必须一样。内存同样如此</p><p>BestEffort： 没有任何限制</p><h3 id="5-kubelet无法启动，报错：kubelet-service-holdoff-time-over">5. kubelet无法启动，报错：<code>kubelet.service holdoff time over</code></h3><p>确认节点swap是否关闭</p><p>临时关闭<code>swapoff -a</code></p><p>永久关闭<code>注释/etc/fstab文件里的swap挂载行</code></p><h3 id="6-拉取私有镜像">6. 拉取私有镜像</h3><pre><code class="language-shell">kubectl create secret docker-registry &lt;secret_name&gt; --docker-server=&lt;FQDN:your-registry-server&gt; --docker-username=&lt;your-name&gt; --docker-password=&lt;your-pword&gt; --docker-email=&lt;your-email&gt;</code></pre><p>deployment 对象中调用：<code>deployment.spec.template.spec.imagePullSecrets: [&lt;secret_name&gt;]</code></p><h3 id="7-pv一直无法删除掉">7. pv一直无法删除掉</h3><pre><code class="language-bash"># 检查是否受保护kubectl describe pv &lt;pv&gt; | grep Finalizers # 重置策略为空kubectl patch pv &lt;pv&gt; -p '&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;'</code></pre><h3 id="8-统计节点容器磁盘使用情况">8. 统计节点容器磁盘使用情况</h3><pre><code class="language-bash">kubectl get --raw /api/v1/nodes/&lt;节点名&gt;/proxy/stats/summary | jq '.pods[0] | &quot;PodName: &quot;, .podRef.name, &quot;usedBytes: &quot;, .containers[].rootfs.usedBytes'</code></pre><h3 id="9-在-Kubernetes-中查看崩溃的-pod-的日志">9. 在 Kubernetes 中查看崩溃的 pod 的日志</h3><p>已崩溃的pod无法通过<code>kubectl logs</code>查看.</p><pre><code class="language-bash">kubectl get events -n &lt;ns_name&gt; --sort-by='.metadata.createTimestamp''</code></pre><h3 id="10-Pod一直处于删除状态">10.Pod一直处于删除状态</h3><p>检查Pod所在节点的kubelet日志，发现出现<code>Error: &quot;UnmountVolume.TearDown failed for volume</code>无法反挂载的错误。</p><p>进一步查看错误后面信息，得知<code>Unmount的Path：/var/lib/kubelet/pods/aa104758-a3a0-4ec3-977b-be9ef8530e5c/volumes/kubernetes.io~projected/kube-api-access-6vltw/</code></p><pre><code class="language-bash">path=/var/lib/kubelet/pods/aa104758-a3a0-4ec3-977b-be9ef8530e5c/volumes/kubernetes.io~projected/kube-api-access-6vltwfind /proc/*/mounts -exec grep $&#123;path&#125; &#123;&#125; +ps -ef | grep &lt;pid 号&gt;</code></pre><p>确认占用的进程信息，是否属于处于删除状态的Pod，如果是，kill掉即可。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞搭建</title>
      <link href="posts/65820315/"/>
      <url>posts/65820315/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>prometheus 监控的构建，相比 zabbix 来说，还是要麻烦一些。</p><p>当然，如果你完全熟悉之后，配置文件化也更容易被其他系统所修改。</p><h1>监控端组件</h1><p>搞之前，先创建一个网络</p><pre><code class="language-bash">docker network create promsnet</code></pre><h2 id="Grafana-展示">Grafana 展示</h2><p><a href="https://grafana.com/docs/grafana/latest/installation/docker/">https://grafana.com/docs/grafana/latest/installation/docker/</a></p><p><a href="https://hub.docker.com/r/grafana/grafana">https://hub.docker.com/r/grafana/grafana</a></p><blockquote><p>数据展示，部署在 proms 端</p></blockquote><h3 id="配置">配置</h3><blockquote><p>/export/docker-data-grafana/conf/default.ini</p></blockquote><p>主配置里参数非常多，修改下列基本配置即可</p><pre><code class="language-ini">[server]domain = 域名http_port = 端口[database]type = sqlite3host = 127.0.0.1:3306name = grafanauser = rootpassword =[smtp]enabled = truehost = smtp.feishu.cn:465user = password = skip_verify = truefrom_address = from_name = GrafanaAdmin</code></pre><h3 id="安装启动">安装启动</h3><pre><code class="language-bash">versionTag=8.3.6mkdir -p /export/docker-data-grafana/&#123;data,conf&#125;chmod 777 /export/docker-data-grafana/datadocker run --name grafana \--restart always \--network promsnet \--mount 'type=bind,src=/export/docker-data-grafana/data,dst=/var/lib/grafana' \--mount 'type=bind,src=/export/docker-data-grafana/conf,dst=/usr/share/grafana/conf' \-p 3000:3000 -d grafana/grafana:$&#123;versionTag&#125;cat &lt;&lt; EOF | tee grafana.docker.commanddocker run --name grafana \--restart always \--network promsnet \--mount 'type=bind,src=/export/docker-data-grafana/data,dst=/var/lib/grafana' \--mount 'type=bind,src=/export/docker-data-grafana/conf,dst=/usr/share/grafana/conf' \-p 3000:3000 -d grafana/grafana:$&#123;versionTag&#125;EOF</code></pre><p>正式版本号，纯数字，没有任何test或者pre后缀</p><blockquote><p>默认账户/初始密码都是admin</p><p>默认配置库是sqlite 3</p></blockquote><h3 id="升级">升级</h3><pre><code class="language-bash">dateTime=`date +%Y%m%d`cp -rp /export/docker-data-grafana /export/docker-data-grafana.$&#123;dateTime&#125;</code></pre><h2 id="Alertmanager-告警管理">Alertmanager 告警管理</h2><p>告警管理器，接收 proms 发来的告警，经过【分组】=&gt;【收敛】=&gt;【静默】处理后通过【receiver】发出。</p><p>其配置中的标签可以是prometheus刮取数据中自带的，也可以是prometheus告警规则中自定义的labels。</p><h3 id="告警分组、收敛、静默">告警分组、收敛、静默</h3><p>分组的意思，就是将某一个组内同一时期的告警合并发送，例如根据实例来分组。</p><p>收敛的意思，就是告警A规则和告警B规则同时触发，但是告警A规则出现的时候，必然会触发告警B，此时只发告警A，例如MYSQL机器挂了，那么只需要发机器挂掉的告警，MYSQL的告警就没必要发送了。</p><p>静默的意思，就是用户已知这个时间点会触发告警规则，但是无需触发，例如高压力定时任务引起磁盘IO告警，虽然会触发平均指标告警，但是在用户认定的安全范围内，因而无需触发。</p><ul><li>分组</li></ul><p>在【route】根路由上通过【label】进行分组，同组的预警尽量一次性发出</p><pre><code class="language-yaml">route:  group_by: ['alertname']  # 告警分组指标, 可以写多个，从而进一步分组。  group_wait: 10s # 分组后，组内需告警的第一条规则需要在 10s 内等待其它可能的告警规则，等待结束后一次性发出。  group_interval: 5m # 在发生告警后，同组内需告警的规则列表需要在 5m 内等待列表可能发生的变动，若有变化，则等待结束后再次发送。  repeat_interval: 1h # 在发生告警后，同组需告警的规则列表若在 1h 内没有发生变化，则再次发送。</code></pre><p>👀<code>alertname</code>标签指的是prometheus告警规则名称</p><p>根路由【route】之后，还可以继续通过子路由【routes】来将不同【label】的告警发送给不同的【receiver】。</p><ul><li>收敛</li></ul><p>举例说明：当主机挂了，此时只需要发送主机挂掉的预警，无需再发送因主机挂掉而产出的其它预警。</p><pre><code class="language-yaml">inhibit_rules:  - source_matchers: [severity=&quot;critical&quot;]    target_matchers: [severity=~&quot;error|warning&quot;]    equal: [cluster] # 针对包含 cluster 标签的告警  - source_matchers: [severity=&quot;critical&quot;]    target_matchers: [severity=~&quot;error|warning&quot;]    equal: [instance] # 针对包含 instance 标签的告警  - source_matchers: [severity=&quot;error&quot;]    target_matchers: [severity=&quot;warning&quot;]    equal: [name] # 针对包含容器标签 name 的告警</code></pre><p>source_matchers 列出匹配标签键值对的【需告警】规则；</p><p>target_matchers  列出匹配标签键值对的【无需告警】规则；</p><p>当 equal 的标签【同时】被 source_matchers 和 target_matchers 匹配的时候，inhibit_rule 规则生效。</p><ul><li>静默</li></ul><p>web控制台上配置。就是临时关闭发送。</p><ul><li>接收者</li></ul><p>【receiver】统一由【receivers】进行配置。</p><h3 id="配置-2">配置</h3><p><a href="https://github.com/prometheus/alertmanager/blob/main/doc/examples/simple.yml">https://github.com/prometheus/alertmanager/blob/main/doc/examples/simple.yml</a></p><p>alertmanager.yml</p><pre><code class="language-bash">global:  resolve_timeout: 5m  smtp_smarthost: 'localhost:25'  smtp_require_tls: true  smtp_from: 'alertmanager@example.org'  smtp_auth_username: 'alertmanager'  smtp_auth_password: 'password'route:  group_by: ['alertname']  group_wait: 10s  group_interval: 10s  repeat_interval: 1h  receiver: 'wechat'  routes:    - matchers:        - app=~&quot;app1|app2&quot;      receiver: email      routes:        - matchers:            - severity=&quot;critical&quot;          receiver: wechatreceivers:- name: 'wechat'  wechat_configs:  - corp_id: ''    to_user: ''    agent_id: ''    api_secret: ''    send_resolved: true- name: 'email'  email_configs:  - to: &lt;tmpl_string&gt;inhibit_rules:  - source_matchers: [severity=&quot;critical&quot;]    target_matchers: [severity=~&quot;error|warning&quot;]    equal: [cluster] # 针对包含 cluster 标签的告警  - source_matchers: [severity=&quot;critical&quot;]    target_matchers: [severity=~&quot;error|warning&quot;]    equal: [instance] # 针对包含 instance 标签的告警  - source_matchers: [severity=&quot;error&quot;]    target_matchers: [severity=&quot;warning&quot;]    equal: [name] # 针对包含容器标签 name 的告警</code></pre><p>上面是微信的，你也可以webhook方式，来走其它方式，例如飞书/钉钉</p><pre><code class="language-bash">global:  resolve_timeout: 5m# 告警根路由route:  group_by: ['alertname']  group_wait: 10s  group_interval: 5m  repeat_interval: 60m  receiver: 'feishu'# 默认的接收队列receivers:- name: 'feishu'  webhook_configs:  - url: ''# 抑制规则inhibit_rules:  - source_matchers: [severity=&quot;critical&quot;]    target_matchers: [severity=~&quot;error|warning&quot;]    equal: [cluster] # 针对包含 cluster 标签的告警  - source_matchers: [severity=&quot;critical&quot;]    target_matchers: [severity=~&quot;error|warning&quot;]    equal: [instance] # 针对包含 instance 标签的告警  - source_matchers: [severity=&quot;error&quot;]    target_matchers: [severity=&quot;warning&quot;]    equal: [name] # 针对包含容器标签 name 的告警</code></pre><h3 id="告警发送中心">告警发送中心</h3><p>💁关于飞书/钉钉的webhook，可以用部署 <a href="https://gitee.com/feiyu563/PrometheusAlert">PrometheusAlert</a>实现。</p><p><a href="https://feiyu563.gitbook.io/prometheusalert/webhook">https://feiyu563.gitbook.io/prometheusalert/webhook</a></p><p>贴一个PrometheusAlert的告警模板：</p><pre><code class="language-go">&#123;&#123; $var := .externalURL&#125;&#125;&#123;&#123; range $k,$v:=.alerts &#125;&#125;&#123;&#123;if eq $v.status "resolved"&#125;&#125;**Prometheus恢复信息**告警项目：&#123;&#123;$v.labels.alertname&#125;&#125;告警级别：&#123;&#123;$v.labels.severity&#125;&#125;持续范围：&#123;&#123;GetCSTtime $v.startsAt&#125;&#125; to &#123;&#123;GetCSTtime $v.endsAt&#125;&#125;故障主机：&#123;&#123;$v.labels.instance&#125;&#125;**&#123;&#123;$v.annotations.summary&#125;&#125;**[点击打开grafana](&#123;&#123;$v.annotations.grafana&#125;&#125;)&#123;&#123;else&#125;&#125;**Prometheus告警信息**告警项目：&#123;&#123;$v.labels.alertname&#125;&#125;告警级别：&#123;&#123;$v.labels.severity&#125;&#125;开始时间：&#123;&#123;GetCSTtime $v.startsAt&#125;&#125;故障主机：&#123;&#123;$v.labels.instance&#125;&#125;**&#123;&#123;$v.annotations.summary&#125;&#125;**[点击打开grafana](&#123;&#123;$v.annotations.grafana&#125;&#125;)&#123;&#123;end&#125;&#125;&#123;&#123; end &#125;&#125;</code></pre><p>👀<code>GetCSTtime</code>是PrometheusAlert自带的一个方法，可以将时区转换成CST。</p><p>从Alertmanager发过来的消息例子，可用于在PrometheusAlert中测试模板。</p><pre><code class="language-json">&#123;&quot;receiver&quot;:&quot;web\\.hook\\.prometheusalert&quot;,&quot;status&quot;:&quot;resolved&quot;,&quot;alerts&quot;:[&#123;&quot;status&quot;:&quot;resolved&quot;,&quot;labels&quot;:&#123;&quot;alertname&quot;:&quot;container-cpu-usage-high&quot;,&quot;cpu&quot;:&quot;cpu00&quot;,&quot;desc&quot;:&quot;容器的cpu总消耗百分比&quot;,&quot;id&quot;:&quot;/docker/91afdbd4c24f22650ce&quot;,&quot;image&quot;:&quot;registry-vpc.cn-zhangjiakou.aliyuncs.com/zyh/app1:latest&quot;,&quot;instance&quot;:&quot;java001.zjk.zyh&quot;,&quot;job&quot;:&quot;auto_discovery_dns&quot;,&quot;name&quot;:&quot;app1&quot;,&quot;severity&quot;:&quot;warning&quot;,&quot;unit&quot;:&quot;%&quot;&#125;,&quot;annotations&quot;:&#123;&quot;grafana&quot;:&quot;http://&lt;grafana_ip&gt;&quot;,&quot;id&quot;:&quot;&quot;,&quot;summary&quot;:&quot;instance: java001.zjk.zyh 下的 container: app1 cpu 使用率持续超过90%.&quot;&#125;,&quot;startsAt&quot;:&quot;2022-02-17T18:06:40.611569268Z&quot;,&quot;endsAt&quot;:&quot;2022-02-17T18:06:55.611569268Z&quot;,&quot;generatorURL&quot;:&quot;http://&lt;prometheus_ip&gt;:9090&quot;,&quot;fingerprint&quot;:&quot;d48ec3&quot;&#125;],&quot;groupLabels&quot;:&#123;&quot;alertname&quot;:&quot;container-cpu-usage-high&quot;&#125;,&quot;commonLabels&quot;:&#123;&quot;alertname&quot;:&quot;container-cpu-usage-high&quot;,&quot;cpu&quot;:&quot;cpu00&quot;,&quot;desc&quot;:&quot;容器的cpu总消耗百分比&quot;,&quot;id&quot;:&quot;/docker/91afdbd4c24f22650ce&quot;,&quot;image&quot;:&quot;registry-vpc.cn-zhangjiakou.aliyuncs.com/zyh/app1:latest&quot;,&quot;instance&quot;:&quot;java001.zjk.zyh&quot;,&quot;job&quot;:&quot;auto_discovery_dns&quot;,&quot;name&quot;:&quot;cms-runner&quot;,&quot;severity&quot;:&quot;warning&quot;,&quot;unit&quot;:&quot;%&quot;&#125;,&quot;commonAnnotations&quot;:&#123;&quot;grafana&quot;:&quot;http://&lt;grafana_ip&gt;&quot;,&quot;id&quot;:&quot;&quot;,&quot;summary&quot;:&quot;instance: java001.zjk.zyh 下的 container: app1 cpu  使用率持续超过90%.&quot;&#125;,&quot;externalURL&quot;:&quot;http://e21437fd4:9093&quot;,&quot;version&quot;:&quot;4&quot;,&quot;groupKey&quot;:&quot;&#123;&#125;:&#123;alertname=\&quot;container-cpu-usage-high\&quot;&#125;&quot;,&quot;truncatedAlerts&quot;:0&#125;</code></pre><p>官方列出的POST json数据：</p><pre><code class="language-json">&#123;  &quot;version&quot;: &quot;4&quot;,  &quot;groupKey&quot;: &lt;string&gt;,              // key identifying the group of alerts (e.g. to deduplicate)  &quot;truncatedAlerts&quot;: &lt;int&gt;,          // how many alerts have been truncated due to &quot;max_alerts&quot;  &quot;status&quot;: &quot;&lt;resolved|firing&gt;&quot;,  &quot;receiver&quot;: &lt;string&gt;,  &quot;groupLabels&quot;: &lt;object&gt;,  &quot;commonLabels&quot;: &lt;object&gt;,  &quot;commonAnnotations&quot;: &lt;object&gt;,  &quot;externalURL&quot;: &lt;string&gt;,           // backlink to the Alertmanager.  &quot;alerts&quot;: [    &#123;      &quot;status&quot;: &quot;&lt;resolved|firing&gt;&quot;,      &quot;labels&quot;: &lt;object&gt;,      &quot;annotations&quot;: &lt;object&gt;,      &quot;startsAt&quot;: &quot;&lt;rfc3339&gt;&quot;,      &quot;endsAt&quot;: &quot;&lt;rfc3339&gt;&quot;,      &quot;generatorURL&quot;: &lt;string&gt;,      // identifies the entity that caused the alert      &quot;fingerprint&quot;: &lt;string&gt;        // fingerprint to identify the alert    &#125;,    ...  ]&#125;</code></pre><h3 id="启动">启动</h3><pre><code class="language-bash">mkdir -p /export/docker-data-alertmanagerdocker run --name alertmanager -d \  --restart always \  --network promsnet \  -p 9093:9093 \  --mount 'type=bind,src=/export/docker-data-alertmanager,dst=/etc/alertmanager' \  prom/alertmanager</code></pre><h2 id="Prometheus-主程">Prometheus 主程</h2><h3 id="概念">概念</h3><ul><li>数据来源：<ul><li>从各种源服务自带的 /metrics 地址中获取指标数据；</li><li>如果源服务（例如redis）里没有提供，则可以从第三方写好的抓取服务中获取指标数据；</li><li>如果是自己写的服务，则可以在服务里嵌入 prometheus 的 sdk 来暴漏指标。</li></ul></li><li>数据存储：<ul><li>默认是本地存储</li><li>可以是远程存储</li></ul></li></ul><pre><code class="language-bash">mkdir /export/docker-data-proms/&#123;data,conf&#125; -pchmod 777 /export/docker-data-proms/data</code></pre><h3 id="配置-3">配置</h3><h4 id="主配置">主配置</h4><p>/export/docker-data-proms/promethesu.yml</p><pre><code class="language-:"># my global configglobal:  scrape_interval:     15s # 从数据源处刮取数据的间隔周期。默认是1分钟  evaluation_interval: 15s # 重新评估告警表达式的间隔周期。默认是1分钟  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:  - static_configs:    - targets:      - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files:  - &quot;/etc/prometheus/conf/rules/*.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.#  - job_name: 'prometheus'#    static_configs:#    - targets: ['proms:9090']#  - job_name: &quot;grafana&quot;#    static_configs:#    - targets: ['grafana:3000']  - job_name: &quot;auto_discovery_dns&quot;    relabel_configs: # 数据获取之前，针对数据源 target 的标签修改    - source_labels: [&quot;__address__&quot;]  # 将 target 中标签键 __address__ = (.*):9100 变更为 instance = .*      regex: &quot;(.*):9100&quot; # 匹配原始标签的值      replacement: &quot;$1&quot; # 提取 regex 中拿到的值      target_label: &quot;instance&quot; # 设定要修改的目标标签      action: replace # 将目标标签的值替换为 replacement 提取的值    - source_labels: [&quot;__address__&quot;]  # 获取原始标签      regex: &quot;(.*):10052&quot; # 匹配原始标签的值      replacement: &quot;$1&quot; # 提取 regex 中拿到的值      target_label: &quot;instance&quot; # 设定要修改的目标标签      action: replace # 将目标标签的值替换为 replacement 提取的值    metric_relabel_configs: # 数据获取后，针对数据指标落盘前的修改    - source_labels: [&quot;__name__&quot;] # 获取原始标签      regex: &quot;^go_.*|^process_.*&quot; # 匹配标签值      action: drop # 删除，不让数据落盘    dns_sd_configs: # 通过 dns srv 记录自动发现    - names: [&quot;_prometheus._tcp.zjk.pj&quot;]# 启用远程存储 TSDB,以阿里云的 TSDB mlarge 规格数据库为例# 阿里云的 TSDB 推荐配置表： https://help.aliyun.com/document_detail/114516.html## Remote write configuration (TSDB).remote_write:  - url: &quot;http://ts-xxxxxxxxxxxx.hitsdb.rds.aliyuncs.com:3242/api/prom_write&quot;    # Configures the queue used to write to remote storage.    queue_config:      # Number of samples to buffer per shard before we start dropping them. 内存事件队列长度，如果满了，则新事件直接丢弃      capacity: 10000      # Maximum number of shards, i.e. amount of concurrency. 并发 shard 数      max_shards: 1      # Maximum number of samples per send. 每个 shard 每次发送多少事件。以500为例，可以简单的理解为每秒发送5000      max_samples_per_send: 500## Remote read configuration (TSDB).remote_read:  - url: &quot;http://ts-xxxxxxxxxxxx.hitsdb.rds.aliyuncs.com:3242/api/prom_read&quot;    read_recent: true</code></pre><p>relabel_configs：针对 scrape target endpoint（刮取器） 的标签修改</p><p>metric_relabel_configs：针对 scrape target endpoint 拿到的 metrics 指标数据的标签修改</p><p><code>__name__</code>是内置的针对标签键的通用名</p><h5 id="原始标签和目标标签">原始标签和目标标签</h5><p>可以在下图位置【Status】-【Service Discovery】中看到详情</p><p><img src="/posts/65820315/image-20210316182617074.png" alt="image-20210316182617074"></p><h5 id="自动发现">自动发现</h5><p>dns srv 自动发现，需要一个内部的dns服务器，例如阿里云的云私有解析等。</p><p>如何添加 srv 记录，例如让proms发现节点服务 all.it.zjk.pj:9100 和 all.it.zjk.pj:10052</p><ol><li>添加私有域 zjk.pj</li><li>添加A记录 all.it.zjk.pj -&gt; 192.168.1.1</li><li>添加srv记录 _prometheus._tcp.zjk.pj -&gt; 10 10 9100 all.it.zjk.pj</li><li>添加srv记录 _prometheus._tcp.zjk.pj -&gt; 10 10 10052 all.it.zjk.pj</li></ol><p>关于A记录就不说了，srv记录里 <code>10 10 9100 all.it.zjk.pj</code>的意思是<code>优先级 权重 服务端口 服务器域名</code></p><p>最终，你可以在 http://&lt;prometheus_ip&gt;:9090/classic/targets 看到自动发现的节点信息。</p><h4 id="定义需要告警的指标规则">定义需要告警的指标规则</h4><p>将采集的数据指标通过 expr 进行运算，并通过 record 进行命名。</p><p>关于规则里的表达式语句的写法涉及到 PromQL，可以看一下文档。</p><p><a href="https://fuckcloudnative.io/prometheus/3-prometheus/basics.html">https://fuckcloudnative.io/prometheus/3-prometheus/basics.html</a></p><h5 id="主机监控指标规则">主机监控指标规则</h5><p>/export/docker-data-proms/conf/rules/node-exporter-record-rules.yml</p><pre><code class="language-bash"># node-exporter-record-rules.yml# 标签 job 关联主配置定义的任务 auto_discovery_dns，获取任务传递的数据，从而抽取信息定义 expr# 给 expr 表达式设置一个别名 record, 别名可以被其它 rules 调用groups:  - name: node_exporter-record    rules:    - expr: up&#123;job=~&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:up      labels:        desc: &quot;节点是否在线, 在线1,不在线0&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;    - expr: time() - node_boot_time_seconds&#123;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:node_uptime      labels:        desc: &quot;节点的运行时间&quot;        unit: &quot;s&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                              cpu                                                           #    - expr: (1 - avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:total:percent      labels:        desc: &quot;节点的cpu总消耗百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:idle:percent      labels:        desc: &quot;节点的cpu idle百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;iowait&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:iowait:percent      labels:        desc: &quot;节点的cpu iowait百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;system&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:system:percent      labels:        desc: &quot;节点的cpu system百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;user&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:user:percent      labels:        desc: &quot;节点的cpu user百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=~&quot;softirq|nice|irq|steal&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:other:percent      labels:        desc: &quot;节点的cpu 其他的百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                    memory                                                  #    - expr: node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:total      labels:        desc: &quot;节点的内存总量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_memory_MemFree_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free      labels:        desc: &quot;节点的剩余内存量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; - node_memory_MemFree_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used      labels:        desc: &quot;节点的已使用内存量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; - node_memory_MemAvailable_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:actualused      labels:        desc: &quot;节点用户实际使用的内存量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: (1-(node_memory_MemAvailable_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used:percent      labels:        desc: &quot;节点的内存使用百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: ((node_memory_MemAvailable_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free:percent      labels:        desc: &quot;节点的内存剩余百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                   load                                                     #    - expr: sum by (instance) (node_load1&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load1      labels:        desc: &quot;系统1分钟负载&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;    - expr: sum by (instance) (node_load5&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load5      labels:        desc: &quot;系统5分钟负载&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;    - expr: sum by (instance) (node_load15&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load15      labels:        desc: &quot;系统15分钟负载&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                 disk                                                       #    - expr: node_filesystem_size_bytes&#123;job=&quot;auto_discovery_dns&quot; ,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:total      labels:        desc: &quot;节点的磁盘总量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_filesystem_avail_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:free      labels:        desc: &quot;节点的磁盘剩余空间&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_filesystem_size_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125; - node_filesystem_avail_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:used      labels:        desc: &quot;节点的磁盘使用的空间&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr:  (1 - node_filesystem_avail_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_size_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:used:percent      labels:        desc: &quot;节点的磁盘的使用百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: irate(node_disk_reads_completed_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:count:rate      labels:        desc: &quot;节点的磁盘读取速率&quot;        unit: &quot;次/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: irate(node_disk_writes_completed_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:count:rate      labels:        desc: &quot;节点的磁盘写入速率&quot;        unit: &quot;次/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (irate(node_disk_written_bytes_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:mb:rate      labels:        desc: &quot;节点的设备读取MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (irate(node_disk_read_bytes_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:mb:rate      labels:        desc: &quot;节点的设备写入MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                filesystem                                                  #    - expr:   (1 -node_filesystem_files_free&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_files&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filesystem:used:percent      labels:        desc: &quot;节点的inode的剩余可用的百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                filefd                                                     #    - expr: node_filefd_allocated&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:count      labels:        desc: &quot;节点的文件描述符打开个数&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: node_filefd_allocated&#123;job=&quot;auto_discovery_dns&quot;&#125;/node_filefd_maximum&#123;job=&quot;auto_discovery_dns&quot;&#125; * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:percent      labels:        desc: &quot;节点的文件描述符打开百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                network                                                    #    - expr: avg by (environment,instance,device) (irate(node_network_receive_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:bit:rate      labels:        desc: &quot;节点网卡eth0每秒接收的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:bit:rate      labels:        desc: &quot;节点网卡eth0每秒发送的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:packet:rate      labels:        desc: &quot;节点网卡每秒接收的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:packet:rate      labels:        desc: &quot;节点网卡发送的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:error:rate      labels:        desc: &quot;节点设备驱动器检测到的接收错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:error:rate      labels:        desc: &quot;节点设备驱动器检测到的发送错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: node_tcp_connection_states&#123;job=&quot;auto_discovery_dns&quot;, state=&quot;established&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:established:count      labels:        desc: &quot;节点当前established的个数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;    - expr: node_tcp_connection_states&#123;job=&quot;auto_discovery_dns&quot;, state=&quot;time_wait&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:timewait:count      labels:        desc: &quot;节点timewait的连接数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;    - expr: sum by (environment,instance) (node_tcp_connection_states&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:total:count      labels:        desc: &quot;节点tcp连接总数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                process                                                    #    - expr: node_processes_state&#123;state=&quot;Z&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:process:zoom:total:count      labels:        desc: &quot;节点当前状态为zoom的个数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                other                                                    #    - expr: abs(node_timex_offset_seconds&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:time:offset      labels:        desc: &quot;节点的时间偏差&quot;        unit: &quot;s&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################    - expr: count by (instance) ( count by (instance,cpu) (node_cpu_seconds_total&#123; mode='system'&#125;) ) * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:count</code></pre><h5 id="容器监控指标规则">容器监控指标规则</h5><p>/export/docker-data-proms/conf/rules/dockersInfo-record-rules.yml</p><pre><code class="language-yaml">groups:  - name: dockersInfo-record    rules:    - expr: count by (instance, name) (count_over_time(container_last_seen&#123;job=&quot;auto_discovery_dns&quot;, name!=&quot;&quot;, container_label_restartcount!=&quot;&quot;&#125;[15m]))      record: dockersInfo:container:restart      labels:        desc: &quot;15m周期内容器发生重启的次数&quot;        unit: &quot;&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                              cpu                                                           #    - expr: rate(container_cpu_usage_seconds_total&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125;[5m])  * 100      record: dockersInfo:container:cpu:total:percent      labels:        desc: &quot;容器的cpu总消耗百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: rate(container_fs_io_time_seconds_total&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125;[5m])  * 100      record: dockersInfo:cpu:iowait:percent      labels:        desc: &quot;容器的cpu iowait百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                    memory                                                  #    - expr: container_spec_memory_limit_bytes&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125;      record: dockersInfo:memory:total      labels:        desc: &quot;容器的内存总量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: container_memory_usage_bytes&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125; / container_spec_memory_limit_bytes&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125; * 100      record: dockersInfo:memory:used:percent      labels:        desc: &quot;容器的内存使用百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;</code></pre><h4 id="定义监控指标阈值规则">定义监控指标阈值规则</h4><h5 id="主机监控指标阈值">主机监控指标阈值</h5><p>/export/docker-data-proms/conf/rules/node-exporter-alert-rules.yml</p><pre><code class="language-bash"># node-exporter-alert-rules.yml# 定义告警规则# 通过前一个 rules 文件拿到定义的 record 别名来编写 expr 判断式# 这里定义的告警规则，在触发的时候，都会传递到 alertmanager，最后从传递的信息中抽取所需数据发送给目标人。groups:  - name: node-alert    rules:    - alert: node-down      expr: node_exporter:up == 0      for: 1m      labels:        severity: critical      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 宕机了&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-cpu-high      expr:  node_exporter:cpu:total:percent &gt; 80      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-cpu-iowait-high      expr:  node_exporter:cpu:iowait:percent &gt;= 12      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu iowait 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-load-load1-high      expr:  (node_exporter:load:load1) &gt; (node_exporter:cpu:count) * 1.2      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; load1 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-memory-high      expr:  node_exporter:memory:used:percent &gt; 85      for: 3m      labels:        severity: info      annotations:        summary: &quot;内存使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-high      expr:  node_exporter:disk:used:percent &gt; 80      for: 3m      labels:        severity: info      annotations:        summary: &quot;&#123;&#123; $labels.device &#125;&#125;:&#123;&#123; $labels.mountpoint &#125;&#125; 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-read:count-high      expr:  node_exporter:disk:read:count:rate &gt; 3000      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; iops read 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-write-count-high      expr:  node_exporter:disk:write:count:rate &gt; 3000      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; iops write 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-read-mb-high      expr:  node_exporter:disk:read:mb:rate &gt; 60      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 读取字节数 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-write-mb-high      expr:  node_exporter:disk:write:mb:rate &gt; 60      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 写入字节数 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-filefd-allocated-percent-high      expr:  node_exporter:filefd_allocated:percent &gt; 80      for: 10m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 打开文件描述符 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netin-error-rate-high      expr:  node_exporter:network:netin:error:rate &gt; 4      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包进入的错误速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netin-packet-rate-high      expr:  node_exporter:network:netin:packet:rate &gt; 35000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包进入速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netout-packet-rate-high      expr:  node_exporter:network:netout:packet:rate &gt; 35000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包流出速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-tcp-total-count-high      expr:  node_exporter:network:tcp:total:count &gt; 40000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; tcp连接数量 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-process-zoom-total-count-high      expr:  node_exporter:process:zoom:total:count &gt; 10      for: 10m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 僵死进程数量 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-time-offset-high      expr:  node_exporter:time:offset &gt; 0.03      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; &#123;&#123; $labels.desc &#125;&#125;  &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;</code></pre><h5 id="容器监控指标阈值">容器监控指标阈值</h5><p>/export/docker-data-proms/conf/rules/dockersInfo-alert-rules.yml</p><pre><code class="language-yaml">groups:  - name: container-alert    rules:    - alert: container-restart-times-high      expr: dockersInfo:container:restart &gt; 5      for: 1m      labels:        severity: warn      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; 15分钟内重启次数超过5次&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: container-cpu-usage-high      expr: dockersInfo:container:cpu:total:percent &gt; 90      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; cpu 使用率持续超过90%.&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: container-cpu-iowait-high      expr: dockersInfo:cpu:iowait:percent &gt; 10      for: 1m      labels:        severity: warn      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; cpu iowait 持续超过10%&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;#    - alert: container-mem-usage-high#      expr: dockersInfo:memory:used:percent &gt; 80#      for: 1m#      labels:#        severity: warn#      annotations:#        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; 内存使用率超过80%&quot;#        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;#        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;#        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;#        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;#        type: &quot;aliyun_meta_ecs_info&quot;</code></pre><h4 id="告警流程解析">告警流程解析</h4><p>关于告警，prometheus存在三个告警状态：</p><ul><li>inactive 未触发expr告警表达式。</li><li>pending 已触发 expr 告警表达式，但还未满足告警持续时间（即 for 语法关键词）。</li><li>firing 已触发最新一次expr 告警表达式且满足告警持续时间。</li></ul><p><a href="http://ip:9090/alerts">http://ip:9090/alerts</a> 可以看到各项告警规则的状态。</p><p>💥需要注意的是，如果不设置for或者将其置为0，则会跳过告警持续时间，直接告警。</p><p>借用网上的一个例子来介绍：</p><pre><code class="language-yaml">groups:-  name: example   rules:   - alert: mysql_uptime      expr: mysql:server_status:uptime &lt; 30      for: 10s      labels:         level: &quot;CRITICAL&quot;      annotations:         detail: 数据库运行时间</code></pre><p><img src="/posts/65820315/image-20220217115556807.png" alt="image-20220217115556807"></p><p>每5秒采集一个数据点；采集第三个点的时候，发现 mysql_uptime = 0; 说明 mysql 发生重启。</p><p>每10秒计算表达式 server_status:uptime &lt; 30 是否为真；在第三个采集点与第四个采集点之间触发了一次计算，致使触发告警。</p><p>等待告警持续时间 10 秒，再次计算表达式 server_status:uptime &lt; 30 是否为真。最终满足【已触发最新一次 expr 告警表达式且满足告警持续时间】，从而触发告警发送给Altertmanager告警管理器。</p><h3 id="部署">部署</h3><h4 id="docker-方式">docker 方式</h4><pre><code class="language-bash">docker run -d \--name proms \--network promsnet \--restart always \-p 9090:9090 \--mount 'type=bind,src=/export/docker-data-proms/prometheus.yml,dst=/etc/prometheus/prometheus.yml'  \--mount 'type=bind,src=/export/docker-data-proms/data,dst=/prometheus' \--mount 'type=bind,src=/export/docker-data-proms/conf,dst=/etc/prometheus/conf' \prom/prometheus \--config.file=/etc/prometheus/prometheus.yml \--storage.tsdb.path=/prometheus \--storage.tsdb.retention=15d \--web.enable-admin-api \--web.enable-lifecycle</code></pre><p>⚠️</p><p><code>--web.enable-admin-api</code> 如果不开启，则无法使用api接口，也就无法自主的删除数据.</p><p><code>--web.enable-lifecycle</code>如果不开启，则无法通过 <code>curl -XPOST http://ip:9090/-/reload</code> 重载 prometheus</p><h4 id="二进制方式">二进制方式</h4><pre><code class="language-bash">wget https://github.com/prometheus/prometheus/releases/download/v2.20.0/prometheus-2.20.0.linux-amd64.tar.gzmkdir prometheustar xf prometheus-2.20.0.linux-amd64.tar.gz --strip-components 1 -C prometheusrm -rf prometheus-2.20.0.linux-amd64.tar.gzcd prometheus &amp;&amp; baseDir=`pwd`cp prometheus.yml&#123;,.bak&#125; #---cat &gt; /usr/lib/systemd/system/prometheus.service &lt;&lt;EOF[Unit]Description=prometheus server daemon[Service]Restart=on-failureExecStart=$&#123;baseDir&#125;/prometheus --config.file=$&#123;baseDir&#125;/prometheus.yml[Install]WantedBy=multi-user.targetEOF#---systemctl daemon-reloadsystemctl start prometheus</code></pre><h3 id="查看">查看</h3><p>被监控端状态：<a href="http://xxx:9090/targets">http://xxx:9090/targets</a></p><p>通过上述地址，你可以看到配置中 job_name 定义的被监控任务的端点状态</p><p><img src="/posts/65820315/image-20210311110821675.png" alt="image-20210311110821675"></p><h3 id="数据删除">数据删除</h3><pre><code class="language-bash"># match[] 表示匹配所有key, &#123;&#125;精确选中key# 删除包含&#123;instance=&quot;10.3.128.202:15692&quot;&#125;的所有数据curl -X POST -g 'http://127.0.0.1:9090/api/v1/admin/tsdb/delete_series?match[]=&#123;instance=~&quot;10.3.128.202:15692&quot;&#125;'# 添加删除时间范围curl -X POST -g 'http://127.0.0.1:9090/api/v1/admin/tsdb/delete_series?start=2021-05-30T00:00:00Z&amp;end=2021-06-07T23:59:59Z'</code></pre><h2 id="pushgateway-未验证">pushgateway (未验证)</h2><blockquote><p>push 模式下的网关</p><p>当前没有现成的推送模式的 node-exporter</p></blockquote><pre><code class="language-bash"># dockerdocker run -d --name=pushgateway -p 9091:9091 prom/pushgateway</code></pre><pre><code class="language-bash"># 一个推送模式的采集脚本示例# cat tcpestab.sh #!/bin/bash# 添加脚本到计划任务中，定时采集# pushgateway ippushgatewayIp=#获取主机名，常传输到Prometheus标签以主机名instance_name=`hostname -f | cut -d'.' -f1`#判断主机名不能是localhost不然发送过的数据不知道是那个主机的 if [ $instance_name == &quot;localhost&quot; ];thenecho &quot;Hostname must not localhost&quot;exit 1fi#自定义key，在Prometheus即可使用key查询label=&quot;count_estab_connections&quot; #获取TCP estab 连接数count_estab_connections=`netstat -an | grep -i 'established' | wc -l`#将数据发送到pushgateway固定格式echo &quot;$label $count_estab_connections&quot;  | curl --data-binary @- http://$pushgatewayIp:9091/metrics/job/pushgateway/instance/$instance_name</code></pre><h1>被监控端组件</h1><p>prometheus官方列出的表单：</p><p><a href="https://prometheus.io/docs/instrumenting/exporters/">https://prometheus.io/docs/instrumenting/exporters/</a></p><p>编写exporter：</p><p><a href="https://prometheus.io/docs/instrumenting/writing_exporters/">https://prometheus.io/docs/instrumenting/writing_exporters/</a></p><h2 id="node-exporter-物理节点监控组件">node-exporter 物理节点监控组件</h2><blockquote><p>宿主数据采集端，部署在被监控主机的9100端口</p></blockquote><h3 id="docker-不太建议">docker 不太建议</h3><pre><code class="language-bash">docker run -d \  --name=node-exporter \  --restart=always \  --net=&quot;host&quot; \  --pid=&quot;host&quot; \  -v &quot;/:/host:ro,rslave&quot; \  quay.io/prometheus/node-exporter:latest \  --path.rootfs=/host</code></pre><h3 id="yum-方式-推荐">yum 方式 推荐</h3><pre><code class="language-bash"># yum包 https://copr.fedorainfracloud.org/coprs/ibotty/prometheus-exporters/curl -Lo /etc/yum.repos.d/_copr_ibotty-prometheus-exporters.repo https://copr.fedorainfracloud.org/coprs/ibotty/prometheus-exporters/repo/epel-7/ibotty-prometheus-exporters-epel-7.repo &amp;&amp; yum install node_exporter -y</code></pre><h3 id="源码包">源码包</h3><pre><code class="language-bash">cd /usr/local/wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gzmkdir node_exportertar xf node_exporter-1.0.1.linux-amd64.tar.gz --strip-components 1 -C node_exportermv node_exporter-1.0.1.linux-amd64.tar.gz srccd node_exporter &amp;&amp; baseDir=`pwd`#---cat &gt; /usr/lib/systemd/system/node_exporter.service &lt;&lt;EOF[Unit]Description=node_exporter server daemon[Service]Restart=on-failureExecStart=$&#123;baseDir&#125;/node_exporter[Install]WantedBy=multi-user.targetEOF</code></pre><h3 id="yum和源码的启动方式">yum和源码的启动方式</h3><pre><code class="language-bash">systemctl daemon-reloadsystemctl start node_exportersystemctl status node_exportercurl http://localhost:9100/metrics # 查看获取的监控数据systemctl enable node_exporter</code></pre><h2 id="cadvisor-容器监控组件">cadvisor 容器监控组件</h2><blockquote><p>容器数据采集端，部署在被监控容器所在宿主的10052端口</p></blockquote><pre><code class="language-bash">dockerRoot=`docker info | awk -F':'  '/Docker Root Dir/&#123;print $2&#125;'|sed 's@^ *@@g'`echo $dockerRootdocker run \  --restart=always \  --volume=/:/rootfs:ro \  --volume=/var/run:/var/run:rw \  --volume=/sys:/sys:ro \  --volume=$&#123;dockerRoot&#125;/:/var/lib/docker:ro \  --volume=/dev/disk/:/dev/disk:ro \  --publish=10052:8080 \  --privileged=true \  --detach=true \  --name=cadvisor \  google/cadvisor:latest</code></pre><h1>使用</h1><h2 id="添加-grafana-数据源-proms">添加 grafana 数据源 : proms</h2><p><img src="/posts/65820315/image-20201218135400147.png" alt="image-20201218135400147"></p><h2 id="添加-grafana-监控模板">添加 grafana 监控模板</h2><p><img src="/posts/65820315/image-20201218135502105.png" alt="image-20201218135502105"></p><p>node模板：<a href="https://grafana.com/grafana/dashboards/8919">https://grafana.com/grafana/dashboards/8919</a></p><p>docker模板：<a href="https://grafana.com/grafana/dashboards/10566">https://grafana.com/grafana/dashboards/10566</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞server安装注意事项</title>
      <link href="posts/ed865d2f/"/>
      <url>posts/ed865d2f/</url>
      
        <content type="html"><![CDATA[<ol><li><p>数据库编码一定要安装官方来命令来设置，例如zabbix5编码是 utf8和utf8_bin</p></li><li><p>最后web控制台安装界面，要确保zabbix-server可以访问设置的 hostname 的 10051 端口。如果hostname写的域名，那么要确保zabbix-server可以通过 hosts本地解析或者外网访问10051端口</p></li><li><p>zabbix-server 的 agent 要使用拉取模式，因为官方给的模板无法直接修正为推流模式</p></li><li><p>报警媒介设置</p><p>脚本设置</p><pre><code class="language-bash">脚本参数&#123;ALERT.SENDTO&#125;&#123;ALERT.SUBJECT&#125;&#123;ALERT.MESSAGE&#125;</code></pre><p>问题模板</p><pre><code class="language-bash">故障: &#123;TRIGGER.NAME&#125;【[骷髅]】=》&#123;TRIGGER.SEVERITY&#125; 事件ID : &#123;EVENT.ID&#125;【UTC+0】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;【告警等级】: &#123;TRIGGER.SEVERITY&#125;【主机信息1】: &#123;HOST.NAME&#125;【主机信息2】: &#123;HOSTNAME1&#125;【告警信息】: &#123;TRIGGER.NAME&#125;【告警项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;</code></pre><p>恢复模板</p><pre><code class="language-bash">恢复: &#123;TRIGGER.NAME&#125;【[OK]】=》&#123;TRIGGER.SEVERITY&#125; 事件ID : &#123;EVENT.ID&#125;【UTC+0】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;【告警等级】: &#123;TRIGGER.SEVERITY&#125;【主机信息1】: &#123;HOST.NAME&#125;【主机信息2】: &#123;HOSTNAME1&#125;【告警信息】: &#123;TRIGGER.NAME&#125;【告警项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;</code></pre></li><li><p>预警用户添加超管权限</p></li><li><p>中文字体</p><pre><code class="language-bash">yum install google-noto-sans-simplified-chinese-fonts.noarch -yrm -rf /etc/alternatives/zabbix-web-fontln -s /usr/share/fonts/google-noto/NotoSansSC-Regular.otf /etc/alternatives/zabbix-web-font</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞从容器中访问宿主机docker命令</title>
      <link href="posts/3e0f2183/"/>
      <url>posts/3e0f2183/</url>
      
        <content type="html"><![CDATA[<p>💥💥💥💥💥💥💥💥💥💥</p><p>不太推荐</p><p>💥💥💥💥💥💥💥💥💥💥</p><h2 id="前言">前言</h2><p>我的jenkins是运行在docker中，但是jenkins官方的镜像里却没有docker命令。</p><p>以至于无法在流水线中打包docker镜像。</p><h2 id="方法">方法</h2><p>首先，需要将docker命令、docker.sock文件以及相关依赖文件映射到容器内。</p><p>其次，以root用户访问容器，在容器中添加docker组，并且组id需要和宿主机中的docker组id一致。</p><p>最后，以root用户访问容器，并将jenkins用户加入到容器中的docker组中。</p><p>最最后，最关键的来了， 一定要重启一下 jenkins 容器。。。</p><h2 id="相关命令">相关命令</h2><pre><code class="language-bash"># 额外的映射文件（宿主机文件和容器内的映射路径，以实际情况为准）-v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7# 以 root 用户访问 jenkins 容器（宿主机的组ID以实际情况为准）docker exec -it -u root jenkins /bin/bashgroupadd -g xxx dockerusermod -aG docker jenkins# 重启 jenkins 容器docker stop jenkins &amp;&amp; docker start jenkins</code></pre><h2 id="注意">注意</h2><p>在容器内执行的 groupadd 和 usermod 命令，需要在每次变更容器镜像后，重新执行，因为命令的相关结果都是容器内数据，清理后不会保留。</p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞ssh-MFA自动登陆</title>
      <link href="posts/5db0cd07/"/>
      <url>posts/5db0cd07/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>ssh 关联MFA后,安全度增加了很多,但是每次手动输入MFA动态口令比较麻烦.所以记录一下自动交互输入MFA口令</p><h2 id="命令安装">命令安装</h2><pre><code class="language-bash">sudo apt install oathtool gnupg2 expect</code></pre><blockquote><p>oathtool 是我们用来生成MFA口令的工具.</p><p>expect 用来编写交互程序</p></blockquote><h2 id="登陆交互脚本">登陆交互脚本</h2><pre><code class="language-bash">#!/usr/bin/expecttrap &#123; set rows [stty rows] set cols [stty columns] stty rows $rows columns $cols &lt; $spawn_out(slave,name)&#125; WINCHset timeout 5set MFAToken &quot;我是MFA的TOKEN&quot;spawn 我是ssh命令expect &quot;MFA auth&quot;send &quot;[exec oathtool -b --totp $MFAToken]\r&quot;;interact</code></pre><blockquote><p>将脚本中的TOKEN和命令替换为自己的.</p></blockquote><h2 id="注意">注意</h2><p>如果你发现你登陆不了, 每次都验证错误, 那么你应该检查下你机器的时间是否正常.</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ssh </tag>
            
            <tag> mfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jdk☞安装和配置</title>
      <link href="posts/ac6b3b36/"/>
      <url>posts/ac6b3b36/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>oracle jdk 现在是商业产品了, 所以线上最好还是用 openjdk.</p><h2 id="源安装">源安装</h2><p><a href="https://adoptopenjdk.net/installation.html#linux-pkg">https://adoptopenjdk.net/installation.html#linux-pkg</a></p><p>rhel之类的，搜索【RPM installation on Centos, RHEL, or Fedora】位置，在添加了repo源之后，可以用 <code>yum list adoptopenjdk*</code>来查看源包含的版本</p><p>包命名示例：adoptopenjdk-11-openj9.x86_64，这里 openj9 指的是 jvm 版本 ，还有 hotspot 版本的 jvm</p><hr><p>ubuntu之类的，搜索【Deb installation on Debian or Ubuntu】位置，在添加了源之后，可以用<code>apt-show-versions -a adoptopenjdk* </code>来查看源包含的版本</p><p>另外听说，openj9 内存管理比 hotspot 牛逼很多，能减少一般的内存占用。</p><h2 id="二进制安装">二进制安装</h2><p><a href="https://adoptopenjdk.net/installation.html?variant=openjdk11&amp;jvmVariant=hotspot#x64_linux-jdk">https://adoptopenjdk.net/installation.html?variant=openjdk11&amp;jvmVariant=hotspot#x64_linux-jdk</a></p><h2 id="二进制配置">二进制配置</h2><pre><code class="language-bash">tar xzf OpenJDK11U-jdk_x64_linux_hotspot_11.0.10_9.tar.gzexport PATH=$PWD/jdk-11.0.10+9/bin:$PATHjava -version</code></pre><h2 id="容器">容器</h2><p><a href="https://hub.docker.com/_/adoptopenjdk?tab=tags">https://hub.docker.com/_/adoptopenjdk?tab=tags</a></p><h2 id="最后">最后</h2><p>OpenJ9 被 IBM 拿回去了，所以adoptium基金会新站点又不提供了。。。<br><a href="https://adoptium.net/installation.html?variant=openjdk8&amp;jvmVariant=hotspot">https://adoptium.net/installation.html?variant=openjdk8&amp;jvmVariant=hotspot</a></p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> jdk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞fcgi中alias的使用</title>
      <link href="posts/58a1c0c7/"/>
      <url>posts/58a1c0c7/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>众所周知, nginx 在配置静态资源的时候, root 和 alias 分别是两种指定资源路径的方式.</p><p>如果：</p><p>​url 是 <code>http://xxx.com/god/a.jpg</code>，且 location 匹配的是 <code>/god/</code>.</p><p>则：</p><p>​当用 root 定位资源路径时, root 配置的值=域名<code>xxx.com</code> 部分所对应的路径, 此时 a.jpg 的物理路径就是  $root/god/a.jpg</p><p>​当用 alias 定位资源路径时, alias 配置的值=url<code>xxx.com/god/</code>部分所对应的路径, 此时 a.jpg 的物理路径就是 ${alias}a.jpg</p><p>即</p><pre><code class="language-bash">location ^~ /god/ &#123;    root /export/webapps/xxx.com;&#125;# 或者location ^~ /god/ &#123;    alias /export/webapps/xxx.com/god/;&#125;</code></pre><p>当我采用上述规则, 使用alias配置fcgi的时候,现实给了我暴击…妥妥的404了.</p><h2 id="在fcgi环境下-alias-的配置">在fcgi环境下, alias 的配置</h2><p>废话不多说, 直接上结果.</p><pre><code class="language-bash">location ^~ /god/ &#123;     index  index.php index.html index.htm;     root /export/webapps/xxx.com/;     location ~* &quot;\.php$&quot; &#123;         try_files      $uri =404;         fastcgi_pass   127.0.0.1:9000;         fastcgi_index  index.php;         include        fastcgi.conf;         fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;     &#125; &#125; location ^~ /god/ &#123;    index  index.php index.html index.htm;    alias /export/webapps/xxx.com/god/;    location ~* &quot;\.php$&quot; &#123;        try_files      $uri =404;        fastcgi_pass   127.0.0.1:9000;        include        fastcgi.conf;        fastcgi_param  SCRIPT_FILENAME  $request_filename;    &#125;&#125;</code></pre><blockquote><p>官方的例子:</p><p><a href="https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/#use-request-filename-for-script-filename">https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/#use-request-filename-for-script-filename</a></p></blockquote><p>关键点：</p><ol><li>当你用 alias 而不是 root 的时候, 你需要将$document_root$fastcgi_script_name替换成$request_filename</li><li>当你不在使用$fastcgi_script_name的时候，你需要显性的添加index，而不是fastcgi_index ,因为$request_filename并不能关联fastcgi_index。</li></ol><p>原因在于$document_root的值，来自于root或者alias，$fastcgi_script_name总是拿url_path</p><p>假设你访问 /god/api.php,  而这个文件的物理路径是/export/webapps/xxx.com/god/api.php。</p><p>此时，</p><p>当你用 alias 的时候</p><p>$document_root = alias = /export/webapps/xxx.com/god/</p><p>$fastcgi_script_name = /god/api.php</p><p>$document_root$fastcgi_script_name=/export/webapps/xxx.com/god//god/api.php</p><p>而$request_filename当前文件的请求路径，由root或者alias+uri相对路径</p>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> fcgi </tag>
            
            <tag> php </tag>
            
            <tag> alias </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞角色</title>
      <link href="posts/720afd15/"/>
      <url>posts/720afd15/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>角色的作用就是规范化，将一个playbook各部分分门别类的放置在规定好的目录中。就如同linux系统一样，/etc/就是放配置的，/bin 就是放程序的，/tmp 就是放临时文件的 …</p><p>ansible 会基于官方规定好的目录结构, 去自动加载目录中的文件. 当一个需求很复杂的时候, 我们就可以基于角色对需求进行分组.</p><p>最后, 如果你不按照这个规定来走, 那么ansible角色模块就找不到相关东西.</p><h2 id="角色结构">角色结构</h2><p>这是一个官方项目的例子</p><blockquote><p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html#">https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html#</a>  官方文档</p></blockquote><pre><code class="language-bash">site.ymlwebservers.ymlfooservers.ymlroles/    common/        tasks/        handlers/        files/        templates/        vars/        defaults/        meta/    webservers/        tasks/        defaults/        meta/</code></pre><p>例子中，common 和 webservers 就是两个角色<br>关于角色的目录，必须包含下面列举出来的目录之一，目录可以为空。但是如果你使用到了某个目录，那么部分目录需要包含main.yml文件。</p><ul><li>tasks 角色用到的主要任务列表</li><li>handlers 角色或者角色外用到的任务后续处理</li><li>defaults 角色默认变量</li><li>vars 角色其余变量，优先级大于 defaults 中定义的变量</li><li>files 角色部署中牵扯到的文件</li><li>templates 角色的jinja2模板</li><li>meta 角色的元数据信息，例如角色的属性(作者, 说明, 一些特殊功能等)</li></ul><p>最后，目录中main.yml用来存储主配置信息。在这里，你可以包含其它的子任务，在子任务中详细描述。<br>例如，在 tasks/main.yml 中，通过 import_tasks，包含其它子任务，就像下面这样</p><blockquote><p>main 包含 redhat 和 debian。当系统是 redhat 时，安装 httpd 包，当系统是 debian 时，安装 apache2 包</p></blockquote><pre><code class="language-yaml"># roles/xxx/tasks/main.yml- name: added in 2.4, previously you used 'include'  import_tasks: redhat.yml  when: ansible_facts['os_family']|lower == 'redhat'- import_tasks: debian.yml  when: ansible_facts['os_family']|lower == 'debian'# roles/xxx/tasks/redhat.yml- yum:    name: &quot;httpd&quot;    state: present# roles/xxx/tasks/debian.yml- apt:    name: &quot;apache2&quot;    state: present</code></pre><h2 id="使用角色">使用角色</h2><pre><code class="language-yaml">---# file: site.yml- include: webservers.yml- include: fooservers.yml</code></pre><pre><code class="language-yaml">---# file: webservers.yml- hosts: webservers  roles:    - common    - webservers</code></pre><p>只运行 webservers 角色，可以通过 site.yml 添加 limit 限制执行，也可以直接调用</p><pre><code class="language-bash">ansible-playbook site.yml --limit webserversansible-playbook webservers.yml</code></pre><h2 id="角色优先级">角色优先级</h2><p>角色中针对 tasks, handlers, vars 有一个优先级概念. 优先级大的会覆盖掉优先级小的配置.</p><p>优先级由大到小如下:</p><p><code>cli 层面参数 &gt; role-xxx-dir &gt; playbook-xxx &gt; role-defaults-dir</code></p><p>这里 cli 层面参数指的是 ansible-playbook -e vara=‘a’ test.play 这种外部传递变量的行为</p><p>这里 role-xxx-dir 指的是 role 特定目录里的配置, 例如 tasks, handlers, vars</p><p>这里 playbook-xxx 指的是直接写入 ploybook 中的部分</p><p>这里 role-defaults-dir 指的是角色目录中的 defaults 默认变量目录</p><h2 id="角色复制">角色复制</h2><p>如果你想让一个角色多次执行，就如同下面这样</p><pre><code class="language-yaml">---- hosts: webservers  roles:    - moo    - moo</code></pre><p>有下列两种方式:</p><ol><li>moo，拥有不同的参数</li><li>将<code>allow_duplicates: true</code>写入 moo/meta/main.yml</li></ol><h2 id="角色标签">角色标签</h2><p>我们一定要针对角色中的tasks添加标签(tags). 原因在于, 有些时候, 当我们只想修改部署很久的一堆机器的某个服务时, 我们只需要在执行角色的 playbook 的时候,追加 --tags 即可帮助我们执行项目中某一个任务，而不必执行所有任务.</p><p>比如, 我们有个部署项目, 其中有一个初始化角色( common ). common 中有众多的初始化任务, 其中一个任务是 ntp 服务的 安装/配置/启动/重启 .</p><p>ntp 服务配置如下:</p><pre><code class="language-yaml">---# file: roles/common/tasks/main.yml- name: be sure ntp is installed  yum: pkg=ntp state=installed  tags: ntp- name: be sure ntp is configured  template: src=ntp.conf.j2 dest=/etc/ntp.conf  notify:    - restart ntpd  tags: ntp- name: be sure ntpd is running and enabled  service: name=ntpd state=running enabled=yes  tags: ntp  ---# file: roles/common/handlers/main.yml- name: restart ntpd  service: name=ntpd state=restarted</code></pre><p>现在, 我们只想修改生产环境的 ntp 服务的配置, 那么只需要将 ntp.conf.j2 模板配置好之后, 重新执行这个playbooks即可. 就想下面这样:</p><pre><code class="language-bash">ansible-playbook -i production site.yml --tags ntp</code></pre><h2 id="写好的playbook">写好的playbook</h2><p><a href="https://galaxy.ansible.com/">https://galaxy.ansible.com/</a></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞模板</title>
      <link href="posts/724fda19/"/>
      <url>posts/724fda19/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>当你用ansible进行多机器的配置调整，且调整的东西都一模一样，此时你不会拒绝模板的诱惑。</p><p>ansible的模板是jinja2，所以jinja2的特性，在这里都可以用。</p><blockquote><p>模板中，不要出现任何你觉得模板会忽略的东西，包括但不限于空格</p></blockquote><h2 id="模块-template">模块 template</h2><p>参数：</p><ul><li>src 模板文件路径</li><li>dest 目的文件路径</li></ul><p>牵扯到目的路径，必然有权限参数</p><ul><li>owner 目的属主</li><li>group 目的属组</li><li>mode 目的权限</li></ul><p>覆盖与备份</p><ul><li>force 覆盖，yes / no</li><li>backup 备份， yes / no ， 若为 yes ，则目的重名文件会先改名</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - template:        src: ~/test.j2        dest: ~/test.info</code></pre><h2 id="模板分隔符">模板分隔符</h2><pre><code class="language-bash">&#123;&#123; &#125;&#125; 一般用来填充变量，可以是过滤器，也可以填充表达式，从而返回相应的值，例如 &#123;&#123; 1==1 &#125;&#125; 返回 True&#123;% %&#125; 一般用来填充控制语句&#123;# #&#125; 模板注释语句，并非渲染后会出现#  ... ## 这一种 ansible 貌似不支持，所以可以忽略</code></pre><h3 id="分隔符1">分隔符1</h3><pre><code class="language-jinja2"># &#123;&#123; &#125;&#125;&#123;# 普通变量 #&#125;&#123;&#123; foo.bar &#125;&#125;&#123;&#123; foo['bar'] &#125;&#125;&#123;# 以过滤器 lookup 为例 #&#125;&#123;&#123; lookup('file', '~/test.file') &#125;&#125;&#123;&#123; lookup('env', 'PATH' )&#125;&#125;</code></pre><p>最终目的文件，会输出<code>~/test.file</code> 内容和 <code>$PATH</code> 内容</p><blockquote><p>字符串拼接需要使用<code>~</code>，例如 <code>&quot;name:&quot;~name</code></p></blockquote><h3 id="分隔符2">分隔符2</h3><pre><code class="language-bash"># &#123;% %&#125;# 官网所有的控制列表https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures</code></pre><h4 id="条件控制语句-if">条件控制语句 if</h4><pre><code class="language-jinja2">&#123;% if 条件1 %&#125;  pass&#123;% elif 条件2 %&#125;  pass&#123;% else %&#125;  pass &#123;% endif %&#125;</code></pre><h4 id="循环语句-for">循环语句 for</h4><pre><code class="language-jinja2">&#123;% for i in 可迭代对象 %&#125;  &#123;&#123; i &#125;&#125;&#123;% endfor %&#125;</code></pre><blockquote><p>默认循环后，每一个循环单体独占一行，如果需要删除独占，则需要给第二个%}和第三个控制符{%加减号，最终变为-%}和{%-。</p></blockquote><p>关于字典类型，可以使用 iteritems() 函数，从而方便的获取到字典的 k 和 v。例如</p><pre><code class="language-jinja2">&#123;% for k,v in &#123;'name':'zhangsan', 'gender':'male'&#125;.iteritems() %&#125;  &#123;&#123; k &#125;&#125;:&#123;&#123; v &#125;&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后name:zhangsangender:male</code></pre><h4 id="条件和循环组合语句">条件和循环组合语句</h4><pre><code class="language-jinja2">&#123;% for i in 可迭代对象 if 条件 %&#125;  满足条件语句&#123;% else %&#125;  不满足条件语句&#123;% endfor %&#125;</code></pre><p>例如</p><pre><code class="language-jinja2">&#123;% for i in [1,2,3,4] if i>2 %&#125;&#123;&#123; i &#125;&#125;'s index is &#123;&#123; loop.index &#125;&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后3's index is 14's index is 2</code></pre><blockquote><p>loop.index 是循环体索引，这里可能会有个疑问。</p><p>正常情况下，3和4的索引应该就是3和4，之所以是1和2，原因在于当条件控制和循环控制位于同一行的时候，先行运算的是 <code>[1,2,3,4] if i&gt;2</code>，之后才开始走<code>for</code>循环。</p><p>如果你想输出原始循环体，则需要将条件控制语句另起一行，放在<code>for</code>循环内部</p></blockquote><pre><code class="language-jinja2">&#123;% for i in [1,2,3,4] %&#125;&#123;% if i>2 %&#125;&#123;&#123; i &#125;&#125;'s index is &#123;&#123; loop.index &#125;&#125;&#123;% endif %&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后3's index is 34's index is 4</code></pre><blockquote><p>上述的 loop.index 只是jinja2的一种使用方式，其它方式具体可见官网文档</p><p><a href="https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures">https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures</a></p></blockquote><h4 id="宏-macro">宏 macro</h4><p>宏就是类似于函数的一个东西。</p><pre><code class="language-jinja2">&#123;# 编写宏 #&#125;&#123;% macro func() %&#125;函数体&#123;% endmacro %&#125;&#123;# 调用宏 #&#125;&#123;&#123; func() &#125;&#125;</code></pre><p>例如：</p><pre><code class="language-jinja2">&#123;% macro func(a,b,c=3,d=4) %&#125;&#123;# 宏编写的时候，宏参数，要遵循默认参数在后&#123;&#123; a &#125;&#125;&#123;&#123; b &#125;&#125;&#123;&#123; c &#125;&#125;&#123;&#123; d &#125;&#125;&#123;% endmacro %&#125;&#123;&#123; func(1,2,5) &#125;&#125;</code></pre><pre><code class="language-bash"># 渲染后1254</code></pre><blockquote><p>当给出参数超出了宏所定义的参数时，根据情况，宏会将多余的参数存在变量中，即：</p><p>超出的为非关键字参数，则存放在一个叫<code>varargs</code>的元组中</p><p>超出的为关键字参数，则存放在一个叫<code>kwargs</code>的字典中</p></blockquote><h4 id="call-方法">call 方法</h4><p>如同当前函数的装饰器，可以扩展当前宏的功能</p><pre><code class="language-jinja2">&#123;# 编写宏 func，并调用 caller #&#125;&#123;% macro func(a) %&#125;我有一个&#123;&#123; a &#125;&#125;。&#123;&#123; caller(a) &#125;&#125;&#123;% endmacro %&#125;&#123;# 编写宏 func_ext #&#125;&#123;% macro func_ext(a,b) %&#125;但&#123;&#123; b &#125;&#125;比&#123;&#123; a &#125;&#125;好吃。&#123;% endmacro %&#125;&#123;# 通过 call 关联 func，加载 func_ext #&#125;&#123;% call(a) func('汉堡') %&#125;&#123;&#123; func_ext(a,'三明治') &#125;&#125;&#123;% endcall %&#125;</code></pre><blockquote><p>caller是call的对象，因此caller也是可以给call传参</p></blockquote><pre><code class="language-bash"># 渲染后我有一个汉堡。但三明治比汉堡好吃</code></pre><h2 id="扩展">扩展</h2><blockquote><p>扩展官方文档，可见 <a href="https://jinja.palletsprojects.com/en/master/extensions/">https://jinja.palletsprojects.com/en/master/extensions/</a></p></blockquote><p>这里我只简单的说一下如何启动 <code>for</code> 循环中的 <code>break</code> 和 <code>continue</code>。</p><p><code>ansible</code> 中添加 <code>jinja2</code> 扩展，需要修改主配置文件 <code>/etc/ansible/ansible.cfg</code>，找到 <code>jinja2_extensions </code>，在后面追加扩展配置即可，每一个扩展用逗号<code>,</code>分割。</p><p><code>break</code>和<code>continue</code> 的扩展名叫：<code>jinja2.ext.loopcontrols</code></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-include</title>
      <link href="posts/6ce01f58/"/>
      <url>posts/6ce01f58/</url>
      
        <content type="html"><![CDATA[<h2 id="引入额外任务">引入额外任务</h2><pre><code class="language-yaml">tasks:  - include: add.yml</code></pre><h2 id="绑定-kv-对，从而改变额外任务里的变量">绑定 kv 对，从而改变额外任务里的变量</h2><pre><code class="language-yaml">tasks:  - include: add.yml    var1=hello    var2=world</code></pre><h2 id="绑定-tags-标记">绑定 tags 标记</h2><blockquote><p>可以通过tags执行相应的额外任务</p></blockquote><pre><code class="language-yaml:">tasks:  - include: add1.yml    tags: add1  - include: add2.yml    tags: add2</code></pre><pre><code class="language-bash">ansible-playbook test.play --tags add1 # 仅执行 add1.yml 任务</code></pre><h2 id="绑定-loop-循环">绑定 loop 循环</h2><pre><code class="language-yaml">tasks:  - include: add.yml    loop:      - [1,2,3]# add.yml- debug:  msg: &quot;loop-item: &#123;&#123; item &#125;&#125; in add.yml &quot;</code></pre><h2 id="绑定-when-条件">绑定 when 条件</h2><pre><code class="language-yaml">tasks:  - include: add.yml    when: 1 &lt; 2</code></pre><hr><p>ansible 在当前版本2.9中，推荐使用 import_tasks 和 include_tasks 来替换 include，include 未来有可能不在支持。（为啥总感觉 ansible 各种变呢）</p><p>import_tasks 静态任务导入，静态任务简单来说，就是不能从任务外传递变量到任务中。</p><p>include_tasks 动态任务导入。支持循环传递变量</p><p>import_tasks 绑定 when 的时候，会将 when 的条件一对一的应用到任务文件中列出的所有任务</p><p>include_tasks 绑定 when 的时候，会将 when 的条件仅应用到任务文件。即只要条件为真，任务文件里的所有任务都会执行。</p><p>关于新版写法，绑定 tags 的方式，和旧版差异比较大，例如</p><h4 id="include-tasks">include_tasks</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: test      include_tasks:         file: add.yml        apply:          tags:            - add      tags:        - always        # add.yml- debug:    msg: &quot;&#123;&#123; item &#125;&#125; is ok&quot;  loop: [1,2,3]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> include </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo☞本地搜索</title>
      <link href="posts/f5c218d/"/>
      <url>posts/f5c218d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>安装完 hexo-generator-search 后，发现搜索结果始终是所有文章.</p><p>如果你用的也是 <a href="https://github.com/Molunerfinn/hexo-theme-melody">Melody</a> 主题，那么可以参考如下信息，来确认。</p><h2 id="效果图">效果图</h2><p><img src="/posts/f5c218d/image-20200518121421437.png" alt="image-20200518121421437"></p><h2 id="软件包">软件包</h2><blockquote><p>截至：2020.05.18，软件包如下</p></blockquote><pre><code class="language-bash">  &quot;dependencies&quot;: &#123;    &quot;gitalk&quot;: &quot;^1.6.2&quot;,    &quot;hexo&quot;: &quot;^4.0.0&quot;,    &quot;hexo-asset-image&quot;: &quot;^1.0.0&quot;,    &quot;hexo-deployer-git&quot;: &quot;^2.1.0&quot;,    &quot;hexo-generator-archive&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-category&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-index&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-search&quot;: &quot;^2.4.0&quot;,    &quot;hexo-generator-tag&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-ejs&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-marked&quot;: &quot;^2.0.0&quot;,    &quot;hexo-renderer-pug&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-stylus&quot;: &quot;^1.1.0&quot;,    &quot;hexo-server&quot;: &quot;^1.0.0&quot;,    &quot;react&quot;: &quot;^15.3.1&quot;,    &quot;react-dom&quot;: &quot;^15.3.1&quot;  &#125;</code></pre><h2 id="config-yml">_config.yml</h2><blockquote><p>追加内容如下</p></blockquote><pre><code class="language-yaml">search:  path: search.xml  field: post  content: true</code></pre><h2 id="主题配置">主题配置</h2><blockquote><p>修改内容</p></blockquote><pre><code class="language-yaml">local_search:  enable: true</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞安装</title>
      <link href="posts/6209085/"/>
      <url>posts/6209085/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>此脚本用于安装 nginx;tengine;openresty. 安装版本为：</p><ul><li>nginx: 1.14</li><li>openresty: 1.15.8.3</li><li>tengine: 2.1.2 # 这是一个很古老的版本…</li></ul><h4 id="目录结构">目录结构</h4><p>因为是编译安装，所以产出目录均在 /usr/local/&lt;nginx/openresty/tengine&gt;，除了 logs 做了软链<code> /usr/local/xxx/logs -&gt; /export/logs/nginx</code></p><p><code>/usr/local/xxx/conf 目录结构</code></p><p><img src="/posts/6209085/image-20200515120037239.png" alt="image-20200515120037239"></p><pre><code class="language-bash"># 下面两个主配置文件会告诉你，相应的上下文配置，应该以什么结尾！！！include /usr/local/$&#123;NginxVer&#125;/nginx/conf/server/*.server;include /usr/local/$&#123;NginxVer&#125;/nginx/conf/upstream/*.upstream;</code></pre><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bashbasedir=/usr/local/srccd $basedirrunuser=`whoami`[[ $runuser == 'root' ]] || &#123;     echo &quot;ERROR:执行用户不是$runuser&quot; &amp;&amp; exit &#125;[[ -d /export/logs/nginx ]] || &#123;     echo &quot;/export/logs/nginx/目录不存在&quot; &amp;&amp; exit &#125;CpuNum=`cat /proc/cpuinfo | grep processor | wc -l`read -p &quot;输入安装的Nginx版本:(nginx;tengine;openresty):&quot; NginxVerread -p &quot;输入开发日常操作用户:&quot; KaifaUserread -p &quot;输入nginx worker用户:&quot; NginxWorkerUseruseradd -s /sbin/nologin $&#123;NginxWorkerUser&#125;usermod -a -G $&#123;KaifaUser&#125; $&#123;NginxWorkerUser&#125;cd /usr/local/srcrm -rf $&#123;NginxVer&#125; &amp;&amp; mkdir $&#123;NginxVer&#125;cat&gt;&gt;$basedir/test.com.server&lt;&lt;EOFserver &#123;    listen 80;    server_name test.com;    root /export/$&#123;NginxWorkerUser&#125;/test.com;    #charset koi8-r;    access_log logs/nginx-test.com.access.log main;    error_log logs/nginx-test.com.error.log;    # 关闭日志    location = /favicon.ico &#123;        log_not_found off;        access_log off;    &#125;    # 关闭日志    location = /robots.txt &#123;        auth_basic off;        allow all;        log_not_found off;        access_log off;    &#125;    # 拒绝探测网站根下的隐藏文件 Deny all attempts to access hidden files such as .htaccess, .htpasswd, .DS_Store (Mac).    location ~ /\. &#123;        deny all;        access_log off;        log_not_found off;    &#125;        #开启浏览器静态文件缓存    location ~ .*\.(html|htm|gif|jpg|jpeg|bmp|png|ico|txt|js|css)\$ &#123;         expires 3h;     &#125;     location / &#123;        #######这个是一个thinkphp框架的伪静态规则，请忽略        if (!-e \$request_filename) &#123;           rewrite ^(.*)\$ /index.php?s=\$1 last;           break;        &#125;        #######        index index.php;    &#125;        # 若php-fpm,请保留这里修改    location ~ \.php &#123;        fastcgi_pass 127.0.0.1：9000;        fastcgi_index index.php;        include fastcgi.conf;        fastcgi_connect_timeout 10s;        fastcgi_send_timeout 10s;        fastcgi_read_timeout 10s;        fastcgi_buffers 8 256k;                                   fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;    # 若 http，请保留这里修改    location / &#123;        proxy_pass http://127.0.0.1:8080;        proxy_connect_timeout 300ms;        proxy_send_timeout 300ms;        proxy_read_timeout 300ms;        proxy_max_temp_file_size 1024m;        proxy_set_header   Host         \$host;        proxy_set_header   X-Real-IP    \$remote_addr;        proxy_set_header   X-Forwarded-For  \$proxy_add_x_forwarded_for;        proxy_buffers 256 4k;        proxy_intercept_errors on;    &#125;&#125;EOFcat&gt;&gt;nginx_status.server&lt;&lt;EOFserver &#123;    listen 80;    server_name 127.0.0.1;   # charset koi8-r;    access_log off;    location /server_status &#123;        stub_status on;        access_log off;        allow 127.0.0.1;        deny all;    &#125;&#125;EOF###################if [[ $NginxVer == 'nginx' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc    wget http://$&#123;NginxVer&#125;.org/download/$&#123;NginxVer&#125;-1.14.0.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-http_v2_module || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125; &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;        deny all;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/conf/upstream/*.upstream;&#125;EOFelif [[ $NginxVer == 'openresty' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc    wget https://openresty.org/download/openresty-1.15.8.3.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-http_v2_module || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125;/nginx &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/nginx/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    log_format mainjson escape=json '&#123;&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,'                    '&quot;@source&quot;:&quot;$server_addr&quot;,'                    '&quot;hostname&quot;:&quot;$hostname&quot;,'                    '&quot;remote_user&quot;:&quot;$remote_user&quot;,'                    '&quot;ip&quot;:&quot;$http_x_forwarded_for&quot;,'                    '&quot;client&quot;:&quot;$remote_addr&quot;,'                    '&quot;request_method&quot;:&quot;$request_method&quot;,'                    '&quot;scheme&quot;:&quot;$scheme&quot;,'                    '&quot;domain&quot;:&quot;$server_name&quot;,'                    '&quot;referer&quot;:&quot;$http_referer&quot;,'                    '&quot;request&quot;:&quot;$request_uri&quot;,'                    '&quot;requesturl&quot;:&quot;$request&quot;,'                    '&quot;args&quot;:&quot;$args&quot;,'                    '&quot;size&quot;:$body_bytes_sent,'                    '&quot;status&quot;: $status,'                    '&quot;responsetime&quot;:$request_time,'                    '&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,'                    '&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,'                    '&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,'                    '&quot;http_cookie&quot;:&quot;$http_cookie&quot;,'                    '&quot;https&quot;:&quot;$https&quot;'                    '&#125;';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;            return 444;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/nginx/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/nginx/conf/upstream/*.upstream;&#125;EOFelif [[ $NginxVer == 'tengine' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc jemalloc-devel    wget http://tengine.taobao.org/download/tengine-2.1.2.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-jemalloc || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125; &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;            deny all;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/conf/upstream/*.upstream;&#125;EOFfi</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo☞图片显示404错误的解决办法</title>
      <link href="posts/662e9d4d/"/>
      <url>posts/662e9d4d/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>写作工具：typora</p><p>部署端：github</p><p>部署包：</p><pre><code># 你可以通过hexo的根目录下的package.json来确认版本&quot;hexo&quot;: &quot;^4.0.0&quot;,&quot;hexo-asset-image&quot;: &quot;^1.0.0&quot;</code></pre><p>目前网上大多数的博文描述的场景，均不是当前场景（截至到2020.05.14），所以博文里虽然展示都正常，但是按照博文的操作却会有路径问题，具体表现是图片前多了一级路径（路径应该是1级域名，<a href="http://xn--bvs393b.com">比如.com</a>，.io等，根据你的域名来定）</p><p>那么，请按照下面我的步骤来操作，如果还有问题，那就不是上述我所说的情况了。</p><h4 id="流程">流程</h4><ol><li><p>修改 hexo-asset-image，修改内容如图所示</p><p><img src="/posts/662e9d4d/image-20200901195338475.png" alt="image-20200901195338475">修改typora的图片存放路径，修改内容如图所示</p><p><img src="/posts/662e9d4d/image-20200514185631903.png" alt="image-20200514185631903"></p><blockquote><p>这不是必须的，但是我想没人会拒绝方便的操作。 typora 在进行如上操作后，就可以在你往文章里粘贴图片的时候，自动生成以文件名前缀命名的目录（效果就如同你开启了hexo的post_asset_folder: true参数），并将图片存放在此目录中。</p></blockquote></li><li><p>开启 hexo 的 _config.yml 中 post_asset_folder: true 参数配置</p></li></ol><h4 id="结论">结论</h4><p>经过上述操作，我想你已经可以在本地 md 文件和线上同时看到图片了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞日志切割</title>
      <link href="posts/9fb0f3a1/"/>
      <url>posts/9fb0f3a1/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>本脚本用于将 nginx 日志进行时间周期切割，并 lzo 压缩，最终上传到 s3。<br>脚本分为三个函数，切割函数，压缩上传函数，删除函数，需要执行哪个，就填写相对应变量。<br>详情可以看脚本注释。</p><blockquote><p>请务必执行前，确认安装了 lzop 和 jq 命令 ，且机器是 aws EC2</p></blockquote><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bash# by zyh# time: 2019-12-13# warning: 使用之前 yum install -y lzop jq# crontab (执行时间周期需要和切割时间周期一致) 重要!!!!!!!# */10 * * * * root bash /export/shell/nginxlog2s3/start.sh &gt; /export/shell/nginxlog2s3/start.log 2&gt;&amp;1# 标识日志名前缀localtag=`curl -sq http://169.254.169.254/latest/dynamic/instance-identity/document/ | jq -r .&quot;accountId&quot;,.&quot;availabilityZone&quot;,.&quot;privateIp&quot; | sed 'N;N;s@\n@_@g'`# ----------------------------------人为变量填写开始区域----------------------------------# 切割时间周期，定位切割后日期的初始写入时间（仅适用于连续切割，且不适用于第一次切割）todaytime=$(date -d &quot;-10 mins&quot; +%Y%m%d)todayhour=$(date -d &quot;-10 mins&quot; +%H)todaytimestr=$(date  -d &quot;-10 mins&quot; +%s)# 企业微信机器人wx_api=''# nginx 日志目录 logs 所在路径, 备份日志目录是 logs/logsbak# 例如日志目录是 /usr/local/nginx/logs，则填写 /usr/loca/nginx, 则切割后本地备份路径是 /usr/local/nginx/logs/logsbaknginx_base=# 日志位于S3的根路径，例如 s3://xxx/logs/xxxdays/nginxS3Base=&quot;&quot;# MvLogList=&quot;a.log b.log c.log&quot;  需要切割的日志，这是必须的MvLogList=&quot;&quot;# LzopS3LogList=&quot;a.log b.log c.log&quot; 需要压缩并上传S3的日志，如果你需要执行此步骤# S3目录格式：$&#123;S3Base&#125;/$&#123;日志名&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/ LzopS3LogList=&quot;&quot;# DeleteLocalLog=&quot;a.log b.log c.log&quot; 需要本地设置保留时间的日志，如果你需要执行此步DeleteLocalLog=&quot;&quot;# 本地保存时间deletetime=$(date -d &quot;72 hours ago&quot; +%s)# ----------------------------------人为变量填写结束区域----------------------------------# 日志原始路径nginx_logs=&quot;$&#123;nginx_base&#125;/logs&quot;# 日志位于本地的切割后备份路径backup_logs=&quot;$&#123;nginx_logs&#125;/logsbak&quot;[[ -d $&#123;backup_logs&#125; ]] || mkdir -p $&#123;backup_logs&#125;# nginx pid 文件路径nginx_pid=&quot;$&#123;nginx_logs&#125;/nginx.pid&quot;[[ -f $&#123;nginx_pid&#125; ]] || &#123;  echo &quot;$&#123;nginx_pid&#125; is not exist!!!!&quot; &amp;&amp; exit&#125;mvlog()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  [[ -d $&#123;backup_logs&#125;/$&#123;NginxLogName&#125; ]] || mkdir -p $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  mv $&#123;nginx_logs&#125;/$&#123;NginxLogName&#125; $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125; &amp;&amp; echo &quot;MV: $&#123;nginx_logs&#125;/$&#123;NginxLogName&#125; to $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;&quot;&#125;lzops3log()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  S3Path=$2  cd $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  lzop $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125; &amp;&amp; aws s3 cp $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo $&#123;S3Path&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/ --quiet &amp;&amp; rm -rf $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo &amp;&amp; echo &quot;UPLOAD: $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo to $&#123;S3Path&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/&quot; || curl &quot;$wx_api&quot; -H 'Content-Type: application/json' -d '&#123;&quot;msgtype&quot;: &quot;markdown&quot;,&quot;markdown&quot;: &#123;&quot;content&quot;: &quot;# `'&quot;$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo&quot;'` 日志上传失败!!!!!!&quot;&#125;&#125;'&#125;deletelocallog()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  cd $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  for logname in `ls`;do    if [[ $&#123;deletetime&#125; -ge $&#123;logname##*_&#125; ]];then      rm -rf $&#123;logname&#125; &amp;&amp; echo &quot;DELETE: $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;deletetime&#125;&quot;    fi  done&#125;#MVfor i in $&#123;MvLogList&#125;;do  mvlog $&#123;i&#125;done#nginx log reloadkill -USR1 `cat $&#123;nginx_pid&#125;`#lzop and to s3for i in $&#123;LzopS3LogList&#125;;do  lzops3log $&#123;i&#125; $&#123;S3Base&#125;/$&#123;i&#125;done#Deletefor i in $&#123;DeleteLocalLog&#125;;do  deletelocallog $&#123;i&#125;done</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> log </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞logrotate安装</title>
      <link href="posts/421d605e/"/>
      <url>posts/421d605e/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>logrotate 可以帮助我们进行日志切割，搭配 cron 服务，就可以自动的进行轮转</p><h4 id="logrotate-版本更新">logrotate 版本更新</h4><blockquote><p>确保 logrotate 支持小时级别的管理，替换/usr/sbin/logrotate,并附加x权限，我这里有一个二进制版本<a href="D:%5Czyh.cool%5Csource_posts%5C%E6%97%A5%E5%BF%97%5C%E5%85%B6%E5%AE%83%5C%E6%97%A5%E5%BF%97%E2%98%9Elogrotate%E5%AE%89%E8%A3%85%5Clogrotate">logrotate</a></p><p>或者也可以直接去 github 上拉取https://github.com/logrotate/logrotate</p></blockquote><h4 id="添加-logrotate-配置">添加 logrotate 配置</h4><pre><code class="language-bash"># 添加所需切割的日志配置cat &gt; /etc/logrotate.d/nginx &lt;&lt; 'EOF'/usr/local/nginx/logs/access.log &#123;  # 定义日志位置 hourly    # 按照小时切割 rotate 2  # 最多保留两份切割日志 missingok nocompress sharedscripts postrotate  /bin/kill -USR1 `cat /usr/local/nginx/logs/nginx.pid 2&gt;/dev/null` 2&gt;/dev/null || true endscript&#125;EOF</code></pre><h4 id="添加-crontab-配置">添加 crontab 配置</h4><pre><code class="language-bash"># 添加logrotate执行脚本cp /etc/cron.daily/logrotate /etc/cron.hourly/</code></pre><h4 id="重载-crond-服务">重载 crond 服务</h4><pre><code class="language-bash">systemctl reload crond</code></pre>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> logrotate </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 邮件预警</title>
      <link href="posts/f8f9b4d0/"/>
      <url>posts/f8f9b4d0/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># $1 收件人# $2 主题# $3 内容smtpServer=  # smtp 服务器地址，例如 smtp.gmail.com:xxxsendUserEmail='it@abc.com'sendUserPassword=  # 一般发件人邮箱密码都是专用密码，并非web密码/usr/local/bin/sendEmail -f $&#123;sendUserEmail&#125; -t $1 -u &quot;$2&quot; -m &quot;$3&quot; -s $&#123;smtpServer&#125; -xu $&#123;sendUserEmail&#125; -xp $&#123;sendUserPassword&#125; -o message-charset=utf-8</code></pre><pre><code class="language-python">#!/usr/bin/python#coding:utf-8# by zyh# python sendmail.py '收件人邮箱地址'  '主题' '邮件内容' '抄送邮箱地址' '附件绝对路径'import smtplibfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartimport sysreload(sys)sys.setdefaultencoding('utf8')mail_host = 'smtp服务器地址'  # 例如 smtp.gmail.com:587mail_user = '发件人邮箱'      # 例如 it@gmail.commail_pass = '发件人密码'def send_mail(*args):    argsNum=len(args[0])    if argsNum == 4:        filename,to_list, subject, content=args[0]    elif argsNum == 5:        filename,to_list, subject, content, cc_list=args[0]    elif argsNum == 6:        filename,to_list, subject, content, cc_list, subfile=args[0]    me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;    #初始化邮件对象 msg    msg = MIMEMultipart()    msg['From'] = me    msg['to'] = to_list    emailList=to_list    # 主题    msg['Subject'] = Header(subject, 'utf-8').encode()    # 内容    msg.attach(MIMEText(content, 'plain', 'utf-8'))    # 抄送    if 'cc_list' in vars():        msg['Cc'] = cc_list        emailstring = to_list+','+cc_list        emailList=emailstring.split(&quot;,&quot;)    # 附件    if 'subfile' in vars():        att1 = MIMEText(open((&quot;%s&quot;%subfile),'rb').read(), 'base64', 'utf8')        att1[&quot;Content-Type&quot;] = 'text/plain'        att1[&quot;Content-Disposition&quot;] = 'attachment; filename='+subfile.split('/')[-1] # 这里的filename可以任意写，写什么名字，邮件中显示什么名字        msg.attach(att1)    # 发送    try:        print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)        print &quot;connect mail server suesscc&quot;        s.starttls()        s.login(mail_user,mail_pass)        print &quot;login mail server suesscc&quot;        s.sendmail(me,emailList,msg.as_string())        s.close()        return True    except Exception,e:        print &quot;%s\t%s&quot;%(to_list,str(e))        return Falseif __name__ == &quot;__main__&quot;:    if len(sys.argv) &lt; 4:        exit()    send_mail(sys.argv)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 邮件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 微信预警</title>
      <link href="posts/f9b08c30/"/>
      <url>posts/f9b08c30/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># conf.ini[wechat]corpid = [app]it = &lt;app_agent_id&gt;:&lt;app_secret&gt;[group]it = usera|userb</code></pre><pre><code class="language-python">#!/usr/bin/env python3&quot;&quot;&quot;@author: zyh@contact: aaa103439@hotmail.com@software: vscode@file: sendchat.py@time: 2020/02/05&quot;&quot;&quot;import sys, os, requests, pathlib, json, configparserimport loggingimport logging.handlersfrom datetime import datetimeclass PySendchat():    def __init__(self, corpid, agentid, secret, touser, content):        self.corpid=corpid        self.agentid=agentid        self.secret=secret        self.touser=touser        self.content=content        LOG_PATHDIR=os.path.dirname(os.path.abspath(__file__))        LOG_FILENAME = '&#123;0&#125;/sendchat.log'.format(LOG_PATHDIR)        self.my_logger = logging.getLogger('SendChat')        self.my_logger.setLevel(logging.INFO)        # Add the log message handler to the logger        handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=102400000, backupCount=5)        # create formatter and add it to the handlers        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')        handler.setFormatter(formatter)        self.my_logger.addHandler(handler)    def gettoken(self):        self.my_logger.info('-----------------------------------------------------------------------------------------')        pwd=os.path.dirname(os.path.abspath(__file__))        tokenfile='&#123;0&#125;/wechat.&#123;1&#125;'.format(pwd,self.agentid)        if pathlib.Path(tokenfile).exists():            tokenfilectime=os.path.getctime(tokenfile)            currenttime=datetime.now().timestamp()            dtime=currenttime-tokenfilectime            self.my_logger.info('&#123;0&#125; lived &#123;1&#125;s.'.format(tokenfile, dtime))            if dtime &gt;= 7200:                try:                    os.remove(tokenfile)                    self.my_logger.info('Token file &#123;0&#125;: delete success'.format(tokenfile))                except Exception as e:                    self.my_logger.error('Token file:&#123;0&#125; delete error.Reason:&#123;1&#125;'.format(tokenfile,e))                    exit        # check token file        try:            tokensize = os.path.getsize(tokenfile)        except Exception as e:            self.my_logger.info('Token file is not exist.Reason:&#123;0&#125;'.format(e))            tokensize = 0        # get token from token file        if tokensize != 0:            with open(tokenfile, 'rb') as fd:                token = fd.read() # get token success                self.my_logger.info('Get token from token file.')            jsonObject = json.loads(token.decode(encoding='utf8'))            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token        # get token from weixin api        else:            try:                self.my_logger.info('New Token Create.')                f = requests.get('https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&#123;0&#125;&amp;corpsecret=&#123;1&#125;'.format(self.corpid, self.secret))                token = f.content                self.my_logger.info('Get token from weixin api.')                jsonObject = json.loads(token.decode(encoding='utf8'))                errcode=int(jsonObject.get(&quot;errcode&quot;))                if errcode != 0:                    errmsg=jsonObject.get(&quot;errmsg&quot;)                    self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(errmsg))                    exit()            except Exception as e:                self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(e))                exit()            try:                self.my_logger.info('Write token to &#123;0&#125;.'.format(tokenfile))                with open(tokenfile, 'wb') as fd:                    fd.write(token)            except Exception as e:                self.my_logger.error('Write &#123;0&#125; error!Reason:&#123;1&#125;'.format(tokenfile,e))                exit()            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token    def sendmsg(self):        accessToken = self.gettoken()        self.my_logger.info('Token:&#123;0&#125;'.format(accessToken))        sendMapDirectroy = &#123;&#125;        sendMapDirectroy[&quot;agentid&quot;] = self.agentid        sendMapDirectroy[&quot;touser&quot;] = self.touser        sendMapDirectroy[&quot;msgtype&quot;] = &quot;text&quot;        sendMapDirectroy[&quot;safe&quot;] = &quot;0&quot;        contentDirectory = &#123;&#125;        sendMapDirectroy[&quot;text&quot;] = contentDirectory        contentDirectory[&quot;content&quot;] = self.content        bodyStr = json.dumps(sendMapDirectroy, ensure_ascii=False).encode(encoding=&quot;utf-8&quot;)        try:            f = requests.post(url=&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s&quot; % accessToken,                          data=bodyStr, timeout=5)            self.my_logger.info(f.content)        except Exception as e:            self.my_logger.error('Send chat network error!Reason:&#123;0&#125;'.format(e))if __name__ == '__main__':    appname = sys.argv[1]    content = sys.argv[3]    # read conf.ini    conf = configparser.ConfigParser()    conf_path = os.path.dirname(os.path.abspath(__file__))    conf_ini = &quot;&#123;0&#125;/conf.ini&quot;.format(conf_path)    if pathlib.Path(conf_ini).exists():        conf.read(conf_ini)        corpid = conf.get(&quot;wechat&quot;, &quot;corpid&quot;)        appinfo = conf.get(&quot;app&quot;, appname)        agentid = appinfo.split(':')[0]        secret = appinfo.split(':')[1]        groupname = conf.get(&quot;group&quot;, appname)        touser = groupname.split(':')[0]        chatobj = PySendchat(corpid, agentid, secret, touser, content)    else:        print('conf.ini error')        exit()    try:        chatobj.sendmsg()    except:        chatobj.my_logger.error(&quot;Send chat failure!&quot;)</code></pre><p>eg:</p><p>python <a href="http://sendchat.py">sendchat.py</a> it ‘’ &lt;预警内容&gt;</p><blockquote><p>创建好app，并关联用户到app</p><p>执行上述命令，会将预警内容通过&lt;app_agent_id&gt; 应用发送给用户usera和userb</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 企业微信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞web监控</title>
      <link href="posts/3d20f83e/"/>
      <url>posts/3d20f83e/</url>
      
        <content type="html"><![CDATA[<h2 id="1-构建-zabbix-agentd-端配置">1. 构建 zabbix_agentd 端配置</h2><p>选择一个主机用于发起监控</p><pre><code class="language-bash"># 目录结构[root@ip-10-230-10-105 zabbix]# pwd/etc/zabbix[root@ip-10-230-10-105 zabbix]# tree &#123;etc,shell&#125;etc├── zabbix_agentd.conf├── zabbix_agentd.conf.bak└── zabbix_agentd.conf.d    └── http_status.conf # 我是 zabbix_agentd 数据项配置shell└── web    ├── http_status.py # 我是自动发现脚本 + 数据采集脚本    └── WEB.txt  # 我是自动发现的数据源2 directories, 5 files</code></pre><pre><code class="language-bash"># zabbix_agentd 配置 http_status.confUserParameter=web.site.code[*],/usr/bin/python /etc/zabbix/shell/web/http_status.py web_site_code $1UserParameter=web.site.discovery,/usr/bin/python /etc/zabbix/shell/web/http_status.py web_site_discovery</code></pre><pre><code class="language-bash"># 自动发现规则配置文件 WEB.txt# 一行一个监控地址# get 原样写入# post 模仿get多加一个?https://abc.com??&lt;post_kv&gt;https://abc.com?&lt;get_kv&gt;https://abc.com</code></pre><pre><code class="language-python">#!/usr/bin/env python#encoding=utf-8# python2.7# 自动发现脚本 + 数据采集脚本 http_status.py# 请将我添加 o+x 权限# ConfigParser 模块是需要安装的import urllib2, sys, json, ConfigParser, os a1 = sys.argv[1]def web_site_code(args):    response = None    WEB_TXT = &quot;%s/WEB.txt&quot; % (os.path.dirname(os.path.realpath(__file__)))    with open(WEB_TXT, 'ro') as f:        for line in f.readlines():            line = line.strip()            if args in line:                if &quot;??&quot; in line:                    line = line.split(&quot;??&quot;)                elif &quot;?&quot; in line:                    line = line.split(&quot;?&quot;)                else:                    pass                try:                    try:                        response = urllib2.urlopen(line[0], data=line[1], timeout=5)                        print response.code                    except IndexError:                        response = urllib2.urlopen(line[0], timeout=5)                        print response.code                    finally:                        response = urllib2.urlopen(line, timeout=5)                        print response.code                except urllib2.URLError,e:                    if hasattr(e, 'code'):                        print e.code                    elif hasattr(e, 'reason'):                        print 500                    else:                        print 500                finally:                    if response:                        response.close()                    exit()def web_site_discovery():    Dict = &#123;&quot;data&quot;:[]&#125;    WEB_TXT = &quot;%s/WEB.txt&quot; % (os.path.dirname(os.path.realpath(__file__)))    for line in open(WEB_TXT):        if &quot;??&quot; in line:            line = line.strip('\n').split(&quot;??&quot;)            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line[0]&#125;        elif &quot;?&quot; in line:            line = line.strip('\n').split(&quot;?&quot;)            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line[0]&#125;        else:            line = line.strip('\n')            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line&#125;        Dict[&quot;data&quot;].append(line)    print json.dumps(Dict, indent=2)if a1 == 'web_site_code':    url = sys.argv[2]    web_site_code(url)elif a1 == 'web_site_discovery':    web_site_discovery()</code></pre><blockquote><p>{ #SITENAME} 自动发现脚本输出的重要变量 servername 地址，将会用于 web 控制台配置</p></blockquote><h2 id="2-构建-web-控制台配置">2. 构建 web 控制台配置</h2><ul><li><p>选择你配置好的zabbix_agentd端的主机所在的 web 控制台配置项，添加一个自动发现规则</p><p><img src="/posts/3d20f83e/image-20210524165435984.png" alt="image-20210524165435984"></p></li><li><p>在自动发现规则里，构建监控项原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20210524165539770.png" alt="image-20210524165539770"></p><p>在自动发现规则里，构建触发器原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20210524165609439.png" alt="image-20210524165609439"></p><blockquote><p>100秒以内，状态为200的次数少于3次告警</p></blockquote></li><li><p>在自动发现规则里，构建图形原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20210524165635429.png" alt="image-20210524165635429"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix ☞ nginx</title>
      <link href="posts/b35b5965/"/>
      <url>posts/b35b5965/</url>
      
        <content type="html"><![CDATA[<blockquote><p>确保相关目录地址如下：</p><p>/etc/zabbix/shell</p><p>/etc/zabbix/zabbix_agentd.d</p><p>/usr/local/nginx/conf/server</p></blockquote><ul><li>1，nginx增加配置 server_status.server</li></ul><pre><code class="language-bash:">cat &gt; /usr/local/nginx/conf/server/server_status.server &lt;&lt; 'EOF'server &#123;    listen       80;    server_name  127.0.0.1;    #charset koi8-r;    access_log  off;    location /server_status &#123;        stub_status on;        access_log off;        allow 127.0.0.1;        deny all;    &#125;&#125;EOF/usr/local/nginx/sbin/nginx -t &amp;&amp; /usr/local/nginx/sbin/nginx -s reload</code></pre><hr><ul><li>2，<a href="http://xn--nginx-e86hk14jmnl025b.sh">添加脚本nginx.sh</a>  (确保a+x权限)</li></ul><pre><code class="language-bash:">mkdir /etc/zabbix/shell -p;cat &gt; /etc/zabbix/shell/nginx.sh &lt;&lt; 'EOF'#!/bin/bash  function active &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Active' | awk '&#123;print $NF&#125;'&#125;function reading &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Reading' | awk '&#123;print $2&#125;'&#125;function writing &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt;/dev/null| grep 'Writing' | awk '&#123;print $4&#125;'&#125;function waiting &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Waiting' | awk '&#123;print $6&#125;'&#125;function accepts &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $1&#125;'&#125;function handled &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $2&#125;'&#125;function requests &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $3&#125;'&#125;function qps &#123;        NGINX_STATUS_URL=&quot;http://127.0.0.1/server_status&quot;        #若是tnginx，则最后应输出d[length(d)-1]        requestold=`/usr/bin/curl -s $&#123;NGINX_STATUS_URL&#125; | /usr/bin/awk '/server accepts handled requests/&#123;getline a;split(a,d);print d[length(d)]&#125;'`        TimeWait=1        sleep $TimeWait        requestnew=`/usr/bin/curl -s $&#123;NGINX_STATUS_URL&#125; | /usr/bin/awk '/server accepts handled requests/&#123;getline a;split(a,d);print d[length(d)]&#125;'`        if [ $requestnew -gt 0 ];then                QPS=`echo &quot;( $requestnew - $requestold ) / $TimeWait&quot; | /usr/bin/bc`        fi        echo $QPS&#125;# Run the requested function  $1EOFchmod a+x /etc/zabbix/shell/nginx.sh</code></pre><hr><ul><li>3，配置zabbix客户端zabbix_agentd.conf</li></ul><pre><code class="language-bash:">cat &gt; /etc/zabbix/zabbix_agentd.d/nginx.conf &lt;&lt; 'EOF'#monitor nginx  UserParameter=nginx.accepts,/etc/zabbix/shell/nginx.sh acceptsUserParameter=nginx.handled,/etc/zabbix/shell/nginx.sh handledUserParameter=nginx.requests,/etc/zabbix/shell/nginx.sh requestsUserParameter=nginx.connections.active,/etc/zabbix/shell/nginx.sh activeUserParameter=nginx.connections.reading,/etc/zabbix/shell/nginx.sh readingUserParameter=nginx.connections.writing,/etc/zabbix/shell/nginx.sh writingUserParameter=nginx.connections.waiting,/etc/zabbix/shell/nginx.sh waitingUserParameter=nginx.connections.qps,/etc/zabbix/shell/nginx.sh qpsEOF</code></pre><hr><ul><li>4，在服务的对应主机上添加模板</li></ul>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 远程磁盘检测</title>
      <link href="posts/1fac36d/"/>
      <url>posts/1fac36d/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">#!/bin/bash # 文件名：disklog.sh # 用途：监视远程系统的磁盘使用情况 BaseDir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $BaseDirlogfile=&quot;disk.log&quot; if [[ -n $1 ]];then     logfile=$1 fishellexecuser=`whoami`if [[ $shellexecuser == root ]];then    rm -rf /root/.ssh/known_hostselse    rm -rf /home/$shellexecuser/.ssh/known_hostsfiprintf &quot;%-8s %-14s %-9s %-8s %-6s %-6s %-6s %s\r\n&quot; &quot;Date&quot; &quot;IP ADDRESS&quot; &quot;Device&quot; &quot;Capacity&quot; &quot;Used&quot; &quot;Free&quot; &quot;Percent&quot; &quot;Status&quot; &gt; $logfile ##################### 手动填写区# 提供远程主机IP地址列表 1.1.1.1 2.2.2.2 3.3.3.3IP_LIST=# 监控阈值(百分比) 只填写数字 1 到 100DiskPct=# 执行用户UserName=''# 执行用户所需私钥, 此文件需要与脚本同级目录PemName=''# 企业微信bot机器人地址wx_api=''##################### 手动填写区for ip in $IP_LIST;do     ssh -i $PemName -o StrictHostKeyChecking=no $&#123;UserName&#125;@$ip 'df -H' | grep ^/dev/ &gt; /tmp/$$.df     while read line;do         cur_date=`date  &quot;+%F_%R&quot;`        printf &quot;%-8s %-14s &quot; $cur_date $ip         echo $line | awk '&#123; printf(&quot;%-9s %-8s %-6s %-6s %-8s&quot;, $1,$2,$3,$4,$5); &#125;'         pusg=$(echo $line | egrep -o &quot;[0-9]+%&quot;)         pusg=$&#123;pusg/\%/&#125;;         if [ $pusg -lt $DiskPct ];then             echo OK        else             echo ALERT         fi     done &lt; /tmp/$$.df     rm -rf /tmp/$$.dfdone &gt;&gt; $&#123;logfile&#125;sed -n '1p' $&#123;logfile&#125; &gt; alert.logawk '$NF == &quot;ALERT&quot;&#123;print $0&#125;' $&#123;logfile&#125; &gt;&gt; alert.log#sed -i '1i &quot;磁盘阈值：'&quot;$DiskPct&quot;'&quot;' alert.logcontent=`cat alert.log`grep -q 'ALERT' alert.log &amp;&amp; &#123;curl &quot;$wx_api&quot;  -H 'Content-Type: application/json'  \-d '   &#123;        &quot;msgtype&quot;: &quot;text&quot;,        &quot;text&quot;: &#123;            &quot;content&quot;: &quot;'&quot;$content&quot;'&quot;        &#125;   &#125;'&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 磁盘 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞redis</title>
      <link href="posts/6c093771/"/>
      <url>posts/6c093771/</url>
      
        <content type="html"><![CDATA[<h2 id="zabbix-模板">zabbix 模板</h2><ol><li>自动发现规则</li></ol><p><img src="/posts/6c093771/image-20200520152740815.png" alt="image-20200520152740815"></p><ol start="2"><li><p>过滤器</p><p><img src="/posts/6c093771/image-20200520152922570.png" alt="image-20200520152922570"></p></li><li><p>监控项原型</p><table><thead><tr><th>名称</th><th>键值</th><th>间隔</th><th>历史记录</th><th>趋势</th><th>类型</th></tr></thead><tbody><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; connected_clients[客户端连接数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,connected_clients]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; maxmemory[redis配置的内存上限]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,maxmemory]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; mem_fragmentation_ratio[内存碎片率]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; redis_instantaneous_ops_per_sec[每秒执行的命令个数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,instantaneous_ops_per_sec]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; redis 存活状态</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,exist]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; rejected_connections[被拒绝的客户端连接数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,rejected_connections]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_sys[redis-master sys-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_sys]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_sys_children[redis-children sys-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_sys_children]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_user[redis-master user-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_user]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_user_children[redis-children user-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_user_children]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory[redis层面已使用内存-不含碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_pct[操作系统层面已使用内存百分比]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_peak[操作系统层面已使用内存历史峰值-含内存碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_peak]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_rss[操作系统层面已使用内存-含内存碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_rss]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr></tbody></table></li><li><p>触发器原型</p><table><thead><tr><th>严重性</th><th>名称</th><th>表达式</th></tr></thead><tbody><tr><td>一般严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  内存碎片化超过50%, 剩余可用内存低于30%</code></td><td><code>&#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio].last()&#125;&gt;1.5  and &#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  可用内存低, 存在使用交换分区</code></td><td><code>&#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio].last()&#125;&lt;1  and &#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>一般严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  操作系统层面内存占用百分比过高</code></td><td><code>&#123;Template  Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>灾难</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis 端口无法访问</code></td><td><code>&#123;Template  Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,exist].last()&#125;&lt;&gt;1</code></td></tr></tbody></table></li><li><p>图形原型，就不详细写了，这里只列出我自己的分类</p><table><thead><tr><th>名称</th><th>宽</th><th>高</th><th>图形类别</th></tr></thead><tbody><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis 连接数监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis 其他监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis qps 监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis mem 监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis cpu 监控</code></td><td>900</td><td>200</td><td>正常</td></tr></tbody></table></li></ol><h2 id="脚本">脚本</h2><p>脚本会生成自动发现进程脚本和redis检测脚本</p><p>zabbix的redis配置路径：ZabbixEtc</p><p>zabbix的脚本路径：ZabbixShell</p><p>redis的cli命令路径：RedisCli</p><p>自动发现脚本：ip_port_discovery.sh</p><ul><li>bash ip_port_discovery.sh 进程名或者端口</li></ul><p>redis检测脚本：redis_info.sh</p><ul><li>bash redis_info.sh ip port item</li></ul><pre><code class="language-redis">ZabbixEtc=/etc/zabbix/zabbix_agentd.dZabbixShell=/etc/zabbix/shellRedisCli='docker exec -t redis redis-cli'[[ -d $&#123;ZabbixShell&#125; ]] || mkdir -p $&#123;ZabbixShell&#125;[[ -z $&#123;ZabbixEtc&#125; ]] &amp;&amp; [[ -z $&#123;ZabbixShell&#125; ]] || [[ -z $&#123;RedisCli&#125; ]] &amp;&amp; exitcat&gt;$&#123;ZabbixEtc&#125;/redis.conf&lt;&lt;EOF##redis monitorUserParameter=redis_info[*],$&#123;ZabbixShell&#125;/redis_info.sh \$1 \$2 \$3 \$4UserParameter=ip_port_discovery[*],$&#123;ZabbixShell&#125;/ip_port_discovery.sh \$1EOFcat&gt;$&#123;ZabbixShell&#125;/redis_info.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashRedisCli=HOST=$1PORT=$2REDIS_INFO=&quot;$RedisCli -h $HOST -p $PORT info&quot;[[ $# -ne 3 ]] &amp;&amp; [[ $# -ne 4 ]] &amp;&amp; &#123; exit&#125;if [[ $# -eq 3 ]];thencase $3 inexist) result=`$RedisCli -h $HOST -p $PORT ping 2&gt;/dev/null |grep -c PONG` echo $result;;cluster)        result=`$REDIS_INFO|/bin/grep cluster|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;uptime_in_seconds)        result=`$REDIS_INFO|/bin/grep uptime_in_seconds|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;connected_clients)        result=`$REDIS_INFO|/bin/grep connected_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_longest_output_list)        result=`$REDIS_INFO|/bin/grep client_longest_output_list|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_biggest_input_buf)        result=`$REDIS_INFO|/bin/grep client_biggest_input_buf|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;blocked_clients)        result=`$REDIS_INFO|/bin/grep blocked_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#内存maxmemory)        result=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory)        result=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_rss)        result=`$REDIS_INFO|/bin/grep -w used_memory_rss|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk -F'k' '&#123;print $1&#125;'`        echo $result;;used_memory_pct) A=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` B=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` result=`echo $A $B | awk '&#123;printf&quot;%0.2f&quot;,$1/$2&#125;'`        echo $result;;used_memory_peak)        result=`$REDIS_INFO|/bin/grep used_memory_peak|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_lua)        result=`$REDIS_INFO|/bin/grep used_memory_lua|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;mem_fragmentation_ratio)        result=`$REDIS_INFO|/bin/grep mem_fragmentation_ratio|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#rdbrdb_changes_since_last_save)        result=`$REDIS_INFO|/bin/grep rdb_changes_since_last_save|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_bgsave_in_progress)        result=`$REDIS_INFO|/bin/grep rdb_bgsave_in_progress|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_save_time)        result=`$REDIS_INFO|/bin/grep rdb_last_save_time|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_bgsave_status)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_last_bgsave_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;rdb_current_bgsave_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_current_bgsave_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#rdbinfoaof_enabled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_enabled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_scheduled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_scheduled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_last_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_current_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_last_bgrewrite_status)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_bgrewrite_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;#aofinfoaof_current_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_base_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_base_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_rewrite)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_rewrite&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_bio_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_bio_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_delayed_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_delayed_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#statstotal_connections_received)        result=`$REDIS_INFO|/bin/grep -w &quot;total_connections_received&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;total_commands_processed)        result=`$REDIS_INFO|/bin/grep -w &quot;total_commands_processed&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;instantaneous_ops_per_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;instantaneous_ops_per_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;rejected_connections)        result=`$REDIS_INFO|/bin/grep -w &quot;rejected_connections&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;expired_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;expired_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;evicted_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;evicted_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_hits)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_hits&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_misses)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_misses&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_patterns)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_patterns&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;latest_fork_usec)        result=`$REDIS_INFO|/bin/grep -w &quot;latest_fork_usec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;connected_slaves)        result=`$REDIS_INFO|/bin/grep -w &quot;connected_slaves&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;master_link_status)        result=`$REDIS_INFO|/bin/grep -w &quot;master_link_status&quot;|awk -F':' '&#123;print $2&#125;'|/bin/grep -c up`        echo $result;;master_last_io_seconds_ago)        result=`$REDIS_INFO|/bin/grep -w &quot;master_last_io_seconds_ago&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;master_sync_in_progress)        result=`$REDIS_INFO|/bin/grep -w &quot;master_sync_in_progress&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;slave_priority)        result=`$REDIS_INFO|/bin/grep -w &quot;slave_priority&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;#cpuused_cpu_sys)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_sys_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;*) echo &quot;argu error&quot;;;esac#db0:key   elif [[ $# -eq 4 ]];thencase $4 inkeys)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;keys&quot; | awk -F'=|,' '&#123;print $2&#125;'`        echo $result;;expires)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;expires&quot; | awk -F'=|,' '&#123;print $4&#125;'`        echo $result;;avg_ttl)        result=`$REDIS_INFO|/bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;avg_ttl&quot; | awk -F'=|,' '&#123;print $6&#125;'`        echo $result;;*)     echo &quot;argu error&quot; ;;esacfiEOFcat&gt;$&#123;ZabbixShell&#125;/ip_port_discovery.sh&lt;&lt;&quot;EOF&quot;#!/bin/bash#$1是要发现的进程部分词汇portarray=(`sudo ss -tnlp|egrep -i &quot;$1&quot;|awk &#123;'print $4'&#125;|awk -F':' '&#123;if ($NF~/^[0-9]*$/) print $NF&#125;'|sort|uniq`)iparray=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`length=$&#123;#portarray[@]&#125;printf &quot;&#123;\n&quot;printf '\t'&quot;\&quot;data\&quot;:[&quot;for ((i=0;i&lt;$length;i++))  do     printf '\n\t\t&#123;'     printf &quot;\&quot;&#123;#IP&#125;\&quot;:\&quot;$&#123;iparray&#125;\&quot;,\&quot;&#123;#TCP_PORT&#125;\&quot;:\&quot;$&#123;portarray[$i]&#125;\&quot;&#125;&quot;     if [ $i -lt $[$length-1] ];then                printf ','     fi  doneprintf &quot;\n\t]\n&quot;printf &quot;&#125;\n&quot;EOFchmod a+x $&#123;ZabbixShell&#125;/redis_info.shchmod a+x $&#123;ZabbixShell&#125;/ip_port_discovery.shsed -i 's#RedisCli=#RedisCli=\&quot;'&quot;$&#123;RedisCli&#125;&quot;'\&quot;#' $&#123;ZabbixShell&#125;/redis_info.shecho '------------------------------------我是 zabbix 监控信息----------------------------------'echo '编辑 visudo，添加如下信息'echo &quot;#zabbix用户可以以sudo执行sszabbix ALL = NOPASSWD: /usr/sbin/ss#zabbix用户使用sudo无需ttyDefaults:zabbix    !requiretty&quot;echo '若redis运行在docker中，执行如下命令'echo 'usermod -a -G docker zabbix'</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞企业微信机器人</title>
      <link href="posts/8c33f887/"/>
      <url>posts/8c33f887/</url>
      
        <content type="html"><![CDATA[<h4 id="shell">shell</h4><pre><code class="language-bash">wechatUrl=wechatData=curl $&#123;wechatUrl&#125; -H 'Content-Type: application/json' -d '&#123;&quot;msgtype&quot;: &quot;markdown&quot;,&quot;markdown&quot;: &#123;&quot;content&quot;: &quot;`'&quot;$wechatData&quot;'`&quot;&#125;&#125;'</code></pre><h4 id="python">python</h4><pre><code class="language-python">import json, requestswechatData=&#123;&quot;msgtype&quot;: &quot;text&quot;,&quot;text&quot;: &#123;&quot;content&quot;: &quot;&quot;&#125;&#125;wechatData['text']['content']='广州今日天气：29度，大部分多云，降雨概率：60%'wechatData['text']['mentioned_list']=[&quot;zyh&quot;]  # all 代表群组所有人wechatData=json.dumps(wechatData)wechatUrl=requests.post(url=wechatUrl, headers=&#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;, data=wechatData, timeout=5)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 企业微信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞agent和proxy（yum）</title>
      <link href="posts/1587f6e8/"/>
      <url>posts/1587f6e8/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://repo.zabbix.com/zabbix/">https://repo.zabbix.com/zabbix/</a></p></blockquote><h2 id="导入-zabbix-源">导入 zabbix 源</h2><pre><code class="language-bash：">rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm</code></pre><h2 id="agent-安装">agent 安装</h2><pre><code class="language-shell">yum install zabbix-agent -ysystemctl enable zabbix-agent</code></pre><h2 id="agent主动模式配置文件">agent主动模式配置文件</h2><h3 id="变量">变量</h3><pre><code class="language-bash"># 主机名前缀name_prefix=# 自动注册action用的元数据host_meta_data=''# server或者proxy地址server_proxy_addr=# 本机ip## 若是 aws： local_ip=`curl -sq http://169.254.169.254/latest/meta-data/local-ipv4`local_ip=</code></pre><pre><code>mv /etc/zabbix/zabbix_agentd.conf /etc/zabbix/zabbix_agentd.conf.bak;cat &gt; /etc/zabbix/zabbix_agentd.conf.xxx &lt;&lt; EOFHostname=$&#123;name_prefix&#125;_$&#123;local_ip&#125;StartAgents=0ServerActive=$&#123;server_proxy_addr&#125;PidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.log#DebugLevel=4Include=/etc/zabbix/zabbix_agentd.d/*.conf#被监控端到服务器获取监控项的周期RefreshActiveChecks=60#被监控端存储监控信息的空间大小BufferSize=1000MaxLinesPerSecond=200#超时时间Timeout=10#自动发现用的元信息HostMetadata=$&#123;host_meta_data&#125;EOFvi  /etc/zabbix/zabbix_agentd.confsystemctl start zabbix-agent</code></pre><blockquote><p>如果无法被 zabbix_server 发现，则可能是上述生成的配置有特殊字符。</p></blockquote><h2 id="proxy-安装命令">proxy 安装命令</h2><pre><code class="language-bash:">yum install zabbix-proxy-mysql -y# 解压数据库文件, 并自行导入zcat /usr/share/doc/zabbix-proxy-mysql-*/schema.sql.gz &gt; schema.sql</code></pre><h2 id="proxy-配置">proxy 配置</h2><pre><code class="language-bash">mv /etc/zabbix/zabbix_proxy.conf /etc/zabbix/zabbix_proxy.conf.bak;cat &gt; /etc/zabbix/zabbix_proxy.conf  &lt;&lt; 'EOF'Server=ServerPort=10051Hostname=LogFile=/var/log/zabbix/zabbix_proxy.logPidFile=/var/run/zabbix/zabbix_proxy.pidDBHost=DBPort=DBName=DBUser=DBPassword=Timeout=4LogSlowQueries=3000ConfigFrequency=60DataSenderFrequency=60StartDiscoverers=5CacheSize=128MStartDBSyncers=20HistoryCacheSize=256MHistoryIndexCacheSize=32MEOFvi /etc/zabbix/zabbix_proxy.conf</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-过滤器</title>
      <link href="posts/f43dca37/"/>
      <url>posts/f43dca37/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>不管是过滤器，lookup，query，with_xxx，很多都是获取我们想要的信息。</p><h4 id="过滤器">过滤器</h4><blockquote><p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html">https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html</a></p></blockquote><blockquote><p>处理变量值，从而获取想要的信息.</p><p>过滤器本身是 jinja2 或者 ansible 官方定义的</p></blockquote><h4 id="简例">简例</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    vara: abcde    varb: [1,2,3,A,b,C,d]    varc: 123    vard: [ 1,2,3,[4,5,6],4,5 ]  tasks:    - name: show upper      debug:        msg: &quot;&#123;&#123; vara | upper&#125;&#125;&quot;</code></pre><blockquote><p>简例中的 upper 即是过滤器，它可以将 vara 中的所有字母元素大写，最终输出 ABCDE</p></blockquote><h4 id="常用的过滤器">常用的过滤器</h4><pre><code class="language-yaml">#将字符串开头和结尾的空格去除msg: &quot;&#123;&#123; vara | trim &#125;&#125;&quot;#返回字符串或列表长度,length与count等效,可以写为countmsg: &quot;&#123;&#123; varb | length &#125;&#125;&quot;# 绝对值msg: &quot;&#123;&#123; varc | abs &#125;&#125;&quot;# 排序(降序排序)msg: &quot;&#123;&#123; varb | sort(reverse=true) &#125;&#125;&quot;# 将列表中第一层嵌套列表元素展开并入列表中,并取出新列表中的最大元素msg: &quot;&#123;&#123; vard | flatten(levels=1) | max &#125;&#125;&quot;# 随机返回一个元素msg: &quot;&#123;&#123; varb | random &#125;&#125;&quot;# 去重msg: &quot;&#123;&#123; vard | unique &#125;&#125;&quot;# 并集msg: &quot;&#123;&#123; varb | union(vard) &#125;&#125;&quot;# 交集msg: &quot;&#123;&#123; varb | intersect(vard) &#125;&#125;&quot;# 补集，取出存在于 varb，但不存在于 vard 中的元素msg: &quot;&#123;&#123; varb | difference(vard) &#125;&#125;&quot;# 去除两个列表交集后的元素msg: &quot;&#123;&#123; varb | symmetric_difference(vard) &#125;&#125;&quot;# 变量未定义，返回默认值 newmsg: &quot;&#123;&#123; vare | default('new')&#125;&#125;&quot;# 变量未定义或者定义但为空，返回默认值 newmsg: &quot;&#123;&#123; vare | default('new', boolean=true)&#125;&#125;&quot;# 变量未定义时，忽略某个参数file: xxxx  mode=&#123;&#123; vare | default(omit)&#125;&#125;&quot;  # 若 vare 不存在，则忽略mode参数</code></pre><h4 id="json-query">json_query</h4><blockquote><p>获取特定数据</p></blockquote><pre><code>1. 查询字符串可用变量代替，增加可读性 loop: &quot;&#123;&#123; domain_definition | json_query(server_name_cluster1_query) &#125;&#125;&quot; vars:    server_name_cluster1_query: &quot;domain.server[?cluster=='cluster1'].port&quot;</code></pre><ol start="2"><li>查询条件</li></ol><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    users:    - name: zhangsan      gender: male    - name: lisi      gender: female  tasks:    - name: test 1      debug:        msg: &quot;&#123;&#123; users | json_query('[?name==`zhangsan`].gender') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;male&quot;    ]&#125;</code></pre><h4 id="map">map</h4><blockquote><p>映射</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    users:    - name: zhangsan      gender: male    - name: lisi      gender: female  tasks:    - name: test 1      debug:        msg: &quot;&#123;&#123; users|map(attribute='name') | list &#125;&#125;&quot;    - name: test 2      debug:        msg: &quot;&#123;&#123; users | json_query('[*].name') &#125;&#125;&quot;</code></pre><pre><code class="language-bash"># test 2 是采用 json_query 方式，test1和test2结果一样ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;zhangsan&quot;,         &quot;lisi&quot;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-变量</title>
      <link href="posts/4235ddf4/"/>
      <url>posts/4235ddf4/</url>
      
        <content type="html"><![CDATA[<h4 id="var-定义">var 定义</h4><blockquote><p>通过变量，修改playbook</p><p>可直接写入 playbook， 也可以写入文件，然后 playbook 通过 vars_files 引用</p><p>关键词:</p><ul><li>vars</li><li>vars_files # 一次性加载文件内部数据，不支持文件动态修改或添加新变量</li></ul></blockquote><h4 id="简例">简例</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  vars_files: ~/vars.yml  vars:    father: Zhang San  tasks:    - name: test vars      shell: echo &quot;&#123;&#123; father &#125;&#125; - &#123;&#123; children.son_name &#125;&#125; success&quot; &gt;&gt; ~/son.log</code></pre><pre><code class="language-yaml">children:  son_name: Zhang Xiaosan</code></pre><pre><code class="language-bash"># son.log 内容Zhang San - Zhang Xiaosan success</code></pre><h4 id="var-注册">var 注册</h4><blockquote><p>当我们想将某个任务的结果写入一个变量的时候，我们可以用register来进行注册</p><p>关键词:</p><ul><li>register</li><li>debug<ul><li>var 输出变量值</li><li>msg 输出字符串</li></ul></li></ul></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  tasks:    - name: test register      shell: echo &quot;register success&quot; &gt; ~/register.log      register: resultInfo    - name: show register result      debug:        msg: &quot;Oh my god&quot;        var: resultInfo</code></pre><h4 id="var-交互">var 交互</h4><blockquote><p>提供一个用户输入信息的机会，和 shell 里面的 read -p 一致。</p><p>关键词：</p><ul><li>vars_prompt</li></ul></blockquote><pre><code class="language-bash">---- hosts: localhost  remote_user: zyh  vars_prompt:    - name: &quot;fatherName&quot;      prompt: &quot;What's your father name&quot;      default: ZhangSan      private: no  tasks:    - name: show father name      shell: echo &quot;&#123;&#123; fatherName &#125;&#125;&quot; &gt; ~/prompt.log</code></pre><blockquote><p>private yes=隐藏输入内容 no=显示输入内容</p><p>default 默认值</p><p>encrypt “sha512_crypt” 将变量值加密，一般用于传递密码，比如传递给 user 模块的 password 参数</p></blockquote><h4 id="var-命令行传入">var 命令行传入</h4><blockquote><p>一般用于临时强制覆盖playbook中定义好的变量</p><p>关键词：</p><ul><li>-e 或者 --extra-vars<ul><li>参数后面，可以跟随多个变量kv对，每一个kv对用空格隔开</li><li>参数后面，@filePath 可以传入变量文件，文件中的变量均可以被引用</li></ul></li></ul></blockquote><pre><code class="language-bash">ansible-play test.play -e &quot;fatherName=Laowang&quot;</code></pre><h4 id="var-作用域">var 作用域</h4><table><thead><tr><th>创建方式</th><th>调用位置</th><th>作用域</th></tr></thead><tbody><tr><td>vars</td><td>play和tasks</td><td>当前play或者当前tasks，无法跨主机</td></tr><tr><td>set_fact</td><td>tasks</td><td>跨play，默认无法跨主机</td></tr><tr><td>register</td><td>tasks</td><td>跨play，默认无法跨主机</td></tr></tbody></table><p>若要使得 set_fact 和 register 跨主机使用，则需要引入内置变量 <code>hostvars</code> 例如 hostvars.&lt;主机名&gt;.&lt;变量名&gt;</p><blockquote><p>其它内置变量：</p><ul><li><p>ansible_version # 版本</p></li><li><p>hostvars # 存储play中的所有主机变量</p></li><li><p>play_hosts # 存储play中的所有主机名</p></li><li><p>inventory_hostname  # 存储当前主机名</p></li><li><p>inventory_hostname_short  # 存储当前主机名简称（其实就是获取主机名第一级，例如001.localhost，那么获取的就是001）</p></li><li><p>groups # 存储所有分组信息，包括all和ungrouped</p></li><li><p>group_names # 存储当前play中主机的所属组名</p></li><li><p>inventory_dir # 存储主机清单文件所在路径</p></li></ul></blockquote><h4 id="var-动态获取新变量">var 动态获取新变量</h4><blockquote><p>关键词：include_vars</p><p>用于任务重载变量文件，从而获取任务期间变量文件修改的数据</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars_files: ~/test.yaml  tasks:    - name: get varb - max      debug:        msg: &quot;&#123;&#123; varb |  max &#125;&#125;&quot;    - name: lineinfile      lineinfile:        regexp: ^varb        line: &quot;varb: [1,2,3,4]&quot;        path: ~/test.yaml    - include_vars: ~/test.yaml    - name: get varb - max again      debug:        msg: &quot;&#123;&#123; varb |  max &#125;&#125;&quot;</code></pre><pre><code class="language-bash">#### test.yaml 变量文件初始内容:vara: 123                                                                 varb: [1,2,3]  #### 最终结果：第一次 get varb 任务输出 3, 第二次 get varb 任务输出 4</code></pre><blockquote><p>include_vars 模块常用参数：</p><ul><li><p>file 读取某个变量文件</p></li><li><p>dir 读取某个目录的所有变量文件</p></li><li><p>depth 递归层深，仅在 dir 启用的时候有意义</p></li><li><p>files_matching 正则匹配文件名，仅在 dir 启用的时候有意义</p></li><li><p>ignore_files 忽略某个列表，列表中的元素可以为正则表达式</p></li><li><p>name: 变量 x  将读取的文件内容集中复制给变量 x，例如上例中变量 x 为 {vara: 123, varb: [1,2,3,4]}</p></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞输出个性化开机状态</title>
      <link href="posts/3988a5f2/"/>
      <url>posts/3988a5f2/</url>
      
        <content type="html"><![CDATA[<h4 id="前沿">前沿</h4><p>觉得默认的登陆不够给力，无法忽悠机器，用wower的话来说，就是先祖忽悠着你</p><h4 id="效果图在此">效果图在此</h4><p><img src="/posts/3988a5f2/image-20200515110044304.png" alt="image-20200515110044304"></p><h4 id="脚本在此">脚本在此</h4><blockquote><p>将脚本放置到 /etc/profile.d/status.sh</p></blockquote><pre><code class="language-bash">#!/bin/bash# Author: zyh# 需先安装 toilet 和 cowsay 命令# yum install epel-release -y# yum install https://rpmfind.net/linux/openmandriva/4.1/repository/x86_64/unsupported/release/toilet-0.2-3-omv4000.x86_64.rpm cowsay -yuser=$USERhome=$HOME## blue to echofunction blue()&#123;    echo -e &quot;\033[34m[Info] $1\033[0m&quot;    &#125;## green to echofunction green()&#123;    echo -e &quot;\033[32m[Success] $1\033[0m&quot;    &#125;## Errorfunction red()&#123;    echo -e &quot;\033[31m\033[01m[Error] $1\033[0m&quot;    &#125;# warningfunction yellow()&#123;    echo -e &quot;\033[33m\033[01m[Warn] $1\033[0m&quot;    &#125;## Error to warning with blinkfunction bred()&#123;    echo -e &quot;\033[31m\033[01m\033[05m[Error] $1\033[0m&quot;    &#125;# Error to warning with blinkfunction byellow()&#123;    echo -e &quot;\033[33m\033[01m\033[05m[Warn] $1\033[0m&quot;    &#125;publicip=`curl -s http://169.254.169.254/latest/meta-data/public-ipv4`localip=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`echo -e &quot;$publicip $localip&quot; | cowsay -f tux | toilet -f term  --gay# * Check if we're somewhere in /homeif [ ! -d $&#123;home&#125; ];then    return 0fi# * Calculate last loginlastlog=`lastlog -u $&#123;user&#125; | grep $&#123;user&#125; | awk '&#123;for(i=3;i&lt;=NF;++i) printf(&quot;%s &quot;,$i)&#125;'`# * Print Outputecho &quot; ::::::::::::::::::::::::::::::::::-STATUS-::::::::::::::::::::::::::::::::::&quot;#  * Check RAM Usagesfree_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemAvailable/&#123;free=$2&#125;END&#123;print free/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;' /proc/meminfo)app_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemFree/&#123;free=$2&#125;/Buffers/&#123;buffers=$2&#125;/^Cached/&#123;cached=$2&#125;END&#123;print (total-free-buffers-cached)/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;'  /proc/meminfo)all_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemFree/&#123;free=$2&#125;END&#123;print (total-free)/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;'  /proc/meminfo)blue &quot; Free Memory : $&#123;free_mem_usages&#125;&quot;blue &quot; Application Memory Usages : $&#123;app_mem_usages&#125;&quot;blue &quot; System Memory Usages : $&#123;all_mem_usages&#125;&quot;# * Check Disk Usagesdiskusages=$(df -PH | awk '&#123;printf &quot;%-40s%-15s%-15s%-15s%-15s%-15s\n&quot;, $1,$2,$3,$4,$5,$6&#125;')blue &quot; Disk Usages :&quot;echo &quot;$&#123;diskusages&#125;&quot; | toilet -f term --metal -w 200# * Check Load Averageloadaverage=$(top -n 1 -b | grep &quot;load average:&quot; | awk '&#123;print $(NF-2) $(NF-1) $NF&#125;')blue &quot; Load Average: $loadaverage&quot;</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>问题记录</title>
      <link href="posts/1ddb9d4e/"/>
      <url>posts/1ddb9d4e/</url>
      
        <content type="html"><![CDATA[<h3 id="Q-包管理器安装软件出现">Q:包管理器安装软件出现</h3><pre><code class="language-bash">insserv:  loop involving service xxx at depth 2</code></pre><h4 id="A-删除-xxx-服务，如果-xxx-服务已删除，清理-xxx-服务的启动脚本-etc-init-d-xxx">A:删除 xxx 服务，如果 xxx 服务已删除，清理 xxx 服务的启动脚本 /etc/init.d/xxx</h4><hr><h3 id="Q-crontab-如何修改时区">Q:crontab 如何修改时区</h3><h4 id="A-在crontab文件最上方添加命令，例如芝加哥时区">A:在crontab文件最上方添加命令，例如芝加哥时区</h4><pre><code class="language-bash">TZ='America/Chicago'                                          CRON_TZ='America/Chicago'  `</code></pre><hr><h3 id="Q-su-user-c-“command”-命令出错">Q:su - user -c “command” 命令出错</h3><h4 id="A-需要用户开启登陆权限，即-etc-passwd-中不能使-sbin-nologin">A:需要用户开启登陆权限，即 /etc/passwd 中不能使 /sbin/nologin</h4><hr><h3 id="Q-zabbix-自动发现异常，表面看不出问题">Q:zabbix 自动发现异常，表面看不出问题</h3><h4 id="A-zabbix-agent-端开启-debug-模式，配置加入参数-DebugLevel-4">A:zabbix-agent 端开启 debug 模式，配置加入参数 DebugLevel=4</h4><hr><h3 id="Q-adminer-无效的CSRF令牌-Invalid-CSRF-token">Q:adminer: 无效的CSRF令牌(Invalid CSRF token)</h3><h4 id="A-nginx-worker-user-用户无法访问-php-session-目录">A:nginx worker user 用户无法访问 php session 目录.</h4><pre><code class="language-bash">chgrp $&#123;nginxWorkerUser&#125; $&#123;phpSessionDir&#125;# 如果是yum或者apt安装,那么php的session一般是 /var/lib/php/session</code></pre><hr><h3 id="Q-python3-No-module-named-‘PIL’">Q:python3 : No module named ‘PIL’</h3><h4 id="A-pip3-install-pillow">A:pip3 install pillow</h4><hr><h3 id="Q-python-workon-命令找不到">Q:python: workon 命令找不到</h3><h4 id="A">A:</h4><pre><code class="language-bash:">pip install virtualenvwrapperecho 'export PATH=~/.local/bin:$PATH' &gt;&gt; ~/.bashrc # 根据所用shell来决定文件路径</code></pre><hr><h3 id="Q-jira配置163的smtp连接超时-但是服务器终端telnet正常">Q:jira配置163的smtp连接超时,但是服务器终端telnet正常</h3><h4 id="A-2">A:</h4><p><img src="/posts/1ddb9d4e/image-20200917111208620.png" alt="image-20200917111208620"></p><h3 id="Q-tomcat-添加-pid-文件">Q: tomcat 添加 pid 文件</h3><h4 id="A-catalina-sh-文件的-PRGDIR-dirname-PRG-下面添加新行-CATALINA-PID-PRGDIR-tomcat-pid-此时将在-bin-目录下创建-tomcat-pid">A:  <a href="http://catalina.sh">catalina.sh</a> 文件的 <code>PRGDIR=dirname &quot;$PRG&quot;</code>下面添加新行 <code>CATALINA_PID=$PRGDIR/tomcat.pid</code> 此时将在 bin/ 目录下创建 tomcat.pid</h4><h3 id="Q：vim中文乱码">Q：vim中文乱码</h3><h4 id="A：-etc-vimrc-中添加">A：/etc/vimrc 中添加</h4><pre><code class="language-bash">set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936set termencoding=utf-8set encoding=utf-8</code></pre><h3 id="Q-mysql-导入本地文件报错：ERROR-3948-42000-at-line-1-Loading-local-data-is-disabled-this-must-be-enabled-on-both-the-client-and-server-sides">Q: mysql 导入本地文件报错：ERROR 3948 (42000) at line 1: Loading local data is disabled; this must be enabled on both the client and server sides</h3><h4 id="A-3">A:</h4><pre><code class="language-bash">服务器执行：set global local_infile=1客户端加载：mysql --local-infile=0 --load-data-local-dir=/my/local/data</code></pre><h3 id="Q-EC2服务器外网不通，内网正常，替换EIP不行">Q: EC2服务器外网不通，内网正常，替换EIP不行</h3><h4 id="A：检查服务器默认路由是否还存在。">A：检查服务器默认路由是否还存在。</h4><pre><code class="language-bash">route -nroute add default gw &lt;gw_ip&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 问题记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-tags</title>
      <link href="posts/960fc783/"/>
      <url>posts/960fc783/</url>
      
        <content type="html"><![CDATA[<h4 id="tags的定义">tags的定义</h4><blockquote><p>tags 可以让你在执行playbook的时候，有选择地执行某些任务，因此 tags 是 tasks 下的关键词</p></blockquote><pre><code class="language-bash">ansible-playbook test.play &lt;--tags-args&gt;</code></pre><h4 id="tags的参数">tags的参数</h4><blockquote><ul><li>–tags=tag_name  执行具有 tag_name 任务</li><li>–skip-tags=tag_name 忽略具有 tag_name 任务</li><li>–list-tags 输出所有</li></ul></blockquote><blockquote><p>tag_name 内置值 ：</p><ul><li>tagged 有tag的task，表示执行具有标记的任务</li><li>untagged 没有tag的task，表示执行不具有标记的任务</li><li>all 所有task，表示执行所有任务</li></ul></blockquote><h4 id="tags的内置标记">tags的内置标记</h4><blockquote><ul><li>always 总是执行某个 task</li><li>never 永远不执行某个 task</li></ul></blockquote><h4 id="tags-的位置">tags 的位置</h4><blockquote><p>位于play或者tasks都可以，本身具有继承属性，也就是tasks里的tags会继承play的tags</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  tags: father  tasks:    - name: test tag son      tags: son,children      shell: echo &quot;son is here!&quot; &gt; ~/son.log    - name: test tag daughter      tags: daughter,children      shell: echo &quot;daughter is here!&quot; &gt; ~/daughter.log</code></pre><pre><code class="language-bash">ansible-playbook test.play --tags=son # 只会生成 son.logansible-playbook test.play --tags=father  # 因继承机制，会生成 son.log 和 daughter.logansible-playbook test.play --tags=children # 因都含有，同样会生成 son.log 和 daughter.log</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-错误捕获</title>
      <link href="posts/c4126d5a/"/>
      <url>posts/c4126d5a/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>类似于 python 中的 try…except…finally，ansible 可以用 block…rescue…always</p><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - block:        - shell: mkdir /file      rescue:        - debug:            msg: &quot;No operation permission&quot;      always:        - debug:            msg: &quot;Task End!&quot;</code></pre><blockquote><p>创建 file 目录失败，则输出&quot;No operation permission&quot;, 最终总是输出“Task End!”</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-条件判断</title>
      <link href="posts/29683589/"/>
      <url>posts/29683589/</url>
      
        <content type="html"><![CDATA[<h4 id="关键词">关键词</h4><ul><li>when</li></ul><h4 id="运算符">运算符</h4><ul><li>==  !=  &gt;  &lt;  &gt;=  &lt;=</li><li>and or not</li><li>( ) 组合，例如 ( a and b ) or c</li></ul><blockquote><p>ansible 某个 task 报错，会导致任务终止，而ignore_errors: true 可以忽略某个任务的条件不满足</p></blockquote><h4 id="is-语句-或者-is-not-语句">is 语句 或者 is not 语句</h4><pre><code class="language-yaml">tasks:  - name: show is xxx    debug:      msg: &quot;xxx is ok&quot;    when: var is xxx</code></pre><blockquote><p>判断文件</p><ul><li>xxx 是 exists ，表示若 var 存在，条件为真</li><li>xxx 是 file, 表示若 var 是文件，条件为真</li><li>xxx 是 directory， 表示若 var 是目录，条件为真</li><li>xxx 是 link，表示若 var 是软连接，条件为真</li><li>xxx 是 mount，表示若 var 是挂载点，条件为真</li></ul></blockquote><blockquote><p>判断变量</p><ul><li>若 xxx 是 defined, 表示若 var 已定义，条件为真</li><li>若 xxx 是 undefined， 表示若 var 未定义，条件为真</li><li>若 xxx 是 none， 表示若 var 是空，条件为真</li></ul></blockquote><blockquote><p>判断任务状态</p><ul><li>若 xxx 是 success， 若 var 为某任务返回结果，则任务状态成功，条件为真</li><li>若 xxx 是 failure， 若 var 为某任务返回结果，则任务状态失败，条件为真</li><li>若 xxx 是 change，若 var 为某任务返回结果，则任务状态改变，条件为真</li><li>若 xxx 是 skip， 若 var 为某任务返回结果，则任务被忽略，条件为真</li></ul></blockquote><blockquote><p>判断字符串</p><ul><li><p>若 xxx 是 string，若 var 是字符串，条件为真</p></li><li><p>若 xxx 是 lower，若 var 是纯小写，条件为真</p></li><li><p>若 xxx 是 upper，若 var 是纯大写，条件为真</p></li></ul></blockquote><blockquote><p>判断数字</p><ul><li><p>若 xxx 是 number， 若 var 是数字，条件为真。 var: “123” ,这里 var 是字符串，不是数字</p></li><li><p>若 xxx 是 even，若 var 是偶数，条件为真</p></li><li><p>若 xxx 是 odd， 若 var 是奇数，条件为真</p></li><li><p>若 xxx 是 divisibleby(num), 若 var 可以被 num 整除，条件为真</p></li></ul></blockquote><blockquote><p>判断集合</p><ul><li>若 xxx 是 subset(list)，若 var 是 list 的子集，条件为真</li><li>若 xxx 是 superset(list), 若 var 是 list 的父集，条件为真</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-循环</title>
      <link href="posts/14bc184e/"/>
      <url>posts/14bc184e/</url>
      
        <content type="html"><![CDATA[<h4 id="常见的循环">常见的循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show debug info      debug:        msg: &quot;&#123;&#123; item &#125;&#125;&quot;      with_items:        - [ a,b ]        - [ A,B, [ D,E,F ]]</code></pre><blockquote><ul><li><p>with_list 输出最表层元素，在简例中，会输出 [ a,b ] 和 [ A,B, [ D,E,F ] ]</p></li><li><p>with_item 递归输出所有层元素</p></li><li><p>with_together 合并两个列表，元素按照对应下标结合，如果某一方列表元素缺失，则用null代替</p></li><li><p>with_indexed_items 最表层所有列表合并为一个新列表并循环。item由{ list.index: list.value } 构成。在简例中，新列表是[ a,b,A,B, [ D,E,F ]]</p><pre><code class="language-yaml"></code></pre></li></ul><p>msg: “Index:&#123;&#123; item.0 &#125;&#125;, Value:&#123;&#123; item.1 &#125;&#125;”</p><pre><code>```bashok: [localhost] =&gt; (item=[0, u'a']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:0, Vaule:a&quot;&#125;ok: [localhost] =&gt; (item=[1, u'b']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:1, Vaule:b&quot;&#125;ok: [localhost] =&gt; (item=[2, u'A']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:2, Vaule:A&quot;&#125;ok: [localhost] =&gt; (item=[3, u'B']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:3, Vaule:B&quot;&#125;ok: [localhost] =&gt; (item=[4, [u'D', u'E', u'F']]) =&gt; &#123;    &quot;msg&quot;: &quot;Index:4, Vaule:[u'D', u'E', u'F']&quot;&#125;</code></pre><ul><li>with_random_choice 随机输出一个最表层列表元素，简例中输出 [a,b] 或者 [ A,B, [ D,E,F ]]</li></ul></blockquote><h4 id="dict-字典循环">dict 字典循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show debug info      debug:        msg: &quot;Name:&#123;&#123; item.key &#125;&#125;, gender:&#123;&#123; item.value &#125;&#125;&quot;      with_dict:        Zhangsan: male        Lisi: female</code></pre><blockquote><p>输出所有字典</p></blockquote><h4 id="sequence-序列循环">sequence 序列循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show sequence info      debug:        msg: &quot;&#123;&#123; item &#125;&#125;&quot;      with_sequence:        start=1        end=5        stride=2        format=&quot;I'm %0.4f&quot;</code></pre><blockquote><ul><li>with_sequence 获取奇偶数，start和end是起止点，stride 是步长（步长可以为负值），format是格式化</li></ul></blockquote><h4 id="nested-嵌套循环">nested 嵌套循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show nested info      debug:        msg: &quot;mkdir /mnt/&#123;&#123; item.0 &#125;&#125;/&#123;&#123; item.1 &#125;&#125;&quot;      with_nested:        - [ a,b ]        - [ A,B,C ]</code></pre><blockquote><p>两个列表做笛卡尔积, 例如构建环境目录</p></blockquote><h4 id="subelements-子元素循环">subelements 子元素循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                    age: 18                    content:                            - eating                            - sleeping                            - play ogre                  - name: Maris                    gender: female                    age: 20                    content:                            - eating                            - sleeping                            - shopping  tasks:          - name: test vars 2            debug:                    msg: &quot;&#123;&#123; item.0.name &#125;&#125; - &#123;&#123; item.0.gender &#125;&#125; - &#123;&#123; item.1 &#125;&#125;&quot;            with_subelements:              - &quot;&#123;&#123; users &#125;&#125;&quot;              - content</code></pre><blockquote><p>分解 , 选中列表内某一个列表元素 content, 与作为一个临时整体的剩余元素构建笛卡尔积，形成 item</p></blockquote><h4 id="file-文件循环">file 文件循环</h4><pre><code class="language-yaml">with_file:  /mnt/a.ini  /mnt/b.ini</code></pre><blockquote><p>始终循环获取ansible主机里文件的内容。（与目标主机无关）</p></blockquote><h4 id="fileglob-寻找通配符匹配的文件">fileglob 寻找通配符匹配的文件</h4><pre><code class="language-yaml">with_fileglob:  - /home/zyh/test/dirA/*  - /home/zyh/test/dirB/[0-9].ini</code></pre><blockquote><p>始终循环获取ansible主机指定目录中匹配的文件名和路径。（与目标主机无关）</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞管理静态文件</title>
      <link href="posts/6a4df400/"/>
      <url>posts/6a4df400/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://docs.djangoproject.com/zh-hans/3.1/howto/static-files/">管理静态文件（比如图片、JavaScript、CSS） | Django 文档 | Django (djangoproject.com)</a></p><p><a href="https://docs.djangoproject.com/zh-hans/3.1/ref/views/">内置视图 | Django 文档 | Django (djangoproject.com)</a></p><h2 id="通用设置">通用设置</h2><p>项目.settings.py中</p><pre><code class="language-python">STATIC_URL = '/static/'if DEBUG:    STATICFILES_DIRS = [        BASE_DIR / &quot;static&quot;,    ]else:    STATIC_ROOT = BASE_DIR / &quot;static&quot;MEDIA_URL = '/media/'MEDIA_ROOT = BASE_DIR / &quot;media</code></pre><h2 id="DEBUG开启">DEBUG开启</h2><p>无需其它改动</p><h2 id="DEBUG关闭">DEBUG关闭</h2><p>项目.urls.py中</p><pre><code class="language-python">from django.views.static import servefrom django.urls import path, include, re_pathfrom . import settingsif not settings.DEBUG:    urlpatterns += [        re_path(r'^media/(?P&lt;path&gt;.*)$', serve, &#123;            'document_root': settings.MEDIA_ROOT,        &#125;),        re_path(r'^static/(?P&lt;path&gt;.*)$', serve, &#123;            'document_root': settings.STATIC_ROOT,        &#125;),    ]</code></pre><h2 id="部署静态文件">部署静态文件</h2><pre><code class="language-bash">python manage.py collectstatic --noinput</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞记忆点</title>
      <link href="posts/3f8b5448/"/>
      <url>posts/3f8b5448/</url>
      
        <content type="html"><![CDATA[<h2 id="git文件">.git文件</h2><pre><code class="language-bash"># Django #*.log*.pot*.pyc__pycache__mediastatic# Backup files #*.bak# other.vscode.gitdblog</code></pre><h2 id="Dockerfile">Dockerfile</h2><pre><code class="language-bash">FROM python:3.8-alpineLABEL maintainer=&quot;aaa103439@hotmail.com&quot;WORKDIR /app COPY . /appRUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories &amp;&amp; \    apk update &amp;&amp; \    apk add gcc musl-dev jpeg-dev zlib-dev freetype-dev lcms2-dev openjpeg-dev tiff-dev tk-dev tcl-dev libffi-dev openssl-dev &amp;&amp; \    pip install --upgrade pip &amp;&amp; pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/CMD [&quot;sh&quot;,&quot;/app/start.sh&quot;]</code></pre><h2 id="生成-settings-SECRET-KEY">生成 settings.SECRET_KEY</h2><pre><code class="language-python">#!/usr/bin/env python# coding=utf-8from django.core.management.utils import  get_random_secret_keyprint(&quot;务必妥善保管好此密码&quot;)print(get_random_secret_key())</code></pre><h2 id="调用-fontawesome-free-图标库">调用 fontawesome-free 图标库</h2><pre><code class="language-bash">pip install fontawesome-free</code></pre><p><code>settings.INSTALLED_APPS</code> 中加入<code>fontawesome-free</code></p><h2 id="外键">外键</h2><p>外键，顾名思义不是自己的字段，而是引用其它表的字段。</p><blockquote><p>外键不是通过字段名定义的，而是通过【外键约束】定义</p></blockquote><p>外键可以实现一对多、多对多和一对一的关系。</p><p>外键既可以通过数据库来约束，也可以不设置约束，仅依靠应用程序的逻辑来保证。</p><p>学生表【student】有主键【sid】，成绩表也有主键【sid】, 学生表和成绩表是1对1的关系</p><p><img src="/posts/3f8b5448/image-20210716114155366.png" alt="image-20210716114155366"></p><p>如果没有【外键约束】的规则建立，则两个表在数据库看来是没有关系的，即：要保持两个表的关系，则需要应用程序去保证逻辑正确，也就是说，应用程序不能插入相同学号的数据到表里。</p><p>若应用程序不想去维持，则就需要数据库【外键约束】去维持。</p><p>在没有django的时候，你需要直接写入SQL语句去实现【外键约束】，例如</p><pre><code class="language-sql">ALTER TABLE scoresADD CONSTRAINT fk_student_sidFOREIGN KEY (student_sid)REFERENCES student (sid);</code></pre><p>django创建外键约束的方式如下，成绩表的【sid】字段定义如下</p><pre><code class="language-python">sid = models.OneToOneField('student', null=True, on_delete=models.CASCADE)</code></pre><h3 id="一对一-models-OneToOneField">一对一 models.OneToOneField()</h3><p>常用于“扩展”另一个模型的主键，就如同上述例子，扩展了学生的额外属性【成绩】</p><h3 id="一对多-models-ForeignKey">一对多 models.ForeignKey()</h3><p>常用于A包含B，外键定义在B表中，引用A表的主键</p><h3 id="多对多-ManyToManyField">多对多 ManyToManyField()</h3><p>你中有我，我中有你，一个班有多个老师，一个老师可以教多个班</p><h3 id="查询">查询</h3><h3 id="on-delete">on_delete</h3><p>字面意思，当删除的时候，应该怎么做</p><p><code>CASCADE</code>：删除</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-系统相关</title>
      <link href="posts/84eb900/"/>
      <url>posts/84eb900/</url>
      
        <content type="html"><![CDATA[<h4 id="cron">cron</h4><blockquote><p>crontab 计划任务</p><p>参数介绍：</p><p>name 计划任务注释，多次操作同名任务，只会修改，而不会新加</p><p>时间参数：</p><ul><li><p>minute hour day month weekday</p></li><li><p>special_time : @reboot @yearly @monthly @weekly @daily @hourly (每xxx执行)</p></li></ul><p>user 添加到指定用户计划任务中</p><p>job 计划任务执行命令</p><p>state 当值为absent时，指删除任务. 只需指定 name.</p><p>disabled 注释任务，若任务信息和之前不一致，会同时修改任务</p><p>backup 先备份再操作, 备份文件位于 /tmp/crontabxxxx</p></blockquote><pre><code class="language-bash">ansible localhost -m cron -a &quot;name='test cron module' user=zyh special_time=hourly job='ls /home/zyh &gt; /home/zyh/cron.log 2&gt;&amp;1'&quot;ansible localhost -m cron -a &quot;name='test cron module' state=absent&quot;</code></pre><h4 id="service">service</h4><blockquote><p>调用远程系统自身的服务管理模块，例如 centos6 的 service ，或者 centos7 的 systemctl</p><p>参数介绍:</p><p>name 服务名</p><p>state 执行动作 started, stopped, restarted, reloaded</p><p>enabled 开机自启动</p></blockquote><h4 id="user">user</h4><blockquote><p>用户管理</p><p>常用参数介绍：</p><p>name 用户名</p><p>group 用户组 groups 用户附加组</p><ul><li>append 额外附加用户附加组</li></ul><p>shell 指定默认shell，比如/usr/sbin/nologin</p><p>state 值为 absent 表示删除用户，值为 present 表示用户必须存在</p><ul><li>remove 删除用户时，同时删除用户家目录</li></ul><p>password 用户密码。（需要传递加密密码，不能是明文密码）</p><pre><code class="language-python">import crypt:passwd=print(crypt.crypt(passwd))</code></pre><p>generate_ssh_key 相当于远程执行 ssh-keygen 命令（不加任何参数，一路回车）。若已经存在~/.ssh/{id_rsa, id_rsa.pub}, 则不执行</p><ul><li>ssh_key_file 自定义私钥名和私钥存放路径, 公钥也会在自定义路径下生成</li></ul></blockquote><h4 id="group">group</h4><blockquote><p>管理用户组</p><p>参数介绍：</p><p>name 组名</p><p>state 组状态, 值为 absent 指删除(组本身并非用户主要组)</p></blockquote><h4 id="setup">setup</h4><blockquote><p>获取机器信息</p><p>参数介绍：</p><p>gather_subset 获取某个子集（all, min, hardware, network, virtual, ohai, facter）</p><p>filter 获取某个集合的某个key</p><p>fact_path 自定义信息存放目录</p></blockquote><pre><code class="language-ini"># setup 默认会搜索目标主机/etc/ansible/facts.d 下的自定义信息,例如 family.ini[family]father=Zhangsanson=Zhangxiaosan</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-handle</title>
      <link href="posts/b0929188/"/>
      <url>posts/b0929188/</url>
      
        <content type="html"><![CDATA[<h4 id="功能">功能</h4><p>用一个短路判断来说，就是两者是串联关系，handlers 用来处理任务后续</p><p>tasks &amp;&amp; handlers</p><p>tasks &amp;&amp; handlers - listen （handlers 组）</p><h4 id="handlers-的-playbook-样本">handlers 的 playbook 样本</h4><pre><code class="language-yaml">---- hosts: localhost  become: true  remote_user: root  tasks:  - name: check nginx    shell: /usr/sbin/nginx -t    notify:      log        - meta: flush_handlers     handlers:  - name: log    shell: echo &quot;nginx check success&quot; &gt; ~/playbook.log</code></pre><blockquote><p>多个tasks的时候，tasks后面的 <code>meta: flush_handlers</code> 可以让tasks执行完，立马执行关联的handlers。否则handlers会在所有tasks执行完后，才开始执行</p></blockquote><h4 id="handlers-listen-的-playbook-样本">handlers-listen 的 playbook 样本</h4><pre><code class="language-yaml">---- hosts: localhost  become: true  remote_user: root  tasks:  - name: check nginx    shell: /usr/sbin/nginx -t    notify:      log group    handlers:  - name: log1    listen: log group    shell: echo &quot;nginx check success 1&quot; &gt; ~/playbook.log  - name: log2    listen: log group    shell: echo &quot;nginx check success 2&quot; &gt;&gt; ~/playbook.log</code></pre><blockquote><p>handlers 通过 listen 绑定在一起， tasks 关联 liasten 绑定 handlers 组，最终 playbook.log 将会写入两行信息</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> handle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-软件包管理</title>
      <link href="posts/3d9b9ec1/"/>
      <url>posts/3d9b9ec1/</url>
      
        <content type="html"><![CDATA[<h4 id="apt-repository">apt_repository</h4><blockquote><p>ubuntu 下：</p><p>repo 指定库地址，例如 nginx 地址 ppa:nginx/stable</p><p>state 值为 absent 时为删除</p></blockquote><pre><code class="language-bash">ansible localhost -m apt_repository -a &quot;repo=ppa:nginx/stable&quot; -b --ask-become-pass</code></pre><pre><code class="language-bash">localhost | CHANGED =&gt; &#123;    &quot;changed&quot;: true,     &quot;repo&quot;: &quot;ppa:nginx/stable&quot;,     &quot;state&quot;: &quot;present&quot;&#125;#( 04/24/20@ 3:10PM )( zyh@zyh ):~   cat  /etc/apt/sources.list.d/ppa_nginx_stable_bionic.listdeb http://ppa.launchpad.net/nginx/stable/ubuntu bionic main</code></pre><h4 id="apt">apt</h4><blockquote><p>常用参数：</p><p>name 包名</p><p>state 包状态 （absent-删除，latest-最新包,  present-默认安装） latest 相当于升级包</p><p>upgrade 升级 （yes，dist，full，no-默认）</p><p><a href="https://docs.ansible.com/ansible/latest/modules/apt_module.html#apt-module">https://docs.ansible.com/ansible/latest/modules/apt_module.html#apt-module</a></p></blockquote><pre><code class="language-bash">ansible localhost -m apt -a &quot;name=nginx state=present&quot; -b --ask-become-pass</code></pre><pre><code class="language-bash">localhost | CHANGED =&gt; &#123;    &quot;cache_update_time&quot;: 1587713114,     &quot;cache_updated&quot;: false,     &quot;changed&quot;: true,     &quot;stderr&quot;: &quot;&quot;,     &quot;stderr_lines&quot;: [],     &quot;stdout&quot;: &quot;Reading package lists...\nBuilding dependency tree.......</code></pre><h4 id="yum-repository">yum_repository</h4><blockquote><p>name 仓库名</p><p>baseurl 仓库地址</p><p>enabled （yes-默认，no)</p><p>gpgcheck (yes, no)</p><p>gpgcakey 指定 gpg ca 公钥</p><p>state (present-默认，absent-删除)</p></blockquote><pre><code class="language-bash">ansible localhost -m yum_repository -a &quot;name=epel baseurl=https://download.fedoraproject.org/pub/epel/$releasever/$basearch/&quot;</code></pre><h4 id="yum">yum</h4><blockquote><p>name 包名</p><p>state (absent-删除，present-安装-默认值，latest-更新)</p><p>disable_gpg_check 关闭gpg检查（用于源gpg检查没有的情况）</p><p>enablerepo 安装包的时候，先临时启用某个源</p><p>disablerepo 安装包的时候，先临时禁用某个源</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-集成SonarQube</title>
      <link href="posts/b7391a97/"/>
      <url>posts/b7391a97/</url>
      
        <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>sonarqube 自动代码审查工具，检查代码错误、漏洞等。</p><p>可以与现有的工作流程集成，实现跨项目分支和拉取请求的持续代码检查。</p><p><img src="/posts/b7391a97/image-20220508163208673.png" alt="image-20220508163208673"></p><p><img src="/posts/b7391a97/image-20220508162806575.png" alt="image-20220508162806575"></p><p>上述描述中几个组件的关系图：</p><p><img src="/posts/b7391a97/image-20220508163300942.png" alt="image-20220508163300942"></p><h2 id="硬件准备">硬件准备</h2><p><img src="/posts/b7391a97/image-20220508155911358.png" alt="image-20220508155911358"></p><h2 id="软件配置">软件配置</h2><p><img src="/posts/b7391a97/image-20220508155947747.png" alt="image-20220508155947747"></p><h2 id="安装-sonarqube">安装 sonarqube</h2><p><a href="https://www.sonarqube.org/downloads/">https://www.sonarqube.org/downloads/</a></p><h3 id="基于docker方式">基于docker方式</h3><p>不过这种方式下数据库默认是个非生产数据库</p><pre><code class="language-bash">## 创建数据目录mkdir -p /data/cicd/sonarqube/&#123;sonarqube_conf,sonarqube_extensions,sonarqube_logs,sonarqube_data&#125;chmod 777 -R /data/cicd/sonarqube/## 运行docker run  -itd  --name sonarqube \    -p 9000:9000 \    -v /data/cicd/sonarqube/sonarqube_conf:/opt/sonarqube/conf \    -v /data/cicd/sonarqube/sonarqube_extensions:/opt/sonarqube/extensions \    -v /data/cicd/sonarqube/sonarqube_logs:/opt/sonarqube/logs \    -v /data/cicd/sonarqube/sonarqube_data:/opt/sonarqube/data \    sonarqube:7.9.6-community## 验证docker logs -f sonarqube## 将上一个步骤容器里生成的lib目录复制到宿主机mkdir -p /data/cicd/sonarqube/sonarqube_libcd  /data/cicd/sonarqube/sonarqube_libdocker cp sonarqube:/opt/sonarqube/lib/* ./docker run  -itd  --name sonarqube \    -p 9000:9000 \    -v /data/cicd/sonarqube/sonarqube_conf:/opt/sonarqube/conf \    -v /data/cicd/sonarqube/sonarqube_extensions:/opt/sonarqube/extensions \    -v /data/cicd/sonarqube/sonarqube_logs:/opt/sonarqube/logs \    -v /data/cicd/sonarqube/sonarqube_data:/opt/sonarqube/data \    -v /data/cicd/sonarqube/sonarqube_lib:/opt/sonarqube/lib \    sonarqube:7.9.6-community</code></pre><p>访问地址： <a href="http://ip:9000">http://ip:9000</a></p><p>账户密码：admin/admin</p><h3 id="基于二进制安装方式">基于二进制安装方式</h3><p><img src="/posts/b7391a97/image-20220508160136785.png" alt="image-20220508160136785"></p><h2 id="安装-sonarqube-中文插件">安装 sonarqube 中文插件</h2><p><img src="/posts/b7391a97/image-20220508170131292.png" alt="image-20220508170131292"></p><p>如果自动安装不上，可能需要手动安装</p><p><a href="https://www.yuque.com/zeyangli/wmaaq7/pcgq4k">https://www.yuque.com/zeyangli/wmaaq7/pcgq4k</a></p><p>选择对应的版本：</p><p><a href="https://github.com/xuhuisheng/sonar-l10n-zh/tree/sonar-l10n-zh-plugin-8.8#the-chinese-translation-pack-for-sonarqube">https://github.com/xuhuisheng/sonar-l10n-zh/tree/sonar-l10n-zh-plugin-8.8#the-chinese-translation-pack-for-sonarqube</a></p><p>下载对应的版本：</p><p><a href="https://github.com/xuhuisheng/sonar-l10n-zh/releases/tag/sonar-l10n-zh-plugin-1.29">https://github.com/xuhuisheng/sonar-l10n-zh/releases/tag/sonar-l10n-zh-plugin-1.29</a></p><pre><code class="language-bash">cd /data/cicd/sonarqube/sonarqube_extensions/downloadswget https://github.com/xuhuisheng/sonar-l10n-zh/releases/download/sonar-l10n-zh-plugin-1.29/sonar-l10n-zh-plugin-1.29.jarchmod +x sonar-l10n-zh-plugin-1.29.jardocker restart sonarqube</code></pre><h2 id="配置-sonarqube">配置 sonarqube</h2><p>开启强制认证</p><p><img src="/posts/b7391a97/image-20220508160310946.png" alt="image-20220508160310946"></p><h2 id="安装-sonar-scanner-到-jenkins-slave">安装 sonar-scanner 到 jenkins slave</h2><p><a href="https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner">https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner</a></p><h3 id="命令行方式安装">命令行方式安装</h3><pre><code class="language-bash">$ wget https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-4.6.0.2311-linux.zip$ tar zxf sonar-scanner-xxxx.tar.gz -C /usr/local$ vim /etc/profileexport SCANNER_HOME=/usr/local/sonar-scanner-4.6.0.2311-linuxexport PATH=$PATH:$SCANNER_HOME/bin$ source /etc/profile $ sonar-scanner -vINFO: Scanner configuration file: /usr/local/sonar-scanner-4.6.0.2311-linux/conf/sonar-scanner.propertiesINFO: Project root configuration file: NONEINFO: SonarScanner 4.6.0.2311INFO: Java 11.0.3 AdoptOpenJDK (64-bit)INFO: Linux 4.18.0-80.el8.x86_64 amd64</code></pre><h3 id="docker-方式安装">docker 方式安装</h3><pre><code class="language-bash">docker run \    --rm \    -e SONAR_HOST_URL=&quot;http://$&#123;SONARQUBE_URL&#125;&quot; \    -e SONAR_LOGIN=&quot;myAuthenticationToken&quot; \    -v &quot;$&#123;YOUR_REPO&#125;:/usr/src&quot; \    sonarsource/sonar-scanner-cli </code></pre><h2 id="sonar-scanner-的使用">sonar-scanner 的使用</h2><p>两种方式：</p><ul><li>项目根路径下加载配置文件</li><li>直接命令行指定项目</li></ul><p>加载配置文件方式：sonar-project.properties</p><pre><code class="language-bash"># 定义唯一的关键字sonar.projectKey=devops-hello-service# 定义项目名称sonar.projectName=My project# 定义项目的版本信息sonar.projectVersion=1.0 # 指定扫描代码的目录位置（多个逗号分隔）sonar.sources=. # 执行项目编码sonar.sourceEncoding=UTF-8# sonarqube 的地址、账户、密码sonar.host.url=sonar.loginsonar.password</code></pre><p>命令行方式：</p><pre><code class="language-bash"># 指定配置文件sonar-scanner -Dproject.settings=myproject.properties# 命令行传参sonar-scanner -Dsonar.projectKey=myproject -Dsonar.sources=src1</code></pre><h3 id="扫java项目">扫java项目</h3><p>👙sonarqube 先安装java语言规则插件：Java code quality and security</p><p>sonar-scanner 扫描java项目会使用一些文件：</p><ul><li>会用到target目录下的classes</li></ul><p><img src="/posts/b7391a97/image-20220508173652730.png" alt="image-20220508173652730"></p><p><a href="http://sonar.links.ci">sonar.links.ci</a> = 项目对应的 jenkins 地址</p><p>sonar.links.homepage = 项目对应的 git 地址，对应扫描后的项目报告里的项目主页</p><pre><code class="language-bash">sonar-scanner -Dsonar.host.url=http://192.168.1.200:9000 \-Dsonar.projectKey=devops-maven-service \-Dsonar.projectName=devops-maven-service \-Dsonar.projectVersion=1.0 \-Dsonar.login=admin \-Dsonar.password=admin \-Dsonar.ws.timeout=30 \-Dsonar.projectDescription=&quot;my first project!&quot; \-Dsonar.links.homepage=http://192.168.1.200/devops/devops-maven-service \-Dsonar.links.ci=http://192.168.1.200:8080/job/demo-pipeline-service/ \-Dsonar.sources=src \-Dsonar.sourceEncoding=UTF-8 \-Dsonar.java.binaries=target/classes \-Dsonar.java.test.binaries=target/test-classes \-Dsonar.java.surefire.report=target/surefire-reports</code></pre><h3 id="扫前端项目">扫前端项目</h3><p>插件：SonarJS</p><pre><code class="language-bash">sonar-scanner -Dsonar.projectKey=devops-web-service \-Dsonar.projectName=devops-web-service \-Dsonar.projectVersion=1.0 \-Dsonar.sources=src \-Dsonar.host.url=http://192.168.1.200:9000 \-Dsonar.login=admin \-Dsonar.password=admin \-Dsonar.sourceEncoding=UTF-8</code></pre><pre><code class="language-bash">#sonar.sources=dist/static/js</code></pre><h3 id="扫golang项目">扫golang项目</h3><p>插件：SonarGo</p><pre><code class="language-bash">sonar-scanner -Dsonar.projectKey=devops-golang-service \-Dsonar.projectName=devops-golang-service \-Dsonar.sources=src \-Dsonar.login=admin \-Dsonar.password=admin \-Dsonar.host.url=http://192.168.1.200:9000## 有测试用例的情况sonar.exclusions=**/*_test.gosonar.tests=.sonar.test.inclusions=**/*_test.go</code></pre><h2 id="登录方式集成">登录方式集成</h2><h3 id="ldap-集成">ldap 集成</h3><p>👙截图里配置要写入到 sonarqube 的主配置文件里</p><p><img src="/posts/b7391a97/image-20220508160503633.png" alt="image-20220508160503633"></p><h3 id="gitlab-集成">gitlab 集成</h3><p>安装 gitlab auth 插件并重启</p><p>gitlab 创建应用</p><p>👙redirect uri 的格式必须是截图里的格式，主地址是 sonarqube 地址。</p><p><img src="/posts/b7391a97/image-20220508161323615.png" alt="image-20220508161323615"></p><p>sonarqube 配置 gitlab auth 插件，填写上面创建的 gitlab 应用信息</p><p><img src="/posts/b7391a97/image-20220508161250818.png" alt="image-20220508161250818"></p><h2 id="正式集成到jenkins">正式集成到jenkins</h2><p>安装 jenkins 插件： SonarQube Scanner，这个插件有以下优点：</p><ul><li>在 jenkinslib 里的 sonar-scanner 就无需指定 sonarqube 的服务器地址、登录信息了。</li><li>在 jenkins 工程里将会提供 sonarqube 的项目地址。</li></ul><p>配置  SonarQube Scanner 插件：</p><ul><li>创建 SonarQube 管理员令牌，并添加 Jenkins 凭据 secret text 类型.</li></ul><p><img src="/posts/b7391a97/image-20220508185130593.png" alt="image-20220508185130593"></p><ul><li>jenkins web 里配置 sonarqube 地址</li></ul><p><img src="/posts/b7391a97/image-20220508185237920.png" alt="image-20220508185237920"></p><p>👙下面两个文件和文章：jenkins-集成gitlab 里的代码并不太一致。</p><p>Jenkinslib：<a href="https://github.com/Spinestars/jenkins/blob/master/src/org/devops/sonarqube.groovy">https://github.com/Spinestars/jenkins/blob/master/src/org/devops/sonarqube.groovy</a></p><p>Jenkinsfile：<a href="https://github.com/Spinestars/jenkins/blob/master/ci.jenkinsfile">https://github.com/Spinestars/jenkins/blob/master/ci.jenkinsfile</a></p><h2 id="sonarqube-规则定义">sonarqube 规则定义</h2><p><img src="/posts/b7391a97/image-20220508191140263.png" alt="image-20220508191140263"></p><h2 id="sonarqube-质量阈">sonarqube 质量阈</h2><p>超过质量阈，sonarqube 项目会报错。</p><p><img src="/posts/b7391a97/image-20220508191231710.png" alt="image-20220508191231710"></p><h3 id="超过质量阈退出流水线">超过质量阈退出流水线</h3><p>Jenkinslib：<a href="https://github.com/Spinestars/jenkins/blob/master/src/org/devops/sonarqube.groovy">https://github.com/Spinestars/jenkins/blob/master/src/org/devops/sonarqube.groovy</a></p><p>质量阈接口的返回json。上述代码用到了这个接口返回值。</p><p><img src="/posts/b7391a97/image-20220508205946591.png" alt="image-20220508205946591"></p><p>涉及的代码如下：</p><pre><code class="language-bash">    //获取扫描结果, 如果超出质量阈，则执行 error 关键词，退出流水线。    result = GetProjectStatus(projectName)    println(result)    if (result.toString() == &quot;ERROR&quot;)&#123;        mytools.EmailUser(userEmail,&quot;代码质量阈错误&quot;)        error &quot; 代码质量阈错误！请及时修复！&quot;    &#125; else &#123;        println(result)    &#125;        ...    def HttpReq(reqType,reqUrl,reqBody)&#123;    def sonarServer = &quot;http://192.168.1.200:9000/api&quot;       response = httpRequest authentication: '4675830a-4330-4dd6-9185-cf62161967f0',            httpMode: reqType,             contentType: &quot;APPLICATION_JSON&quot;,            consoleLogResponseBody: true,            ignoreSslErrors: true,             requestBody: reqBody,            url: &quot;$&#123;sonarServer&#125;/$&#123;reqUrl&#125;&quot;            //quiet: true        return response&#125;//获取Sonar质量阈状态// 这个接口从 sonarqube 项目详情页里通过F12就可以看到，是一个get请求,并返回json字符串，包含质量阈的结果def GetProjectStatus(projectName)&#123;    apiUrl = &quot;project_branches/list?project=$&#123;projectName&#125;&quot;    response = HttpReq(&quot;GET&quot;,apiUrl,'')        response = readJSON text: &quot;&quot;&quot;$&#123;response.content&#125;&quot;&quot;&quot;    result = response[&quot;branches&quot;][0][&quot;status&quot;][&quot;qualityGateStatus&quot;]        //println(response)       return result&#125;</code></pre><h2 id="自动绑定规则和质量阈到项目">自动绑定规则和质量阈到项目</h2><p>Api地址：http://<sonarqube>/web_api/api/</sonarqube></p><p><img src="/posts/b7391a97/image-20220508212018844.png" alt="image-20220508212018844"></p><p>Jenkinslib：<a href="https://github.com/Spinestars/jenkins/blob/master/src/org/devops/sonarqube.groovy">https://github.com/Spinestars/jenkins/blob/master/src/org/devops/sonarqube.groovy</a></p><h3 id="查找项目">查找项目</h3><p>接口返回数据，通过判断 total 是否为 0，来判断项目是否存在。</p><p><img src="/posts/b7391a97/image-20220508212753835.png" alt="image-20220508212753835"></p><p>涉及代码：</p><pre><code class="language-bash">    //搜索项目， 返回项目是否存在的结果    result = SerarchProject(projectName)    println(result)        //判断项目是否存在    if (result == &quot;false&quot;)&#123;        println(&quot;$&#123;projectName&#125;---项目不存在,准备创建项目---&gt; $&#123;projectName&#125;！&quot;)        CreateProject(projectName)    &#125; else &#123;        println(&quot;$&#123;projectName&#125;---项目已存在！&quot;)    &#125;    ...//搜索Sonar项目def SerarchProject(projectName)&#123;    apiUrl = &quot;projects/search?projects=$&#123;projectName&#125;&quot;    response = HttpReq(&quot;GET&quot;,apiUrl,'')    response = readJSON text: &quot;&quot;&quot;$&#123;response.content&#125;&quot;&quot;&quot;    result = response[&quot;paging&quot;][&quot;total&quot;]    if(result.toString() == &quot;0&quot;)&#123;       return &quot;false&quot;    &#125; else &#123;       return &quot;true&quot;    &#125;&#125;</code></pre><h3 id="新建项目">新建项目</h3><pre><code class="language-bash">//创建Sonar项目def CreateProject(projectName)&#123;    apiUrl =  &quot;projects/create?name=$&#123;projectName&#125;&amp;project=$&#123;projectName&#125;&quot;    response = HttpReq(&quot;POST&quot;,apiUrl,'')    println(response)&#125;</code></pre><h3 id="更新语言规则集">更新语言规则集</h3><p>👙需要先在 sonarqube 服务上定义好质量规则</p><pre><code class="language-bash">    // 绑定项目质量规则,从项目名里获取质量规则名.    // 项目名规则： 质量规则名-项目名    qpName=projectName.split(&quot;-&quot;)[0]   //anyops    //qpName=&quot;myjava&quot;    if (projectType == &quot;npm&quot;)&#123;        ConfigQualityProfiles(projectName,&quot;js&quot;,qpName)        ConfigQualityProfiles(projectName,&quot;ts&quot;,qpName)    &#125; else if (projectType == &quot;maven&quot;) &#123;        ConfigQualityProfiles(projectName,&quot;java&quot;,qpName)    &#125;    ...//绑定项目质量规则def ConfigQualityProfiles(projectName,lang,qpname)&#123;    apiUrl = &quot;qualityprofiles/add_project?language=$&#123;lang&#125;&amp;project=$&#123;projectName&#125;&amp;qualityProfile=$&#123;qpname&#125;&quot;    response = HttpReq(&quot;POST&quot;,apiUrl,'')    println(response)&#125;</code></pre><h3 id="获取质量阈ID">获取质量阈ID</h3><p>要绑定质量阈到项目，需要先获取质量阈ID</p><p>其API地址是：http://<sonarqube>/api/qualitygates/show?name=&lt;质量阈规则名&gt;</sonarqube></p><p>json数据</p><p><img src="/posts/b7391a97/image-20220508224048527.png" alt="image-20220508224048527"></p><p>代码如下：</p><pre><code class="language-bash">//获取质量阈IDdef GetQualtyGateId(gateName)&#123;    apiUrl= &quot;qualitygates/show?name=$&#123;gateName&#125;&quot;    response = HttpReq(&quot;GET&quot;,apiUrl,'')    response = readJSON text: &quot;&quot;&quot;$&#123;response.content&#125;&quot;&quot;&quot;    result = response[&quot;id&quot;]        return result&#125;</code></pre><h3 id="绑定质量阈到项目">绑定质量阈到项目</h3><pre><code class="language-bash">    //配置质量阈    ConfigQualityGates(projectName,qpName)    ...//配置项目质量阈def ConfigQualityGates(projectName,gateName)&#123;    gateId = GetQualtyGateId(gateName)    apiUrl = &quot;qualitygates/select?gateId=$&#123;gateId&#125;&amp;projectKey=$&#123;projectName&#125;&quot;    response = HttpReq(&quot;POST&quot;,apiUrl,'')    println(response)println(response)&#125;</code></pre><h2 id="扫描结果输出到-gitlab-的-commit-上">扫描结果输出到 gitlab 的 commit 上</h2><p>仅支持 sonarqube 6.7 版本，通过在 sonar-scanner 添加额外的命令行参数启用。</p><p>教程链接：<a href="https://youdianzhishi.com/web/course/1013/1290">https://youdianzhishi.com/web/course/1013/1290</a></p><h2 id="配置-sonarqube-多分支模式的支撑">配置 sonarqube 多分支模式的支撑</h2><p>默认社区版 sonarqube 不支持多分支项目。不过这个插件本身作者是不保证持续维护的。</p><p>两种方式使用这个多分支插件。</p><p>第一种，使用作者打包好的 docker 镜像：<a href="https://hub.docker.com/r/mc1arke/sonarqube-with-community-branch-plugin">https://hub.docker.com/r/mc1arke/sonarqube-with-community-branch-plugin</a></p><p>第二种，在社区版里添加插件</p><p>插件地址：<a href="https://github.com/mc1arke/sonarqube-community-branch-plugin">https://github.com/mc1arke/sonarqube-community-branch-plugin</a></p><p>插件安装方法：</p><ol><li>Copy the plugin JAR file to the <code>extensions/plugins/</code> directory of your SonarQube instance</li><li>Add <code>-javaagent:./extensions/plugins/sonarqube-community-branch-plugin-$&#123;version&#125;.jar=web</code> to the <code>sonar.web.javaAdditionalOpts</code> property in your Sonarqube installation’s <code>config/sonar.properties</code> file, e.g. <code>sonar.web.javaAdditionalOpts=-javaagent:./extensions/plugins/sonarqube-community-branch-plugin-1.8.0.jar=web</code></li><li>Add <code>-javaagent:./extensions/plugins/sonarqube-community-branch-plugin-$&#123;version&#125;.jar=ce</code> to the <code>sonar.ce.javaAdditionalOpts</code> property in your Sonarqube installation’s <code>config/sonar.properties</code> file, e.g. <code>sonar.ce.javaAdditionalOpts=-javaagent:./extensions/plugins/sonarqube-community-branch-plugin-1.8.0.jar=ce</code></li><li>Start Sonarqube, and accept the warning about using third-party plugins</li></ol><p>扫描的时候加上额外的参数：<code>-Dsonar.branch.name = branch_name (e.g master)</code></p><p>最终效果：</p><p><img src="/posts/b7391a97/image-20220508231335349.png" alt="image-20220508231335349"></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
            <tag> sonarqube </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-集成gitlab</title>
      <link href="posts/2a5275e1/"/>
      <url>posts/2a5275e1/</url>
      
        <content type="html"><![CDATA[<h2 id="简要">简要</h2><p>jenkins集成gitlab有两种方式：</p><ul><li>通过 jenkins 自带的通用 webhook 触发器。</li><li>通过 gitlab 或者 gitlab-branch-source 之类的插件。</li></ul><p>本文档介绍通用 webhook 触发器，更加灵活。</p><ul><li>可以自定义 webhook 地址的请求参数，用于限定 webhook 的 get 参数。即仅当 webhook 包含这个参数的时候，才会触发构建。</li><li>可以接收 gitlab 通过 webhook 地址传递过来的 body（json数据），并且可以从 json 数据中任意的获取数据，例如分支名等</li><li>可以自定义 webhook 的 token</li></ul><h2 id="安装通用-webhook-插件">安装通用 webhook 插件</h2><p><a href="https://plugins.jenkins.io/generic-webhook-trigger/">Generic Webhook Trigger</a></p><h2 id="代码-demo">代码 demo</h2><p>将demo放在 <a href="http://git.xxx.com/devops/simple-java-maven-app.git">http://git.xxx.com/devops/simple-java-maven-app.git</a></p><pre><code class="language-bash">git clone https://github.com/Spinestars/simple-java-maven-appcd simple-java-maven-appgit remote add origin http://git.xxx.com/devops/simple-java-maven-app.gitgit add .git commit -m &quot;Initial commit&quot;git push -u origin master</code></pre><h2 id="maven工具">maven工具</h2><p>参见文章：jenkins-构建工具集成</p><h2 id="共享库-jenkinslib">共享库 jenkinslib</h2><p><a href="http://git.xxx.com/devops/jenkinslib.git">http://git.xxx.com/devops/jenkinslib.git</a></p><p>jenkinslib在jenkins的配置，参见文章：jenkins☞003共享库</p><p>src/org/devops/build.groovy</p><p>👙此代码的执行一定需要先配置 jenkins maven 全局工具</p><pre><code class="language-bash">package org.devopsdef Build(buildType, buildShell)&#123;def buildTools = [&quot;mvn&quot;:&quot;M2&quot;, &quot;npm&quot;:&quot;NPM&quot;]println(&quot;当前选择构建类型：$&#123;buildType&#125;&quot;)buildHome = tool buildTools[buildType]if (&quot;$&#123;buildType&#125;&quot; == &quot;npm&quot;)&#123;sh &quot;&quot;&quot;export NODE_HOME=$&#123;buildHome&#125;export PATH=\$NODE_HOME/bin:\$PATH$&#123;buildHome&#125;/bin/$&#123;buildType&#125; $&#123;buildShell&#125;&quot;&quot;&quot;&#125;else&#123;sh &quot;$&#123;buildHome&#125;/bin/$&#123;buildType&#125; $&#123;buildShell&#125;&quot;&#125;&#125;</code></pre><p>src/org/devops/gitlab.groovy</p><p>👙jenkins-gitlab-secretText-token 是标题：创建jenkins访问gitlab的凭据 的内容。</p><pre><code class="language-bash">package org.devops// 封装HTTP,用于jenkins向gitlab发起请求def HttpReq(reqType, reqUrl,reqBody )&#123;    def gitServer = &quot;http://git.xxx.com/api/v4&quot;    withCredentials([string(credentialsId: 'jenkins-gitlab-secretText-token', variable: 'GITLABTOKEN')]) &#123;        response = httpRequest acceptType: 'APPLICATION_JSON_UTF8',                           consoleLogResponseBody: true,                           contentType: 'APPLICATION_JSON_UTF8',                           customHeaders: [[maskValue: false, name: 'PRIVATE-TOKEN', value: &quot;$&#123;GITLABTOKEN&#125;&quot;]],                           httpMode: &quot;$&#123;reqType&#125;&quot;,                           url: &quot;$&#123;gitServer&#125;/$&#123;reqUrl&#125;&quot;,                           wrapAsMultipart: false,                          requestBody: &quot;$&#123;reqBody&#125;&quot;    &#125;    return response&#125;// 变更 gitlab 提交状态def ChangeCommitStatus(projectId,commitSha,status)&#123;    commitApi = &quot;projects/$&#123;projectId&#125;/statuses/$&#123;commitSha&#125;?state=$&#123;status&#125;&quot;    response = HttpReq('POST', commitApi, '')    println(response)    return response&#125;</code></pre><p>src/org/devops/tools.groovy</p><pre><code class="language-bash">package org.devopsdef toEmail(userEmail,status)&#123; emailext body: &quot;&quot;&quot;            &lt;!DOCTYPE html&gt;             &lt;html&gt;             &lt;head&gt;             &lt;meta charset=&quot;UTF-8&quot;&gt;             &lt;/head&gt;             &lt;body leftmargin=&quot;8&quot; marginwidth=&quot;0&quot; topmargin=&quot;8&quot; marginheight=&quot;4&quot; offset=&quot;0&quot;&gt;                 &lt;img src=&quot;http://&lt;jenkins&gt;/static/0eef74bf/images/headshot.png&quot;&gt;                &lt;table width=&quot;95%&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif&quot;&gt;                       &lt;tr&gt;                         &lt;td&gt;&lt;br /&gt;                             &lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建信息&lt;/font&gt;&lt;/b&gt;                         &lt;/td&gt;                     &lt;/tr&gt;                     &lt;tr&gt;                         &lt;td&gt;                             &lt;ul&gt;                                 &lt;li&gt;项目名称：$&#123;JOB_NAME&#125;&lt;/li&gt;                                         &lt;li&gt;构建编号：$&#123;BUILD_ID&#125;&lt;/li&gt;                                 &lt;li&gt;Git用户：$&#123;userFullName&#125;($&#123;userName&#125;)&lt;/li&gt;                                &lt;li&gt;Git分支：$&#123;branch&#125;&lt;/li&gt;                                &lt;li&gt;CommitShortID：$&#123;commitShort&#125;&lt;/li&gt;                                 &lt;li&gt;CommitID：$&#123;commitSha&#125;&lt;/li&gt;                                &lt;li&gt;构建状态: $&#123;status&#125;&lt;/li&gt;                                                         &lt;li&gt;项目地址：&lt;a href=&quot;$&#123;BUILD_URL&#125;&quot;&gt;$&#123;BUILD_URL&#125;&lt;/a&gt;&lt;/li&gt;                                    &lt;li&gt;构建日志：&lt;a href=&quot;$&#123;BUILD_URL&#125;console&quot;&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt;                             &lt;/ul&gt;                         &lt;/td&gt;                     &lt;/tr&gt;                     &lt;tr&gt;                  &lt;/table&gt;             &lt;/body&gt;             &lt;/html&gt;  &quot;&quot;&quot;,            subject: &quot;Jenkins-$&#123;JOB_NAME&#125;项目构建信息 &quot;,            to: userEmail&#125;</code></pre><h2 id="Pipeline文件-jenkinsfile">Pipeline文件 jenkinsfile</h2><p>定义一个 jenkinsfile 库，存 jenkinsfile 文件。</p><p><a href="http://git.xxx.com/devops/jenkinsfiles.git">http://git.xxx.com/devops/jenkinsfiles.git</a></p><pre><code class="language-bash">#!groovy@Library('jenkinslib@master') _def build = new org.devops.build()def gitlab = new org.devops.gitlab()def tools = new org.devops.tools()//def deploy = new org.devops.deploy()String buildType = &quot;$&#123;env.buildType&#125;&quot;String buildShell = &quot;$&#123;env.buildShell&#125;&quot;//String deployHosts = &quot;$&#123;env.deployHosts&#125;&quot;String gitUrl = &quot;$&#123;env.gitUrl&#125;&quot;String branchName = &quot;$&#123;env.branchName&#125;&quot;// 当存在 runOpts 变量并且值等于 GitlabPush 的时候，则分支名从 webhook 数据中获取。if (&quot;$&#123;runOpts&#125;&quot; == &quot;GitlabPush&quot;) &#123;   branchName = branch - 'refs/heads/'env.commitShort = commitSha.take(8)currentBuild.description = &quot;&quot;&quot;Git用户: $&#123;userFullName&#125;($&#123;userName&#125;)\nGit分支：$&#123;branchName&#125;\nCommit: $&#123;commitShort&#125;&quot;&quot;&quot;gitlab.ChangeCommitStatus(projectId, commitSha, &quot;running&quot;)&#125;pipeline&#123;    agent &#123; node &#123; label &quot;agent01&quot; &#125; &#125;    stages &#123;        stage(&quot;checkout&quot;) &#123;            steps &#123;                script &#123;                    checkout([$class: 'GitSCM', branches: [[name: &quot;$&#123;branchName&#125;&quot;]], userRemoteConfigs: [[credentialsId: 'gitlab-jenkins-pull-token', url: &quot;$&#123;giturl&#125;&quot;]]])                &#125;            &#125;        &#125;        stage(&quot;build&quot;) &#123;            steps &#123;                script &#123;                    build.Build(buildType, buildShell)                &#125;            &#125;        &#125;    &#125;        post &#123;    always &#123;    script &#123;    println('always')    &#125;    &#125;        success &#123;    script &#123;    println('success')gitlab.ChangeCommitStatus(projectId, commitSha, &quot;success&quot;)tools.toEmail(userEmail, &quot;success&quot;)    &#125;    &#125;        failure &#123;    script &#123;    println('failure')gitlab.ChangeCommitStatus(projectId, commitSha, &quot;failed&quot;)tools.toEmail(userEmail, &quot;failed&quot;)    &#125;    &#125;        aborted &#123;    script &#123;    println('aborted')gitlab.ChangeCommitStatus(projectId, commitSha, &quot;canceld&quot;)tools.toEmail(userEmail, &quot;aborted&quot;)    &#125;    &#125;    &#125;&#125;</code></pre><h2 id="jenkins-web-配置">jenkins web 配置</h2><p>如果一切配置好，则 jenkins 构建的时候会输出 gitlab 通过 webhook 传递过来的信息，如下示例：</p><pre><code class="language-json">&#123;&quot;object_kind&quot;:&quot;push&quot;,&quot;event_name&quot;:&quot;push&quot;,&quot;before&quot;:&quot;332e5cfeff94b9dec3d766955c533d2ab7e62bf4&quot;,&quot;after&quot;:&quot;39cf0cd49ff6f156e76a171b6b8ab85ce512a8e5&quot;,&quot;ref&quot;:&quot;refs/heads/master&quot;,&quot;checkout_sha&quot;:&quot;39cf0cd49ff6f156e76a171b6b8ab85ce512a8e5&quot;,&quot;message&quot;:null,&quot;user_id&quot;:1,&quot;user_name&quot;:&quot;zyh&quot;,&quot;user_username&quot;:&quot;root&quot;,&quot;user_email&quot;:&quot;&quot;,&quot;user_avatar&quot;:&quot;http://git.xxx.com/uploads/-/system/user/avatar/1/avatar.png&quot;,&quot;project_id&quot;:68,&quot;project&quot;:&#123;&quot;id&quot;:68,&quot;name&quot;:&quot;simple-java-maven-app&quot;,&quot;description&quot;:&quot;&quot;,&quot;web_url&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app&quot;,&quot;avatar_url&quot;:null,&quot;git_ssh_url&quot;:&quot;ssh://git@git.xxx.com:2222/devops/simple-java-maven-app.git&quot;,&quot;git_http_url&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app.git&quot;,&quot;namespace&quot;:&quot;devops&quot;,&quot;visibility_level&quot;:0,&quot;path_with_namespace&quot;:&quot;devops/simple-java-maven-app&quot;,&quot;default_branch&quot;:&quot;master&quot;,&quot;ci_config_path&quot;:&quot;&quot;,&quot;homepage&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app&quot;,&quot;url&quot;:&quot;ssh://git@git.xxx.com:2222/devops/simple-java-maven-app.git&quot;,&quot;ssh_url&quot;:&quot;ssh://git@git.xxx.com:2222/devops/simple-java-maven-app.git&quot;,&quot;http_url&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app.git&quot;&#125;,&quot;commits&quot;:[&#123;&quot;id&quot;:&quot;39cf0cd49ff6f156e76a171b6b8ab85ce512a8e5&quot;,&quot;message&quot;:&quot;Update pom.xml&quot;,&quot;title&quot;:&quot;Update pom.xml&quot;,&quot;timestamp&quot;:&quot;2022-05-08T01:37:58+00:00&quot;,&quot;url&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app/-/commit/39cf0cd49ff6f156e76a171b6b8ab85ce512a8e5&quot;,&quot;author&quot;:&#123;&quot;name&quot;:&quot;zyh&quot;,&quot;email&quot;:&quot;zhangyanghua@pengwin.com&quot;&#125;,&quot;added&quot;:[],&quot;modified&quot;:[&quot;pom.xml&quot;],&quot;removed&quot;:[]&#125;,&#123;&quot;id&quot;:&quot;2bf69c80b1713021be07700c4598db4884de8e1a&quot;,&quot;message&quot;:&quot;Update pom.xml&quot;,&quot;title&quot;:&quot;Update pom.xml&quot;,&quot;timestamp&quot;:&quot;2022-05-08T01:36:22+00:00&quot;,&quot;url&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app/-/commit/2bf69c80b1713021be07700c4598db4884de8e1a&quot;,&quot;author&quot;:&#123;&quot;name&quot;:&quot;zyh&quot;,&quot;email&quot;:&quot;zhangyanghua@pengwin.com&quot;&#125;,&quot;added&quot;:[],&quot;modified&quot;:[&quot;pom.xml&quot;],&quot;removed&quot;:[]&#125;,&#123;&quot;id&quot;:&quot;332e5cfeff94b9dec3d766955c533d2ab7e62bf4&quot;,&quot;message&quot;:&quot;Update README.md&quot;,&quot;title&quot;:&quot;Update README.md&quot;,&quot;timestamp&quot;:&quot;2020-09-06T18:16:16+08:00&quot;,&quot;url&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app/-/commit/332e5cfeff94b9dec3d766955c533d2ab7e62bf4&quot;,&quot;author&quot;:&#123;&quot;name&quot;:&quot;lizeyang&quot;,&quot;email&quot;:&quot;610556220@qq.com&quot;&#125;,&quot;added&quot;:[],&quot;modified&quot;:[&quot;README.md&quot;],&quot;removed&quot;:[]&#125;],&quot;total_commits_count&quot;:3,&quot;push_options&quot;:&#123;&#125;,&quot;repository&quot;:&#123;&quot;name&quot;:&quot;simple-java-maven-app&quot;,&quot;url&quot;:&quot;ssh://git@git.xxx.com:2222/devops/simple-java-maven-app.git&quot;,&quot;description&quot;:&quot;&quot;,&quot;homepage&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app&quot;,&quot;git_http_url&quot;:&quot;http://git.xxx.com/devops/simple-java-maven-app.git&quot;,&quot;git_ssh_url&quot;:&quot;ssh://git@git.xxx.com:2222/devops/simple-java-maven-app.git&quot;,&quot;visibility_level&quot;:0&#125;&#125;</code></pre><p>👙通过 <a href="http://json.cn">json.cn</a> 格式化。</p><p>通过示例，我们可以在 jenkins webhook 通用触发器里配置变量来获取所需的数据，例如本次 git 提交人、本次 git commit等</p><p>同时还会输出 jenkins webhook 触发器用户配置的一些参数或者变量，示例：</p><pre><code class="language-bash">Contributing variables:    branch = refs/heads/dev    runOpts = GitlabPush    runOpts_0 = GitlabPush</code></pre><p>branch 变量就是用户自定义的获取 gitlab 通过 webhook 传递过来的 json 数据的变量。</p><p>runOpts 变量就是用户自定义的 webhook get 参数。</p><h3 id="webhook-通用触发器配置">webhook 通用触发器配置</h3><p>通过pipeline方式设置 webhook 通用触发器配置。</p><p><img src="/posts/2a5275e1/image-20220509113201663.png" alt="image-20220509113201663"></p><p>通过 jenkins webui 设置通用触发器配置。</p><h4 id="设置webhook传递过来的post内容的映射变量">设置webhook传递过来的post内容的映射变量</h4><p>具体的设置可以通过上述的 post content 示例来对照配置。</p><p>需要设置的变量有：</p><ul><li>branch=$.ref</li><li>userFullName=$.user_name</li><li>userName=$.user_username</li><li>userEmail=$.user_email   # 这个user_email 取得是gitlab用户的公共邮箱（默认gitlab不会将账户注册邮箱传递到这个公共邮箱，需要用户自己在web用户配置里设置）</li><li><a href="http://projectId=$.project.id">projectId=$.project.id</a></li><li>commitSha=$.checkout_sha</li><li>before=$.before</li><li>after=$.after</li><li>object_kind=$.object_kind</li></ul><p><img src="/posts/2a5275e1/image-20220508101641308.png" alt="image-20220508101641308"></p><h4 id="设置webhook-get参数和token">设置webhook get参数和token</h4><p>token可以随便写</p><p><img src="/posts/2a5275e1/image-20220508101703780.png" alt="image-20220508101703780"></p><h4 id="开启各种控制台日志输出">开启各种控制台日志输出</h4><p><img src="/posts/2a5275e1/image-20220508101721454.png" alt="image-20220508101721454"></p><p>经过上述配置，最终生成的 webhook 地址是：</p><p>http://<Jenkins>/generic-webhook-trigger/invoke?token=test.simple-java-maven-app&amp;runOpts=GitlabPush</Jenkins></p><h3 id="checkout-方式获取Jenkinsfile">checkout 方式获取Jenkinsfile</h3><p>👙需要先配置 jenkins 拉取 gitlab 代码的凭证</p><p>凭证类型：username with password</p><p>凭证id：jenkins-gitlab-pull-code</p><p><img src="/posts/2a5275e1/image-20220508115409236.png" alt="image-20220508115409236"></p><h2 id="gitlab-web-配置">gitlab web 配置</h2><p>将webhook地址填入网址</p><p><img src="/posts/2a5275e1/image-20220508102131452.png" alt="image-20220508102131452"></p><h2 id="增加构建描述信息">增加构建描述信息</h2><p>Jenkins构建工程中对应的一些变量：</p><p><a href="http://jkspj.pengwin.com:18080/job/test.simple-java-maven-app/pipeline-syntax/globals#currentBuild">http://jkspj.pengwin.com:18080/job/test.simple-java-maven-app/pipeline-syntax/globals#currentBuild</a></p><p>我们可以给这些变量赋值，从而在jenkins pipeline或者jenkins web上展示一些自定义信息。</p><p>例如：</p><ul><li>currentBuild.description 变量，这个变量里的值会显示在构建列表里，如图所示</li></ul><p><img src="/posts/2a5275e1/image-20220508115119649.png" alt="image-20220508115119649"></p><p>👙在上面的jenkinsfile已经包含了上面截图的效果.</p><pre><code class="language-bash">if (&quot;$&#123;runOpts&#125;&quot; == &quot;GitlabPush&quot;) &#123;       branchName = branch - 'refs/heads/'commitShort = commitSha.take(8)    currentBuild.description = &quot;&quot;&quot;Git用户: $&#123;userFullName&#125;($&#123;userName&#125;)\nGit分支：$&#123;branchName&#125;\nCommit: $&#123;commitShort&#125;&quot;&quot;&quot;&#125;</code></pre><h2 id="将构建状态反馈给gitlab">将构建状态反馈给gitlab</h2><p>如果你用 gitlab 插件，则这个实现不难，但如果用通用 webhook 触发器，则这个实现是需要调用 gitlab api 接口。</p><p>相关的 gitlab api 接口文档：</p><p><a href="https://docs.gitlab.com/ee/api/commits.html#post-the-build-status-to-a-commit">https://docs.gitlab.com/ee/api/commits.html#post-the-build-status-to-a-commit</a></p><p>官方示例：</p><pre><code class="language-bash">curl --request POST --header &quot;PRIVATE-TOKEN: &lt;your_access_token&gt;&quot; &quot;https://gitlab.example.com/api/v4/projects/17/statuses/18f3e63d05582537db6d183d9d557be09e1f90c8?state=success&quot;</code></pre><p>gitlab build 状态：<code>pending</code>, <code>running</code>, <code>success</code>, <code>failed</code>, <code>canceled</code></p><p>👙可以看到 gitlab 的状态单词和 jenkins 的状态单词是不一样的，因此jenkins pipeline里调用 gitlab api 传递的状态要用 gitlab 的状态单词。</p><h3 id="创建jenkins访问gitlab的凭据">创建jenkins访问gitlab的凭据</h3><ol><li>创建 gitlab 专属用户：jenkins</li><li>将所有需要自动化的仓库授予 jenkins，权限为 Maintainer</li><li>用 jenkins 用户登录 gitlab 网站，并创建个人令牌，令牌权限是 api</li><li>登录 jenkins 网站，创建凭据：<ul><li>凭据类型：secret text</li><li>凭据ID：jenkins-gitlab-secretText-token</li></ul></li></ol><h2 id="过滤-gitlab-push-请求">过滤 gitlab push 请求</h2><p>默认新建分支都会触发jenkins构建。</p><p>通用webhook触发器，可以进行如下设定：</p><ul><li>当给定的正则可以匹配给定的字符串时，则此次构建不执行。</li></ul><p>👙给定的字符串，从 post json 中获取，这样，正则就可以过滤推送的事件。</p><p>官方提供的例子：<a href="https://github.com/jenkinsci/generic-webhook-trigger-plugin/blob/master/src/test/resources/org/jenkinsci/plugins/gwt/bdd/gitlab/gitlab-push-ignore-create-remove-branch.feature">https://github.com/jenkinsci/generic-webhook-trigger-plugin/blob/master/src/test/resources/org/jenkinsci/plugins/gwt/bdd/gitlab/gitlab-push-ignore-create-remove-branch.feature</a></p><pre><code class="language-bash">    Given the following generic variables are configured:      | variable        | expression               | expressionType  | defaultValue | regexpFilter  |      | object_kind     | $.object_kind            | JSONPath        |              |               |      | before          | $.before                 | JSONPath        |              |               |      | after           | $.after                  | JSONPath        |              |               |      | ref             | $.ref                    | JSONPath        |              |               |      | git_ssh_url     | $.repository.git_ssh_url | JSONPath        |              |               |    Given filter is configured with text: $object_kind $before $after    Given filter is configured with expression: ^push\s(?!0&#123;40&#125;).&#123;40&#125;\s(?!0&#123;40&#125;).&#123;40&#125;$</code></pre><p>第一组是根据 post json 设置的变量</p><p>第二组是根据变量定义的字符串</p><p>第三组是匹配字符串的正则</p><p>对应的 jenkins web 配置截图：</p><p><img src="/posts/2a5275e1/image-20220508141037178.png" alt="image-20220508141037178"></p><h2 id="配置邮件发送">配置邮件发送</h2><p>jenkins web 配置 smtp 服务器，见截图:</p><p><img src="/posts/2a5275e1/image-20220508152447885.png" alt="image-20220508152447885"></p><p>邮件模板和发邮件的代码，参见标题：jenkinslib.src/org/devops/tools.groovy</p><p>pipeline调用，参见标题： jenkinsfile.post()部分</p><h2 id="合并分支触发流水线">合并分支触发流水线</h2><p>👙依赖标题：将构建状态反馈给gitlab。</p><p>只有当源分支最后一次流水线成功的时候，才允许合并到目标分支。</p><p>实现上述需求，只需要开启gitlab的功能即可：</p><p><img src="/posts/2a5275e1/image-20220508154034888.png" alt="image-20220508154034888"></p><p>效果如下：</p><p><img src="/posts/2a5275e1/image-20220508155409563.png" alt="image-20220508155409563"></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-构建工具集成</title>
      <link href="posts/67a8b607/"/>
      <url>posts/67a8b607/</url>
      
        <content type="html"><![CDATA[<h2 id="简要">简要</h2><p>所有的构建工具，都应该安装在 jenkins 工作节点上。</p><h2 id="maven-java打包工具">maven(java打包工具)</h2><p>jenkins 工作节点安装</p><blockquote><p><a href="https://maven.apache.org/download.cgi">https://maven.apache.org/download.cgi</a></p></blockquote><pre><code class="language-bash">wget --no-check-certificate https://dlcdn.apache.org/maven/maven-3/3.8.5/binaries/apache-maven-3.8.5-bin.tar.gz -O mvn.tar.gztar xf mvn.tar.gz -C /usr/local/cd /usr/local/apache-maven-3.8.5# /etc/profile 加入export M2_HOME=/usr/local/apache-maven-3.8.5export PATH=$M2_HOME/bin:$PATH</code></pre><p>jenkins web 界面配置，引用安装好的 mvn</p><p><img src="/posts/67a8b607/image-20220507205451384.png" alt="image-20220507205451384"></p><p>pipeline中引用</p><pre><code class="language-yaml">pipeline &#123;agent &#123; node &#123; label &quot;master&quot;&#125;&#125;stages &#123;stage(&quot;build&quot;)&#123;steps&#123;script&#123;mvnHome tools &quot;M2&quot;sh &quot;$&#123;mvnHome&#125;/bin/mvn -v&quot;&#125;&#125;&#125;&#125;&#125;</code></pre><p>👙也可以不用 tools 引用配置好的。而是直接使用 mvnHome = “/usr/local/apache-maven-3.8.5” 写死。</p><p>常用的 mvn 命令：</p><pre><code class="language-bash">mvn clean package # 移除项目路径下的 target 目录文件后，再根据 pom.xml 的 packaging 标签来打包成 jar 或者 warmvn clean install # 移除项目路径下的 target 目录文件后，再根据 pom.xml 的 packaging 标签来打包成 jar 或者 war，同时回解决依赖，包含 package 命令。mvn clean test # 移除项目路径下的 target 目录文件后，再进行单元测试。</code></pre><h2 id="Ant-java打包工具">Ant(java打包工具)</h2><p><a href="https://zeyangli.github.io/chapter4/chapter4-1/2/">https://zeyangli.github.io/chapter4/chapter4-1/2/</a></p><blockquote><p><a href="https://ant.apache.org/bindownload.cgi">https://ant.apache.org/bindownload.cgi</a></p></blockquote><pre><code class="language-bash">tar zxf apache-ant-1.10.5-bin.tar.gz -C /usr/local/#添加全局变量（/etc/profile）export ANT_HOME=/usr/local/apache-ant-1.10.5export PATH=$PATH:$MAVEN_HOME/bin:$ANT_HOME/binsource /etc/profile</code></pre><p>pipeline</p><pre><code class="language-bash">node &#123;stage (&quot;build&quot;)&#123;    antHome = tool 'ANT'    sh &quot;$&#123;antHome&#125;/bin/ant -version&quot;&#125;&#125;</code></pre><p>常用命令</p><pre><code class="language-bash">ant -buildfile -f build.xml</code></pre><h2 id="Gradle-java打包工具">Gradle(java打包工具)</h2><p><a href="https://zeyangli.github.io/chapter4/chapter4-1/3/">https://zeyangli.github.io/chapter4/chapter4-1/3/</a></p><blockquote><p><a href="https://downloads.gradle.org/distributions/gradle-5.3-bin.zip">https://downloads.gradle.org/distributions/gradle-5.3-bin.zip</a></p></blockquote><pre><code class="language-bash">unzip gradle-5.3-bin.zip -d /usr/local/#添加全局变量（/etc/profile）export GRADLE_HOME=/usr/local/gradle-5.3export PATH=$PATH: $GRADLE_HOME/binsource /etc/profile</code></pre><p>pipeline</p><pre><code class="language-bash">node &#123;stage (&quot;gradlebuild&quot;)&#123;    gradleHome = tool 'GRADLE'    sh &quot;$&#123;gradleHome&#125;/bin/gradle -v&quot;&#125;&#125;</code></pre><p>常用命令</p><pre><code class="language-bash">- ./gradlew -v 版本号，首次运行，没有gradle的要下载的哦。- ./gradlew clean 删除HelloWord/app目录下的build文件夹- ./gradlew build 检查依赖并编译打包- ./gradlew assembleDebug 编译并打Debug包- ./gradlew assembleRelease 编译并打Release的包- ./gradlew installRelease Release模式打包并安装- ./gradlew uninstallRelease 卸载Release模式包</code></pre><h2 id="NPM-前端打包工具">NPM(前端打包工具)</h2><p><a href="https://zeyangli.github.io/chapter4/chapter4-1/4/">https://zeyangli.github.io/chapter4/chapter4-1/4/</a></p><blockquote><p><a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a></p></blockquote><pre><code class="language-bash">tar xf node-v10.15.3-linux-x64.tar.xz -C /usr/local/#添加全局变量（/etc/profile）export NODE_HOME=/usr/local/node-v10.15.3-linux-x64export PATH=$PATH: $NODE_HOME/binsource /etc/profile</code></pre><p>pipeline</p><p>👙在Jenkins全局工具配置中默认并没有node，需要安装NodeJS插件。</p><p>可以直接通过Jenkinsfile定义使用。</p><pre><code class="language-bash">node &#123;stage (&quot;npmbuild&quot;)&#123;    sh &quot;&quot;&quot;       export npmHome=/usr/local/node-v10.15.3-linux-x64       export PATH=\$PATH:\$npmHome/bin       npm -v       &quot;&quot;&quot;&#125;    &#125;</code></pre><p>常用命令</p><pre><code class="language-bash">npm install &amp;&amp; npm run build</code></pre><h2 id="Jenkinslib-整合工具">Jenkinslib 整合工具</h2><h3 id="方式1">方式1</h3><p>工具不通过 jenkins tool 调用，纯粹用 pipeline</p><p>src/org/devops/builds.groovy</p><pre><code class="language-bash">package org.devops// 构建函数def Build(buildTools, buildType)&#123;    switch(buildType)&#123;        case &quot;maven&quot;:            sh &quot;$&#123;buildTools[&quot;maven&quot;]&#125;/bin/mvn clean package&quot;            break        case &quot;gradle&quot;:            sh &quot;$&#123;buildTools[&quot;gradle&quot;]&#125;/bin/gradle build -x test&quot;            break                case &quot;golang&quot;:            sh &quot;$&#123;buildTools[&quot;golang&quot;]&#125;/bin/go build demo.go&quot;            break                case &quot;npm&quot;:            sh &quot;&quot;&quot; $&#123;buildTools[&quot;npm&quot;]&#125;/bin/npm install  &amp;&amp; $&#123;buildTools[&quot;npm&quot;]&#125;/bin/npm run build &quot;&quot;&quot;            break                default :            println(&quot;buildType ==&gt; [maven|gralde|golang|npm]&quot;)            break    &#125;&#125;</code></pre><p>pipeline 引用</p><pre><code class="language-bash">@Library(&quot;devopslib@master&quot;) _  // 这里的意思是调用 devopslib引用下的 git 库的 master 分支版本def builds  = new org.devops.builds()// 定义打包工具集def buildTools = [  &quot;maven&quot; : &quot;/usr/local/apache-maven-3.8.1&quot;,                    &quot;gradle&quot;: &quot;/usr/local/gradle-6.8.3/&quot;,                    &quot;golang&quot;: &quot;/usr/local/go&quot;,                    &quot;npm&quot;   : &quot;/usr/local/node-v14.16.1-linux-x64/&quot;,                    &quot;sonar&quot; : &quot;/usr/local/sonar-scanner-4.6.0.2311-linux/&quot;]//WEBUI上面的参数获取String branchName = &quot;$&#123;env.branchName&#125;&quot;String gitHttpURL = &quot;$&#123;env.gitHttpURL&#125;&quot;String buildType  = &quot;$&#123;env.buildType&#125;&quot;         // 传递打包命令String skipSonar  = &quot;$&#123;env.skipSonar&#125;&quot;String repoName   = &quot;$&#123;JOB_NAME.split('/')[0]&#125;&quot;pipeline &#123;    agent &#123; label  &quot;build&quot; &#125;        options &#123;        skipDefaultCheckout true    &#125;    stages &#123;        stage(&quot;GetCode&quot;)&#123;            steps&#123;                script&#123;                    mytools.GetCode(&quot;git&quot;,branchName,gitHttpURL)                &#125;            &#125;                    &#125;        stage(&quot;Build&quot;)&#123;            steps &#123;                script &#123;                    builds.Build(buildTools, buildType)                &#125;            &#125;        &#125;...</code></pre><h3 id="方式2">方式2</h3><p>工具通过 jenkins tool 调用，需要提前配置好 jenkins 全局工具</p><p>src/org/devops/builds.groovy</p><pre><code class="language-bash">package org.devopsdef Build(buildType, buildShell)&#123;def buildTools = [&quot;mvn&quot;:&quot;M2&quot;, &quot;npm&quot;:&quot;NPM&quot;]println(&quot;当前选择构建类型：$&#123;buildType&#125;&quot;)buildHome = tool buildTools[buildType]if (&quot;$&#123;buildType&#125;&quot; == &quot;npm&quot;)&#123;sh &quot;&quot;&quot;export NODE_HOME=$&#123;buildHome&#125;export PATH=\$NODE_HOME/bin:\$PATH$&#123;buildHome&#125;/bin/$&#123;buildType&#125; $&#123;buildShell&#125;&quot;&quot;&quot;&#125;else&#123;sh &quot;$&#123;buildHome&#125;/bin/$&#123;buildType&#125; $&#123;buildShell&#125;&quot;&quot;&#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins构建工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-pipeline dsl 常用方法</title>
      <link href="posts/57e9683e/"/>
      <url>posts/57e9683e/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>这些方法都在jenkins里内置了，因此可以直接在pipeline里使用</p><p>图形化的代码配置可以访问：http://<jenkins>/job/prod_saas_cms/pipeline-syntax/</jenkins></p><p>👙如果你找不到某个DSL，则说明需要相关插件</p><h2 id="json-读取-readJSON">json 读取 readJSON</h2><p>(需要额外插件) Pipeline Utility Steps</p><p>处理json数据格式化，处理json数据</p><pre><code class="language-bash">def response = readJSON text: &quot;$&#123;scanResult&#125;&quot;println(scanResult)def response = readJSON file: 'dir/input.json'</code></pre><h2 id="调用凭据-withCredentials">调用凭据 withCredentials</h2><p>调用jenkins里存储的凭据</p><pre><code class="language-bash">withCredentials([usernamePassword(credentialsId: 'registry-jenkins_harbor-rw', passwordVariable: 'password', usernameVariable: 'username')]) &#123;    println(username)    println(password)&#125;</code></pre><p>上面代码的意思是调用 usernamePassword 类型的凭据 registry-jenkins_harbor-rw，并将用户名存储在username变量里，密码存储在password变量里。</p><h2 id="下载代码-checkout">下载代码 checkout</h2><p>下载代码</p><pre><code class="language-bash">checkout([$class: 'GitSCM', branches: [[name: 'master']], extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: 'docker']], userRemoteConfigs: [[credentialsId: 'gitlab-jenkins-pull-token', url: 'http://git.xxx.com/cms/mapi-agent.git']]])</code></pre><p>上面代码的意思是通过 jenkins 凭据 gitlab-jenkins-pull-token  下载 <a href="http://git.xxx.com/cms/mapi-agent.git">http://git.xxx.com/cms/mapi-agent.git</a> 的 master 分支代码（不包含分支名目录）到 jenkins 工作目录下的 docker 目录中。</p><h2 id="推送HTML报告-publishHTML">推送HTML报告 publishHTML</h2><p>(需要额外插件) HTML Publisher</p><pre><code class="language-bash">publishHTML([allowMissing: false,alwaysLinkToLastBuild: false,keepAll: true,reportDir: './report/',reportFiles: 'a.html, b.html',reportName: 'InterfaceTestReport',reportTitles: 'HTML'])</code></pre><h2 id="交互方式-input">交互方式 input</h2><p>交互方式</p><h2 id="构建用户信息-BuildUser">构建用户信息 BuildUser</h2><p>(需要额外插件)</p><p>获取构建用户信息</p><pre><code class="language-bash">wrap([$class: 'BuildUser'])&#123;echo &quot;full name: $&#123;BUILD_USER&#125;&quot;echo &quot;user id: $&#123;BUILD_USER_ID&#125;&quot;echo &quot;user email: $&#123;BUILD_USER_EMAIL&#125;&quot;&#125;</code></pre><h2 id="web请求-httpRequest">web请求 httpRequest</h2><p>发起http请求</p><pre><code class="language-bash">ApiUrl = &quot;http://xxx/api/project_branches/list?project=$&#123;projectName&#125;&quot;Result = httpRequest authentication: 'xxx',quiet: true,contentType: 'APPLICATION_JSON',url: &quot;ApiUrl&quot;</code></pre><h2 id="邮件-email">邮件 email</h2><pre><code class="language-bash">def EmailUser(userEmail,status)&#123; emailext body: &quot;&quot;&quot;            &lt;!DOCTYPE html&gt;             &lt;html&gt;             &lt;head&gt;             &lt;meta charset=&quot;UTF-8&quot;&gt;             &lt;/head&gt;             &lt;body leftmargin=&quot;8&quot; marginwidth=&quot;0&quot; topmargin=&quot;8&quot; marginheight=&quot;4&quot; offset=&quot;0&quot;&gt;                 &lt;!-- &lt;img src=&quot;http://192.168.1.200:8080/static/0eef74bf/images/headshot.png&quot;&gt; --&gt;                &lt;table width=&quot;95%&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; style=&quot;font-size: 11pt; font-family: Tahoma, Arial, Helvetica, sans-serif&quot;&gt;                       &lt;tr&gt;                         &lt;td&gt;&lt;br /&gt;                             &lt;b&gt;&lt;font color=&quot;#0B610B&quot;&gt;构建信息&lt;/font&gt;&lt;/b&gt;                         &lt;/td&gt;                     &lt;/tr&gt;                     &lt;tr&gt;                         &lt;td&gt;                             &lt;ul&gt;                                 &lt;li&gt;项目名称：$&#123;JOB_NAME&#125;&lt;/li&gt;                                         &lt;li&gt;构建编号：$&#123;BUILD_ID&#125;&lt;/li&gt;                                 &lt;li&gt;构建状态: $&#123;status&#125; &lt;/li&gt;                                                         &lt;li&gt;项目地址：&lt;a href=&quot;$&#123;BUILD_URL&#125;&quot;&gt;$&#123;BUILD_URL&#125;&lt;/a&gt;&lt;/li&gt;                                    &lt;li&gt;构建日志：&lt;a href=&quot;$&#123;BUILD_URL&#125;console&quot;&gt;$&#123;BUILD_URL&#125;console&lt;/a&gt;&lt;/li&gt;                             &lt;/ul&gt;                         &lt;/td&gt;                     &lt;/tr&gt;                     &lt;tr&gt;                  &lt;/table&gt;             &lt;/body&gt;             &lt;/html&gt;  &quot;&quot;&quot;,            subject: &quot;Jenkins-$&#123;JOB_NAME&#125;项目构建信息 &quot;,            to: userEmail&#125;</code></pre><h2 id="清理空间-cleanWS">清理空间 cleanWS</h2><pre><code class="language-bash">cleanWs cleanWhenSuccess: false</code></pre><p>表示仅当工程执行成功的时候不清理，其余工程状态都清理。</p><h2 id="java单元测试报告-junit">java单元测试报告 junit</h2>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
            <tag> jenkins-dsl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-groovy基础</title>
      <link href="posts/e804084e/"/>
      <url>posts/e804084e/</url>
      
        <content type="html"><![CDATA[<h2 id="字符串和列表">字符串和列表</h2><pre><code class="language-bash">&quot;devopstestops&quot;.contains(&quot;ops&quot;)  // 包含 opstrue&quot;devopstestops&quot;.endsWith(&quot;ops)   // 已 ops 结尾true&quot;devopstestops&quot;.size() // 字符串长度 length() 依然是计算长度13&quot;devops&quot;.toUpperCase() // 转大写DEVOPS&quot;DEVOPS&quot;.toLowerCase() // 转小写devops--- 字符串分割hosts = &quot;host01,host02,host03&quot;.split(',')[host01,host02,host03]for ( i in hosts )&#123;    println(i)&#125;host01host02host03--- 添加元素result = [1,2,3,4]  result.add(5)println(result)--- 去重[2,3,4,5,5,6,6].unique()[2,3,4,5,6]--- 链接[2,3,4,5,6].join(&quot;-&quot;)2-3-4-5-6--- 遍历result = [1,2,3,4]result.each&#123;    println it&#125;1234</code></pre><h2 id="字典">字典</h2><pre><code class="language-bash">[key:value]</code></pre><pre><code class="language-bash">--- 获取key[1:2,3:4,5:6].keySet()[1,3,5]--- 获取值[1:2,3:4,5:6].values()[2,4,6]--- 追加[1:2,3:4,5:6] + [7:8][1:2,3:4,5:6,7:8]--- 减少[1:2,3:4,5:6,7:8] - [7:8][1:2,3:4,5:6]</code></pre><h2 id="if">if</h2><pre><code class="language-bash">buildType = &quot;maven&quot;if (buildType == &quot;maven&quot;)&#123;    println(&quot;This is a maven project!&quot;)&#125; else if ()&#123;&#125; else &#123;&#125;</code></pre><h2 id="switch">switch</h2><pre><code class="language-bash">buildType = &quot;gradle&quot;switch(&quot;$&#123;buildType&#125;&quot;)&#123;case 'maven':println(&quot;This is maven&quot;)break;;;case 'gradle':println(&quot;This is a gradle&quot;)break;;;default:println(&quot;Project Type Error&quot;);;&#125;This is a gradle</code></pre><p>👙以上写法，如果不加 break，则始终会执行 default</p><h2 id="for和while">for和while</h2><pre><code class="language-bash">langs = ['java', 'python', 'groovy']for (lang in langs)&#123;if (lang == 'java')&#123;println('lang error in java')&#125;else &#123;println(&quot;lang is $&#123;lang&#125;&quot;)&#125;&#125;lang error in javalang is pythonlang is groovy</code></pre><pre><code class="language-bash">while(1==1)&#123;println('true')&#125;</code></pre><h2 id="function">function</h2><pre><code class="language-bash">def PrintMes()&#123;println('true')&#125;PrintMes()def PrintMes(info)&#123;println(info)&#125;PrintMes(&quot;devops&quot;)</code></pre><h2 id="正则">正则</h2><pre><code class="language-bash">@NonCPS // 解释器，处理重复运行情况下 json 序列化问题String getBranch(String branchName)&#123;def matcher = (branchName =~ &quot;RELEASE-[0-9]&#123;4&#125;&quot;) // 构建判断 branchName是否包含 RELEASE-4位数字的匹配器if (matcher.find())&#123;newBranchName = matcher[0]&#125; else &#123;newBranchName = branchName&#125;newBranchName&#125;newBranchName = getBranch(branchName)println(&quot;新分支名 --&gt; $&#123;newBranchName&#125;&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
            <tag> groovy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-示例1</title>
      <link href="posts/192a4f4c/"/>
      <url>posts/192a4f4c/</url>
      
        <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>本例子是传统例子，执行步骤如下：</p><ol><li>拉取代码</li><li>根据代码的Dockerfile，打包 docker 镜像，并清理24小时之前的镜像</li><li>推送镜像到代码仓库</li><li>获取更新服务所在的服务器的ip，并写入到jenkins工作目录的.hosts文件中。ansible从.hosts中获取远程服务器ip</li><li>在jenkins工作目录中，调用ansible在远程服务器中执行用户传递的shell命令</li><li>如果成功，就推送jenkins工程信息到企业微信</li></ol><h2 id="Jenkinslib">Jenkinslib</h2><pre><code class="language-bash">package org.devops//格式化输出, 需要AnsiColor插件支持def myPrint(content, color)&#123;    colors = ['red'   : &quot;\033[40;31m &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;$&#123;content&#125;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; \033[0m&quot;,              'green' : &quot;\033[40;32m &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;$&#123;content&#125;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; \033[0m&quot;,              'yellow' : &quot;\033[40;33m &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;$&#123;content&#125;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; \033[0m&quot;,              'blue'  : &quot;\033[47;34m &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;$&#123;content&#125;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; \033[0m&quot;]    ansiColor('xterm') &#123;        println(colors[color])    &#125;&#125;//封装wechatBot请求， 需要 http request 插件def myHttp(reqMode,reqUrl,reqBody)&#123;    result = httpRequest httpMode: reqMode,                accept: &quot;APPLICATION_JSON_UTF8&quot;,                contentType: &quot;APPLICATION_JSON_UTF8&quot;,                consoleLogResponseBody: true,                ignoreSslErrors: true,                requestBody: reqBody,                url: reqUrl                quiet: true    return result&#125;//根据aws ec2服务器上用户设定的一对一的特定标签对，获取服务器的公有ip或者私有ip, 并写入到 .hosts 文件中//ipType: PublicIpAddress PrivateIpAddress//需要jenkins工作节点拥有 aws ec2 describe-instances 权限def getEc2Ip(ipType, tagKey, tagValue, Region) &#123;   sh &quot;&quot;&quot;      export AWS_DEFAULT_REGION=$&#123;Region&#125;      aws ec2 describe-instances --filters &quot;Name=tag:$&#123;tagKey&#125;,Values=$&#123;tagValue&#125;&quot; --query 'Reservations[*].Instances[*].[$&#123;ipType&#125;]' --output text &gt; .hosts      cat .hosts   &quot;&quot;&quot; &#125;//ansible 远程执行 shellCommand 传递的 shell 命令//需要提前在jenkins工作节点上部署好私钥，对应的公钥需要放置在 .hosts 文件中记录的服务器用户 ec2-user 下的 .ssh/authorized_keys 文件中def ansible(remoteUser,shellCommand) &#123;    sh &quot;&quot;&quot;        ansible -i .hosts --private-key /var/jenkins_home/.ec2-user.pem -u $&#123;remoteUser&#125; -b --become-user root all -m shell -a &quot;$&#123;shellCommand&#125;&quot;    &quot;&quot;&quot;&#125;</code></pre><h2 id="Jenkinsfile">Jenkinsfile</h2><pre><code class="language-bash">#!groovy// 需要先编写共享库和配置共享库, 这里配置的共享库名叫 jenkinslib.@Library('jenkinslib') _def mytools = new org.devops.mytools()def JOB_MY_RESULT=&quot;&quot;&quot;       - 项目:$&#123;JOB_NAME&#125;       - 编号:$&#123;BUILD_ID&#125;    &quot;&quot;&quot;pipeline &#123;    agent any    parameters &#123;        // git pull        string(defaultValue: 'http://git.xxx.com/demo/test.git', description: 'git url', name: 'gitUrl')        string(defaultValue: 'master', description: 'git branch', name: 'branchVersion')        string(defaultValue: 'jenkins-pull-token', description: 'jenkins git credentialsId', name: 'gitCredId')                // docker build and docker push and docker pull and docker run        string(defaultValue: 'xxx.dkr.ecr.us-east-1.amazonaws.com', description: 'build docker image domain', name: 'dockerImageDomain')        string(defaultValue: 'it/busybox:latest', description: 'build docker image version', name: 'dockerImageTag')                // get ec2 ip        choice(choices: 'PublicIpAddress\nPrivateIpAddress', description: 'get aws ec2 wan_ip or lan_ip', name: 'ipType')        string(defaultValue: 'dockerImageName', description: 'tag key of the app', name: 'tagKey')        string(defaultValue: 'boggle/boggle_server', description: 'tag value of the app', name: 'tagValue')        string(defaultValue: 'us-east-1', description: 'region where app is running', name: 'Region')                // ansible         string(defaultValue: 'ec2-user', description: 'ansible remote user', name: 'ansibleRemoteUser')        string(defaultValue: &quot;docker pull xxx.dkr.ecr.us-east-1.amazonaws.com/it/busybox:latest;docker rm -f busybox;docker run --name busybox xxx.dkr.ecr.us-east-1.amazonaws.com/it/busybox:latest&quot;, description: 'ansible shell module args', name: 'ansibleShellCommand')                //set wechatbot        string(defaultValue: 'POST', description: 'http request method(GET,POST)', name: 'reqMode')        string(defaultValue: 'https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=&lt;chatbotid&gt;', description: 'http request url', name: 'reqUrl')    &#125;    tools &#123;        git &quot;git&quot;    &#125;    stages &#123;        stage('Clone Code') &#123;            steps &#123;                git branch: params.branchVersion, changelog: false, credentialsId: params.gitCredId, poll: false, url: params.gitUrl                sh &quot;rm -rf boggle_common &amp;&amp; cp -rp ../boggle_common ./&quot;            &#125;        &#125;        stage('Build Docker Image') &#123;            steps &#123;                sh &quot;&quot;&quot;                    whoami                    docker image list                    docker build . -t $&#123;dockerImageDomain&#125;/$&#123;dockerImageTag&#125;                    docker image prune -a -f --filter &quot;until=24h&quot;                &quot;&quot;&quot;            &#125;        &#125;        stage('Push Docker Image To AWS ECR') &#123;            steps &#123;                sh &quot;&quot;&quot;                    aws ecr get-login-password --region $&#123;Region&#125; | docker login --username AWS --password-stdin $&#123;dockerImageDomain&#125;                    docker push $&#123;dockerImageDomain&#125;/$&#123;dockerImageTag&#125;                &quot;&quot;&quot;            &#125;        &#125;        stage('Make Ansible Hosts') &#123;            steps &#123;                script&#123;                    mytools.getEc2Ip(ipType, tagKey, tagValue, Region)                &#125;            &#125;        &#125;        stage('Start Latest Tag Container On Remote Machine') &#123;            steps &#123;                script&#123;                    mytools.ansible(ansibleRemoteUser,&quot;aws ecr get-login-password --region $&#123;Region&#125; | docker login --username AWS --password-stdin $&#123;dockerImageDomain&#125;;$&#123;ansibleShellCommand&#125;&quot;)                &#125;            &#125;        &#125;    &#125;    post&#123;        success&#123;            script&#123;                mytools.myPrint(JOB_MY_RESULT, 'green')                // 构建http post请求, 需要安装HTTP Request Plugin                wechatData=&quot;&quot;&quot;&#123;&quot;msgtype&quot;:&quot;markdown&quot;,&quot;markdown&quot;:&#123;&quot;content&quot;: &quot;$&#123;JOB_MY_RESULT&#125;&quot;&#125;&#125;&quot;&quot;&quot;                wechatResult = mytools.myHttp(params.reqMode, params.reqUrl, wechatData)            &#125;        &#125;        failure&#123; script&#123; mytools.myPrint(JOB_MY_RESULT, 'red') &#125;&#125;        aborted&#123; script&#123; mytools.myPrint(JOB_MY_RESULT, 'yellow') &#125;&#125;        unstable&#123; script&#123; mytools.myPrint(JOB_MY_RESULT, 'yellow') &#125;&#125;        changed&#123; script&#123; mytools.myPrint(JOB_MY_RESULT, 'yellow') &#125;&#125;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-常用插件</title>
      <link href="posts/75dc22a9/"/>
      <url>posts/75dc22a9/</url>
      
        <content type="html"><![CDATA[<h2 id="gitlab">gitlab</h2><p><a href="https://plugins.jenkins.io/gitlab-plugin/">https://plugins.jenkins.io/gitlab-plugin/</a></p><p>这个插件会在 jenkins 系统配置中添加额外的选项框：<code>Gitlab</code>，通过设定这个选项框后，你可以在 Jenkinsfile 中调用这个选项，从而将 jenkins 工程步骤的结果通过 gitlab api 回传给 gitlab，gitlab 将这个结果在 ci/cd 流水线界面体现出来。</p><p>💥配置<code>Gitlab</code>选项框的时候，需要一个gitlab的用户的API token，且这个用户需要是gitlab工程的Maintainer用户。如果权限不对，会导致jenkins工程结果无法反馈给gitlab.</p><p>示例：</p><pre><code class="language-groovy">pipeline&#123;    agent &#123;        node &#123;            label &quot;agent01&quot;        &#125;    &#125;    options &#123;                          // 调用 jenkins 系统配置中配置好的 Gitlab        gitLabConnection('gitlab')    &#125;    environment &#123;    dockerImage = &quot;&quot;                  &#125;    stages &#123;        stage(&quot;set custom env&quot;)&#123;            steps&#123;                script&#123;                    imageName = env.GIT_URL.split(&quot;/|\\.&quot;)[-2]                    imageTag = env.BRANCH_NAME + &quot;-&quot; + env.GIT_COMMIT.take(8)                    if (env.BRANCH_NAME == 'master') &#123;                        registryCredential = &quot;registry-apps_acr-rw&quot;                     registryUrl = &quot;https://registry-vpc.cn-zhangjiakou.aliyuncs.com/&quot;                    registryNamespace = env.GIT_URL.split(&quot;/|\\.&quot;)[-3]                    &#125; else &#123;                        registryCredential = &quot;registry-jenkins_harbor-rw&quot;                                           registryUrl = &quot;https://harbor.xxx.com:10443&quot;                     registryNamespace = env.GIT_URL.split(&quot;/|\\.&quot;)[-3]                    &#125;                &#125;            &#125;        &#125;        stage(&quot;code: pull&quot;) &#123;            steps &#123;                cleanWs()                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'    // 回传阶段结果给 gitlab                    try &#123;                        checkout scm                    &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;            &#125;        &#125;        stage(&quot;code: test&quot;) &#123;            steps &#123;                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'                    try &#123;                        echo &quot;单元测试&quot;                    &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;            &#125;        &#125;        stage('docker: build') &#123;steps &#123;                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'                    try &#123;                        dockerImage = docker.build( registryNamespace + &quot;/&quot; + imageName + &quot;:&quot; + imageTag, &quot;docker&quot;)                      &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;&#125;&#125;        stage('docker: push') &#123;steps &#123;                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'                    try &#123;                        docker.withRegistry( registryUrl, registryCredential ) &#123;                            dockerImage.push()                        &#125;                    &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;&#125;&#125;    &#125;    post &#123;        success&#123;            updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'            //成功清理工作空间，失败保留现场            cleanWs()        &#125;        failure&#123;            updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'            //成功清理工作空间，失败保留现场            cleanWs()        &#125;    &#125;&#125;</code></pre><h3 id="插件本身内置变量">插件本身内置变量</h3><p><a href="https://plugins.jenkins.io/gitlab-plugin/#plugin-content-defined-variables">https://plugins.jenkins.io/gitlab-plugin/#plugin-content-defined-variables</a></p><p>💥多分支工程无法使用</p><pre><code>gitlabBranchgitlabSourceBranchgitlabActionTypegitlabUserNamegitlabUserUsernamegitlabUserEmailgitlabSourceRepoHomepagegitlabSourceRepoNamegitlabSourceNamespacegitlabSourceRepoURLgitlabSourceRepoSshUrlgitlabSourceRepoHttpUrlgitlabMergeRequestTitlegitlabMergeRequestDescriptiongitlabMergeRequestIdgitlabMergeRequestIidgitlabMergeRequestStategitlabMergedByUsergitlabMergeRequestAssigneegitlabMergeRequestLastCommitgitlabMergeRequestTargetProjectIdgitlabTargetBranchgitlabTargetRepoNamegitlabTargetNamespacegitlabTargetRepoSshUrlgitlabTargetRepoHttpUrlgitlabBeforegitlabAftergitlabTriggerPhrase</code></pre><h2 id="gitlab-branch-source-多分支流水线">gitlab-branch-source(多分支流水线)</h2><p><a href="https://plugins.jenkins.io/gitlab-branch-source/">https://plugins.jenkins.io/gitlab-branch-source/</a></p><p>这个插件，旨在多分支流水线的分支源中提供额外的gitlab选项，从而实现 jenkins 和 gitlab 之间的继承。从设置上，比<code>gitlab</code>插件或许要简单些。</p><p>✨这个插件有一个小缺点，就是jenkins是被动的以某个时间间隔监听gitlab的事件，所以gitlab事件触发后，jenkins这边可能会在十几秒以内某个点才会开启工程。当然，这只是观察到的现象，并不一定准确。</p><p>首先，插件会在 jenkins 系统配置中添加额外的选项框：<code>GitLab</code>，需要在这里设置 gitlab 服务链接信息。</p><p>💥注意这个和<code>gitlab</code>插件添加的选项卡名称及其相似。。。</p><p><img src="/posts/75dc22a9/image-20220413114803485.png" alt="image-20220413114803485"></p><p>jenkins多分支工程里配置</p><p>添加分支源：GitLab项目：</p><ul><li>服务器：选择jenkins系统配置里设定好的</li><li>所有者：就是代码检查凭据拥有权限的 gitlab group</li><li>项目列表：会根据设定的<code>所有者</code>列出对应的git工程，选择要执行CI流的git工程即可</li></ul><p>行为列表：</p><ul><li>根据发现的分支构建CI工程：但只限于不在MR中的分支，也就是说master分支不会触发CI</li><li>根据发现的来自于origin的MR请求构建CI工程：但只限于MR的目标分支构建CI工程，也就是说MR之后的master分支</li></ul><p><img src="/posts/75dc22a9/image-20220413115205901.png" alt="image-20220413115205901"></p><p><img src="/posts/75dc22a9/image-20220413115340464.png" alt="image-20220413115340464"></p><p><img src="/posts/75dc22a9/image-20220413115405051.png" alt="image-20220413115405051"></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-命令调用</title>
      <link href="posts/1e274151/"/>
      <url>posts/1e274151/</url>
      
        <content type="html"><![CDATA[<h4 id="shell">shell</h4><blockquote><p>远程执行一个命令</p><p>部分参数解析：</p><p>chdir 远程工作目录</p><p>executable 远程执行shell，需要绝对路径</p></blockquote><pre><code class="language-bash">ansible localhost -m shell -a &quot;chdir=/ ls&quot;</code></pre><pre><code class="language-bash">localhost | CHANGED | rc=0 &gt;&gt;binbootdevetchome...</code></pre><h4 id="script">script</h4><blockquote><p>远程执行一个ansible主机环境的脚本</p><p>部分参数解析：</p><p>chdir 远程工作目录</p></blockquote><pre><code class="language-bash">cat test.sh#!/bin/bashfor i in `ls /export`;do        echo $idone#-------------ansible -i hosts test -m script -a &quot;/home/zyh/test.sh&quot;</code></pre><pre><code class="language-bash">10.200.10.212 | CHANGED =&gt; &#123;    &quot;changed&quot;: true,     &quot;rc&quot;: 0,     &quot;stderr&quot;: &quot;Shared connection to 10.200.10.212 closed.\r\n&quot;,     &quot;stderr_lines&quot;: [        &quot;Shared connection to 10.200.10.212 closed.&quot;    ],     &quot;stdout&quot;: &quot;jdk1.8.0_191\r\njdk8\r\nsen\r\n&quot;,     &quot;stdout_lines&quot;: [        &quot;jdk1.8.0_191&quot;,         &quot;jdk8&quot;,         &quot;sen&quot;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-文本文件操作</title>
      <link href="posts/4235ddf4/"/>
      <url>posts/4235ddf4/</url>
      
        <content type="html"><![CDATA[<h2 id="文本操作">文本操作</h2><h4 id="file">file</h4><blockquote><p>path 文件对象地址<br>state 文件类型或者动作状态 （touch: 针对文件, directory：针对目录, link：针对软连接, hard：针对硬链接)</p><p>src 软硬链接的源文件</p><p>owner 属主</p><p>group 属组</p><p>mode 数字权限</p><p>recurse 递归操作</p></blockquote><h4 id="blockinfile">blockinfile</h4><blockquote><p>在指定位置，插入文本块，并在文本块开头和结尾添加标记. 标记用来确认文本块的位置，一些参数会通过标记位置来修改文本块。</p><p>注释格式:</p><p># BEGIN xxx</p><p># END xxx</p><p>参数简介：</p><p>path 文件对象地址</p><p>block 需要添加的文本块</p><p>marker 自定义标记 xxx 部分，如果存在相同标记，则优先处理相同标记的文本块。</p><p>state 状态为absent时，删除标记包括的文本块</p><p>insertafter 正则或者EOF，插入匹配的指定行之后</p><p>insertbefore 正则或者BOF，插入匹配的指定行之前</p><p>backup 先备份，再操作，备份文件后缀是时间戳</p><p>create 文件不存在，则创建</p></blockquote><pre><code class="language-bash"># 这条命令中，如果marker标记已经存在，则insertafter将无效-m blockinfile -a 'path= block=&quot; &quot; marker=&quot;#&#123;mark&#125; xxx&quot; insertafter=&quot;正则&quot;'</code></pre><h4 id="lineinfile">lineinfile</h4><blockquote><p>根据指定的内容，进行替换或删除</p><p>参数简介：</p><p>path 文件对象地址。</p><p>line 指定行内容（在没有正则的情况下，需要全匹配）。</p><p>regexp 通过正则匹配行，并将此行替换成 line 指定的内容，regexp有额外扩展参数，例如 backref。</p><ul><li><p>line  若line匹配到某行，则不修改，若无匹配，则添加line至末尾。</p></li><li><p>regexp + line  若有regexp匹配到某行，替换匹配行为line；如果没有匹配，则将line追加到行尾。此时，regexp不支持分组。</p></li><li><p>regexp + backrefs （true）+ line  若有regexp匹配到某行，替换匹配行为line；如果没有匹配，则保持源文件不变；此时，regexp支持分组。</p></li></ul><p>state 状态为absent时，删除匹配行</p><p>insertafter 正则或者EOF，插入匹配的指定行之后</p><p>insertbefore 正则或者BOF，插入匹配的指定行之前</p><p>backup 先备份，再操作</p><p>create 文件不存在，则创建</p></blockquote><pre><code class="language-bash">-m lineinfile -a 'path= line=&quot; &quot; insertafter=&quot;正则&quot;'</code></pre><h4 id="replace">replace</h4><blockquote><p>替换文件对象中符合匹配的字符串</p><p>path 文件对象地址</p><p>regexp 正则匹配</p><p>replace 替换后的字符串</p><p>backup 先备份，再操作</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-aws</title>
      <link href="posts/15aa7d2e/"/>
      <url>posts/15aa7d2e/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://github.com/ansible/ansible/blob/stable-2.9/contrib/inventory/">https://github.com/ansible/ansible/blob/stable-2.9/contrib/inventory/</a></p><p><a href="https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html">https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html</a></p><p><a href="https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html#inventory-script-example-aws-ec2">https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html#inventory-script-example-aws-ec2</a> （深坑，脚本404，找到了脚本，各种错误，请扔一边）</p></blockquote><h3 id="前言">前言</h3><p>通过 ansible 获取大区下 ec2 资源信息</p><h3 id="授权">授权</h3><pre><code class="language-shell">export AWS_ACCESS_KEY_ID='AK123'export AWS_SECRET_ACCESS_KEY='abc123'export EC2_INI_PATH=ec2.ini</code></pre><h3 id="库存-inventory">库存(inventory)</h3><pre><code class="language-ini">[local]localhost</code></pre><h3 id="Playbook">Playbook</h3><pre><code class="language-yaml">---  - name: test ec2    hosts: local    gather_facts: no   # 我们要这信息干什么？我们是有目标的    connection: local # 木有定义资源    tasks:      - name: get ec2 info        ec2_instance_info:          region: cn-north-1        register: data_output      - name: show ec2 info        debug:          msg: &quot;&#123;&#123; data_output|json_query('instances[*].network_interfaces[*].private_ip_address') &#125;&#125;&quot;</code></pre><h3 id="执行">执行</h3><pre><code class="language-shell">ansible-playbook -i hosts ec2.yml</code></pre><h3 id="输出">输出</h3><pre><code class="language-shell">TASK [show ec2 info] ******************************************************************************************************************************************************ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        [            &quot;10.100.10.250&quot;        ],         [            &quot;10.100.10.252&quot;        ],         [            &quot;10.100.10.210&quot;        ],         [            &quot;10.100.10.251&quot;        ]    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>windows 包管理工具 scoop</title>
      <link href="posts/22b82d75/"/>
      <url>posts/22b82d75/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-powershell">Set-ExecutionPolicy RemoteSigned -scope CurrentUseriwr -useb get.scoop.sh | iexscoop install aria2scoop config aria2-max-connection-per-server 16scoop config aria2-split 16scoop config aria2-min-split-size 1Mscoop install git scoop bucket add extrasscoop install windows-terminal</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> windows </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows </tag>
            
            <tag> scoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-gitops-cd</title>
      <link href="posts/9011aa7a/"/>
      <url>posts/9011aa7a/</url>
      
        <content type="html"><![CDATA[<h2 id="目的">目的</h2><p>根据gitlab的程序配置仓库，当配置变更的时候，执行jenkins cd工程，部署程序。</p><p>在kubernetes中，配置仓库里就是kubernetes清单。</p><h2 id="操作步骤">操作步骤</h2><p>与gitlab的关联，方法与ci一致.</p><h2 id="配置仓库的Jenkinsfile">配置仓库的Jenkinsfile</h2><pre><code class="language-groovy">pipeline&#123;    agent&#123;        kubernetes&#123;            //label &quot;jenkins-agent&quot; // pod 模板标签            //cloud 'kubernetes'  // 插件配置名            inheritFrom &quot;kubectl&quot; // 从标签为 kubectl 的 pod 模板继承        &#125;    &#125;    stages &#123;stage(&quot;git&quot;) &#123;steps &#123;git branch: 'master', url: 'https://github.com/abc-deployment.git' //从https://github.com/abc-deployment.git的master分支克隆代码&#125;&#125;stage('deploy') &#123;steps &#123;container('kubectl') &#123;                    withKubeConfig([credentialsId: 'k8s-cluster-admin-kubeconfig'])&#123; // 这个指令基于 Kubernetes CLI 插件，k8s-cluster-admin-kubeconfig 是 secret file 类型凭据，加载 kubeconfig                        sh 'kubectl apply -f kubernetes/'                    &#125;&#125;&#125;&#125;    &#125;&#125;</code></pre><p>✨关于kubectl的容器，可以选用 kubesphere/kubectl 镜像.</p><p>💥关于k8s-cluster-admin-kubeconfig凭据，应该只拥有对应命名空间的RBAC权限.</p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins-gitops-ci</title>
      <link href="posts/eea0d6c7/"/>
      <url>posts/eea0d6c7/</url>
      
        <content type="html"><![CDATA[<h2 id="目的">目的</h2><p>gitlab 推送后，jenkins 可以自动执行流水线。</p><p>流程：</p><p>用户推送代码到gitlab工程 -》 gitlab 收到推送信息到 jenkins webhook -》Jenkins 收到gitlab的事件 -》根据 Jenkins 流水线工程配置的可执行事件来启动流水线工程 -》 执行pipeline(从配置的scm里获取) -》根据 pipeline 执行代码拉取、代码测试、代码打包、docker镜像包、docker镜像推送。</p><h2 id="涉及到的插件">涉及到的插件</h2><ul><li>GitLab Plugin<ul><li>✨安装 gitlab 插件的时候，可能提示 git 插件版本过低，以至于gitlab插件最后安装异常。升级 git 插件后，gitlab插件可用。</li></ul></li><li>Gitlab Hook Plugin</li><li>Gitlab API Plugin</li><li>ruby-runtime</li><li>git Plugin</li><li><a href="https://plugins.jenkins.io/gitlab-branch-source/">https://plugins.jenkins.io/gitlab-branch-source/</a></li></ul><h2 id="涉及到的账户">涉及到的账户</h2><ul><li>git只读账户，供jenkins使用</li><li>容器镜像仓库读写账户，供jenkins使用</li></ul><h2 id="操作步骤">操作步骤</h2><ol><li><p>gitlab - 管理中心 - 设置 - 网络 - 外发请求 - 允许Webhook和服务对本地网络的请求（启用）</p><p>✨仅当jenkins通过内网访问gitlab的时候需要.</p></li><li><p>在 gitlab 上创建 Jenkins 专属用户 jenkins.</p></li><li><p>生成 jenkins 用户拉取代码的凭证</p><p>✨当jenkins通过ssh方式拉取代码：</p><p>jenkins 创建证书凭据，并将公钥放在 gitlab 专属用户 jenkins 上</p><p>✨当jenkins通过http方式拉取代码：</p><p>gitlab 专属用户 jenkins 创建 read_repository 级别的访问令牌</p></li><li><p>生成 jenkins 回传工程执行过程到 gitlab ci 的凭证</p><p>✨创建 API级别的访问令牌，且用户必须是 git 项目 Maintainer 用户，否则 jenkins 无法将工程结果回传给 gitlab</p></li><li><p>在 jenkins 上添加拉取代码的凭据<code>gitlab-jenkins-ro-token</code>。</p><p>✨以 http 方式为例，凭据类型 Username with password。password是 gitlab 专属用户 jenkins 的read_repository级别的访问令牌</p></li><li><p>授权 Jenkins 访问 gitlab api。</p><ol><li>jenkins 安装 gitlab 插件，并创建凭据<code>gitlab-jenkins-api-token</code>，凭据类型为Gitlab api token，将 gitlab 专属用户 jenkins 的API级别的访问令牌填入。</li><li>jenkins 启用和配置 gitlab 插件。</li></ol></li></ol><p><img src="/posts/eea0d6c7/image-20220327163856124.png" alt="image-20220327163856124"></p><ol start="5"><li>流水线工程启用 gitlab 事件推送，从而获取 jenkins 提供 webhook 地址，便于 gitlab 将代码变更事件推送到 jenkins.</li></ol><p>✨截图里黑色区域，即为jenkins为当前流水线工程创建的webhook地址</p><p><img src="/posts/eea0d6c7/image-20220327164429407.png" alt="image-20220327164429407"></p><p>点击【高级】，即可弹出高级配置，从而可以给 webhook 地址创建 token</p><p><img src="/posts/eea0d6c7/image-20220327164615263.png" alt="image-20220327164615263"></p><ol start="6"><li><p>打开对应的 gitlab 工程 - 设置 - webhooks，将上一步骤里的 webhooks 地址和 token 填入其中。</p></li><li><p>将Pipeline文件Jenkinsfile放入gitlab工程中，并将jenkins工程的pipeline通过scm指向gitlab工程。</p><p>✨scm调用<code>gitlab-jenkins-ro-token</code>凭据。</p></li></ol><h2 id="配置仓库的Jenkinsfile">配置仓库的Jenkinsfile</h2><pre><code>pipeline&#123;    agent &#123;        node &#123;            label &quot;agent01&quot;        &#125;    &#125;    options &#123;        gitLabConnection('gitlab')          // 链接到 gitlab 上    &#125;    environment &#123;    dockerImage = &quot;&quot;                                                                  // 空的环境变量名    &#125;    stages &#123;        stage(&quot;set custom env&quot;)&#123;            steps&#123;                script&#123;                    imageName = env.GIT_URL.split(&quot;/|\\.&quot;)[-2]                    imageTag = env.GIT_COMMIT.take(8)                                 // 获取 git commit short id                    appVersion = env.BRANCH_NAME + &quot;-&quot; + imageTag                     // 镜像标签                    if (env.BRANCH_NAME == 'master') &#123;                        registryCredential = &quot;registry-apps_acr-rw&quot;                       // 镜像注册表登录凭据名，需提前在jenkins凭据功能里添加                    registryUrl = &quot;https://registry-vpc.cn-zhangjiakou.aliyuncs.com/&quot; // 镜像注册表地址                    registryNamespace = &quot;pj-zjk-it&quot;                                   // gitlab组/registry命名空间                    &#125; else &#123;                        registryCredential = &quot;registry-jenkins_harbor-rw&quot;                                           registryUrl = &quot;https://localhbgg.pengwin.com:10443&quot;                     registryNamespace = env.GIT_URL.split(&quot;/|\\.&quot;)[-3]                    &#125;                &#125;            &#125;        &#125;        stage(&quot;code: pull&quot;) &#123;            steps &#123;                cleanWs()                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'                    try &#123;                        checkout scm                    &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;            &#125;        &#125;        stage(&quot;code: test&quot;) &#123;            steps &#123;                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'                    try &#123;                        echo &quot;单元测试&quot;                    &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;            &#125;        &#125;        stage('docker: build') &#123;steps &#123;                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'                    try &#123;                        dockerImage = docker.build( registryNamespace + &quot;/&quot; + imageName + &quot;:&quot; +appVersion, &quot;docker&quot;)                          // build方法接收两个参数，第一个是镜像tag，第二个是dockerfile在代码里的相对目录                    &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;&#125;&#125;        stage('docker: push') &#123;steps &#123;                script &#123;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'running'                    try &#123;                        docker.withRegistry( registryUrl, registryCredential ) &#123;   // 登录注册表                            dockerImage.push()                                     // dockerImage 即上一阶段的 dockerImage 变量.                        &#125;                    &#125; catch(Exception ex) &#123;                        updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'                        throw ex;                    &#125;                    updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'                &#125;&#125;&#125;    &#125;    post &#123;        success&#123;            updateGitlabCommitStatus name: env.STAGE_NAME, state: 'success'            //成功清理工作空间，失败保留现场            cleanWs()        &#125;        failure&#123;            updateGitlabCommitStatus name: env.STAGE_NAME, state: 'failed'            //成功清理工作空间，失败保留现场            cleanWs()        &#125;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞安装和基本配置</title>
      <link href="posts/d9cdb70/"/>
      <url>posts/d9cdb70/</url>
      
        <content type="html"><![CDATA[<h1>安装Jenkins Master</h1><blockquote><p><a href="https://github.com/jenkinsci/docker/blob/master/README.md">https://github.com/jenkinsci/docker/blob/master/README.md</a></p></blockquote><h2 id="通过二进制安装">通过二进制安装</h2><p>略过。。。</p><h2 id="通过容器安装">通过容器安装</h2><p>通过 <a href="https://hub.docker.com/r/jenkins/jenkins/tags?page=1&amp;name=2.318">https://hub.docker.com/r/jenkins/jenkins/tags?page=1&amp;name=2.318</a> 过滤版本</p><pre><code class="language-bash"># 定义版本，例如centos7+jdk8的jenkinsjenkinsTag=2.318-centos7-jdk8# 定义jenkins主服务访问端口jenkinsPort=18080# 定义容器所用的dnsjenkinsDns=119.29.29.29# 定义jenkins的web域名jenkinsDomain=docker volume create jenkins_homedocker run --name jenkins_$&#123;jenkinsTag&#125; \--hostname $&#123;jenkinsDomain&#125; \--restart always \--mount 'type=volume,src=jenkins_home,dst=/var/jenkins_home' \-p $&#123;jenkinsPort&#125;:8080 \-p 50000:50000 \--dns $&#123;jenkinsDns&#125; \-d jenkins/jenkins:$&#123;jenkinsTag&#125;echo &quot;docker run --name jenkins_$&#123;jenkinsTag&#125; --hostname $&#123;jenkinsDomain&#125;  --restart always --mount 'type=volume,src=jenkins_home,dst=/var/jenkins_home' -p $&#123;jenkinsPort&#125;:8080 -p 50000:50000 --dns $&#123;jenkinsDns&#125; -d jenkins/jenkins:$&#123;jenkinsTag&#125;&quot; &gt; docker-jenkins-install.command</code></pre><ol><li>调用宿主机 docker 命令所需的依赖(不一定可用，特别是最后一个库文件，不同版本位置不一定一致)</li></ol><blockquote><p>💥此操作仅当你需要在master上运行工程，并且工程中涉及到了docker命令。</p><p>✨推荐在宿主机上以二进制方式运行从节点，授权从节点代理程序的执行用户访问宿主机docker命令</p></blockquote><pre><code class="language-bash">--mount 'type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock' \--mount 'type=bind,src=/usr/bin/docker,dst=/usr/bin/docker' \--mount 'type=bind,src=/usr/lib64/libltdl.so.7,dst=/usr/lib/x86_64-linux-gnu/libltdl.so.7'</code></pre><ol start="2"><li>如果私有git不对外，则添加额外的host解析</li></ol><pre><code class="language-bash">#定义私有git服务的域名gitlabDomain=#定义私有git服务的ipgitlabWanIP=--add-host $gitlabDomain&#125;:$&#123;gitlabWanIP&#125;</code></pre><ol start="3"><li>宿主机添加hosts内网解析</li></ol><pre><code class="language-bash">localIP=`curl http://100.100.100.200/latest/meta-data/private-ipv4`echo &quot;$&#123;localIP&#125; $&#123;jenkinsDomain&#125;&quot; &gt;&gt; /etc/hosts</code></pre><blockquote><p>上述localIP的获取仅适用于阿里云ecs，如果你不是，则可以通过下面命令获取，不一定保证可行<br>localIP=<code>ip addr show | grep -A3 &quot; eth0&quot; | grep &quot;inet &quot; | awk -F&quot;/&quot; '&#123;print $1&#125;' | awk '&#123;print $NF&#125;'</code></p></blockquote><h2 id="通过访问WEB界面初始化安装过程">通过访问WEB界面初始化安装过程</h2><p>浏览器访问：http://<jenkinsDomain>:18080</jenkinsDomain></p><ol><li>解锁秘钥：</li></ol><p>根据页面提示访问【容器里的】<code>/var/jenkins_home/secrets/initialAdminPassword</code>获得解锁秘钥</p><pre><code class="language-bash">docker exec -it jenkins_$&#123;jenkinsTag&#125; /bin/bash cat /var/jenkins_home/secrets/initialAdminPassword</code></pre><ol start="2"><li>插件安装：选择【安装推荐的插件】</li><li>管理员用户创建，并点击【使用admin账户继续】，并点击【保存完成】</li></ol><h2 id="添加Jenkins-从节点">添加Jenkins 从节点</h2><p>默认没有独立的Node节点，也就是说默认情况下构建是运行在Jenkins Master本身所在容器上。</p><p>但是我们可以添加一个额外的Node节点，并安装构建所需的环境。</p><p>路径：[manage jenkins]-[manage nodes and clouds]-[create node] - 填入agent名-创建，随后参照下图配置</p><p><img src="/posts/d9cdb70/image-20211102164349471.png" alt="image-20211102164349471"></p><p>保存之后，在列表里点击创建的【agent01】，Jenkins 会列出创建命令。</p><p>✨在使用列出的命令之前，需要提前在执行命令的系统环境中满足两个条件：</p><ol><li><p>需要提前在Node节点上安装一个jdk环境</p></li><li><p>需要先下载代理程序 agent.jar，下载地址位于Jenkins Master服务：<a href="http://$">http://$</a>{jenkinsDomain}:18080/jnlpJars/agent.jar</p></li></ol><p>满足上述条件之后，根据 Jenkins 列出的命令，执行下列命令：</p><pre><code class="language-bash"># 创建目录(截图里填入的地址：/&lt;远程工作目录&gt;/&lt;内部数据目录&gt;)mkdir -p /export/jenkins_agent_home/remoting# 授权密码(从jenkins列出的命令里可以拿到)echo 6ffa296c09a3bee21e7217d &gt; secret-file# 运行命令nohup java -jar agent.jar -jnlpUrl http://$&#123;jenkinsDomain&#125;:18080/computer/agent01/jenkins-agent.jnlp -secret @secret-file -workDir &quot;/export/jenkins_agent_home&quot; -failIfWorkDirIsMissing &gt;&gt; agent.log &amp;# 将运行命令写入本地文件，方便之后启动echo &quot;nohup java -jar agent.jar -jnlpUrl http://$&#123;jenkinsDomain&#125;:18080/computer/agent01/jenkins-agent.jnlp -secret @secret-file -workDir &quot;/export/jenkins_agent_home&quot; -failIfWorkDirIsMissing &gt;&gt; agent.log &amp;&quot; &gt; jenkins_agent.command</code></pre><h1>配置Jenkins 从节点构建环境</h1><ol><li>安装awscli、ansible、aliyun</li></ol><pre><code class="language-bash">cd /export/jenkins_agent_home[[ -d src ]] || mkdir srccd srccurl https://bootstrap.pypa.io/get-pip.py -o get-pip.pymkdir ~/.pip/cat &gt; ~/.pip/pip.conf &lt;&lt; EOF[global]index-url = https://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.comEOFyum install python3python3 get-pip.pypip install awsclipip install ansiblecurl 'https://aliyuncli.alicdn.com/aliyun-cli-linux-latest-amd64.tgz' -o aliyun-cli-linux-latest-amd64.tgztar xf aliyun-cli-linux-latest-amd64.tgzcp aliyun /usr/bin/</code></pre><blockquote><p>如果 <a href="http://get-pip.py">get-pip.py</a> 无法下载，可以访问链接：<a href="https://www.aliyundrive.com/s/aVJppvovbCf">https://www.aliyundrive.com/s/aVJppvovbCf</a> 下载</p></blockquote><ol start="2"><li>调用宿主机docker命令</li></ol><pre><code class="language-bash">usermod -aG docker jenkins</code></pre><h1>WEB界面的额外配置</h1><h2 id="其它插件安装">其它插件安装</h2><p><a href="https://plugins.jenkins.io/">https://plugins.jenkins.io/</a></p><p>实用工具集合插件：<a href="https://plugins.jenkins.io/pipeline-utility-steps/">Pipeline Utility Steps</a></p><p>输出着色：<a href="https://plugins.jenkins.io/ansicolor"> AnsiColor</a></p><p>新界面：<a href="https://plugins.jenkins.io/blueocean">Blue Ocean</a></p><p>docker插件：<a href="https://plugins.jenkins.io/docker-workflow/">Docker Pipeline</a>、<a href="https://plugins.jenkins.io/docker-plugin/">Docker</a></p><ul><li><a href="https://docs.cloudbees.com/docs/admin-resources/latest/plugins/docker-workflow">https://docs.cloudbees.com/docs/admin-resources/latest/plugins/docker-workflow</a></li></ul><p>通用webhook触发器：<a href="https://plugins.jenkins.io/generic-webhook-trigger/">Generic Webhook Trigger</a></p><p>gitlab插件：<a href="https://plugins.jenkins.io/gitlab-plugin/">gitlab</a>，<a href="https://plugins.jenkins.io/gitlab-branch-source/">GitLab Branch Source</a></p><p>用户角色管理：<a href="https://plugins.jenkins.io/role-strategy">Role-based Authorization Strategy</a></p><p>HTTP请求插件：<a href="https://plugins.jenkins.io/http_request">HTTP Request</a></p><p>Kubernetes：<a href="https://plugins.jenkins.io/kubernetes">Kubernetes</a>、<a href="https://plugins.jenkins.io/kubernetes-cli/">Kubernetes CLI</a></p><ul><li>kubernetes 插件提供额外的agent功能</li><li>kubernetes cli 插件提供了一些 pipeline 扩展语法</li></ul><p>多分支基于webhook扫描插件：<a href="https://plugins.jenkins.io/multibranch-scan-webhook-trigger/">Multibranch Scan Webhook Trigger</a></p><ul><li><code>openssl rand -hex 8</code> 可以生成用户webhook的token</li><li>webhook地址示例：<code>http://&lt;jenkins&gt;/multibranch-webhook-trigger/invoke?token=e632bc43e840e631</code></li></ul><p>多分支构建策略扩展插件：<a href="https://plugins.jenkins.io/multibranch-build-strategy-extension">Pipeline: Multibranch build strategy extension</a></p><p>构建超时（最新版本默认已有）：<a href="https://plugins.jenkins.io/build-timeout">Build Timeout</a></p><p>构建期间用户信息（最新版本默认已有）：<a href="https://plugins.jenkins.io/build-user-vars-plugin">build user vars plugin</a></p><p>中文包（最新版本默认已有）：<a href="https://plugins.jenkins.io/localization-zh-cn">Localization: Chinese (Simplified)</a></p><p>时间戳（最新版本默认已有）：<a href="https://plugins.jenkins.io/timestamper">Timestamper</a></p><p>工作空间清道夫（最新版本默认已有）：<a href="https://plugins.jenkins.io/ws-cleanup">Workspace Cleanup</a></p><h2 id="用户和角色管理">用户和角色管理</h2><ol><li><p>安装插件 Role-based Authorization Strategy</p></li><li><p>启用插件 Configure Global Security 中启用 Role-Based Strategy 策略</p><p><img src="/posts/d9cdb70/image-20200515180703925.png" alt="image-20200515180703925"></p></li><li><p>配置全局角色和项目角色 Manage and Assign Roles - Manage Roles</p><p>全局角色<strong>Global roles</strong> 设置两个： admin 和 read</p><p><img src="/posts/d9cdb70/image-20200515181449965.png" alt="image-20200515181449965"></p><p>项目角色<strong>Project roles</strong>：每一个项目设置一个</p><p>Pattern: <code>.*\.&lt;项目名&gt; </code></p><p>权限: 看图</p><p><img src="/posts/d9cdb70/image-20200515181359533.png" alt="image-20200515181359533"></p></li><li><p>创建项目用户</p></li><li><p>分配角色 Manage and Assign Roles - Assign Roles</p><p>给管理员分配 admin，给项目用户分配 read 和 cp (cp是我设置的项目角色)</p><p><img src="/posts/d9cdb70/image-20200515181740598.png" alt="image-20200515181740598"></p></li></ol><ul><li><p>安装配置文件插件 Config File Provider</p></li><li><p>自动安装 git ，jdk，maven （这种方式只有在进行了一次构建后，才会安装）</p></li><li><p>maven 私服配置文件</p><ol><li><p>通过 Config File Provider 添加一个项目maven配置</p></li><li><pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;  &lt;servers&gt;    &lt;server&gt;     &lt;id&gt;&lt;Repository Policy名&gt;&lt;/id&gt;     &lt;username&gt;&lt;/username&gt;     &lt;password&gt;&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;     &lt;id&gt;&lt;Repository Policy名&gt;&lt;/id&gt;      &lt;username&gt;&lt;/username&gt;      &lt;password&gt;&lt;/password&gt;    &lt;/server&gt;  &lt;/servers&gt;  &lt;profiles&gt;  &lt;profile&gt;&lt;id&gt;&lt; 仓库名 &gt;&lt;/id&gt;&lt;repositories&gt;  &lt;repository&gt;&lt;id&gt;&lt;maven 仓库组或仓库ID&gt;&lt;/id&gt;&lt;url&gt;&lt;maven 私服具体仓库组或仓库地址&gt;&lt;/url&gt;&lt;releases&gt;  &lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt;&lt;snapshots&gt;  &lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt;  &lt;/repository&gt;&lt;/repositories&gt;  &lt;/profile&gt;  &lt;/profiles&gt;  &lt;activeProfiles&gt; &lt;activeProfile&gt;项目名&lt;/activeProfile&gt;  &lt;/activeProfiles&gt;&lt;/settings&gt;</code></pre></li><li><p>然后构建项目的时候，构建环境-Provide Configuration files-Files，并且Build-高级-Settings file-Provided settings.xml</p></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker-多阶段构建</title>
      <link href="posts/cdd230fc/"/>
      <url>posts/cdd230fc/</url>
      
        <content type="html"><![CDATA[<h2 id="示例文件">示例文件</h2><pre><code class="language-dockerfile">## 别名 ui-buildFROM node:10 AS ui-buildWORKDIR /usr/src/appCOPY WebApp/ ./WebApp/RUN cd WebApp &amp;&amp; npm install @angular/cli &amp;&amp; npm install &amp;&amp; npm run buildFROM node:10 AS server-buildWORKDIR /root/## 从 ui-build:/usr/src/app/WebApp/dist 复制到 server-build:./WebApp/distCOPY --from=ui-build /usr/src/app/WebApp/dist ./WebApp/distCOPY package*.json ./RUN npm installCOPY index.js .EXPOSE 3070ENTRYPOINT [&quot;node&quot;]CMD [&quot;index.js&quot;]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞安装-包管理方式</title>
      <link href="posts/b73a61bf/"/>
      <url>posts/b73a61bf/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>介绍如何通过yum或者apt-get安装php和php-fpm<br>适合php7.2</p><h4 id="centos">centos</h4><blockquote><p>安装源 <a href="https://webtatic.com/">https://webtatic.com/</a></p></blockquote><pre><code class="language-bash">yum install epel-releaserpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpmyum install gcc-c++ geoip-devel -yyum install php72w-cli php72w-devel mod_php72w php72w-fpm php72w-opcache php72w-gd php72w-bcmath php72w-xml -y# php72w-lzo php72w-yaf 没有直接的包mkdir /export/logs/php -pcd /etc/php-fpm.d/ &amp;&amp; mv www.conf www.conf.bakwebName=wwwcat &gt; www.conf &lt;&lt; EOF[$&#123;webName&#125;]user = webappsgroup = webappslisten = 0.0.0.0:9000pm = staticpm.max_children = 20pm.max_requests = 1024pm.status_path = /php-fpm_statusrequest_slowlog_timeout = 2sslowlog = /export/logs/php/php-slow.logphp_admin_value[error_log] = /export/logs/php/www-error.logphp_admin_flag[log_errors] = onphp_value[session.save_handler] = filesphp_value[session.save_path]    = /var/lib/php/sessionphp_value[soap.wsdl_cache_dir]  = /var/lib/php/wsdlcacheEOF# 安装源中没有的模块, 假设模块是rediscd /usr/local/srcphpModule=yafwget https://pecl.php.net/get/$&#123;phpModule&#125; &amp;&amp; mkdir $&#123;phpModule&#125;-src &amp;&amp; tar xf $&#123;phpModule&#125; --strip-components 1 -C $&#123;phpModule&#125;-srccd $&#123;phpModule&#125;-src &amp;&amp; phpize &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make installcat &gt; /etc/php.d/$&#123;phpModule&#125;.ini &lt;&lt; EOF; Enable zip extension moduleextension=$&#123;phpModule&#125;.soEOF</code></pre><h4 id="ubuntu">ubuntu</h4><pre><code class="language-bash">add-apt-repository ppa:ondrej/phpapt-get install php7.2 php7.2-dev php7.2-fpm php7.2-mysql php7.2-curl php7.2-json php7.2-mbstring php7.2-xml  php7.2-intl php7.2-yac php7.2-yaf php7.2-redis php7.2-lzo php7.2-geoip php7.2-pecl php7.2-pear php7.2-dev php7.2-gd php7.2-zip php7.2-xml php7.2-bcmath</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞常用命令记录</title>
      <link href="posts/106410b0/"/>
      <url>posts/106410b0/</url>
      
        <content type="html"><![CDATA[<h4 id="创建Pod拉取镜像的权限账户">创建Pod拉取镜像的权限账户</h4><pre><code class="language-bash">kubectl create secret docker-registry &lt;obj_name&gt; \--docker-server=https://image-registry \--docker-username=pod-ro \--docker-password=pod@123 \--docker-email=&lt;email&gt; \-n &lt;namespace&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞基础信息</title>
      <link href="posts/d3e413b7/"/>
      <url>posts/d3e413b7/</url>
      
        <content type="html"><![CDATA[<h3 id="安装">安装</h3><pre><code class="language-shell">apt-get install python3-pippip3 install ansible --user -i https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><blockquote><p>pip 安装方式，不会生成默认配置<br><a href="https://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg">https://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg</a></p></blockquote><h3 id="关闭-known-hosts-检查">关闭 known_hosts 检查</h3><pre><code class="language-ini"># /etc/ansible/ansible.cfg or ~/.ansible.cfg[defaults]host_key_checking = False</code></pre><h3 id="库存和变量">库存和变量</h3><pre><code class="language-ini"># /etc/ansible/hosts 默认位置，但可自定义，并通过 -i 来调用############################################################################# 单主机mail.example.com# 主机组，其中 http_port 是主机变量[webservers]www[01:50].example.comhttp_port=80[dbservers]db-[a:f].example.comansible_connection=ssh        ansible_ssh_user=mysql# 定义 dbservers 组的组变量 vars[dbservers:vars]mysql_port=3306# 定义 webproject 组包含子组 children[webproject:children]webserversdbservers</code></pre><blockquote><p>中括号表示分组，可以用组名代替组资源 ;</p></blockquote><h3 id="结构化变量">结构化变量</h3><blockquote><p>采用 yaml 配置，格式：</p><pre><code class="language-yaml">---  变量:值</code></pre></blockquote><pre><code class="language-shell">/etc/ansible/group_vars/&lt;组名&gt; # &lt;组名&gt;文件或路径下的文件，均会被认为是&lt;组名&gt;变量/etc/ansible/host_vars/&lt;主机名&gt; # &lt;主机名&gt;文件或路径下的文件，均会被认为是&lt;组名&gt;变量</code></pre><h4 id="常用的变量">常用的变量</h4><pre><code class="language-shell">ansible_ssh_host      将要连接的远程主机名.与你想要设定的主机的别名不同的话,可通过此变量设置.ansible_ssh_port      ssh端口号.如果不是默认的端口号,通过此变量设置.ansible_ssh_user      默认的 ssh 用户名ansible_ssh_pass      ssh 密码(这种方式并不安全,我们强烈建议使用 --ask-pass 或 SSH 密钥)ansible_sudo_pass      sudo 密码(这种方式并不安全,我们强烈建议使用 -b --ask-become-pass)ansible_sudo_exe (new in version 1.8)      sudo 命令路径(适用于1.8及以上版本)ansible_connection      与主机的连接类型.比如:local, ssh 或者 paramiko. Ansible 1.2 以前默认使用 paramiko.1.2 以后默认使用 'smart','smart' 方式会根据是否支持 ControlPersist, 来判断'ssh' 方式是否可行.ansible_ssh_private_key_file      ssh 使用的私钥文件.适用于有多个密钥,而你不想使用 SSH 代理的情况.ansible_shell_type      目标系统的shell类型.默认情况下,命令的执行使用 'sh' 语法,可设置为 'csh' 或 'fish'.ansible_python_interpreter      目标主机的 python 路径.适用于的情况: 系统中有多个 Python, 或者命令路径不是&quot;/usr/bin/python&quot;,比如  \*BSD, 或者 /usr/bin/python      不是 2.X 版本的 Python.我们不使用 &quot;/usr/bin/env&quot; 机制,因为这要求远程用户的路径设置正确,且要求 &quot;python&quot; 可执行程序名不可为 python以外的名字(实际有可能名为python26).      与 ansible_python_interpreter 的工作方式相同,可设定如 ruby 或 perl 的路径....</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工具库</title>
      <link href="posts/3827a60a/"/>
      <url>posts/3827a60a/</url>
      
        <content type="html"><![CDATA[<p><a href="https://encycolorpedia.cn/323e4e">十六进制颜色代码表，图表，调色板，绘图&amp;油漆</a></p><p><a href="http://gosspublic.alicdn.com/ram-policy-editor/index.html">阿里云 ram 策略生成器</a></p><p><a href="https://develop.aliyun.com/tools/sdk?#/python">阿里云SDK频道</a></p><p><a href="https://app.xuty.tk/static/app/index.html">表情锅</a></p><p><a href="https://whoer.net/zh">测试国外出口ip</a></p><p><a href="http://www.ip-api.com/">测试国外出口ip</a></p><p><a href="https://www.wondercv.com/">超级简历WonderCV - HR推荐简历模板,智能简历制作工具,专业中英文简历模板免费下载</a></p><p><a href="https://help.aliyun.com/knowledge_detail/50270.html?spm=a2c4g.11186623.6.621.483534bfFo31Sm">各地区管局备案规则</a></p><p><a href="https://www.ipplus360.com/">更精准的全球IP地址定位平台_IP问问 -埃文科技(ipplus360.com)</a></p><p><a href="http://xn--eqrt2g.xn--vuq861b/">工信部-域名.信息</a></p><p><a href="http://explainshell.com/">解析命令 explainshell.com - match command-line arguments to their help text</a></p><p><a href="https://haveibeenpwned.com/Passwords">密码泄露检测</a></p><p><a href="https://tuna.moe/">清华大学 TUNA 协会 - Home</a></p><p><a href="http://zh.thetimenow.com/time-zone-converter.php">时区转换生成</a></p><p><a href="https://help.aliyun.com/document_detail/116378.html?spm=a2c4g.11186623.2.17.6f36578flUOrcy#concept-188715">使用redis-shake迁移RDB文件内的数据</a></p><p><a href="https://www.17ce.com/">网站测速|网站速度测试|网速测试|电信|联通|网通|全国|监控|CDN|PING|DNS 17CE.COM</a></p><p><a href="https://devhints.io/">语言/工具语法常用摘要</a></p><p><a href="https://ipchaxun.com/">域名反查ip</a></p><p><a href="https://www.whatsmydns.net/">域名解析检查</a></p><p><a href="https://intodns.com/">域名状态报告</a></p><p><a href="https://github.com/ireaderlab/alex">alex:web压力测试工具</a></p><p><a href="http://www.kammerl.de/ascii/AsciiSignature.php">Ascii Text / Signature Generator motd动态开机提醒</a></p><p><a href="https://www.json2yaml.com/convert-yaml-to-json">Convert YAML to JSON</a></p><p><a href="https://csr.chinassl.net/generator-csr.html">CSR文件生成工具-中国数字证书CHINASSL</a></p><p><a href="https://apps.evozi.com/apk-downloader/">gp apk 下载</a></p><p><a href="http://tool.520101.com/wangluo/ipjisuan/">ip地址在线计算器</a></p><p><a href="https://ipv6-test.com/validate.php">IPv6 站点测试</a></p><p><a href="http://grokconstructor.appspot.com/do/match#result">logstash grok 测试</a></p><p><a href="https://github.com/DoubleLabyrinth/MobaXterm-keygen">MobaXterm-keygen 密钥</a></p><p><a href="http://msdn.itellyou.cn/">MSDN, 我告诉你</a></p><p><a href="https://api.aliyun.com/#/cli">OpenAPI Explorer</a></p><p><a href="https://www.pyman.com.cn/">pyman网址导航</a></p><p><a href="http://rpm.pbone.net/">RPM Search</a></p><p><a href="https://www.cnblogs.com/zhaoruiqing/articles/12870209.html">Typora的Emoji指令</a></p><p><a href="https://paste.ubuntu.com/">Ubuntu Pastebin</a></p><p><a href="http://www.92csz.com/study/UnixToolbox-zh_CN.html">Unix Toolbox - 中文版</a></p><p><a href="https://www.dotcom-tools.com/">Website Performance Test Tools</a></p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞admin管理视图</title>
      <link href="posts/86059ffb/"/>
      <url>posts/86059ffb/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://docs.djangoproject.com/zh-hans/3.2/ref/contrib/admin/">Django 管理站点 | Django 文档 | Django (djangoproject.com)</a></p><h2 id="基本使用">基本使用</h2><p>若要在管理视图中看到某个模型，则需要先注册</p><p><code>&lt;app&gt;.admin</code></p><p>例如注册<code>Author</code></p><pre><code class="language-python">from django.contrib import adminfrom myproject.myapp.models import Authorclass AuthorAdmin(admin.ModelAdmin):    list_display = []    list_filter = []    search_fileds = ()admin.site.register(Author, AuthorAdmin)</code></pre><p>其中类<code>AuthorAdmin</code>通过继承<code>admin.ModelAdmin</code>获取默认的一些方法，这些默认方法可以查看：</p><p><a href="https://docs.djangoproject.com/zh-hans/3.2/ref/contrib/admin/#custom-template-options">自定义|Django 管理站点 | Django 文档 | Django (djangoproject.com)</a></p><h2 id="自定义">自定义</h2><h3 id="自定义侧边栏过滤器">自定义侧边栏过滤器</h3><p><a href="https://docs.djangoproject.com/zh-hans/3.2/ref/contrib/admin/#django.contrib.admin.ModelAdmin.list_filter">list_filter | Django 管理站点 | Django 文档 | Django (djangoproject.com)</a></p><p>例如资源表有字段<code>user</code>,<code>user</code>是 User 表的外键，确保过滤器里仅显示拥有资源的用户。</p><p>则需要设置一个元组，第一个元素是字段<code>user</code>，第二个元素是<code>admin.RelatedOnlyFieldListFilter</code>。</p><p>第二个元素继承自 <code>django.contrib.admin.FieldListFilter</code></p><pre><code class="language-python">    list_filter = (        ('user', admin.RelatedOnlyFieldListFilter),    )</code></pre><blockquote><p>列表过滤器通常只有在过滤器有多个选择时才会出现</p></blockquote><h3 id="过滤展示数据">过滤展示数据</h3><p>例如登录用户如果是超级用户，则展示所有数据；</p><p>登录用户如果是普通用户，则展示自己的数据</p><pre><code class="language-python">class MyModelAdmin(admin.ModelAdmin):    def get_queryset(self, request):        qs = super().get_queryset(request)         if request.user.is_superuser:            return qs        return qs.filter(user=request.user)</code></pre><p>这里利用<code>super()</code>调用父类<code>ModelAdmin</code>的<code>get_queryset</code>方法返回<a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/querysets/#queryset-api">QuerySet</a>的所有模型实例.</p><p><code>QuerySet</code>有很多细化方法，例如上述代码里的<code>filter</code>，关于细化方法里的参数格式，则参考<a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/querysets/#field-lookups">Field | QuerySet</a></p><h3 id="表格保存前后添加额外逻辑">表格保存前后添加额外逻辑</h3><p><code>save_model</code>方法被赋予 <code>HttpRequest</code>、一个模型实例、一个 <code>ModelForm</code> 实例和一个基于是否添加或更改对象的布尔值。覆盖这个方法可以进行保存前或保存后的操作。</p><p>例如保存前，自动将模型<code>user</code>字段设置为登录用户</p><pre><code class="language-python">    def save_model(self, request, obj, form, change):        obj.user = request.user  # 保存前的逻辑        super().save_model(request, obj, form, change)        pass # 保存后的逻辑</code></pre><h3 id="设定表格操作权限">设定表格操作权限</h3><p>设定普通登录用户也拥有表格修改和删除权限</p><pre><code class="language-python">    def has_change_permission(self, request, obj=None):        if not obj:            return True # So they can see the change list page        if request.user.is_superuser or obj.user == request.user:            return True        else:            return False        has_delete_permission = has_change_permission  </code></pre><p><code>if not obj</code>返回<code>True</code>，确保模型没有数据的时候，用户也可以看到</p><p><code>if request.user.is_superuser or obj.user == request.user:</code>，确保超级用户和登录登录用户，可以自行修改</p><p>最后其它用户没有权限访问页面</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞日志</title>
      <link href="posts/26bd5165/"/>
      <url>posts/26bd5165/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://docs.djangoproject.com/zh-hans/3.1/topics/logging/">https://docs.djangoproject.com/zh-hans/3.1/topics/logging/</a></p><p><a href="https://docs.python.org/3/library/logging.config.html#configuration-dictionary-schema">https://docs.python.org/3/library/logging.config.html#configuration-dictionary-schema</a></p><p><a href="https://docs.python.org/3/library/logging.html">https://docs.python.org/3/library/logging.html</a></p><p>django 采用 python 的 logger 模块，配置方式选择字典方式</p><h2 id="字典方式基本介绍">字典方式基本介绍</h2><p>有一些必选的键值对 <a href="https://docs.python.org/3/library/logging.config.html#dictionary-schema-details">https://docs.python.org/3/library/logging.config.html#dictionary-schema-details</a></p><p>例如<code>version</code>，<code>formatters</code>等，具体看上面官方文档和下面的例子</p><h2 id="关键对象介绍">关键对象介绍</h2><p><code>loggers</code>根据消息界别接收日志，但不负责输出</p><p><code>LogRecord</code><code>logger</code>接收一条日志，就创建一个<code>logrecord</code>对象</p><p><code>handlers</code>绑定在<code>logger</code>，用于处理日志<code>logrecord</code>对象，例如输出到哪里. 可以将多个<code>handler</code>绑定到同一个<code>logger</code></p><p><code>formatters</code>绑定在<code>handler</code>，用来格式化日志<code>logrecord</code>对象</p><p><code>filters</code>通过设定过滤条件，过滤从<code>logger</code>到<code>handler</code>的日志<code>logrecord</code>对象。非必须</p><h3 id="额外的">额外的</h3><p><code>LoggerAdapter(logger, extra)</code>用来包装<code>logger</code>，它接收一个<code>logger</code>和一个字典<code>extra</code>，从而将<code>extra</code>添加到<code>logrecord</code></p><p>之后，通过<code>LoggerAdapter</code>对象接收的日志，再经过<code>formatter</code>对<code>extra.key</code>的编排，就实现了添加信息到原始日志的功能.</p><h2 id="日志级别">日志级别</h2><ul><li><code>DEBUG</code>：排查故障时使用的低级别系统信息</li><li><code>INFO</code>：一般的系统信息</li><li><code>WARNING</code>：描述系统发生了一些小问题的信息</li><li><code>ERROR</code>：描述系统发生了大问题的信息</li><li><code>CRITICAL</code>：描述系统发生严重问题的信息</li></ul><h2 id="流程">流程</h2><p>给接收的日志定级别：<code>logger.&lt;level&gt;(&quot;log message&quot;)</code></p><p><code>logger</code>判断接收的日志级别是否符合<code>logger</code>定义的级别</p><p>如果符合，<code>logger</code>无差别的推送消息到多个<code>handler</code>，如果不符合，则忽略；另外，如果<code>logger.propagate</code>为真，则消息会复制一份到更高级别的<code>logger</code></p><p><code>handler</code>判断接收的日志级别是否符合<code>handler</code>定义的级别</p><p>如果符合，<code>handler</code>通过<code>formatter</code>排列<code>logrecord</code>对象属性，并通过<code>handler.class</code>将日志输出，如果不符合，则忽略；另外，如果<code>handler</code>有<code>filter</code>，则只会输出满足<code>filter</code>的日志</p><h2 id="例子">例子</h2><p>写入 <a href="http://settings.py">settings.py</a> 中</p><pre><code class="language-python"># LoggingLOGGING = &#123;    'version': 1,  # 指明版本，目前就只有一个版本    'disable_existing_loggers': False,  # 表示是否禁用所有的已经存在的日志配置    # 声明两个格式化id    'formatters': &#123;  # 格式器        'verbose': &#123;  # 详细            'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'        &#125;,        'standard': &#123;  # 标准            'format': '[%(asctime)s] [%(levelname)s] %(message)s'        &#125;,    &#125;,    # 定义过滤器 special,它有一个条件函数 project.logging.SpecialFilter(foo), 函数被实例化的时候，foo='bar'    # project.logging.SpecialFilter 需要自己实现, 判断 logrecord 对象,或者满足一些条件才输出日志    'filters': &#123;        'special': &#123;            '()': 'project.logging.SpecialFilter',            'foo': 'bar',        &#125;,    &#125;,    # handlers：用来定义具体处理日志的方式，可以定义多种，&quot;console&quot;就是打印到控制台方式。file是写入到文件的方式，&quot;mail_admins&quot;是邮件通知    # 这里的 console/file/mail_admins 仅仅是自定义的名称    'handlers': &#123; # 处理器        'console': &#123;            'level': 'DEBUG',            'class': 'logging.StreamHandler',            'stream': 'ext://sys.stdout',   # 文件重定向的配置，将打印到控制台的信息都重定向出去            'formatter': 'standard'   # 制定输出的格式，注意 在上面的formatters配置里面选择一个，否则会报错        &#125;,        'file_all': &#123;            'level': 'DEBUG',            'class': 'logging.handlers.TimedRotatingFileHandler',            'filename': os.path.dirname(os.path.dirname(os.path.realpath(__file__)))+'/log/all.log',              'formatter': 'standard'            'maxBytes': 1024            'when': 'midnight'            'interval': 1            'backupCount': 3                 'utc': False            'encoding': 'utf-8'                    &#125;,        'file_request': &#123;            'level': 'DEBUG',            'class': 'logging.handlers.TimedRotatingFileHandler',            'filename': os.path.dirname(os.path.dirname(os.path.realpath(__file__)))+'/log/request.log',              'formatter': 'standard'            'maxBytes': 1024            'when': 'midnight'            'interval': 1            'backupCount': 3                 'utc': False            'encoding': 'utf-8'                    &#125;,        'file_db': &#123;            'level': 'DEBUG',            'class': 'logging.handlers.TimedRotatingFileHandler',            'filename': os.path.dirname(os.path.dirname(os.path.realpath(__file__)))+'/log/db.log',              'formatter': 'standard'            'maxBytes': 1024            'when': 'midnight'            'interval': 1            'backupCount': 3                 'utc': False            'encoding': 'utf-8'                    &#125;,        'mail_admins': &#123;            'level': 'ERROR',            'class': 'django.utils.log.AdminEmailHandler',            'filters': ['special']        &#125;,    &#125;,    'loggers': &#123;  # log记录器，配置之后就会对应的输出日志        'django': &#123;            'handlers': ['file_all'],            'level': 'INFO',            'propagate': False,        &#125;,        'django.request ':&#123;            'handlers': ['file_request','mail_admins'],            'level': 'INFO',            'propagate': False,        &#125;,         'django.server ':&#123;            'handlers': ['console'],            'level': 'INFO',            'propagate': False,        &#125;,               'django.db.backends': &#123;            'handlers': ['file_db'],            'level':'INFO',            'propagate': False,        &#125;,        # 根记录器        'root': &#123;        'handlers': ['console'],        'level': 'WARNING',        &#125;,    &#125;,&#125;</code></pre><h2 id="logger">logger</h2><p><a href="https://docs.python.org/3/library/logging.html#logger-objects">https://docs.python.org/3/library/logging.html#logger-objects</a></p><p>从上面的例子可以看到，<code>logger</code>有一个属性是<code>propagate</code>，这个属性的作用是将收到的消息复制一份到更高级别的<code>logger</code></p><p>关于<code>logger</code>的层级意思是：通过<code>.</code>分割的<code>logger_name</code></p><p>上述列子中，层级高低排序是：</p><p><code>root</code>&gt;<code>django</code>&gt;<code>django.request</code>=<code>django.server</code></p><p><code>root</code>&gt;<code>django</code>&gt;``django.db.backends`</p><h2 id="handler">handler</h2><p><a href="https://docs.python.org/3/library/logging.handlers.html#module-logging.handlers">https://docs.python.org/3/library/logging.handlers.html#module-logging.handlers</a></p><p>在上述例子中，<code>handler</code>的<code>class</code>都是<code>python</code>的内置类型</p><h2 id="formatter">formatter</h2><p>格式化<code>logrecord</code>对象，排版<code>logrecord</code>的属性</p><h2 id="logrecord">logrecord</h2><p><a href="https://docs.python.org/zh-cn/3.9/library/logging.html#logrecord-attributes">https://docs.python.org/zh-cn/3.9/library/logging.html#logrecord-attributes</a></p><h2 id="LoggerAdapter">LoggerAdapter</h2><p><a href="https://docs.python.org/zh-cn/3.9/library/logging.html#loggeradapter-objects">https://docs.python.org/zh-cn/3.9/library/logging.html#loggeradapter-objects</a></p><h2 id="调用">调用</h2><pre><code class="language-python">import logging, logging.configlogging.config.dictConfig(LOGGING) # django 无需此步骤，django 会自动加载logger = logging.getLogger(__name__)  # 这里的logger名称要匹配上 dictConfig 定义的 logger, 匹配遵循名称层级规则logger.info(&quot;message&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jquery☞伪进度条</title>
      <link href="posts/5d527900/"/>
      <url>posts/5d527900/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>点击-&gt;触发脚本jquery-&gt;脚本访问url-&gt;django view(show_progress)-&gt;view返回数据给脚本jquery-&gt;脚本jquery拿到进度数据百分比-&gt;更新进度条-&gt;重复访问url-&gt;直至进度条数据100%-&gt;终止访问url.</p><h2 id="具体步骤">具体步骤</h2><h3 id="点击">点击</h3><blockquote><p>点击提交按钮，通过id “submit” 找到脚本</p></blockquote><pre><code class="language-html">        &lt;div class=&quot;item1&quot;&gt;          &lt;a href=&quot;&#123;% url 'assets:ssllist' %&#125;&quot;&gt;返回&lt;/a&gt;          &lt;button id=&quot;submit&quot; class=&quot;mybtn&quot; type=&quot;submit&quot;&gt;提交&lt;/button&gt;        &lt;/div</code></pre><h3 id="脚本jquery">脚本jquery</h3><pre><code class="language-javascript">    &lt;script&gt;    $(function () &#123;      $('#submit').on('click', function () &#123;     // 触发 click 动作，执行 function        $('#prog_out').attr(&quot;class&quot;,&quot;progress&quot;);      var sitv = setInterval(function()&#123;    // 添加一个间隔期函数setInterval，它有两个参数，参数1是执行函数function，参数2是间隔时间              var show_progress_url = &quot;&#123;% url 'assets:showprogress' %&#125;&quot;; // 变量 show_progress_url                $.getJSON(show_progress_url, function(res)&#123;    // 访问 show_progress_url，并将view视图的结果res传递给 function 函数，并执行 function(res)                 if(res === 100)&#123;       // 若 view视图返回100，则修改动作条颜色为 success，并且清空计时器并退出                  $('#prog_out').attr(&quot;class&quot;, &quot;progress progress-bar bg-success&quot;);                  clearInterval(sitv);                &#125;                console.log(&quot;currect progresspct:&quot;+res);  // 打印当前进度                  $('#prog_in').width(res + '%');     // 改变进度条进度，注意这里是内层的div， res是后台返回的进度                  $('#prog_in').val(res);   //改变进度条的值                &#125;);              &#125;,           1000);   // 间隔期函数setInterval,第二个参数，每1秒查询一次后台进度      &#125;);    &#125;)    &lt;/script&gt;</code></pre><h3 id="脚本访问URL">脚本访问URL</h3><p>通过上述JS脚本，访问show_progress_url变量</p><h3 id="django-view视图">django view视图</h3><pre><code class="language-python">def show_progress(request):    if request.user.is_authenticated:        loginuser = request.user.username        #progresspct = cache.get_or_set(loginuser,0)        print('当前创建证书的用户是:' + loginuser)        progresspct = cache.get(loginuser)        print(&quot;百分比:&#123;0&#125;&quot;.format(progresspct))        if progresspct == 100:            print('删除缓存')            cache.delete(loginuser)            return JsonResponse(progresspct, safe=False)        progresspct += 1        cache.set(loginuser,progresspct)        return JsonResponse(progresspct, safe=False)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> jquery </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jquery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞restful简单使用</title>
      <link href="posts/dd4c9efa/"/>
      <url>posts/dd4c9efa/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>官方快速教程：<a href="https://www.django-rest-framework.org/tutorial/quickstart/">Quickstart - Django REST framework (django-rest-framework.org)</a></p><h2 id="安装">安装</h2><pre><code class="language-bash">pip install djangorestframework</code></pre><h2 id="配置">配置</h2><p><code>settings.INSTALLED_APPS</code>加入<code>'rest_framework.authtoken','rest_framework'</code></p><blockquote><p>其中<code>rest_framework.authtoken</code>是简单的 Token 认证</p></blockquote><p><code>settings.py</code> 中加入以下代码：</p><pre><code class="language-python">REST_FRAMEWORK = &#123;    'DEFAULT_AUTHENTICATION_CLASSES': [        'rest_framework.authentication.BasicAuthentication',        'rest_framework.authentication.SessionAuthentication',        'rest_framework.authentication.TokenAuthentication'  # 开启 Token 认证    ],    'DEFAULT_PERMISSION_CLASSES': [        'rest_framework.permissions.IsAuthenticated', # 全局接口权限：开启全局认证    ],    # 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination', # 接口分页    # 'PAGE_SIZE': 10&#125;</code></pre><h2 id="具体">具体</h2><p><img src="/posts/dd4c9efa/image-20210805152047616.png" alt="image-20210805152047616"></p><p>大概的界面就如上图所示：</p><ol><li>右上角是一个登录UI</li><li>正面是请求地址/方法之类的，以及返回的数据</li></ol><h3 id="关于右上角的登录UI">关于右上角的登录UI</h3><pre><code class="language-python"># 项目根APP里的 urls.pyfrom django.urls import path, includeurlpatterns += [    path('api-auth/', include('rest_framework.urls')), ]</code></pre><h3 id="大致的一个流程">大致的一个流程</h3><p>Request☞DjangoUrLs☞AppRootUrL☞APIUrL☞APIView☞API序列化☞DjangoModel</p><p>假设你的API地址是 /api-user/user</p><h3 id="DjangoUrLs路由">DjangoUrLs路由</h3><blockquote><p>项目根路由</p></blockquote><pre><code class="language-python">urlpatterns += [    path('api-user/', include(loginurls, namespace=&quot;login&quot;)),]</code></pre><h3 id="AppRoutUrL">AppRoutUrL</h3><blockquote><p>loginurls文件：App根路由</p></blockquote><pre><code class="language-python">app_name=&quot;login&quot;from rest_framework.decorators import api_viewfrom rest_framework.response import Responsefrom rest_framework.reverse import reverse@api_view(['GET'])def api_root(request, format=None):    return Response(&#123;        'users': reverse('login:user-list', request=request, format=format),        'snippets': reverse('login:asset-list', request=request, format=format)    &#125;)# API根路由urlpatterns += [    path('', api_root),]</code></pre><h3 id="APIURL">APIURL</h3><pre><code class="language-python">user_list = views.UserViewSet.as_view(&#123;    'get': 'list'&#125;)user_detail = views.UserViewSet.as_view(&#123;    'get': 'retrieve'&#125;)urlpatterns += [    path('user/', user_list, name='user-list'),    path('user/&lt;int:pk&gt;/', user_detail, name='user-detail')]</code></pre><h3 id="APIViews">APIViews</h3><pre><code class="language-python">from login import models as loginmodelsfrom login.serializers import UserSerializerfrom django.contrib.auth.models import Userfrom rest_framework import permissionsfrom rest_framework import viewsetsclass UserViewSet(viewsets.ModelViewSet):    &quot;&quot;&quot;    This viewset automatically provides `list` and `retrieve` actions.    &quot;&quot;&quot;    queryset = User.objects.all()    serializer_class = UserSerializer    permission_classes = [permissions.IsAdminUser]</code></pre><h3 id="API序列化">API序列化</h3><pre><code class="language-python">from rest_framework import serializersfrom login import models as loginmodelsfrom django.contrib.auth.models import Userclass UserSerializer(serializers.HyperlinkedModelSerializer):    # 显示的定义两个数据字段的查找方法，一个是user的关联外键 user_asset，一个是 HyperlinkedModelSerializer 所需的链接字段 url    user_asset = serializers.HyperlinkedRelatedField(     # user_asset 是 User主表的外键名(related_name)        view_name=&quot;login:asset-detail&quot;, read_only=True    )    url = serializers.HyperlinkedIdentityField(        view_name='login:user-detail',   # 默认 HyperlinkedModelSerializer 的 url 字段查找的视图名是 &#123;model_name&#125;-detail。 这里显示指定视图    )    class Meta:        model = User        fields = ['url', 'id', 'username', 'user_asset']  # api返回的数据</code></pre><h2 id="简单的Token认证">简单的Token认证</h2><blockquote><p>通过API拿到数据，一般我们会开启认证，例如Token方式</p></blockquote><pre><code class="language-python">&quot;&quot;&quot;Token URL&quot;&quot;&quot;from login import views as loginviewsurlpatterns += [    path('api/gettoken/', loginviews.CustomAuthToken.as_view()), # 通过发起POST获取token，&#123;&quot;username&quot;: &quot;xxx&quot;, &quot;password&quot;:xxx&#125;]</code></pre><pre><code class="language-python">&quot;&quot;&quot;views&quot;&quot;&quot;from rest_framework.authtoken.views import ObtainAuthTokenfrom rest_framework.authtoken.models import Tokenfrom rest_framework.response import Responseclass CustomAuthToken(ObtainAuthToken):    #throttle_classes =     def post(self, request, *args, **kwargs):        serializer = self.serializer_class(data=request.data,                                        context=&#123;'request': request&#125;)        serializer.is_valid(raise_exception=True)        user = serializer.validated_data['user']        token, created = Token.objects.get_or_create(user=user)        return Response(&#123;            'token': token.key,            'user_id': user.pk,            'email': user.email        &#125;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞验证码</title>
      <link href="posts/d60c38c/"/>
      <url>posts/d60c38c/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://django-simple-captcha.readthedocs.io/en/latest/advanced.html#configuration-toggles">高级主题 — Django Simple Captcha 0.5.14 文档 (django-simple-captcha.readthedocs.io)</a></p><h2 id="安装">安装</h2><pre><code class="language-python">pip install django-simple-captcha</code></pre><p><code>settings.INSTALLED_APPS</code>加入<code>captcha</code></p><h2 id="配置">配置</h2><p><code>settings.py</code>末尾中加入以下代码：</p><pre><code class="language-python"># 验证码设置CAPTCHA_FIELD_TEMPLATE = &quot;captcha/field.html&quot; # 字段排列CAPTCHA_TEXT_FIELD_TEMPLATE = &quot;captcha/text_field.html&quot; # 文本输入框#CAPTCHA_HIDDEN_FIELD_TEMPLATE = &quot;captcha/hidden_field.html&quot; CAPTCHA_IMAGE_TEMPLATE = &quot;captcha/image.html&quot; # 验证码图标框CAPTCHA_CHALLENGE_FUNCT = 'captcha.helpers.random_char_challenge' # 验证码类型CAPTCHA_IMAGE_SIZE=(100,36) # 验证码尺寸CAPTCHA_TIMEOUT=1#CAPTCHA_OUTPUT_FORMAT=u'%(text_field)s %(hidden_field)s %(image)s'</code></pre><h2 id="具体">具体</h2><p>以<code>app:login</code>为例，在其根目录构建以下内容</p><ol><li>构建<code>templates/captcha</code>目录，并创建<code>field.html</code>、<code>text_field.html</code>、<code>image.html</code>模板</li></ol><pre><code class="language-bash">❯ cat field.html&#123;&#123; text_field &#125;&#125;&#123;&#123; hidden_field &#125;&#125;&#123;&#123; image &#125;&#125;❯ cat text_field.html&lt;input id=&quot;id_&#123;&#123; name &#125;&#125;_1&quot; name=&quot;&#123;&#123; name &#125;&#125;_1&quot; type=&quot;text&quot; placeholder=&quot;captcha&quot; /&gt;❯ cat image.html&lt;img src=&quot;&#123;&#123; image &#125;&#125;&quot; alt=&quot;&#123;&#123; name &#125;&#125;&quot; class=&quot;&#123;&#123; name &#125;&#125;&quot;/&gt;</code></pre><ol start="2"><li>构建<code>static/login/js</code>目录，并创建<code>captcha.js</code>，用于动态刷新验证码</li></ol><pre><code class="language-bash">❯ cat captcha.js$('img.captcha').click(function() &#123;  $.getJSON('/captcha/refresh/',function(json) &#123; // This should update your captcha image src and captcha hidden input      console.log(json);      $(&quot;img.captcha&quot;).attr(&quot;src&quot;,json.image_url);      $(&quot;#id_captcha_0&quot;).val(json.key);    &#125;);    return false;&#125;);</code></pre><ol start="3"><li>创建<code>form</code>表单调用<code>captcha</code></li></ol><pre><code>❯ cat forms.py&quot;&quot;&quot;导入表单模块&quot;&quot;&quot;from django import formsfrom captcha.fields import CaptchaField, CaptchaTextInputclass UserForm(forms.Form):    &quot;&quot;&quot;    用户表单    &quot;&quot;&quot;    username = forms.CharField(label='用户名', max_length=128, widget=forms.TextInput(attrs=&#123;'class': 'form-control'&#125;))    password = forms.CharField(label='密码', max_length=256, widget=forms.PasswordInput(attrs=&#123;'class': 'form-control'&#125;))    captcha = CaptchaField(label='验证码')</code></pre><ol start="4"><li>构建<code>templates/login</code>目录，并创建<code>login.html</code>模板调用<code>caphtcha</code></li></ol><pre><code class="language-html">&#123;% load static %&#125;&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;  &lt;head&gt;    &lt;!-- Required meta tags --&gt;    &lt;meta charset=&quot;utf-8&quot;&gt;    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, shrink-to-fit=no&quot;&gt;    &lt;!-- 上述meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt;    &lt;!-- Bootstrap CSS --&gt;    &lt;link href=&quot;&#123;% static 'login/css/login.css' %&#125;&quot; rel=&quot;stylesheet&quot; /&gt;    &lt;link href=&quot;&#123;% static 'fontawesome_free/css/all.min.css' %&#125;&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;    &lt;title&gt;登录&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;    &lt;div id=&quot;container&quot;&gt;      &lt;h1&gt;登录&lt;/h1&gt;      &#123;% if login_form.captcha.errors %&#125;        &#123;% for error in login_form.captcha.errors %&#125;        <small>😢😢😢&#123;&#123; error &#125;&#125;😢😢😢</small>        &#123;% endfor %&#125;      &#123;% elif message %&#125;        <small>😢😢😢&#123;&#123; message &#125;&#125;😢😢😢</small>      &#123;% else %&#125;        <small>😊😊😊欢迎您的到来😊😊😊</small>      &#123;% endif %&#125;      &lt;form class=&quot;form&quot; action=&quot;/login/&quot; method=&quot;post&quot;&gt;&#123;% csrf_token %&#125;        &lt;!-- 判断用户输入信息，并返回错误信息 --&gt;        &#123;% for item in login_form %&#125;          &#123;% if item.html_name == "username" %&#125;          <div class="item">            <i class="far fa-user"></i>            <input id="&#123;&#123; item.id_for_label &#125;&#125;" name="&#123;&#123; item.html_name &#125;&#125;" type="&#123;&#123; item.field.widget.input_type &#125;&#125;" placeholder="&#123;&#123; item.html_name &#125;&#125;">            <text>            &#123;% for error in item.errors %&#125;                &#123;&#123; error &#125;&#125;            &#123;% endfor %&#125;            &lt;/text&gt;          &lt;/div&gt;          &#123;% endif %&#125;          &#123;% if item.html_name == "password" %&#125;          <div class="item">            <i class="fas fa-key"></i>            <input id="&#123;&#123; item.id_for_label &#125;&#125;" name="&#123;&#123; item.html_name &#125;&#125;" type="&#123;&#123; item.field.widget.input_type &#125;&#125;" placeholder="&#123;&#123; item.html_name &#125;&#125;">            <text>            &#123;% for error in item.errors %&#125;                &#123;&#123; error &#125;&#125;            &#123;% endfor %&#125;            </text>          </div>          &#123;% endif %&#125;          &#123;% if item.html_name == "captcha" %&#125;          <div class="item">            <i class="fas fa-robot"></i>            &#123;&#123; login_form.captcha &#125;&#125;            </div>            &#123;% endif %&#125;        &#123;% endfor %&#125;        &lt;div class=&quot;item1&quot;&gt;          &lt;button type=&quot;submit&quot;&gt;Go~~~~&lt;/button&gt;          &lt;a href=&quot;/register/&quot;&gt;Register&lt;/a&gt;        &lt;/div&gt;      &lt;/form&gt;    &lt;/div&gt;    &lt;script src=&quot;https://cdn.bootcss.com/jquery/3.5.1/jquery.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;https://cdn.bootcss.com/popper.js/1.16.0/umd/popper.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;https://cdn.bootcss.com/twitter-bootstrap/4.5.3/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;&#123;% static 'login/js/captcha.js' %&#125;&quot;&gt;&lt;/script&gt;  &lt;/body&gt;&lt;/html&gt;</text></div></code></pre><ol start="5"><li>构建<code>static/login/css</code>目录，并创建<code>login.css</code></li></ol><pre><code class="language-bash">❯ cat login.cssbody &#123;background: url('../image/backend.jpg') center center no-repeat;background-attachment: fixed;background-size: cover;&#125;input::-webkit-input-placeholder&#123;    color:#000000;&#125;input::-moz-placeholder&#123;   /* Mozilla Firefox 19+ */    color:#000000;&#125;input:-moz-placeholder&#123;    /* Mozilla Firefox 4 to 18 */    color:#000000;&#125;input:-ms-input-placeholder&#123;  /* Internet Explorer 10-11 */    color:#000000;&#125;#container &#123;    width: 35%;    height: 400px;    margin: 0 auto;    margin-top: 15%;    padding: 20px 50px;    text-align: center;    background: #ffffff50;&#125;#container .alert &#123;    margin: 0 auto;    width: 350px;    text-align: left;&#125;#container .form &#123;    margin-top: 30px;&#125;#container .form .item &#123;    margin-top: 15px;    position: relative;&#125;#container .form .item i &#123;    font-size: 20px;&#125;#container .form .item input &#123;    border: 0;    border-bottom: 2px solid #000;    padding: 10px 30px;    background: #ffffff00;    font-size: 20px;    padding-left: 11px;&#125;#container .form .item text &#123;    position: absolute;    text-size: 5px;    text-align: left;    padding-top: 10px;&#125;#container .form .item img &#123;    position: absolute;&#125;#container .form .item1 &#123;    display: flex;    justify-content: space-between;    margin-top: 15px;    padding-top: 5%;    padding-left: 22%;    padding-right: 22%;&#125;#container .form .item1 button &#123;    width: 180px;    height: 30px;    font-size: 20px;    font-weight: 600;    color: #ffffff;    background-image: linear-gradient(60deg, #64b3f4 0%, #c2e59c 100%);    border-radius: 15px;    border: 0;&#125;#container .form .item1 a &#123;    text-decoration:none;    width: 100px;    font-size: 20px;    font-weight: 600;    color: #ffffff;    background-image: linear-gradient(60deg, #64b3f4 0%, #c2e59c 100%);    border-radius: 15px;    border: 0;&#125;</code></pre><ol start="6"><li>添加路由</li></ol><p><code>项目.urls.urlpatterns</code>中添加<code>path('captcha/', include('captcha.urls')),</code></p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python☞json</title>
      <link href="posts/876c5852/"/>
      <url>posts/876c5852/</url>
      
        <content type="html"><![CDATA[<h2 id="Json与python之间类型关系">Json与python之间类型关系</h2><table><thead><tr><th><strong>JSON类型</strong></th><th><strong>python类型</strong></th></tr></thead><tbody><tr><td><strong>{}</strong></td><td><strong>dict</strong></td></tr><tr><td><strong>[]</strong></td><td><strong>list</strong></td></tr><tr><td><strong>“string”</strong></td><td><strong>str</strong></td></tr><tr><td><strong>“123456”</strong></td><td><strong>int或float</strong></td></tr><tr><td><strong>true/false</strong></td><td><strong>True/False</strong></td></tr><tr><td><strong>null</strong></td><td><strong>None</strong></td></tr></tbody></table><h2 id="序列化">序列化</h2><pre><code class="language-python">import jsonjson.dumps(python_dict)  # 将py类型转为json字符串类型</code></pre><h2 id="反序列化">反序列化</h2><pre><code class="language-python">import jsonjson.loads(json_str)  # 将json字符串类型转为py类型</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> json </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞时间</title>
      <link href="posts/cd32aa48/"/>
      <url>posts/cd32aa48/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>本文中描述的并不能涵盖所有时间设置命令，只是记录了经常用到的一些。</p><h2 id="chrony-时间同步">chrony 时间同步</h2><pre><code>sudo yum erase ntp*sudo yum -y install chronysed -i '1i server time1.aliyun.com prefer iburst' /etc/chrony.confsudo service chronyd start</code></pre><blockquote><p><a href="http://time1.aliyun.com">time1.aliyun.com</a> 替换成ntp服务器，<a href="http://time01.aliyun.com">time01.aliyun.com</a> 是阿里云提供的公共ntp</p></blockquote><h2 id="timedatectl-命令修改时区">timedatectl 命令修改时区</h2><p>如果你用的是7以上的centos或者ubuntu</p><p>那么可以用 timedatectl 命令来设置时区</p><p>timedatectl --list-timezones</p><p>timedatectl --set-timezone Asia/Shanghai</p><hr><h2 id="没有timedatectl命令">没有timedatectl命令</h2><h3 id="修改会话时区">修改会话时区</h3><pre><code class="language-bash">echo &quot;TZ='UTC+0'; export TZ&quot; &gt;&gt; ~/.bash_profile</code></pre><blockquote><p>需要注意以下几点：</p><ol><li>UTC8 表示西8区</li><li>tzselect 可以帮你查看时区有哪些</li><li>UTC 方式，无法识别冬令时和夏令时，所以建议用地区名称，例如 asia/shanghai</li></ol></blockquote><h3 id="修改crontab时区">修改crontab时区</h3><p>在 crontab 用户配置最上面加入，例如添加芝加哥时区</p><pre><code class="language-bash">TZ='America/Chicago'CRON_TZ='America/Chicago'</code></pre><blockquote><p>关于时区设置方面，不建议修改配置，因为不够灵活</p></blockquote><h3 id="修正时间-写入硬件时钟">修正时间, 写入硬件时钟</h3><pre><code class="language-bash">yum install ntp -yntpdate cn.ntp.org.cnhwclock -wecho '0 12 * * * /usr/sbin/ntpdate cn.ntp.org.cn &gt; /dev/null 2&gt;&amp;1' &gt;&gt; /etc/crontab</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 时区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rabbitmq-集群简要</title>
      <link href="posts/188d54be/"/>
      <url>posts/188d54be/</url>
      
        <content type="html"><![CDATA[<h2 id="引用">引用</h2><p><a href="https://www.rabbitmq.com/clustering.html#cluster-formation">https://www.rabbitmq.com/clustering.html#cluster-formation</a></p><h2 id="节点数量">节点数量</h2><p>奇数，最少3个，不建议超过7个。</p><h2 id="节点名">节点名</h2><p>节点启动的时候，rabbitmq通过<code>RABBITMQ_NODENAME</code>来获取节点名，如果没有获取到，则通过自动生成<code>rabbit@hostname</code>的节点名</p><p>💥如果<code>hostname</code>是FQDN，则rabbitmq需要将环境变量<code>RABBITMQ_USE_LONGNAME</code>设定为<code>true</code>。</p><p>如果是CLI，则也可以通过<code>--longnames</code>来设定。</p><p>配置文件~/.erlang.cookie确保信息一致。</p><h3 id="节点名解析检查">节点名解析检查</h3><p><a href="https://www.rabbitmq.com/networking.html#dns-verify-resolution">https://www.rabbitmq.com/networking.html#dns-verify-resolution</a></p><p>将node2.cluster.local.svc解析为ipv4</p><pre><code class="language-bash">rabbitmq-diagnostics resolve_hostname node2.cluster.local.svc --address-family IPv4 --offline</code></pre><h2 id="数据复制">数据复制</h2><p>虚拟主机、交换器、用户和权限会<a href="https://www.rabbitmq.com/clustering.html#cluster-membership">自动复制</a>到集群中的所有节点都在节点之间复制。但默认情况下，消息队列只会驻留在一个节点上，不过其它节点依然可以访问这个消息队列。</p><h2 id="共享密钥">共享密钥</h2><p>集群之间以及客户端访问通过共享密钥<code>erlang.cookie</code>来确定是否允许互相访问。共享密钥文件权限为<code>600</code></p><p>✨不通的部署方式，其密钥文件的路径不一样。</p><h3 id="通过包管理器部署">通过包管理器部署</h3><p>通常位于/var/lib/rabbitmq/.erlang.cookie（由服务器使用）和$HOME/.erlang.cookie（由 CLI 工具使用）</p><h3 id="通过docker">通过docker</h3><p>使用环境变量<code>RABBITMQ_ERLANG_COOKIE</code>定义</p><h3 id="通过kubernetes">通过kubernetes</h3><p>暂无</p><h3 id="认证失败">认证失败</h3><p>以下信息均为cookie认证失败</p><pre><code>Connection attempt from node 'rabbitmqcli-99391-rabbit@warp10' rejected. Invalid challenge reply</code></pre><pre><code>* epmd reports node 'rabbit' running on port 25672* TCP connection succeeded but Erlang distribution failed* suggestion: hostname mismatch?* suggestion: is the cookie set correctly?* suggestion: is the Erlang distribution using TLS?</code></pre><pre><code class="language-bash">* connected to epmd (port 4369) on warp10* epmd reports node 'rabbit' running on port 25672* TCP connection succeeded but Erlang distribution failed* Authentication failed (rejected by the remote node), please check the Erlang cookie</code></pre><h3 id="检查cookie信息">检查cookie信息</h3><pre><code class="language-bash">-&gt; rabbitmq-diagnostics erlang_cookie_sourcesCookie FileEffective user: antaresEffective home directory: /home/cli-userCookie file path: /home/cli-user/.erlang.cookieCookie file exists? trueCookie file type: regularCookie file access: readCookie file size: 20Cookie CLI Switch--erlang-cookie value set? false--erlang-cookie value length: 0Env variable  (Deprecated)RABBITMQ_ERLANG_COOKIE value set? falseRABBITMQ_ERLANG_COOKIE value length: 0</code></pre><h2 id="节点类型">节点类型</h2><p>节点之间是平等的，没有领导者和追随者。</p><h2 id="客户端访问">客户端访问</h2><p>大多数客户端库都应该接受端点列表（主机名或 IP 地址）作为连接选项。因为节点是平等的，所以客户端可以任意访问端点列表里的节点，而节点会将操作路由到<a href="https://www.rabbitmq.com/quorum-queues.html">仲裁队列领导者</a>或<a href="https://www.rabbitmq.com/ha.html#leader-migration-data-locality">队列领导者副本</a>。这是对客户端透明的。</p><p>关于这个客户端透明，在经典镜像队列中，某些情况下不生效。</p><h2 id="指令范围">指令范围</h2><p>在集群中，一些非特意返回本地数据的指令均会返回集群范围的数据。例如：<br><code>rabbitmqctl list_connections</code>、<code>rabbitmqctl list_mqtt_connections</code>、<code>rabbitmqctl list_stomp_connections</code>、<code>rabbitmqctl list_users</code>、 <code>rabbitmqctl list_vhosts</code></p><p>而<code>rabbitmq-diagnostics environment</code>和<code>rabbitmq-diagnostics status</code>则会返回当前节点的数据，毕竟主机名是不一致的。</p><p>因此，以一些监控API为例，无需向每一个节点发出请求，接收http api请求的节点会将请求散布到其它节点并最终将响应数据聚合在一起。</p><p>如果多个节点均启用了管理插件，则管理员可以通过任意节点访问管理UI。</p><h2 id="三节点集群部署">三节点集群部署</h2><ol><li>正常方式启动三节点服务</li></ol><pre><code class="language-bash"># on rabbit1rabbitmq-server -detached# on rabbit2rabbitmq-server -detached# on rabbit3rabbitmq-server -detached</code></pre><ol start="2"><li>节点2加入</li></ol><p>💥任何节点加入之前，都需要先reset</p><pre><code class="language-bash"># on rabbit2rabbitmqctl stop_app# =&gt; Stopping node rabbit@rabbit2 ...done.rabbitmqctl reset# =&gt; Resetting node rabbit@rabbit2 ...rabbitmqctl join_cluster rabbit@rabbit1# =&gt; Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...done.rabbitmqctl start_app# =&gt; Starting node rabbit@rabbit2 ...done.</code></pre><ol start="3"><li>节点3加入</li></ol><pre><code class="language-bash"># on rabbit3rabbitmqctl stop_app# =&gt; Stopping node rabbit@rabbit3 ...done.# on rabbit3rabbitmqctl reset# =&gt; Resetting node rabbit@rabbit3 ...rabbitmqctl join_cluster rabbit@rabbit2# =&gt; Clustering node rabbit@rabbit3 with rabbit@rabbit2 ...done.rabbitmqctl start_app# =&gt; Starting node rabbit@rabbit3 ...done.</code></pre>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> rabbitmq </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> rabbitmq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rabbitmq☞简要安装</title>
      <link href="posts/b43013ef/"/>
      <url>posts/b43013ef/</url>
      
        <content type="html"><![CDATA[<h2 id="docker方式">docker方式</h2><pre><code class="language-bash">mkdir -p /export/docker-data-rabbitmq/&#123;data,log,conf.d&#125;chown -R 999:999 /export/docker-data-rabbitmq/&#123;data,log,conf.d&#125;</code></pre><p>✨<code>999</code>是rabbitmq官方镜像里的<code>rabbitmq</code>的<code>uid</code></p><h3 id="添加额外的日志配置">添加额外的日志配置</h3><p>/export/docker-data-rabbitmq/conf.d/log.conf</p><pre><code>#log.file = rabbit.loglog.dir = /var/log/rabbitmqlog.file.level = info# rotate every night at midnightlog.file.rotation.date = $D0# keep up to 5 archived log files in addition to the current onelog.file.rotation.count = 5</code></pre><pre><code class="language-bash"># https://hub.docker.com/_/rabbitmq, 然后搜索 management 版本 [docker pull rabbitmq:management]pwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`echo mq@$&#123;pwd&#125;;container_name=rabbitmqnetwork=cmsdocker run --name $&#123;container_name&#125; \--hostname $&#123;container_name&#125; \--network $&#123;network&#125; \--restart always \-p 5672:5672 -p 15672:15672 -p 15692:15692 \--cpus=1 \--memory=1G --memory-swap=1G \--ulimit nofile=204800 \--mount &quot;type=bind,src=/export/docker-data-rabbitmq/data,dst=/var/lib/rabbitmq&quot; \--mount &quot;type=bind,src=/export/docker-data-rabbitmq/conf.d,dst=/etc/rabbitmq/conf.d&quot; \--mount &quot;type=bind,src=/export/docker-data-rabbitmq/log,dst=/var/log/rabbitmq&quot; \-e RABBITMQ_DEFAULT_VHOST=rabbitmq \-e RABBITMQ_DEFAULT_USER=admin \-e RABBITMQ_DEFAULT_PASS=mq@$&#123;pwd&#125; \-e RABBITMQ_LOGS=rabbitmq.log \-d rabbitmq:3.8.13-management</code></pre><p>需要注意的是，docker方式，<code>RABBITMQ_LOGS</code>默认值是<code>-</code>也就是输出到<code>stdout</code>, 这里我改成了具体文件</p><p>💥任何配置，环境变量的优先级大于文本配置</p><p>安装logrotate，rabbitmq的日志轮转依托它</p><pre><code class="language-bash">docker exec -it rabbitmq /bin/bashapt updateapt install logrotateexit</code></pre><p>重启 rabbitmq</p><pre><code class="language-bash">docker restart rabbitmq</code></pre><h2 id="rpm方式-过时">rpm方式(过时)</h2><pre><code># 安装 erlang 的 rabbitmq 兼容版方式 (卸载命令 yum erase erlang-*)yum install -y https://github.com/rabbitmq/erlang-rpm/releases/download/v22.0.7/erlang-22.0.7-1.el6.x86_64.rpm# 安装 rabbitmq-serveryum install -y https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.15/rabbitmq-server-3.7.15-1.el6.noarch.rpm;## 开启管理rabbitmq-plugins enable rabbitmq_management;# 修改一些小配置vim /etc/rabbitmq/rabbitmq.conflog.file.level = warninglog.file = rabbitmq.logvim /etc/rabbitmq/rabbitmq-env.confRABBITMQ_LOG_BASE=/export/rabbitmq/logsNODENAME=rabbit@&lt;当前的hostname&gt;mkdir -p /export/rabbitmq/logs &amp;&amp; chown rabbitmq:rabbitmq /export/rabbitmq/logschown rabbitmq.rabbitmq /var/lib/rabbitmq/.erlang.cookie;echo '* soft nofile 202400' &gt;&gt; /etc/security/limits.confecho '* hard nofile 202400' &gt;&gt; /etc/security/limits.conf#/etc/init.d/rabbitmq-server restart;systemctl restart rabbitmq-server</code></pre><h2 id="Prometheus-监控">Prometheus 监控</h2><p>15692 端口是 prometheus 采集器的暴露端口，docker方式默认采集插件已经开启。</p><p>若 prometheus 采集器插件没有启动，可以进容器里通过 rabbitmq-plugins enable rabbitmq_prometheus 启动</p><p>宿主机里执行 <code>curl -s localhost:15692/metrics | head -n 3</code>确保可以返回采集器数据。</p><p>prometheus的告警规则，可以访问 <a href="https://awesome-prometheus-alerts.grep.to/rules#rabbitmq">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a> 获取</p><p>grafana的模板，可以访问 <a href="https://grafana.com/grafana/dashboards/10991">RabbitMQ-Overview dashboard for Grafana | Grafana Labs</a></p><h2 id="一些基本命令">一些基本命令</h2><h3 id="添加用户和虚拟主机">添加用户和虚拟主机</h3><pre><code class="language-bash">#输入http://ip:15672可以登录管理界面,# rpm 安装后，只有默认账户guest/guest.# docker 安装后，有 admin/admin 用户# 添加一个 vhostpro_vhost=rabbitmqctl add_vhost $&#123;pro_vhost&#125;# 添加一个新用户user_name=pwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`rabbitmqctl add_user $&#123;user_name&#125; mq@$&#123;pwd&#125;;echo &quot;$&#123;user_name&#125;用户密码是：mq@$&#123;pwd&#125;&quot;#该命令使用户 user_name 具有 pro_vhost 中所有资源的配置、写、读权限以便管理其中的资源# rabbitmqctl set_permissions -p vhost User ConfP WriteP ReadPrabbitmqctl set_permissions -p $&#123;pro_vhost&#125; $&#123;user_name&#125; &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;; # 用户绑定 tags# tags决定了用户更高级的权限，## 超级管理员(administrator)：可登陆管理控制台(启用management plugin的情况下)，可查看所有的信息，并且可以对用户，策略(policy)进行操作。## 监控者(monitoring)：可登陆管理控制台(启用management plugin的情况下)，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)## 策略制定者(policymaker)：可登陆管理控制台(启用management plugin的情况下), 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)## 普通管理者(management)：仅可登陆管理控制台(启用management plugin的情况下)，无法看到节点信息，也无法对策略进行管理。## 其他(other)：无法登陆管理控制台，通常就是普通的生产者和消费者。user_tag=administratorrabbitmqctl set_user_tags $&#123;user_name&#125; $&#123;user_tag&#125;;#查看所有用户rabbitmqctl list_users;</code></pre><p>👙超管用户如果不添加对某个vhost的权限，则无法直接对这个vhost下的队列进行删除。</p><h3 id="状态">状态</h3><pre><code class="language-bash">rabbitmqctl statusrabbitmqctl list_queues -p &lt;vhost_name&gt;</code></pre><h3 id="队列">队列</h3><pre><code class="language-bash">rabbitmqctl list_queues --vhost &lt;&gt;</code></pre><h2 id="客户端调用">客户端调用</h2><h3 id="php">php</h3><pre><code class="language-bash">wget https://github.com/alanxz/rabbitmq-c/releases/download/v0.7.1/rabbitmq-c-0.7.1.tar.gz;tar xf rabbitmq-c-0.7.1.tar.gz;cd rabbitmq-c-0.7.1;./configure --prefix=/usr/local/rabbitmq-c-0.7.1;make &amp;&amp; make install;cd ..;wget https://pecl.php.net/get/amqp-1.9.3.tgz;tar xf amqp-1.9.3.tgz;cd amqp-1.9.3;/usr/local/php/bin/phpize;./configure --with-php-config=/usr/local/php/bin/php-config --with-amqp --with-librabbitmq-dir=/usr/local/rabbitmq-c-0.7.1;make &amp;&amp; make install;php.ini里添加生成的amqp.so的路径信息extension = /usr/local/php/lib/php/extensions/no-debug-non-zts-20160303/amqp.so</code></pre>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> rabbitmq </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> rabbitmq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志-06Elasticsearch索引生命周期</title>
      <link href="posts/13e95dda/"/>
      <url>posts/13e95dda/</url>
      
        <content type="html"><![CDATA[<h1>引用</h1><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html</a></p><p><a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-ilm">https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-ilm</a></p><h1>涉及的概念</h1><ol><li>索引生命周期</li><li>索引模板</li><li>索引别名</li></ol><h1>索引生命周期</h1><table><thead><tr><th><strong>phases.hot.min_age</strong></th><th>设置索引进入hot阶段所需的时间。</th></tr></thead><tbody><tr><td><strong>phases.hot.actions.set_priority.priority</strong></td><td>设置hot阶段索引的优先级。</td></tr><tr><td><strong>phases.warm.min_age</strong></td><td>设置索引进入warm阶段所需的时间。</td></tr><tr><td><strong>phases.warm.actions.allocate.number_of_replicas</strong></td><td>设置warm阶段索引的副本数。</td></tr><tr><td><strong>phases.warm.actions.allocate.require.box_type</strong></td><td>设置warm阶段索引分片分配的策略。例如将分片分配到warm节点。</td></tr><tr><td><strong>phases.warm.actions.set_priority.priority</strong></td><td>设置warm阶段索引的优先级。</td></tr><tr><td><strong>phases.cold.min_age</strong></td><td>设置索引进入cold阶段所需的时间。</td></tr><tr><td><strong>phases.cold.actions.set_priority.priority</strong></td><td>设置cold阶段索引的优先级。</td></tr></tbody></table><pre><code class="language-bash">PUT /_ilm/policy/&lt;索引生命周期名&gt;&#123;    &quot;policy&quot;: &#123;        &quot;phases&quot;: &#123;            &quot;hot&quot;: &#123;                &quot;actions&quot;: &#123;                    &quot;rollover&quot;: &#123;                        &quot;max_age&quot;: &quot;1d&quot;,                        &quot;max_size&quot;: &quot;1gb&quot;,                        &quot;max_docs&quot;: 10000                    &#125;,                    &quot;set_priority&quot;: &#123;                        &quot;priority&quot;: 100                    &#125;                &#125;            &#125;,            &quot;warm&quot;: &#123;                &quot;min_age&quot;: &quot;7d&quot;,                &quot;actions&quot;: &#123;                    &quot;allocate&quot;: &#123;                      &quot;require&quot;: &#123;                        &quot;box_type&quot;: &quot;warm&quot;                    &#125;,                    &quot;forcemerge&quot;: &#123;                        &quot;max_num_segments&quot;: 1                    &#125;,                    &quot;shrink&quot;: &#123;                        &quot;number_of_shards&quot;: 1                    &#125;,                    &quot;set_priority&quot;: &#123;                        &quot;priority&quot;: 50                    &#125;                &#125;            &#125;,            &quot;cold&quot;: &#123;                &quot;min_age&quot;: &quot;30d&quot;,                &quot;actions&quot;: &#123;                    &quot;allocate&quot;: &#123;                      &quot;require&quot;: &#123;                        &quot;box_type&quot;: &quot;cold&quot;                      &#125;                    &#125;,                    &quot;set_priority&quot;: &#123;                        &quot;priority&quot;: 0                    &#125;                &#125;            &#125;,            &quot;delete&quot;: &#123;                &quot;min_age&quot;: &quot;90d&quot;,                &quot;actions&quot;: &#123;                    &quot;delete&quot;: &#123;&#125;                &#125;            &#125;        &#125;    &#125;&#125;</code></pre><p>上面 json 的配置意思如下：</p><p>假设初始索引是<code>-000001</code>，它当前位于 hot 阶段。（所有索引初始的时候都是 hot 阶段，因为<strong>phases.hot.min_age</strong>默认是0）</p><p>hot 阶段下，<code>-000001</code>满足下列任一条件，滚动生成新索引<code>-000002</code>：</p><ul><li>存活1天</li><li>索引大于1GB</li><li>索引大于10000行</li></ul><hr><p>滚动更新后，当<code>-000001</code></p><ul><li><p>存活 7 天，就会进入warm 阶段，并开始强制合并和压缩，以及将<code>-000001</code>转移到 <code>warm</code> 节点上。</p><p>​🚨需要先存在<code>warm</code>角色的节点</p></li><li><p>存活 30 天，就会进入 cold 阶段，并将<code>-000001</code>转移到<code>cold</code>节点上。</p><p>​🚨需要先存在<code>cold</code>角色的节点</p></li><li><p>存活 90 天，就会进入 delete 阶段，将<code>-000001</code>索引删除。</p></li></ul><h1>索引模板</h1><table><thead><tr><th>index_patterns</th><th>匹配索引名的正则，凡是满足这个值的索引都会附加索引模板定义的属性</th></tr></thead><tbody><tr><td>number_of_shards</td><td>分片数</td></tr><tr><td><a href="http://index.lifecycle.name">index.lifecycle.name</a></td><td>生命周期名</td></tr><tr><td>index.lifecycle.rollover_alias</td><td>生命周期滚动更新的时候，需要的索引别名</td></tr></tbody></table><pre><code class="language-bash">PUT _template/&lt;索引模板名&gt;&#123;  &quot;index_patterns&quot;: [&quot;app-prod-*&quot;],  &quot;settings&quot;: &#123;    &quot;number_of_shards&quot;: 1,    &quot;index.lifecycle.name&quot;: &quot;app-prod&quot;,     &quot;index.lifecycle.rollover_alias&quot;: &quot;app-prod&quot;  &#125;&#125;</code></pre><h1>索引别名</h1><p>生命周期滚动更新必须的东西。</p><p>因为要支持生命周期滚动更新，则索引初始名必须是xxx-000001</p><pre><code class="language-bash">PUT /&lt;索引名&gt;/_alias/&lt;索引别名&gt;</code></pre><h1>logstash</h1><pre><code class="language-bash">    output &#123;      elasticsearch &#123;        ilm_rollover_alias =&gt; &quot;app-prod&quot;        ilm_pattern =&gt; &quot;&#123;now/d&#125;-00001&quot;        ilm_policy =&gt; &quot;app-prod&quot;      &#125;    &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05ELK基本使用-kibana</title>
      <link href="posts/d8390732/"/>
      <url>posts/d8390732/</url>
      
        <content type="html"><![CDATA[<h2 id="安装">安装</h2><p><a href="https://www.elastic.co/guide/en/kibana/current/targz.html">https://www.elastic.co/guide/en/kibana/current/targz.html</a></p><pre><code class="language-bash">appName=kibanaesVersion=7.15.1cd /export/srccurl &quot;https://mirrors.huaweicloud.com/$&#123;appName&#125;/$&#123;esVersion&#125;/$&#123;appName&#125;-$&#123;esVersion&#125;-linux-x86_64.tar.gz&quot; -o $&#123;appName&#125;.tgztar xf $&#123;appName&#125;.tgzmv $&#123;appName&#125;-$&#123;esVersion&#125;-linux-x86_64 ../$&#123;appName&#125; &amp;&amp; cd ..useradd -U $&#123;appName&#125;chown -R $&#123;appName&#125;:$&#123;appName&#125; /export/$&#123;appName&#125;cd /export/$&#123;appName&#125;</code></pre><h2 id="配置">配置</h2><p><a href="https://www.elastic.co/guide/en/kibana/current/settings.html">https://www.elastic.co/guide/en/kibana/current/settings.html</a></p><pre><code class="language-yaml">cp /export/kibana/config/kibana.yml /export/kibana/config/kibana.yml.defaultcat &gt; /export/kibana/config/kibana.yml &lt;&lt; EOFserver.name: kibanaserver.host: 0.0.0.0server.port: 5601elasticsearch.hosts: [&quot;http://hadoop001:9200&quot;,&quot;http://hadoop002:9200&quot;,&quot;http://hadoop003:9200&quot;]xpack.monitoring.ui.container.elasticsearch.enabled: truei18n.locale: zh-CNEOF</code></pre><pre><code class="language-bash">bin/kibana-encryption-keys generate# 将输出的 `Settings` 配置追加到 kibana.ymlxpack.encryptedSavedObjects.encryptionKey: 40233b82c57c14127363xpack.reporting.encryptionKey: f5550ba4b241b820bxpack.security.encryptionKey: 9732d0b256555b</code></pre><h2 id="系统用户-可选">系统用户(可选)</h2><p>仅当ES开启安全配置后需要.</p><p>追加下列配置到<code>kibana.yml</code>，kibana通过kibana_system用户访问elasticsearch执行后台任务.</p><pre><code class="language-bash">elasticsearch.username: &quot;kibana_system&quot;</code></pre><p>执行下列命令,将密码加密存放.</p><pre><code class="language-bash">./bin/kibana-keystore create./bin/kibana-keystore add elasticsearch.password=&gt;输入ES安全配置期间，生成的`kibana_system`内置用户密码</code></pre><h2 id="启动">启动</h2><pre><code class="language-bash">su - kibanacd /export/kibananohup ./bin/kibana &amp;</code></pre><p>ℹ️如果启用安全设置和用户配置，则kibana的web访问需要<code>elastic</code>用户权限.</p><h2 id="用户权限管理">用户权限管理</h2><p><a href="https://www.elastic.co/guide/en/kibana/7.15/tutorial-secure-access-to-kibana.html">https://www.elastic.co/guide/en/kibana/7.15/tutorial-secure-access-to-kibana.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04ELK基本使用-elasticsearch</title>
      <link href="posts/41bf4066/"/>
      <url>posts/41bf4066/</url>
      
        <content type="html"><![CDATA[<h2 id="准备">准备</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.15/system-config.html">https://www.elastic.co/guide/en/elasticsearch/reference/7.15/system-config.html</a></p><pre><code class="language-bash"># elasticsearch 具体安装命令和版本请以下载页中对应的docker安装方式页里命令为基准sysctl -a | egrep &quot;(vm.max_map_count|net.ipv4.tcp_retries2)&quot;  # 查看是否过小, 如果过小执行下一条echo 'vm.max_map_count=262144' &gt;&gt; /etc/sysctl.confecho 'net.ipv4.tcp_retries2=5' &gt;&gt; /etc/sysctl.confsysctl -p## https://www.elastic.co/guide/en/elasticsearch/reference/7.15/setting-system-settings.htmlcat &gt;&gt; /etc/security/limits.conf &lt;&lt; EOF  *           soft   nofile       102400  *           hard   nofile       102400  *           soft   nproc        102400  *           hard   nproc        102400  *           soft  memlock      unlimited  *           hard  memlock      unlimitedEOF## 关闭swap 同时应该在 /etc/fstab 中取消 swap 的挂载sudo swapoff -a</code></pre><h2 id="安装">安装</h2><p><a href="https://mirrors.huaweicloud.com/elasticsearch/">https://mirrors.huaweicloud.com/elasticsearch/</a></p><pre><code>esVersion=7.15.1cd /export/srccurl &quot;https://mirrors.huaweicloud.com/elasticsearch/$&#123;esVersion&#125;/elasticsearch-$&#123;esVersion&#125;-linux-x86_64.tar.gz&quot; -o es.tgztar xf es.tgzmv elasticsearch-$&#123;esVersion&#125; ../elasticsearchuseradd -U elasticsearchchown -R elasticsearch:elasticsearch /export/elasticsearchsu - elasticsearchcd /export/elasticsearch</code></pre><h2 id="配置">配置</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.15/important-settings.html">https://www.elastic.co/guide/en/elasticsearch/reference/7.15/important-settings.html</a></p><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.15/configuring-stack-security.html">https://www.elastic.co/guide/en/elasticsearch/reference/7.15/configuring-stack-security.html</a></p><h3 id="角色">角色</h3><p>角色介绍的官方文档：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#node-roles">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#node-roles</a></p><p>elasticsearch 可以通过<code>node.roles: [xxx]</code>来设定节点角色。如果不设置<code>node.roles</code>，则默认节点拥有所有角色。</p><p>角色可分为：master、data和data_xxx系列、ingest、ml、remote_cluster_client、transform。</p><p>💢在一个集群中，master和data是必须的角色。</p><h3 id="一份通用节点配置">一份通用节点配置</h3><p>配置中<code>discovery.seed_hosts</code>替换成所有节点的hostname</p><pre><code class="language-bash">cp config/elasticsearch.yml config/elasticsearch.yml.defaulthostName=`hostname`cat &gt; /export/elasticsearch/config/elasticsearch.yml &lt;&lt; EOFcluster.name: es-clusterdiscovery.seed_hosts: xxx1,xxx2,xxx3node.name: $&#123;hostName&#125;#node.master: true#node.data: falsehttp.port: 9200network.host: 0.0.0.0http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;bootstrap.memory_lock: truexpack.security.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.client_authentication: requiredxpack.security.transport.ssl.keystore.path: elastic-certificates.p12xpack.security.transport.ssl.truststore.path: elastic-certificates.p12resource.reload.interval.high: 5sEOF</code></pre><p>ℹ️<code>xpack.security.transport.ssl.keystore.path、xpack.security.transport.ssl.truststore.path</code>指定的文件名与下面启用TLS中的默认文件名一致。</p><p>💁 如果是单节点，则应该加入<code>discovery.type: single-node</code>，并删除<code>discovery.seed_hosts</code>。</p><h2 id="启用TLS">启用TLS</h2><p>当启用了<code>xpack.security.enabled: true</code>后，多节点生产模式集群必须启用TLS，否则无法启动集群。</p><p>创建CA证书和节点证书</p><pre><code class="language-bash">./bin/elasticsearch-certutil ca=&gt;回车，接受默认文件名 elastic-stack-ca.p12=&gt;输入CA证书密码./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12=&gt;输入CA证书密码=&gt;回车，接受默认文件名 elastic-certificates.p12=&gt;输入节点证书密码（可选）</code></pre><p>将生成的<code>elastic-certificates.p12</code>复制到每一个节点的<code>config</code>目录中，并确保授权<code>600</code></p><p>💔我添加了节点证书密码后，ES校验证书失败，不知原因。</p><p>💢如果节点证书开启了密码，则需要存在ES中，通过下列命令存储，会存放在<code>config/elasticsearch.keystore</code></p><pre><code class="language-bash">./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password=&gt;输入节点证书密码./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password=&gt;输入节点证书密码</code></pre><h2 id="启动、关闭">启动、关闭</h2><h3 id="启动">启动</h3><p>ES默认会自动调整JVM参数，非必要情况下无需调整.</p><p>命令中<code>-Ecluster.initial_master_nodes</code>替换成所有节点的hostname</p><pre><code class="language-bash">## export ES_JAVA_OPTS=&quot;$ES_JAVA_OPTS xxx&quot;export ES_TMPDIR=&quot;/export/elasticsearch/tmp&quot;mkdir -p /export/elasticsearch/tmpsu - elasticsearchcd /export/elasticsearch &amp;&amp; ./bin/elasticsearch -d -p es.pid -Ecluster.initial_master_nodes=data01,data02,data03</code></pre><p>💢<code>-Ecluster.initial_master_nodes=data01,data02,data03</code> 仅当首次启动的时候需要添加。当集群成功启动后，应删除此配置。重启集群、新节点入集群都不应该加入此配置。</p><h3 id="关闭">关闭</h3><pre><code class="language-bash">su - elasticsearchcd /export/elasticsearch &amp;&amp; pkill -F es.pid</code></pre><h2 id="内置用户密码生成">内置用户密码生成</h2><p>通过设置用户密码后，kibana就需要配置<code>elastic</code>账户来访问web。</p><p><code>elastic</code>是超级用户；</p><p><code>logstash_system</code> 是一个监控用户，仅用于 Elastic Stack 监控功能监控 Logstash 实例，并将监控数据存储在安全的 Elasticsearch 集群中，不可用于logstash.output。</p><p>✨关于logstash.output去操作ES所需的索引写权限，可以通过 <a href="https://www.elastic.co/guide/en/logstash/current/ls-security.html">https://www.elastic.co/guide/en/logstash/current/ls-security.html</a> 文档的介绍创建。创建方式可以通过ES API或者kibana控制台。</p><p>关于内置用户对应的权限，可以看相关内置角色的权限范围：</p><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.15/built-in-roles.html">https://www.elastic.co/guide/en/elasticsearch/reference/7.15/built-in-roles.html</a></p><p>创建命令如下：</p><pre><code class="language-bash">./bin/elasticsearch-setup-passwords autoChanged password for user apm_systemPASSWORD apm_system = va40HqefLmamecrNSUuvChanged password for user kibana_systemPASSWORD kibana_system = 34hem76rnIB1XFv6oWGpChanged password for user kibanaPASSWORD kibana = 34hem76rnIB1XFv6oWGpChanged password for user logstash_systemPASSWORD logstash_system = KEx2vLvyUu4cY5KOK95sChanged password for user beats_systemPASSWORD beats_system = 86ZVXoxinMESc5iR9vyAChanged password for user remote_monitoring_userPASSWORD remote_monitoring_user = yFTGkenceHBnZFOS6rX5Changed password for user elasticPASSWORD elastic = Fb5wzMz7UJTCJJNipJfz</code></pre><p>✨设置了<code>elastic</code>用户密码后，你将无法再次运行<code>bin/elasticsearch-setup-passwords</code>命令进行设置。</p><h2 id="校验集群状态">校验集群状态</h2><p>使用elastic用户访问健康状态接口。</p><pre><code class="language-bash">curl --user elastic:Fb5wzMz7UJTCJJNipJfz -XGET 'http://data01:9200/_cluster/health?pretty'&#123;  &quot;cluster_name&quot; : &quot;es-cluster&quot;,  &quot;status&quot; : &quot;green&quot;,  &quot;timed_out&quot; : false,  &quot;number_of_nodes&quot; : 3,  &quot;number_of_data_nodes&quot; : 3,  &quot;active_primary_shards&quot; : 2,  &quot;active_shards&quot; : 4,  &quot;relocating_shards&quot; : 0,  &quot;initializing_shards&quot; : 0,  &quot;unassigned_shards&quot; : 0,  &quot;delayed_unassigned_shards&quot; : 0,  &quot;number_of_pending_tasks&quot; : 0,  &quot;number_of_in_flight_fetch&quot; : 0,  &quot;task_max_waiting_in_queue_millis&quot; : 0,  &quot;active_shards_percent_as_number&quot; : 100.0&#125;</code></pre><p>在这之后，你可以通过kibana从而方便的进行图形化用户管理.</p><h2 id="备份集群">备份集群</h2><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.15/backup-cluster.html">https://www.elastic.co/guide/en/elasticsearch/reference/7.15/backup-cluster.html</a></p><h2 id="问题点">问题点</h2><ol><li>如果磁盘不够用，则因添加新节点，而不是添加新磁盘路径</li><li><code>cluster.name</code>是集群的唯一标识，任何不一致的<code>cluster.name</code>节点无法加入集群</li><li><code>node.name</code>是节点的唯一标识名，默认取值主机名</li><li><code>network.host</code>的设置是ES判断开发模式和生产模式的依据，默认值是回环地址，ES将认为是开发模式。</li><li>没有<code>data</code>角色的节点如果在启动时在磁盘上找到任何分片数据将拒绝启动，而没有<code>master</code>角色和<code>data</code>角色的节点如果在启动时磁盘上有任何索引元数据将拒绝启动。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞03ELK基本使用-logstash组件02-插件介绍</title>
      <link href="posts/56da51f2/"/>
      <url>posts/56da51f2/</url>
      
        <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>logstash的主要功能就是转换数据。</p><p>官方将一些主流的插件分为了四类并进行了介绍。</p><p><a href="https://www.elastic.co/guide/en/logstash/7.15/transformation.html">https://www.elastic.co/guide/en/logstash/7.15/transformation.html</a></p><h2 id="核心操作">核心操作</h2><p>ℹ️ 以下各种插件并非全部功能，具体可以查看官方文档.</p><h3 id="date-filter">date filter</h3><pre><code class="language-yaml">filter &#123;  date &#123;    match =&gt; [ &quot;logdate&quot;, &quot;MMM dd yyyy HH:mm:ss&quot; ]  &#125;&#125;</code></pre><p>按照<code>MMM dd yyyy HH:mm:ss</code>格式化<code>logdate</code>事件字段，并将格式化后的时间作为事件的【logstash】时间戳。通常用来为es索引添加真实的事件时间戳.</p><h3 id="drop-filter">drop filter</h3><pre><code class="language-yaml">filter &#123;  if [loglevel] == &quot;debug&quot; &#123;    drop &#123; &#125;  &#125;&#125;</code></pre><p>通过if条件删除包含<code>loglevel: debug</code>字段的事件.</p><h3 id="fingerprint-filter">fingerprint filter</h3><pre><code class="language-yaml">filter &#123;  fingerprint &#123;    source =&gt; [&quot;IP&quot;, &quot;@timestamp&quot;, &quot;message&quot;]    method =&gt; &quot;SHA1&quot;    key =&gt; &quot;0123&quot;    target =&gt; &quot;[@metadata][generated_id]&quot;  &#125;&#125;</code></pre><p>通过事件原始字段加额外的key，以<code>method</code>加密方式构成<code>[@metadata][generated_id]</code>唯一ID。</p><h3 id="mutate-filter">mutate filter</h3><pre><code class="language-yaml">filter &#123;  mutate &#123;    rename =&gt; &#123; &quot;HOSTORIP&quot; =&gt; &quot;client_ip&quot; &#125;  &#125;&#125;</code></pre><p>针对字段执行rename, remove, replace, modify，strip等操作行为.</p><h3 id="ruby-filter">ruby filter</h3><pre><code class="language-yaml">filter &#123;  ruby &#123;    code =&gt; &quot;event.cancel if rand &lt;= 0.90&quot;  &#125;&#125;</code></pre><p>执行ruby代码，例子中的意思是：取消90%的人.</p><h2 id="序列操作">序列操作</h2><h3 id="csv-filter">csv filter</h3><pre><code class="language-yaml">filter &#123;  csv &#123;    separator =&gt; &quot;,&quot;    columns =&gt; [ &quot;Transaction Number&quot;, &quot;Date&quot;, &quot;Description&quot;, &quot;Amount Debit&quot;, &quot;Amount Credit&quot;, &quot;Balance&quot; ]  &#125;&#125;</code></pre><p>将csv数据，按照<code>separator</code>进行分割，且字段名通过<code>columns</code>定义</p><h3 id="xml-filter">xml filter</h3><pre><code class="language-yaml">filter &#123;  xml &#123;    source =&gt; &quot;message&quot;  &#125;&#125;</code></pre><p>解析事件中的<code>message</code>字段（message字段存储的是xml文档）</p><h3 id="json-coder">json coder</h3><pre><code class="language-yaml">input &#123;  file &#123;    path =&gt; &quot;/path/to/myfile.json&quot;    codec =&gt;&quot;json&quot;&#125;</code></pre><p>coder编解码器一般用于input或者output，还有protobuf/fluent/avro等。</p><h2 id="提取字段、整理数据">提取字段、整理数据</h2><h3 id="dissect-filter">dissect filter</h3><p>日志数据</p><pre><code class="language-bash">Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...</code></pre><pre><code class="language-yaml">filter &#123;  dissect &#123;    mapping =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;ts&#125; %&#123;+ts&#125; %&#123;+ts&#125; %&#123;src&#125; %&#123;prog&#125;[%&#123;pid&#125;]: %&#123;msg&#125;&quot; &#125;  &#125;&#125;</code></pre><p>分组映射<code>message</code>，新的事件字段如下：</p><pre><code class="language-json">&#123;  &quot;msg&quot;        =&gt; &quot;Starting system activity accounting tool...&quot;,  &quot;@timestamp&quot; =&gt; 2017-04-26T19:33:39.257Z,  &quot;src&quot;        =&gt; &quot;localhost&quot;,  &quot;@version&quot;   =&gt; &quot;1&quot;,  &quot;host&quot;       =&gt; &quot;localhost.localdomain&quot;,  &quot;pid&quot;        =&gt; &quot;1&quot;,  &quot;message&quot;    =&gt; &quot;Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...&quot;,  &quot;type&quot;       =&gt; &quot;stdin&quot;,  &quot;prog&quot;       =&gt; &quot;systemd&quot;,  &quot;ts&quot;         =&gt; &quot;Apr 26 12:20:02&quot;&#125;</code></pre><h3 id="kv-filter">kv filter</h3><p>日志数据：</p><pre><code class="language-bash">ip=1.2.3.4 error=REFUSED</code></pre><p>配置：</p><pre><code class="language-yaml">filter &#123;  kv &#123; &#125;&#125;</code></pre><p>新事件字段：</p><pre><code class="language-bash">ip: 1.2.3.4error: REFUSED</code></pre><h3 id="grok-filter">grok filter</h3><p>类似于 dissect filter。</p><p>日志数据：</p><pre><code class="language-bash">55.3.244.1 GET /index.html 15824 0.043</code></pre><p>配置：</p><pre><code class="language-yaml">filter &#123;  grok &#123;    match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IP:client&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;&quot; &#125;  &#125;&#125;</code></pre><p>新事件字段：</p><pre><code class="language-bash">client: 55.3.244.1method: GETrequest: /index.htmlbytes: 15824duration: 0.043</code></pre><p>调试工具1： <a href="http://grokdebug.herokuapp.com/patterns%EF%BC%8C%E5%B9%B6%E4%B8%94%E9%87%8C%E9%9D%A2%E4%B9%9F%E5%8C%85%E5%90%AB%E4%B8%80%E4%BA%9B%E5%B7%B2%E6%9C%89%E7%9A%84%E7%A4%BA%E4%BE%8B%E3%80%82">http://grokdebug.herokuapp.com/patterns，并且里面也包含一些已有的示例。</a></p><p>调试工具2：通过kibana-开发工具-Grok Debugger来调试</p><p><img src="/posts/56da51f2/image-20211115151242262.png" alt="image-20211115151242262"></p><h2 id="丰富数据">丰富数据</h2><h3 id="dns-filter">dns filter</h3><p>将字段ip进行反差，并执行后续动作，例如replace，用反查的域名替换掉ip</p><pre><code class="language-yaml">filter &#123;  dns &#123;    reverse =&gt; [ &quot;source_host&quot; ]    action =&gt; &quot;replace&quot;  &#125;&#125;</code></pre><h3 id="geoip-filter">geoip filter</h3><p>通过对ip进行地理位置查询，添加一个额外的<code>geoip</code>字段</p><pre><code class="language-yaml">filter &#123;  geoip &#123;    source =&gt; &quot;clientip&quot;  &#125;&#125;</code></pre><h3 id="jdbc-static-filter">jdbc_static filter</h3><p>通过外部数据库的数据构建本地缓存库，并通过本地缓存库查询数据丰富事件</p><pre><code class="language-yaml">filter &#123;  jdbc_static &#123;    loaders =&gt; [       &#123;        id =&gt; &quot;remote-servers&quot;        query =&gt; &quot;select ip, descr from ref.local_ips order by ip&quot;        local_table =&gt; &quot;servers&quot;      &#125;,      &#123;        id =&gt; &quot;remote-users&quot;        query =&gt; &quot;select firstname, lastname, userid from ref.local_users order by userid&quot;        local_table =&gt; &quot;users&quot;      &#125;    ]    local_db_objects =&gt; [       &#123;        name =&gt; &quot;servers&quot;        index_columns =&gt; [&quot;ip&quot;]        columns =&gt; [          [&quot;ip&quot;, &quot;varchar(15)&quot;],          [&quot;descr&quot;, &quot;varchar(255)&quot;]        ]      &#125;,      &#123;        name =&gt; &quot;users&quot;        index_columns =&gt; [&quot;userid&quot;]        columns =&gt; [          [&quot;firstname&quot;, &quot;varchar(255)&quot;],          [&quot;lastname&quot;, &quot;varchar(255)&quot;],          [&quot;userid&quot;, &quot;int&quot;]        ]      &#125;    ]    local_lookups =&gt; [       &#123;        id =&gt; &quot;local-servers&quot;        query =&gt; &quot;select descr as description from servers WHERE ip = :ip&quot;        parameters =&gt; &#123;ip =&gt; &quot;[from_ip]&quot;&#125;        target =&gt; &quot;server&quot;      &#125;,      &#123;        id =&gt; &quot;local-users&quot;        query =&gt; &quot;select firstname, lastname from users WHERE userid = :id&quot;        parameters =&gt; &#123;id =&gt; &quot;[loggedin_userid]&quot;&#125;        target =&gt; &quot;user&quot;       &#125;    ]    # using add_field here to add &amp; rename values to the event root    add_field =&gt; &#123; server_name =&gt; &quot;%&#123;[server][0][description]&#125;&quot; &#125;    add_field =&gt; &#123; user_firstname =&gt; &quot;%&#123;[user][0][firstname]&#125;&quot; &#125;     add_field =&gt; &#123; user_lastname =&gt; &quot;%&#123;[user][0][lastname]&#125;&quot; &#125;    remove_field =&gt; [&quot;server&quot;, &quot;user&quot;]    jdbc_user =&gt; &quot;logstash&quot;    jdbc_password =&gt; &quot;example&quot;    jdbc_driver_class =&gt; &quot;org.postgresql.Driver&quot;    jdbc_driver_library =&gt; &quot;/tmp/logstash/vendor/postgresql-42.1.4.jar&quot;    jdbc_connection_string =&gt; &quot;jdbc:postgresql://remotedb:5432/ls_test_2&quot;  &#125;&#125;</code></pre><p><code>loaders</code> 查询外部库</p><p><code>local_db_objects</code> 映射<code>loaders</code>的库，构建本地表</p><p><code>local_lookups</code> 通过本地表查询数据，存放于<code>target</code>指定的变量</p><p><code>add_field</code> 添加新字段</p><p><code>remove_field</code> 删除老字段</p><h3 id="translate-filter">translate filter</h3><p>针对某个事件字段翻译，需要提供字典。</p><pre><code class="language-yaml">filter &#123;  translate &#123;    field =&gt; &quot;response_code&quot;    target =&gt; &quot;http_response&quot;    dictionary =&gt; &#123;      &quot;200&quot; =&gt; &quot;OK&quot;      &quot;403&quot; =&gt; &quot;Forbidden&quot;      &quot;404&quot; =&gt; &quot;Not Found&quot;      &quot;408&quot; =&gt; &quot;Request Timeout&quot;    &#125;    remove_field =&gt; &quot;response_code&quot;  &#125;&#125;</code></pre><p><code>field</code> 指定原字段</p><p><code>target</code> 指定翻译后的字段</p><p><code>dictionary</code> 指定翻译字典.左边是原字段，右边是翻译后字段</p><h3 id="useragent-filter">useragent filter</h3><p>将agent字符串转化成kv对</p><pre><code class="language-yaml">filter &#123;  useragent &#123;    source =&gt; &quot;agent&quot;    target =&gt; &quot;user_agent&quot;    remove_field =&gt; &quot;agent&quot;  &#125;&#125;</code></pre><p>事件的新字段：</p><pre><code class="language-json">        &quot;user_agent&quot;: &#123;          &quot;os&quot;: &quot;Mac OS X 10.12&quot;,          &quot;major&quot;: &quot;50&quot;,          &quot;minor&quot;: &quot;0&quot;,          &quot;os_minor&quot;: &quot;12&quot;,          &quot;os_major&quot;: &quot;10&quot;,          &quot;name&quot;: &quot;Firefox&quot;,          &quot;os_name&quot;: &quot;Mac OS X&quot;,          &quot;device&quot;: &quot;Other&quot;        &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞02ELK基本使用-logstash组件01-安装</title>
      <link href="posts/f73d84da/"/>
      <url>posts/f73d84da/</url>
      
        <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>logstash 使用<code>inputs</code>接收创建数据，<code>filters</code>过滤修改数据，<code>outputs</code>输出数据。三种动作均通过插件实现。</p><p>其中，inputs和outputs支持将<code>codecs</code>编码器作为自身的一部分。</p><h3 id="inputs">inputs</h3><p>inputs负责创建摄入管道(pipeline)，每一个管道根据插件类型来决定是客户端还是服务端。</p><p>例如，beats插件，通常是服务端，接收beats客户端发过来的数据。此时，logstash.input开启一个服务端端口作为监听端口，这个端口不可重复。</p><p>kafka插件，通常是客户端，从kafka服务端拉取数据，此时，logstash.input无需监听端口。</p><p>ℹ️具体插件介绍查看：<a href="https://www.elastic.co/guide/en/logstash/current/input-plugins.html">https://www.elastic.co/guide/en/logstash/current/input-plugins.html</a></p><h3 id="filters">filters</h3><p>filters支持多个插件类型，常用的有：</p><ul><li>grok 用于将非结构化事件塑造成结构化事件</li><li>mutate 针对事件字段进行增删改</li><li>drop 完全删除一个事件</li><li>geoip 给事件添加地理位置</li></ul><p>ℹ️具体插件介绍查看：<a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">https://www.elastic.co/guide/en/logstash/current/filter-plugins.html</a></p><h3 id="outputs">outputs</h3><p>大部分情况下，你应该始终用elasticsearch作为output输出对象。</p><p>ℹ️具体插件介绍查看：<a href="https://www.elastic.co/guide/en/logstash/current/output-plugins.html">https://www.elastic.co/guide/en/logstash/current/output-plugins.html</a></p><h3 id="codecs">codecs</h3><p>比较常用的插件有，json、json_lines</p><p>ℹ️具体插件介绍查看：<a href="https://www.elastic.co/guide/en/logstash/current/codec-plugins.html">https://www.elastic.co/guide/en/logstash/current/codec-plugins.html</a></p><p>三者的合作方式是：</p><p>inputs将接收事件，并将事件写入内存队列中，每一个管道的工作线程从队列里获取数据，交给filters处理，最后通过output输出。</p><p>需要注意的是，位于内存中的事件队列无法保证可靠性，因此如果不设置持久队列，则一旦出现意外，数据将丢失。</p><p>关于队列持久，有两大类，第一类是inputs将事件写入到本地磁盘存储，第二类是在logstash前放一个其它服务，例如kafka或者redis，通过其它服务来提前构建高可用的事件队列.</p><p>具体参考：<a href="https://www.elastic.co/guide/en/logstash/current/persistent-queues.html">https://www.elastic.co/guide/en/logstash/current/persistent-queues.html</a></p><h2 id="安装">安装</h2><p>详细的安装文档参考 <a href="https://www.elastic.co/guide/en/logstash/current/installing-logstash.html#_yum">https://www.elastic.co/guide/en/logstash/current/installing-logstash.html#_yum</a></p><p>以二进制安装为例</p><h3 id="二进制">二进制</h3><p><a href="https://mirrors.huaweicloud.com/logstash/">https://mirrors.huaweicloud.com/logstash/</a></p><pre><code class="language-bash">lsVersion=7.15.1curl &quot;https://mirrors.huaweicloud.com/logstash/$&#123;lsVersion&#125;/logstash-$&#123;lsVersion&#125;-linux-x86_64.tar.gz&quot; -o logstash.tgztar xf logstash.tgzmv logstash-$&#123;lsVersion&#125; ../logstash &amp;&amp; cd ..cd logstash</code></pre><h3 id="docker">docker</h3><pre><code class="language-bash">version: '2.3'services:  logstash:    image: docker.elastic.co/logstash/logstash:7.15.1    container_name: logstash    volumes:      - /export/logstash/pipeline:/usr/share/logstash/pipeline      - /export/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml      - /export/logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml      - /export/logstash/data:/usr/share/logstash/data    ports:      - &quot;5044:5044&quot;      - &quot;9600:9600&quot;    networks:      - esnet    cpus: 1    mem_limit: 1g    memswap_limit: 1g    mem_reservation: 1g    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9600&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failurenetworks:  esnet:</code></pre><p>ℹ️通过自定义的pipelines覆盖掉默认的pipelines，<a href="http://xn--pipelinespipeline-ys50a2w950g962gzf7c2s7e7kqe.id">默认的pipelines只有一个pipeline.id</a>:main</p><h2 id="主配置">主配置</h2><pre><code class="language-bash">cp /export/logstash/config/logstash.yml /export/logstash/config/logstash.yml.defaulthostName=`hostname`cat &gt; /export/logstash/config/logstash.yml &lt;&lt; EOFnode.name: hadoop001http.host: 0.0.0.0http.port: 9600log.level: infoconfig.reload.automatic: trueconfig.reload.interval: 10sconfig.support_escapes: false## 通过ES提供的内置用户logstash_system将logstash的状态数据发送给ES。#xpack.monitoring.elasticsearch.username: logstash_system#xpack.monitoring.elasticsearch.password: $&#123;ES_PWD&#125;EOF</code></pre><p>logstash 主配置写法：<a href="https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html">https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html</a></p><p>ℹ️logstash 通过 path.settings 来寻找目录下的 pipelines.yml 配置</p><p>✨xpack 开头的选项用于仅当ES启用了xpack和内置用户。需要提前先在ES那边创建。</p><p>步骤如下：</p><ol><li>开通ES的安全配置xpack，并初始化内置用户</li><li>创建logstash密钥库，存放加密密码到变量中</li></ol><pre><code class="language-bash">bin/logstash-keystore create=&gt;输入y,表示不设置密钥库的密码bin/logstash-keystore add ES_PWD=&gt;输入ES_PWD</code></pre><ol start="3"><li>删除主配置里的xpack两个选项的注释，并通过密码变量调用密码。</li></ol><pre><code class="language-bash">bin/logstash-keystore listbin/logstash-keystore remove</code></pre><h2 id="管道配置">管道配置</h2><p>管道配置的编写一般是需要依托于beats组件，或者beats组件后的消息队列组件.</p><p>管道配置分多个步骤：<code>input</code>、<code>filter</code>、<code>output</code></p><p><a href="https://www.elastic.co/guide/en/logstash/current/input-plugins.html">https://www.elastic.co/guide/en/logstash/current/input-plugins.html</a></p><p><a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">https://www.elastic.co/guide/en/logstash/current/filter-plugins.html</a></p><p><a href="https://www.elastic.co/guide/en/logstash/current/output-plugins.html">https://www.elastic.co/guide/en/logstash/current/output-plugins.html</a></p><p>配置文件：config/pipelines.yml</p><h3 id="示例1">示例1</h3><pre><code class="language-yaml">- pipeline.id: upstream_server  config.string: |    input &#123;      kafka &#123;        topics_pattern =&gt; &quot;zz.it.elk\..*&quot;        group_id =&gt; &quot;logstash&quot;        bootstrap_servers =&gt; &quot;hadoop001:8123,hadoop002:8123,hadoop003:8123&quot;        security_protocol =&gt; &quot;SASL_PLAINTEXT&quot;        sasl_mechanism =&gt; &quot;SCRAM-SHA-256&quot;        sasl_jaas_config =&gt; &quot;org.apache.kafka.common.security.scram.ScramLoginModule required username='elk'  password='123456';&quot;        codec =&gt;&quot;json&quot;      &#125;    &#125;    output &#123;      if [fields][log_topic] == &quot;zz.it.elk.syslog.secure&quot; &#123;        pipeline &#123; send_to =&gt; syslog &#125;      &#125; else &#123;        pipeline &#123; send_to =&gt; apache &#125;      &#125;    &#125;- pipeline.id: apache  pipeline.workers: 3  queue.type: persisted  #path.config: &quot;/usr/share/logstash/pipeline/beats-5044.conf&quot;  config.string: |    input &#123;      pipeline &#123;        address =&gt; apache      &#125;    &#125;    filter &#123;      mutate &#123;        remove_tag =&gt; [&quot;beats_input_codec_plain_applied&quot;]      &#125;    &#125;    output &#123;      elasticsearch &#123;          hosts =&gt; [&quot;hadoop001:9200&quot;,&quot;hadoop002:9200&quot;,&quot;hadoop003:9200&quot;]          user =&gt; &quot;index_admin&quot;          password =&gt; &quot;$&#123;INDEX_PWD&#125;&quot;          manage_template =&gt; false          index =&gt; &quot;%&#123;[tags][0]&#125;-%&#123;[tags][1]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;    &#125;- pipeline.id: syslog  pipeline.workers: 1  queue.type: persisted  #path.config: &quot;/usr/share/logstash/pipeline/beats-5045.conf&quot;  config.string: |    input &#123;      pipeline &#123;        address =&gt; syslog      &#125;    &#125;    filter &#123;      mutate &#123;        remove_tag =&gt; [&quot;beats_input_codec_plain_applied&quot;]      &#125;    &#125;    output &#123;      elasticsearch &#123;          hosts =&gt; [&quot;hadoop001:9200&quot;,&quot;hadoop002:9200&quot;,&quot;hadoop003:9200&quot;]          user =&gt; &quot;index_admin&quot;          password =&gt; &quot;$&#123;INDEX_PWD&#125;&quot;          manage_template =&gt; false          index =&gt; &quot;%&#123;[tags][0]&#125;-%&#123;[tags][1]&#125;-all-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;    &#125;</code></pre><p>⚠️数据进kafka后，会转为json格式，因此需要在input中加入json编码器</p><h3 id="示例2">示例2</h3><pre><code class="language-yaml">- pipeline.id: k8scontainers  pipeline.workers: 3  queue.type: persisted  config.string: |    input &#123;      kafka &#123;        topics_pattern =&gt; &quot;zz.it.elk.k8s.containers&quot;        group_id =&gt; &quot;logstash&quot;        bootstrap_servers =&gt; &quot;data01:8123,data02:8123,data03:8123&quot;        security_protocol =&gt; &quot;SASL_PLAINTEXT&quot;        sasl_mechanism =&gt; &quot;SCRAM-SHA-256&quot;        sasl_jaas_config =&gt; &quot;org.apache.kafka.common.security.scram.ScramLoginModule required username='elk'  password='123456';&quot;        codec =&gt;&quot;json&quot;      &#125;    &#125;    filter &#123;      mutate &#123;        remove_tag =&gt; [&quot;beats_input_codec_plain_applied&quot;]      &#125;    &#125;    output &#123;      elasticsearch &#123;          hosts =&gt; [&quot;data01:9200&quot;,&quot;data02:9200&quot;,&quot;data03:9200&quot;]          user =&gt; &quot;index_admin&quot;          password =&gt; &quot;index_admin&quot;          manage_template =&gt; false          index =&gt; &quot;%&#123;[tags][0]&#125;-%&#123;[kubernetes][container][name]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;    &#125;</code></pre><h3 id="创建管道所需的ES写权限账户">创建管道所需的ES写权限账户</h3><p>⚠️<code>output.elasticsearch.user</code>的用户需要在ES中创建。可以使用kibana用户管理中去创建，其用户的角色权限示例如下图所示：</p><p><a href="https://www.elastic.co/guide/en/logstash/current/ls-security.html">https://www.elastic.co/guide/en/logstash/current/ls-security.html</a></p><p><img src="/posts/f73d84da/image-20211129164142226.png" alt="image-20211129164142226"></p><h2 id="测试">测试</h2><pre><code class="language-bash">cd /export/logstashbin/logstash --config.test_and_exit</code></pre><h3 id="数据debug">数据debug</h3><p>pipelines.output配置<code>stdout</code>插件</p><pre><code class="language-bash">    output &#123;      stdout &#123;&#125;    &#125;</code></pre><pre><code class="language-bash">bin/logstash -e</code></pre><p>可以观察 logstash 获取的事件</p><h2 id="启动">启动</h2><pre><code class="language-bash">cd /export/logstashnohup bin/logstash &amp;</code></pre><h2 id="配置自动重载">配置自动重载</h2><p>docker模式下的logstash已经是自动重载，如果你是二进制方式，则启动的时候追加<code>--config.reload.automatic</code></p><p>当 Logstash 检测到配置文件中的更改时，它会通过停止所有输入来停止当前管道，并尝试创建使用更新配置的新管道。在验证新配置的语法后，Logstash 会验证所有输入和输出是否都可以初始化（例如，所有必需的端口都已打开）。如果检查成功，Logstash 会将现有管道与新管道交换。如果检查失败，旧管道将继续运行，并将错误传播到控制台。</p><p>在自动配置重新加载期间，JVM 不会重新启动。管道的创建和交换都发生在同一个进程中。</p><p>对filter-grok部分的更改也会重新加载，但仅在配置文件中的更改触发重新加载（或管道重新启动）时才会重新加载。</p><h2 id="管道配置：多重管道">管道配置：多重管道</h2><p><a href="https://www.elastic.co/guide/en/logstash/current/multiple-pipelines.html">https://www.elastic.co/guide/en/logstash/current/multiple-pipelines.html</a></p><p>在一个logstash实例中配置多个<code>pipeline</code>，甚至，还可以管道到管道</p><p><a href="https://www.elastic.co/guide/en/logstash/current/pipeline-to-pipeline.html#pipeline-to-pipeline">https://www.elastic.co/guide/en/logstash/current/pipeline-to-pipeline.html#pipeline-to-pipeline</a></p><p>管道到管道涉及一个概念：【虚拟地址】，以下面例子说明：</p><pre><code class="language-yaml"># config/pipelines.yml- pipeline.id: upstream  config.string: input &#123; stdin &#123;&#125; &#125; output &#123; pipeline &#123; send_to =&gt; [myVirtualAddress] &#125; &#125;- pipeline.id: downstream  config.string: input &#123; pipeline &#123; address =&gt; myVirtualAddress &#125; &#125;</code></pre><p>ℹ️<code>send_to</code>连接虚拟地址，<code>address</code>创建虚拟地址</p><ol><li>下游管道输入充当虚拟服务器，侦听本地进程中的单个虚拟地址。</li><li>只有在同一个 Logstash 上运行的上游管道输出才能将事件发送到此虚拟地址列表。</li><li>如果下游管道被阻塞或不可用，则上游管道输出将被阻塞。</li><li>当事件跨管道发送时，它们的数据被完全复制，因此对下游管道中事件的修改不会影响上游管道中的该事件。</li><li>管道插件可能是管道之间通信的最有效方式，但它仍然会产生性能成本。因为 Logstash 必须在 Java 堆上为每个下游管道完整复制每个事件。使用此功能可能会影响 Logstash 的堆内存利用率。</li></ol><h3 id="多重分销商模式">多重分销商模式</h3><p>通过 beats-server.input 管道接收，在 beats-server.output 中进行事件字段判断后，通过虚拟地址分发到多个管道</p><p>缺点：分发的数据是复制的，因此内存使用会增大。</p><pre><code class="language-yaml">cat &gt; /export/docker-compose-data/logstash/pipelines.yml &lt;&lt; EOF- pipeline.id: beats_server  config.string: |    input &#123;      beats &#123;        port =&gt; 5044      &#125;    &#125;    output &#123;      if [tags][1] == &quot;access&quot; &#123;        pipeline &#123; send_to =&gt; access &#125;      &#125; else &#123;        pipeline &#123; send_to =&gt; error &#125;      &#125;    &#125;- pipeline.id: apache_access  pipeline.workers: 3  pipeline.batch.size: 500  queue.type: persisted  config.string: |    input &#123;      pipeline &#123;        address =&gt; access      &#125;    &#125;    filter &#123;      mutate &#123;        remove_tag =&gt; [&quot;beats_input_codec_plain_applied&quot;]      &#125;    &#125;    output &#123;      elasticsearch &#123;          hosts =&gt; [&quot;es01:9200&quot;]          manage_template =&gt; false          index =&gt; &quot;%&#123;[tags][0]&#125;-%&#123;[tags][1]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;    &#125;- pipeline.id: apache_error  pipeline.workers: 1  pipeline.batch.size: 125  queue.type: persisted  config.string: |    input &#123;      pipeline &#123;        address =&gt; error      &#125;    &#125;    filter &#123;      mutate &#123;        remove_tag =&gt; [&quot;beats_input_codec_plain_applied&quot;]      &#125;    &#125;    output &#123;      elasticsearch &#123;          hosts =&gt; [&quot;es01:9200&quot;]          manage_template =&gt; false          index =&gt; &quot;%&#123;[tags][0]&#125;-%&#123;[tags][1]&#125;-all-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;    &#125;EOF</code></pre><p>ℹ️</p><p>每一个管道的内存队列大小=pipeline.workers*pipeline.batch.size, 内存队列大小决定了同一时间可处理的事件数.</p><p>默认情况下,pipeline.workers = vcpu , pipeline.batch.size = 125.</p><p>⚠️当队列满的时候，logstash会阻塞input,防止新事件流入。</p><h3 id="分叉路径模式">分叉路径模式</h3><p>这个模式下，beats_server.input 收到数据后，beats_server.output会不经条件判断，直接将数据通过虚拟地址（es,http）复制到额外的两个管道（bufferd-es,bufferd-http）。</p><p>优点：bufferd-es 和 bufferd-http管道互不干扰，且可以分别设定过滤规则.</p><p>缺点：资源消耗方面是成倍数增加的，每多一个管道，就需要多一倍的资源.例如下列配置中，bufferd-es和bufferd-http均开启了持久化（queue.type: persisted），这意味着额外的双倍磁盘消耗。</p><pre><code class="language-yaml">cat &gt; /export/docker-compose-data/logstash/pipelines.yml &lt;&lt; EOF- pipeline.id: beats_server  config.string: |    input &#123;      beats &#123;        port =&gt; 5044      &#125;    &#125;    output &#123;        pipeline &#123; send_to =&gt; [es, s3] &#125;    &#125;- pipeline.id: es  pipeline.workers: 3  queue.type: persisted  config.string: |    input &#123;      pipeline &#123;        address =&gt; es      &#125;    &#125;    filter &#123;      mutate &#123;        remove_tag =&gt; [&quot;beats_input_codec_plain_applied&quot;]      &#125;    &#125;    output &#123;      elasticsearch &#123;          hosts =&gt; [&quot;es01:9200&quot;]          manage_template =&gt; false          index =&gt; &quot;%&#123;[tags][0]&#125;-%&#123;[tags][1]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;        &#125;    &#125;- pipeline.id: s3  pipeline.workers: 1  queue.type: persisted  config.string: |    input &#123;      pipeline &#123;        address =&gt; s3      &#125;    &#125;    filter &#123;      mutate &#123;        remove_tag =&gt; [&quot;beats_input_codec_plain_applied&quot;]      &#125;    &#125;    output &#123;      s3 &#123;&#125;    &#125;EOF</code></pre><p>⚠️根据 input 的类型，input 插件可能是 client 也可能是 server，在充当 server 角色的时候，应该保证端口唯一。</p><h2 id="管道配置：持久队列">管道配置：持久队列</h2><p>默认事件队列是内存级别，因此数据可能丢失，而持久队列是定期将内存数据写入到磁盘。</p><p>工作流是：输入→队列(磁盘)→过滤器+输出</p><p>当输入有事件准备好处理时，它会将事件写入队列。当写入队列成功时，输入可以向其数据源发送确认。因此，要求输入源必须支持事件确认。（并不是所有的input插件都支持，但beats和http插件是支持的）.</p><p>经过过滤器和输出成功后，事件将被标记为【确认(ACKed)】.</p><p>⚠️ 如果事件在经历过滤器和输出成功后，且还未来得及标记为【确认(ACKed)】的情况下，logstash突然挂了，那么在logstash启动后，事件会被重复发送，这会导致输出端接收到重复事件。</p><p>持久队列分为两部分文件，一个是保存事件的多份数据队列文件<code>page.&lt;num&gt;</code>，一个是记录事件信息的单个检查点文件<code>checkpoint.xxx</code>。</p><ul><li>每有x个事件被写入到队列文件，检查点文件就会更新一次。</li><li>每有x个事件被确认，检查点文件也会更新一次。</li></ul><p>数据队列文件: 按照<code>queue.page_capacity</code>定义的大小进行切分。【最新】的一份数据文件被称之为头文件（接收input事件,只可追加），其余的叫尾文件（只读不可变），事件确认完的尾页将被删除。</p><p>检查点文件: 包含事件所在的数据文件，是否确认等信息。凡是没有被检查点记录的事件，均可以认为暂不安全.</p><p>最后，一个至关重要的点，logstash始终通过【检查点文件】来判断事件。</p><p>⚠️ logstash突然故障：</p><ul><li>会导致没有记录在检查点文件里的<code>buffered</code>事件丢失。(也就是还未写入磁盘的内存队列里的事件)</li><li>会导致已经完成处理但却来不及更新进检查点文件的事件被重复输出。</li></ul><p><code>pipelines.yml</code>示例配置：</p><pre><code class="language-yaml">- pipeline.id: bufferd-to-disk  queue.type: persisted  queue.drain: false # 关闭logstash的时候，不等待【持久队列】的事件确认完成  queue.page_capacity: 250mb  queue.max_bytes: 1024mb # 所有【包含】未处理事件的数据队列页总大小，过大会影响性能，过小会在output压力大的时候阻塞input  queue.checkpoint.writes: 1024 # 数据队列【头页】每写入1024个事件，就刷新一次检查点文件。值越小，故障后丢失的事件就越少，磁盘io压力越大。丢失数&lt;=值。  queue.checkpoint.acks: 1024 # 数据队列【尾页】每确认1024个事件，就刷新一次检查点文件。值越小，故障后重复发送的事件就越少，磁盘io压力越大。重复数&lt;=值</code></pre><h2 id="主配置：死信队列">主配置：死信队列</h2><p><a href="https://www.elastic.co/guide/en/logstash/7.15/dead-letter-queues.html">https://www.elastic.co/guide/en/logstash/7.15/dead-letter-queues.html</a></p><p>当logstash无法解析事件的时候，默认情况下logstash会丢弃或者挂起。</p><p>开启死信队列可以收集这些异常事件，确保logstash正常运行。之后，死信队列的事件需要【人工】处理。</p><p>⚠️哪怕你开启了死信队列，也要确保pipeline.output是elasticsearch，因为logstash通过elasticsearch返回的状态码来定性死信，当前判断值为400/404。</p><p><img src="/posts/f73d84da/image-20211112170234694.png" alt="image-20211112170234694"></p><p>死信队列开启方式：<code>dead_letter_queue.enable: true</code>，它以文件形式存储，位于<code>path.dead_letter_queue</code>/&lt;<a href="http://pipeline.id">pipeline.id</a>&gt;。任意输出</p><p>⚠️两个logstash实例不能设置相同的<code>path.dead_letter_queue</code></p><p>文件切分分两种方式：</p><ol><li>达到了文件大小上限<code>dead_letter_queue.max_bytes</code>。默认值：1024mb</li><li>达到了刷新时间<code>dead_letter_queue.flush_interval</code>。默认值: 5000ms，不能低于 1000ms</li></ol><p>ℹ️上面几个<code>dead_letter_queue</code>均为主配置</p><h3 id="人工处理死信">人工处理死信</h3><p><a href="https://www.elastic.co/guide/en/logstash/7.15/plugins-inputs-dead_letter_queue.html">https://www.elastic.co/guide/en/logstash/7.15/plugins-inputs-dead_letter_queue.html</a></p><p>处理死信，就是新构建一个pipeline，从死信中读取异常事件，并再次处理的过程.这期间需要用到<code>Dead_letter_queue input plugin</code></p><p>假设异常事件是<code>&#123;&quot;geoip&quot;:&#123;&quot;location&quot;:&quot;home&quot;&#125;&#125;</code> 可以看出<code>home</code>是不对的，不符合geoip所需要值类型.</p><pre><code class="language-yaml">input &#123;  dead_letter_queue &#123;    path =&gt; &quot;/path/to/data/dead_letter_queue&quot; # path.dead_letter_queue    start_timestamp =&gt; &quot;2017-06-06T23:40:37&quot;    commit_offsets =&gt; true     pipeline_id =&gt; &quot;main&quot;   &#125;&#125;filter &#123;  mutate &#123;    remove_field =&gt; &quot;[geoip][location]&quot;   &#125;&#125;output &#123;  elasticsearch&#123;    hosts =&gt; [ &quot;localhost:9200&quot; ]   &#125;&#125;</code></pre><p><code>commit_offsets</code> 表示记录偏移点，意思就是<code>input.dead_letter_queue</code>每次不会从开头重新处理，而是从上一次的尾端开始。</p><p><code>start_timestamp</code> 表示仅处理指定时间点之后进入死信队列的事件.</p><p><code>pipeline_id</code> 指的是往死信队列里写异常事件的管道。默认是main</p><p>通过<code>remove_field</code>删除异常的字段，从而解决。</p><h3 id="删除死信数据">删除死信数据</h3><p>关掉pipeline，然后直接物理层面删除<code>path.dead_letter_queue</code>/&lt;<a href="http://pipeline.id">pipeline.id</a>&gt;即可</p>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞01ELK基本使用-beats组件</title>
      <link href="posts/3475106c/"/>
      <url>posts/3475106c/</url>
      
        <content type="html"><![CDATA[<p>☢ 以下信息仅是个人笔记，不保证时效性，不保证准确度☢</p><p>☢ 以下信息仅是个人笔记，不保证时效性，不保证准确度☢</p><p>☢ 以下信息仅是个人笔记，不保证时效性，不保证准确度☢</p><h2 id="Beats">Beats</h2><p>以filebeat为例，这里我选择二进制方式</p><p>当然还有指标收集器：Merticbeat</p><h2 id="简单原理">简单原理</h2><p>filebeat 通过 input 管理 harvesters，harvesters 负责逐行读取单个文件的内容，并将数据发送给 output。</p><p>filebeat 通过注册表来记录 harvesters 正在读取的文件位置，从而避免重复读取。因此，filebeat 重启的时候，始终会通过注册表得知最后一个已知的位置从而继续运行 harvesters。</p><p>filebeat 通过注册表来记录 harvesters 读取的每一行事件的传输状态，来确保事件不会丢失。若 filebeat 已将某个事件传递到 output ，但还未收到确认的时候就 down 掉，则 filebeat 会重发数据，这可能会出现重复数据。</p><p>💥应确保filebeat的日志处理速度大于日志的写入速度，或者日志轮转保留一定量的日志数量。如果日志文件写入磁盘的速度超过 FileBeat 可以处理的速度，或者在<code>output</code>不可用时删除了日志文件，数据有可能会丢失。在 Linux 上，FileBeat 也可能因为 inode 重用而跳过行。</p><h2 id="安装">安装</h2><p><a href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html">https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.html</a></p><pre><code class="language-bash">mkdir -p /export/src &amp;&amp; cd /export/srccurl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.15.1-linux-x86_64.tar.gztar xzvf filebeat-7.15.1-linux-x86_64.tar.gzmv filebeat-7.15.1-linux-x86_64 ../filebeat &amp;&amp; cd ..cd filebeat</code></pre><h2 id="配置">配置</h2><p><a href="https://www.elastic.co/guide/en/beats/filebeat/current/configuration-general-options.html#libbeat-configuration-fields">https://www.elastic.co/guide/en/beats/filebeat/current/configuration-general-options.html#libbeat-configuration-fields</a></p><pre><code class="language-bash"># 添加配置文件cat &gt; filebeat.yml &lt;&lt; EOFfilebeat.config:  modules:    path: $&#123;path.config&#125;/modules.d/*.yml    reload.enabled: false#filebeat.autodiscover:#  providers:#    - type: docker#      hints.enabled: truefilebeat.shutdown_timeout: 5s # Filebeat 在关闭时等待发布者完成发送事件的时间# 注册表记录了收割机读取文件的信息以及读取的最后指针位置close_removed: true # 开启状态下，如果文件被删除或者改名，则收割进程将立即关闭文件描述符并终止读取。反之，收割进程将会在完全读取完毕后，才会关闭文件描述符。（如果你要关闭这个选项，则 clean_removed 也需要关闭）.clean_removed: true # 只要注册表里的文件名在磁盘上找不到，则清除对应的注册表项。开启它（默认开启）可以一定程度上解决注册表过大。但这会有一个问题，当共享数据盘短时间内取消挂载并重新挂载，则注册表会重新注册文件，这将导致文件被重新读取发送（即重复读取）.filebeat.inputs:- type: log  enabled: true  paths:    - /var/log/secure-*  tags: [&quot;syslog&quot;,&quot;secure&quot;]  fields:    log_topic: zz.it.elk.syslog.secure- type: log  enabled: true  paths:    - /export/logs/*.log  tags: [&quot;apache&quot;,&quot;access&quot;]  fields:    log_topic: zz.it.elk.apache.access- type: log  enabled: true  paths:  - /export/logs/*.json  tags: [&quot;apache&quot;,&quot;error&quot;]# keys_under_root可以让字段位于根节点，默认为false  json.keys_under_root: true# 对于同名的key，覆盖原有key值  json.overwrite_keys: true  json.message_key: message# 将解析错误的消息记录储存在error.message字段中  json.add_error_key: true# 日志多行合并采集（配置的时候一定要顶格写）# 匹配以&#123;开头之后的数据，一直到下一个&#123;之前  multiline.type: pattern  multiline.pattern: '^&#123;'  multiline.negate: true  multiline.match: after# 处理器：在output阶段之前，优化event## ecs 字段删除processors:  - drop_fields:      fields: [&quot;ecs&quot;,&quot;input&quot;,&quot;agent&quot;,&quot;log&quot;]output.kafka:  # initial brokers for reading cluster metadata  hosts: [&quot;hadoop001:8123&quot;, &quot;hadoop002:8123&quot;, &quot;hadoop003:8123&quot;]  username: elk  password: 123456  sasl.mechanism: SCRAM-SHA-256  # message topic selection + partitioning  # 一定要注意 log_topic 字段在事件中的位置  topic: '%&#123;[fields.log_topic]&#125;'  partition.round_robin:    reachable_only: false  required_acks: 1  compression: gzip  max_message_bytes: 1000000EOF</code></pre><ol><li><p>modules 定义了filebeat内置的针对不同程序的采集模块，默认后缀都是<code>.disabled</code>，也就是不启用。</p><p>关于各种模块的介绍，参考https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules.html。本文后面会有所介绍。</p></li><li><p>配置文件中有一个<code>$&#123;path.config&#125;</code> ，引用了配置<code>path.config</code>，在二进制部署中<code>path.config</code>就是二进制程序所在的目录。</p><p>关于filebeat程序内诸如路径的配置，参考https://www.elastic.co/guide/en/beats/filebeat/current/directory-layout.html#directory-layout</p></li><li><p><code>reload.enabled</code>表示开启动态重载，开启之后，每间隔<code>reload.period</code>时间，Beats就会重载一次<code>path</code>指定的配置.可以从https://www.elastic.co/guide/en/beats/filebeat/current/_live_reloading.html中获取.</p></li><li><p><code>filebeat.inputs</code>中我们添加了额外的<code>tags</code>，这个<code>tags</code>字段会出现在事件(json)的根下，因此可以方便的通过<code>tags</code>来动态的生成ES索引名.</p></li><li><p>%{[fields.log_topic]} 调用事件字段，仅在output阶段使用</p></li></ol><h3 id="多行匹配配置">多行匹配配置</h3><pre><code class="language-yaml"># 日志多行合并采集（配置的时候一定要顶格写）# 匹配以&#123;开头之后的数据，一直到下一个&#123;之前  multiline.type: pattern  multiline.pattern: '^&#123;'  multiline.negate: true  multiline.match: after</code></pre><p>下图表示：</p><p>例1：匹配（false）b开头的多行，直到遇到多行之后（after）的某行c不匹配，则不匹配行c就是事件起始点。</p><p>例2：匹配（false）b开头的多行，直到遇到多行之前（before）的某行a不匹配，则不匹配行a就是事件重点。</p><p>例3：不匹配（true）b开头的多行，直到遇到多行之后（after）的某行b匹配，则匹配行b就是事件起始点。</p><p>例4：不匹配（true）b开头的多行，直到遇到多行之前（before）的某行b匹配，则匹配行b就是事件重点。</p><p><img src="/posts/3475106c/image-20220330114830848.png" alt="image-20220330114830848"></p><h3 id="processors处理器配置">processors处理器配置</h3><p><a href="https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html">https://www.elastic.co/guide/en/beats/filebeat/current/filtering-and-enhancing-data.html</a></p><p>作用是在output阶段之前，优化event，例如添加字段、删除字段等。</p><p>如果存在多个处理器，按定义顺序执行。</p><p>filebeat 内置了众多处理器。</p><h4 id="add-kubernetes-metadata-处理器">add_kubernetes_metadata 处理器</h4><p>这个处理器给每个事件附加pod元数据。附加的元数据有：</p><ul><li>Pod Name</li><li>Pod UID</li><li>Namespace</li><li>Labels</li></ul><p>如何给每个事件附加对应的pod元数据，依赖于两个构造字段：</p><ul><li>Indexers</li><li>Matchers</li></ul><p>Indexers 索引器：通过 indexer 构建一个标识符作为事件分类后的索引；</p><ul><li><code>container</code> 索引器，使用Pod下的容器 ID 作为标识符构建索引。</li></ul><p>Matchers 匹配器：作用是以 matcher 构建一个和 indexer 标识符相同的查询 key，从而过滤 input 事件。</p><ul><li><code>logs_path</code> 匹配器，基于 pod uid 或者 container id 作为查询 key 过滤 input 事件，并将事件关联到对应的标识符索引下。</li></ul><p>这样，对应标识符索引下的事件，都会附加上此索引标识符对应pod的元数据。</p><p>简单来看， Indexer 的值应该与 Matcher 的查询 key 保持一致。</p><p>举例：</p><pre><code class="language-yaml">    processors:                                                                                                   - add_kubernetes_metadata:          in_cluster: true  # filebeat 位于集群内          default_indexers.enabled: false          default_matchers.enabled: false          indexers:            - container:          matchers:            - logs_path:                resource_type: 'container'                logs_path: '/var/log/containers/'       - drop_fields:          fields: [&quot;host.name&quot;]          ignore_missing: true      - copy_fields:          fields:          - from: kubernetes.node.name            to: host.name          fail_on_error: false          ignore_missing: true</code></pre><h4 id="add-kubernetes-metadata-可能出现的故障">add_kubernetes_metadata 可能出现的故障</h4><p>💥启用add_kubernetes_metadata报错，根据提示给filebeat添加NODE_NAME环境变量</p><pre><code class="language-bash">add_kubernetes_metadata/kubernetes.go:177  Couldn't discover Kubernetes node: %!w(*errors.fundamental=&amp;&#123;kubernetes: Node could not be discovered with any known method. Consider setting env var NODE_NAME 0xc000560ae0&#125;) &#123;&quot;libbeat.processor&quot;: &quot;add_kubernetes_metadata&quot;&#125;</code></pre><pre><code class="language-yaml">      containers:      - name: filebeat          env:        - name: NODE_NAME          valueFrom:            fieldRef:              fieldPath: spec.nodeName</code></pre><p>💥add_kubernetes_metadata无法通过system:serviceaccount:kube-system:default获取api，根据提示创建新的SA对象。</p><pre><code class="language-bash">reflector.go:138] pkg/mod/k8s.io/client-go@v0.21.1/tools/cache/reflector.go:167: Failed to watch *v1.Node: failed to list *v1.Node: nodes &quot;k8s01&quot; is forbidden: User &quot;system:serviceaccount:kube-system:default&quot; cannot list resource &quot;nodes&quot;in API group &quot;&quot; at the cluster scope</code></pre><pre><code class="language-yaml">apiVersion: v1kind: ServiceAccountmetadata:  name: filebeat  namespace: kube-system  labels:    app: filebeat---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: filebeat  labels:    app: filebeatrules:- apiGroups: [&quot;&quot;]  resources:  - namespaces  - pods  - nodes  verbs:  - get  - watch  - list---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: filebeatsubjects:- kind: ServiceAccount  name: filebeat  namespace: kube-systemroleRef:  kind: ClusterRole  name: filebeat  apiGroup: rbac.authorization.k8s.io</code></pre><h3 id="output">output</h3><p>output用来指定filebeat读取的数据输出到哪。</p><p>💥不同版本的filebeat能支撑的output插件的参数都可能不同。例如老版本的 filebeat 的 output.kafka.sasl.mechanism 支撑就有问题。</p><h3 id="output负载均衡">output负载均衡</h3><p>仅支持redis/logstash/es，kafka是内部自身负载</p><pre><code class="language-yaml">output.logstash:  hosts: [&quot;localhost:5044&quot;, &quot;localhost:5045&quot;]  loadbalance: true  worker: 2</code></pre><p>不开启loadbalance，主机列表里随机选一个作为输出，若故障就切换另一个。</p><p>开启loadbalance，将事件平衡发送到主机列表，具体逻辑官网没说。</p><h3 id="测试-output-是否可用">测试 output 是否可用</h3><pre><code class="language-bash">./filebeat test output</code></pre><h3 id="启动">启动</h3><ol><li>测试数据(一定不要针对大文件使用)</li></ol><pre><code class="language-bash">./filebeat -e -d &quot;*&quot;# -e 表示关闭文件日志，输出到控制台# -d 表示开启debug，后面&quot;*&quot;指输出所有debug</code></pre><blockquote><p>如果上面的命令有事件输出，则说明filebeat配置基本无误，如果没有实际数据对象输出，则直接检查filebeat配置或者日志文件本身。</p><p>默认情况下，filebeat 的每一个事件都会包含以下事件字段：</p><pre><code class="language-json">&#123;    ...    &quot;@metadata&quot;: &#123;       &quot;beat&quot;: &quot;filebeat&quot;,       &quot;version&quot;: &quot;7.15.1&quot;     &#125;&#125;</code></pre></blockquote><ol start="2"><li>后台启动</li></ol><pre><code class="language-bash">nohup ./filebeat &amp; 2&gt;&amp;1</code></pre><p>ℹ️ 启动之后，会在<code>$&#123;home.config&#125;/data</code>下生成注册表。注册表不能随意删除，删除意味着filebeat无法确定是否已读过文件，这会导致数据重复导入.</p><h3 id="模块化">模块化</h3><p>除了玩家自定义output，并搭配logstash的pipeline进行事件过滤转换。Beats组件一般还内置了一些通用场景的模块，这些模块已经包含了对应的默认配置，elasticsearch的ingest pipeline、索引模板以及kibana仪表，当你开启这些模块，Beats就可以自动将这些对象帮你创建好。</p><p>详情参见：<a href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules.html">https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules.html</a></p><p>ℹ️模块的功能要想使用，output必须是elasticsearch，否则你即便启用，也无法安装.</p><p>官方CSDN也有一个博文，进行了使用简介 <a href="https://elasticstack.blog.csdn.net/article/details/109178666">https://elasticstack.blog.csdn.net/article/details/109178666</a></p><p>现在，以采集系统日志为例.</p><p>ℹ️ 模块的采集可以单独用一个filebeat去跑。</p><p>在你执行下面的操作之前，请先确认两个条件：</p><ol><li>filebeat的output是Elasticsearch</li><li>filebeat配置了kibana地址</li></ol><p>首先，在kibana中创建图表，在elasticsearch中创建索引管理模板</p><pre><code class="language-bash">./filebeat setup --dashboards --index-management </code></pre><p>其次，开启模块</p><pre><code class="language-bash">./filebeat modules enable system</code></pre><p>然后，在elasticsearch中创建ingest pipeline</p><pre><code class="language-bash">./filebeat setup --pipelines --modules system</code></pre><p>此时，你可以在kibana后台找到</p><ol><li>索引生命周期策略：filebeat</li><li>索引模式：filebeat-*</li><li>各种dashboard</li><li>以及ingest pipeline</li></ol><p>ℹ️模块默认创建的索引都是filebeat-xxx，因此索引模式也是 filebeat-*，其 dashboard 数据设置的时候通过内置的字段来区分，例如system是通过 event.dataset:system.syslog 过滤.</p><p>最后，针对开启的模块，配置相关文件。默认这些文件都在 models.d，在没有 enable 的时候，文件后缀都是 disabled。</p><pre><code class="language-yml"># Module: system# Docs: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-system.html- module: system  syslog:    enabled: true    var.paths: [&quot;/path/to/log/syslog*&quot;]  auth:    enabled: true    var.paths: [&quot;/path/to/log/auth.log*&quot;]</code></pre><p>最后启动filebeat，查看数据是否有录入</p><pre><code class="language-bash">./filebeat -e -d &quot;*&quot;</code></pre><p>ℹ️因为通过模块创建的默认索引名字都是 filebeat-开头的，所以为了避免索引模式匹配到其它索引，建议非模块创建的索引名不以Beats组件名开头.</p><p>如果使用模块的时候，filebeat.output 指向了 logstash，那么logstash.output应该参照如下配置</p><pre><code class="language-conf">output &#123;  if [@metadata][pipeline] &#123;    elasticsearch &#123;      hosts =&gt; &quot;https://myEShost:9200&quot;      manage_template =&gt; false      index =&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;      pipeline =&gt; &quot;%&#123;[@metadata][pipeline]&#125;&quot;       user =&gt; &quot;elastic&quot;      password =&gt; &quot;secret&quot;    &#125;  &#125; else &#123;    elasticsearch &#123;      hosts =&gt; &quot;https://myEShost:9200&quot;      manage_template =&gt; false      index =&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;      user =&gt; &quot;elastic&quot;      password =&gt; &quot;secret&quot;    &#125;  &#125;</code></pre><p>ℹ️pipeline =&gt; “%{[@metadata][pipeline]}” 确保了 logstash.output 可以正确的找到 elasticsearch 中的 ingest pipeline.</p><h3 id="遇到的问题点">遇到的问题点</h3><ul><li>⚠️filebeat读取的文件，一定要有行结尾符，否则将无法载入。</li></ul><pre><code class="language-bash">[root@es logs]# file 16033741222_2021_11_04_09_33_23_1703_15_1.6.13.json16033741222_2021_11_04_09_33_23_1703_15_1.6.13.json: ASCII text, with very long lines, with no line terminators</code></pre><p>上述文件中，通过file指令可以看到提示<code>with no line terminators</code></p><ul><li><p>⚠️模块并不能适配所有情况，例如system模块读取centos的auth级别日志（secure)就无法适配dashboard，可能只适配ubuntu的日志。</p></li><li><p>⚠️filebeat当前的output.logstash插件无法实现根据fields字段来动态选择管道.</p></li><li><p><code>Dropping event: no topic could be selected</code> output.kafka 中 topic 定义的变量有问题。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞00ELK基本使用-简介</title>
      <link href="posts/f8ca35d2/"/>
      <url>posts/f8ca35d2/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><ul><li>各组件总下载页: <a href="https://www.elastic.co/cn/downloads/">https://www.elastic.co/cn/downloads/</a></li><li>容器下载页: <a href="https://www.docker.elastic.co">https://www.docker.elastic.co</a></li></ul><ol><li>Elasticsearch 搜索分析 <a href="https://www.elastic.co/cn/downloads/elasticsearch">https://www.elastic.co/cn/downloads/elasticsearch</a></li><li>Logstash 转换输出 <a href="https://www.elastic.co/cn/downloads/logstash">https://www.elastic.co/cn/downloads/logstash</a></li><li>Beats 收集 <a href="https://www.elastic.co/cn/downloads/beats/">https://www.elastic.co/cn/downloads/beats/</a></li><li>Kibana 展示 <a href="https://www.elastic.co/cn/downloads/kibana">https://www.elastic.co/cn/downloads/kibana</a></li></ol><h2 id="数据过程">数据过程:</h2><p>data ☞ beats ☞ kafka ☞ Logstash ☞ Elasticsearch (master node) + data node ☞ Kibana</p><p>beats 是轻量的数据提取端，它有很多类型，例如：日志文件（Filebeat），网络数据（Packetbeat），服务器指标（Metricbeat），这些 beats 都依赖<a href="https://github.com/elastic/beats/tree/master/libbeat">libbeat</a></p><p>logstash 不是必须的，但当你需要充当beats的聚合器或者过滤数据的时候，logstash可以帮你.</p><p>ℹ️beats 发送数据以事件为一个单位，例如一行就是一个<code>事件</code></p><h2 id="结构">结构</h2><p><img src="https://www.elastic.co/guide/en/logstash/7.15/static/images/deploy4.png" alt="部署4"></p><p>上图是logstash官方的一个图。kafka作为数据中心，来保证消息的持久性。Beats将数据持久存储在kafka，logstash从kafka中消费。</p>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞基本认证</title>
      <link href="posts/18de0eed/"/>
      <url>posts/18de0eed/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># 安装 htpasswd 工具yum install httpd-tools -y# 生成密码文件htpasswd -bc /usr/local/nginx/conf/.passwd usera pwd # 创建用户usera, 并写入 .passwdhtpasswd -b /usr/local/nginx/conf.passwd userb pwd  # 追加用户 userbhtpasswd -D /usr/local/nginx/conf/.passwd usera # 删除用户# nginx配置server &#123;    listen       80;    server_name  xxx.com;    index index.html;    location /auth &#123;        auth_basic &quot;nginx auth&quot;;        auth_basic_user_file /usr/local/nginx/conf/.passwd;        alias /export/webapps/xxx.com;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞软raid创建</title>
      <link href="posts/a7611bc1/"/>
      <url>posts/a7611bc1/</url>
      
        <content type="html"><![CDATA[<h2 id="linux-☞-软-raid-创建">linux ☞ 软 raid 创建</h2><ul><li>创建</li></ul><pre><code class="language-bash"># 磁盘分区fdisk /dev/sdafdisk /dev/sdb# 构建raid0# --level raid级别# --raid-devices 盘数# --chunk 条带深度，决定了数据分割的标准单位大小，数值越小，则数据越分散，性能越低(如若没有特殊优化需求，建议选默认值即可)mdadm -Cv /dev/md0 --level=0 --raid-devices=2 /dev/sda1 /dev/sdb1# 已上配置中, 也可以不分区, 直接进行 raid 构建mdadm --create --verbose /dev/md0 --level=0 --name=MY_RAID --raid-devices=number_of_volumes device_name1 device_name2# 观察和等待阵列初始化cat /proc/mdstat# 观察初始化后的阵列信息mdadm --detail /dev/md0# 格式化 （加卷标）mke2fs -t ext4 -L raid0 /dev/md0# 写入配置# 不同的操作系统 mdadm.conf 位置不同, 具体以 man mdadm.conf 为准mdadm --detail --scan | tee -a /etc/mdadm.conf# echo &quot;DEVICE /dev/sda1 /dev/sdb1 &quot; &gt;&gt; /etc/mdadm/mdadm.conf# mdadm -Ds &gt;&gt; /etc/mdadm/mdadm.conf# 创建新的 Ramdisk Image 以为新的 RAID 配置正确地预加载块储存设备模块sudo dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)# 写入挂载 （用卷标挂载，有些系统重启后，设备名会从md0变成md127）echo &quot;LABEL=raid0 /data ext4 defaults,nofail 0 2&quot; &gt;&gt; /etc/fstabmkdir /datamount -a# 确认挂载成功df -h</code></pre><ul><li>删除</li></ul><pre><code class="language-bash"># 删除/etc/fstab的挂载信息$ mdadm -S /dev/md0$ mdadm --misc --zero-superblock /dev/sda$ mdadm --misc --zero-superblock /dev/sdb# 删除/etc/mdadm/mdadm.conf文件中添加的DEVICE行和ARRAY行</code></pre><ul><li>额外信息</li></ul><pre><code class="language-bash">#2T以上大小分区parted /dev/sda mklabel gpt mkpart primary1 0% 100%partprobe</code></pre><ul><li>关于云</li></ul><pre><code class="language-bash">当使用云端磁盘构建raid的时候,且又想进行 raid 备份,则务必先停止io操作,停止io操作的方法最好是 umount 或者停机. 否则会导致 raid 数据完整性出现问题.</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> raid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞s3 初始化安装</title>
      <link href="posts/2c5a64ff/"/>
      <url>posts/2c5a64ff/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>脚本的目的：</p><ol><li>创建S3</li><li>添加生命周期</li><li>创建iam规则</li></ol><blockquote><p>s3官方授权示例：<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/example-policies-s3.html">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/example-policies-s3.html</a></p></blockquote><h3 id="主体脚本">主体脚本</h3><pre><code class="language-bash">#!/bin/bash# http://docs.amazonaws.cn/general/latest/gr/rande.html#s3_region# us-east-1 us-west-1 等# 需要s3权限和iam权限# 需要先编写 s3-lifecycle.json#桶名read -p &quot;输入s3桶名=&quot; S3BucketName#所属项目read -p &quot;输入项目名=&quot; TeamNameread -p &quot;输入AWS_ACCESS_KEY_ID=&quot; AWS_ACCESS_KEY_IDread -p &quot;输入AWS_SECRET_ACCESS_KEY=&quot; AWS_SECRET_ACCESS_KEYread -p &quot;输入AWS_DEFAULT_REGION=&quot; AWS_DEFAULT_REGIONexport AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_IDexport AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEYexport AWS_DEFAULT_REGION=$AWS_DEFAULT_REGIONS3LocationConstraint=$&#123;AWS_DEFAULT_REGION&#125;[[ $&#123;S3LocationConstraint&#125; == 'us-east-1' ]] &amp;&amp; aws s3api create-bucket --bucket $&#123;S3BucketName&#125; --region $&#123;S3LocationConstraint&#125; || aws s3api create-bucket --bucket $&#123;S3BucketName&#125; --region $&#123;S3LocationConstraint&#125; --create-bucket-configuration LocationConstraint=$&#123;S3LocationConstraint&#125;aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'conf/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'data/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'backup/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/7days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/15days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/30days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/60days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/90days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/longlasting/'aws s3api put-bucket-tagging --bucket $&#123;S3BucketName&#125; --tagging &quot;TagSet=[&#123;Key=Team,Value=$&#123;TeamName&#125;&#125;]&quot;aws s3api put-bucket-lifecycle-configuration --bucket $&#123;S3BucketName&#125; --lifecycle-configuration file://s3-lifecycle.jsoncat &gt; s3-program.rule &lt;&lt; EOF&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:PutObject&quot;,                &quot;s3:GetObject&quot;,                &quot;s3:DeleteObject&quot;,                &quot;s3:PutObjectAcl&quot;,                &quot;s3:GetObjectAcl&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/7days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/15days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/30days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/60days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/90days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/longlasting/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/conf/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/data/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/backup/*&quot;            ]        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListBucket&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;        &#125;    ]&#125;EOFcat &gt; s3-local.rule &lt;&lt; EOF&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListBucket&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:PutObject&quot;,                &quot;s3:GetObject&quot;,                &quot;s3:DeleteObject&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/7days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/15days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/30days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/60days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/90days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/longlasting/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/conf/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/data/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/backup/*&quot;            ],            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor2&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:GetBucketLocation&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor3&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListAllMyBuckets&quot;,            &quot;Resource&quot;: &quot;*&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;    ]&#125;EOFaws iam create-policy --policy-name s3-$&#123;S3BucketName&#125;-role --description &quot;For role use only!!!!!!!!!!!!&quot; --policy-document  file://s3-program.ruleaws iam create-policy --policy-name s3-$&#123;S3BucketName&#125;-local --description &quot;Limit the source IP!!!!!!!!!!!!&quot; --policy-document file://s3-local.ruleecho &quot;程序用户规则: s3-$&#123;S3BucketName&#125;-role 已生成&quot;echo &quot;本地用户规则：s3-$&#123;S3BucketName&#125;-local 已生成&quot;echo &quot;请将 local 规则关联到对应个人用户或组&quot;echo &quot;请将 role 规则关联到角色&quot;</code></pre><h3 id="生命周期规则-s3-lifecycle-json">生命周期规则 s3-lifecycle.json</h3><blockquote><p>规则说明：</p><ol><li>所有对象，30天之后转为ONEZONE_IA;</li><li>logs 前缀单独定义：<ul><li>days 路径下的对象保存对应的天数</li><li>longlasting 路径下的对象永久保存，但是 90 天之后的对象转换为 GLACIER</li></ul></li></ol></blockquote><pre><code class="language-shell">&#123;  &quot;Rules&quot;: [      &#123;          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;&quot;          &#125;,           &quot;Status&quot;: &quot;Enabled&quot;,           &quot;Transitions&quot;: [              &#123;                  &quot;Days&quot;: 30,                   &quot;StorageClass&quot;: &quot;ONEZONE_IA&quot;              &#125;          ],           &quot;NoncurrentVersionTransitions&quot;: [              &#123;                  &quot;NoncurrentDays&quot;: 30,                   &quot;StorageClass&quot;: &quot;ONEZONE_IA&quot;              &#125;          ],           &quot;ID&quot;: &quot;30days_onezone_ia&quot;      &#125;,      &#123;          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/longlasting/&quot;          &#125;,           &quot;Status&quot;: &quot;Enabled&quot;,           &quot;Transitions&quot;: [              &#123;                  &quot;Days&quot;: 90,                   &quot;StorageClass&quot;: &quot;GLACIER&quot;              &#125;          ],           &quot;ID&quot;: &quot;logs_90day_glacier&quot;      &#125;,       &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/7days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 7          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_7days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;:  1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/15days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 15          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_15days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;:  1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/30days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 30          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_30days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/60days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 60          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_60days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/90days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 90          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_90days_before&quot;      &#125;  ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> s3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker☞01安装</title>
      <link href="posts/b7cf7b1f/"/>
      <url>posts/b7cf7b1f/</url>
      
        <content type="html"><![CDATA[<h2 id="依赖">依赖</h2><pre><code class="language-bash">yum install -y yum-utils \  device-mapper-persistent-data \  lvm2</code></pre><h2 id="仓库">仓库</h2><blockquote><p>官方 repo: <a href="https://download.docker.com/linux/centos/docker-ce.repo">https://download.docker.com/linux/centos/docker-ce.repo</a></p></blockquote><pre><code class="language-bash:">yum-config-manager \    --add-repo \    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</code></pre><h2 id="安装">安装</h2><pre><code class="language-bash">yum install docker-ce -y### 安装 compose ， 可选yum install python3-pip -ypip3 install --upgrade pip &amp;&amp; pip3 install docker-composepip3 install docker-compose### 开机自启动systemctl enable docker</code></pre><h3 id="安装-runlike-命令">安装 runlike 命令</h3><p>runlike命令可以输出docker启动的时候的指令</p><pre><code class="language-bash">pip3 install runlikerunlike &lt;container_id&gt; # 输出run指令不过需要注意的是，这个命令并不完美，比如输出不了mount参数，因为你还需要通过docker inspect &lt;container_id&gt;来查看mount指令</code></pre><h2 id="修改-docker-默认数据目录">修改 docker 默认数据目录</h2><pre><code class="language-bash">[[ -f /etc/docker/daemon.json ]] &amp;&amp; mv /etc/docker/daemon.json /etc/docker/daemon.json.default || &#123; mkdir -p /etc/docker/ &amp;&amp;  touch /etc/docker/daemon.json &#125;cat &gt;  /etc/docker/daemon.json &lt;&lt; EOF&#123;  &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;],  &quot;data-root&quot;: &quot;/export/docker-data-root&quot;,  &quot;storage-driver&quot;: &quot;overlay2&quot;,  #&quot;insecure-registries&quot;: [&quot;&quot;]&#125;EOF</code></pre><p>✨insecure-registries 指向 http 协议的镜像仓库。</p><h2 id="启动">启动</h2><pre><code class="language-bash">systemctl start docker</code></pre><h2 id="镜像库">镜像库</h2><pre><code class="language-bash">docker hubquay.io</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞01常见术语概念</title>
      <link href="posts/aaf9b7/"/>
      <url>posts/aaf9b7/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p><a href="https://kubernetes.io/zh/docs/concepts/overview/components/">Kubernetes 组件 | Kubernetes</a></p><p><a href="https://kubernetes.io/zh/docs/reference/glossary/?all=true">词汇表 | Kubernetes</a></p><h2 id="结构图">结构图</h2><p><img src="https://d33wubrfki0l68.cloudfront.net/2475489eaf20163ec0f54ddc1d92aa8d4c87c96b/e7c81/images/docs/components-of-kubernetes.svg" alt="Kubernetes 组件"></p><p><img src="/posts/aaf9b7/image-20220326095545303.png" alt="image-20220326095545303"></p><ol><li><p>Control Plane 是 k8s 的 master 节点，负责协调集群中的所有活动，例如调度应用程序、维护应用程序的所需状态、扩展应用程序和滚动更新。</p><blockquote><p>control plane 和 master 是同义</p></blockquote></li><li><p>Node 是 k8s 的工作节点，也就是实际主要跑容器的节点。</p></li></ol><h2 id="组件">组件</h2><p><a href="https://kubernetes.io/docs/concepts/overview/components/">https://kubernetes.io/docs/concepts/overview/components/</a></p><p><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/">https://kubernetes.io/docs/reference/command-line-tools-reference/</a></p><p>Control Plane 节点组件</p><ol><li>kube-apiserver  是集群入口，对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件行为均通过 apiserver 进行通信。</li><li>kube-controller-manager 维护管理各种类型的crontroller，例如常见的Deployment Controller，因此kube-controller-manager通过不同类型的Controller来实现故障检测、滚动升级、node扩展等，是集群所有资源对象的自动化管理中心。</li><li>kube-scheduler 是负责 pod 的调度, 即如何将 pod 分发到 node。</li><li>etcd 注册中心，保存集群状态。</li></ol><p>Node 节点组件</p><ol><li><p>kubelet 是节点代理程序，部署在 node 上。它是k8s master和k8s node之间的纽带。处理 pod 创建/启动/监控/重启/销毁等工作。</p><p>开启 register-node = true 的情况下 会自动向 apiserver 服务注册自己。</p></li><li><p>kube-proxy 是节点的网络代理，是 service 资源对象的一部分功能实现，解决节点上各资源的通信</p></li><li><p>Container Runtime 是节点上负责运行容器的环境。它可以是Docker、containerd等。</p></li></ol><p>插件</p><p>常用的有DNS插件CoreDNS、网络插件Flannel、可视化插件DashBoard等</p><p><a href="https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/">https://kubernetes.io/zh/docs/concepts/cluster-administration/addons/</a></p><h2 id="对象">对象</h2><p>kubernetes 通过实体化的对象来表述资源、策略、状态。</p><p>正常情况下，kubernetes 将会尽可能的达到对象的期望状态。</p><p>上句话的意思就是，对象基本都包含两个字段，分别是 spec 和 status。</p><ul><li><p>spec 表示对象应该拥有的状态特性</p></li><li><p>status 表示对象当前拥有的状态特性</p></li></ul><p>kubernetes 总是想尽办法将 status 向 spec 靠拢。</p><p>创建一个对象，需要一份 yaml 配置，配置里包含如下必备信息：</p><ul><li><code>apiVersion</code> - 创建该对象所使用的 Kubernetes API 的版本</li><li><code>kind</code> - 想要创建的对象的类别</li><li><code>metadata</code> - 帮助唯一性标识对象的一些数据，包括一个 <code>name</code> 字符串、UID 和可选的 <code>namespace</code></li><li><code>spec</code> - 你所期望的该对象的状态</li></ul><p>kubectl 命令调用配置，并将信息转换为 json 发给 Apiserver。</p><p>一个 yaml 配置例子如下：</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deploymentspec:  selector:    matchLabels:      app: nginx  replicas: 2 # tells deployment to run 2 pods matching the template  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80</code></pre><blockquote><p>不同对象的 spec 必然是不一样的，通过 API 文档可以获取</p><p><a href="https://kubernetes.io/docs/reference/kubernetes-api/">https://kubernetes.io/docs/reference/kubernetes-api/</a></p></blockquote><p>介绍一些常用的对象：</p><ol><li><p>Namespace</p><p>命名空间是一个逻辑概念，起到了隔离作用。通过它，你可以找到它内部的其它资源对象，例如 pod，svc，deployment等</p></li><li><p>Pod</p><p>pod 内包含多个容器，所以多个容器共享以下资源。</p><ul><li>PID命名空间: pod内的进程能互相看到PID</li><li>网络命名空间: pod中的多个容器共享一个ip (唯一)</li><li>IPC命名空间: pod中的多个容器之间可以互相通信</li><li>UTS命名空间: pod中的多个容器共享一个主机名 (唯一)</li><li>存储卷: pod多个容器可以共同访问pod定义的存储卷</li></ul></li><li><p>Label</p><p>label用于表示资源，从而方便的被其它资源找到。它很重要。</p><p>标签定义 <code>key: value</code></p><p>选择器: <code>key &lt;= !=&gt; value</code>  <code>key &lt;not&gt; in (value1, value2)</code></p><p>ReplicaSet  和 Service 通过 selector 选择器来选择 Pod 对象, 从而精细化的将 Pod 进行分组。一旦某个 pod 的 Label 被修改，那么这个 pod 将从 ReplicaSet 中脱离，而 ReplicaSet 会重新创建新的 pod 补足 ReplicaSet 定义的副本数。</p></li><li><p>Deployment</p><p>无状态副本集。用来部署一个 pod 组，它通过 ReplicaSet 来进行缩放， 并需要 pod 模板创建 pod， 需要 Label 监控 pod。Deployment 通过 ReplicaSet 严格执行配置所定义的pod副本数量。应该始终使用 Deployment 来创建 pod。</p></li><li><p>Satefulset</p><p>有状态副本集。</p></li><li><p>Service</p><p>svc 用来构建一个负载配置。可以比喻为aws的elb内网负载均衡器或者nginx的service proxy配置。</p><p>svc 通过 pod 定义的 label 发现一组 pod。这些 pod 本身有 endpoint 地址。</p><p>svc 创建后，会拿到一个集群内部ip和dns。kube-proxy 服务会将 svc 的 ip 和 pod 的 endpoint 地址关联起来。</p><p>最终，其它容器可以通过这个ip和dns来访问svc关联的pod资源。</p></li><li><p>Ingress</p><p>管理集群服务外部访问的API对象，典型的访问方式是 HTTP</p></li></ol><h2 id="对象操作实际转化">对象操作实际转化</h2><p>客户端对 api server 发起的请求，都会转化为 http 请求</p><p>例如发起一个针对 deployment.default 对象的请求，</p><p>其 request path 是：</p><p>http://<apiserver>:6443/apis/apps/v1/namespaces/default/deployments/my-deployment/</apiserver></p><blockquote><p>这里 /apps/v1 指的是 kubectl api-versions 列出的 versions。</p></blockquote><p>可以针对对象发起的http请求动作是：</p><p>GET/POST/PUT/DELETE</p><p>转化为API的动作是：（即你用kubectl调用的动作）</p><p>get/list/create/update/patch/watch/proxy/redirect/delete/deletecollection</p><h2 id="服务组件流程图">服务组件流程图</h2><blockquote><ul><li>apiserver 对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件行为均通过 apiserver 进行通信</li><li>apiserver 也会直接调用 kubelet API（如 logs, exec, attach 等），默认不校验 kubelet 证书，但可以通过 <code>--kubelet-certificate-authority</code> 开启</li></ul></blockquote><p><img src="/posts/aaf9b7/image-20200823103640732.png" alt="image-20200823103640732"></p><blockquote><p>以一个pod的构建为例:</p><ul><li>用户通过提交请求到Apiserver创建一个 Pod，apiserver 将 pod 状态写入 etcd。</li><li>scheduluer 通过 etcd 检测到未绑定 Node 的 Pod，开始调度并给pod分配node，并更新etcd的pod绑定状态。</li><li>kubelet 通过 etcd 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod，并更新Pod运行状态。</li></ul></blockquote><p><img src="/posts/aaf9b7/image-20200823102430144.png" alt="image-20200823102430144"></p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>007-kafka基本保活</title>
      <link href="posts/41d19ce/"/>
      <url>posts/41d19ce/</url>
      
        <content type="html"><![CDATA[<h2 id="保活工具">保活工具</h2><p>monit</p><h3 id="配置">配置</h3><pre><code class="language-bash">check process zookeeper with pidfile /export/kafka/zookeeper.pid  start program = &quot;/bin/bash -c 'source /etc/profile &amp;&amp; cd /export/kafka &amp;&amp; ./zookeeper.start'&quot;    as uid &quot;root&quot; and gid &quot;root&quot;  stop program = &quot;/bin/bash -c 'source /etc/profile &amp;&amp; cd /export/kafka &amp;&amp; ./bin/zookeeper-server-stop.sh &amp;&amp; rm -rf zookeeper.pid'&quot;    as uid &quot;root&quot; and gid &quot;root&quot;if failed port 2181 then alertcheck process kafka with pidfile /export/kafka/kafka.pid  depends on zookeeper  start program = &quot;/bin/bash -c 'source /etc/profile &amp;&amp; cd /export/kafka &amp;&amp; ./kafka.start'&quot;    as uid &quot;root&quot; and gid &quot;root&quot;  stop program = &quot;/bin/bash -c 'source /etc/profile &amp;&amp; cd /export/kafka &amp;&amp; ./bin/kafka-server-stop.sh &amp;&amp; rm -rf kafka.pid'&quot;    as uid &quot;root&quot; and gid &quot;root&quot;</code></pre><h3 id="脚本">脚本</h3><p>/export/kafka/zookeeper.start</p><pre><code class="language-bash">#!/bin/bashset -eucd /export/kafka[[ -d zk_data ]] &amp;&amp; nohup bin/zookeeper-server-start.sh config/zookeeper.properties &gt;&gt; zookeeper.nohup &amp;sleep 1ps aux | grep 'zookeeper.properties' | grep -v grep | awk '&#123;print $2&#125;' &gt; zookeeper.pid</code></pre><p>/export/kafka/kafka.start</p><pre><code class="language-bash">#!/bin/bashset -eucd /export/kafka[[ -d data ]] &amp;&amp; nohup bin/kafka-server-start.sh config/server.properties &gt;&gt; kafka.nohup &amp;sleep 1ps aux | grep 'server.properties' | grep -v grep | awk '&#123;print $2&#125;' &gt; kafka.pid</code></pre>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> kafka </tag>
            
            <tag> monit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>006-kafka基本监控</title>
      <link href="posts/1a545f72/"/>
      <url>posts/1a545f72/</url>
      
        <content type="html"><![CDATA[<h2 id="采集工具">采集工具</h2><p><a href="https://github.com/danielqsj/kafka_exporter">https://github.com/danielqsj/kafka_exporter</a></p><h3 id="安装">安装</h3><pre><code class="language-bash">appVersion=1.4.2mkdir -p /export/srccd /export/srccurl &quot;https://github.com/danielqsj/kafka_exporter/releases/download/v$&#123;appVersion&#125;/kafka_exporter-$&#123;appVersion&#125;.linux-amd64.tar.gz&quot; -o kafka_exporter.tgz &amp;&amp; tar xf kafka_exporter.tgz &amp;&amp; mv kafka_exporter-$&#123;appVersion&#125;.linux-amd64 ../kafka_exportercd ../kafka_exporter./kafka_exporter \--kafka.server=kafka001:8123 \--sasl.enabled \--sasl.username=&quot;broker&quot; \--sasl.password=&quot;broker&quot; \--sasl.mechanism=&quot;scram-sha256&quot;</code></pre><p>ℹ️默认<code>kafka_exporter</code>监听在<code>9308</code>端口上.</p><p>⚠️在1.4.2版本中，kafka_exporter在部分系统中存在无法正确的解析hostname的问题，需直接指明IP。</p><h3 id="添加到开机启动">添加到开机启动</h3><p>/etc/crontab</p><pre><code class="language-bash">@reboot root sleep 120;/export/kafka_exporter/kafka_exporter --kafka.server=kafka001:8123 --sasl.enabled --sasl.username=&quot;broker&quot; --sasl.password=&quot;broker&quot; --sasl.mechanism=&quot;scram-sha256&quot; &gt;&gt; /export/kafka_exporter/kafka_exporter.nohup 2&gt;&amp;1</code></pre><h2 id="监控系统">监控系统</h2><h3 id="Prometheus">Prometheus</h3><p>主配置里加入</p><pre><code class="language-yml">  - job_name: &quot;kafka_exporter&quot;    metrics_path: /metrics    static_configs:      - targets: ['kafka001:9038'] # kafka_exporter地址和端口        labels:          instance: it-zz-kafka-cluster</code></pre><p>告警规则里加入</p><p><a href="https://awesome-prometheus-alerts.grep.to/rules#kafka-1">https://awesome-prometheus-alerts.grep.to/rules#kafka-1</a></p><pre><code class="language-yml">  - alert: KafkaTopicsReplicas    expr: sum(kafka_topic_partition_in_sync_replica) by (topic) &lt; 3    for: 0m    labels:      severity: critical    annotations:      summary: Kafka topics replicas (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Kafka topic in-sync partition\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;  - alert: KafkaConsumersGroup    expr: sum(kafka_consumergroup_lag) by (consumergroup) &gt; 50    for: 1m    labels:      severity: critical    annotations:      summary: Kafka consumers group (instance &#123;&#123; $labels.instance &#125;&#125;)      description: &quot;Kafka consumers group\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h3 id="grafana">grafana</h3><p>Grafana Dashboard ID: 7589, name: Kafka Exporter Overview.</p>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>005-kafka基本维护</title>
      <link href="posts/124a3d6d/"/>
      <url>posts/124a3d6d/</url>
      
        <content type="html"><![CDATA[<h1>分区重新分配工具</h1><p>bin/kafka-reassign-partitions.sh 可以用来调整分区副本的重新分布，包括增加副本，迁移副本等。</p><p>此工具有三大选项：</p><ul><li>–generate：创建分配计划，用户通过指定topic列表和broker列表来生成指定topic列表的【分区重新分配计划】。</li><li>–execute:  执行分配计划，通过【分区重新分配计划】或者自定义分配计划进行实际行动。</li><li>–verify: 核实计划执行情况，返回计划执行状态，也可以重复的添加/删除分配过程中的网络限流。</li></ul><h2 id="扩展集群">扩展集群</h2><p>集群扩展意味着增加一个新的broker。</p><p>kafka不会自动平衡数据。</p><p>需要手动启动分区重新分配工具（bin/kafka-reassign-partitions.sh）来进行自动化分配。</p><h3 id="创建分配计划">创建分配计划</h3><p>编写【指定要迁移】的topic列表文件<code>topics-to-move.json</code></p><pre><code class="language-json"> &#123;&quot;topics&quot;: [&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;,              &#123;&quot;topic&quot;: &quot;foo2&quot;&#125;],  &quot;version&quot;:1  &#125;</code></pre><p>将topic列表里的topic迁移到<code>broker.id</code>为&quot;5和6&quot;的节点上.</p><pre><code class="language-bash">bin/kafka-reassign-partitions.sh \--command-config config/my.tools.properties \--bootstrap-server kafka1:8123 \--topics-to-move-json-file topics-to-move.json \--broker-list &quot;5,6&quot; \--generate</code></pre><p>输出如下：先输出当前topic分区的分布配置，然后输出调整后的分布配置</p><pre><code class="language-json">Current partition replica assignment  &#123;&quot;version&quot;:1,  &quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,                &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,                &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,                &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,                &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;,                &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;]  &#125;Proposed partition reassignment configuration  &#123;&quot;version&quot;:1,  &quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,                &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,                &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,                &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,                &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;,                &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;]  &#125;</code></pre><p>将上面输出的两部分配置，分别保存为当前topic分区落点配置<code>topics-current-plan.json</code>和预期topic分区落点配置<code>topics-expect-plan.json</code></p><h3 id="执行分配计划">执行分配计划</h3><pre><code class="language-bash">bin/kafka-reassign-partitions.sh \--command-config config/my.tools.properties \--bootstrap-server kafka1:8123 \--reassignment-json-file topics-expect-plan.json \--execute</code></pre><p>ℹ️自定义分配计划只需要满足上面分配计划的文件格式即可.</p><blockquote><p>添加限速：–throttle 50000000  限速50MB/s  你可以多次重复执行上述命令并通过新的限速值来动态调整限速.</p></blockquote><h3 id="检查分配计划">检查分配计划</h3><pre><code class="language-bash">bin/kafka-reassign-partitions.sh \--command-config config/my.tools.properties \--bootstrap-server kafka1:8123 \--reassignment-json-file topics-expect-plan.json \--verify</code></pre><p>ℹ️如果–execute期间添加了–throttle，则–verify会在迁移完成后清空–throttle</p><h2 id="分区副本因子调整">分区副本因子调整</h2><p>调整副本因子，其实就是自定义分区配置，参考上面的配置，仅针对需要调整的topic的分区自定义配置并执行即可。</p><h2 id="问题点">问题点</h2><h3 id="限速带来的问题">限速带来的问题</h3><p>使用限速可能会导致复制速度过慢，以至于复制速度低于生产者写入速度。这会导致复制数据滞后越来越大。因此，应该在复制期间，监控滞后状态。这里的复制速度，就是消费速度。</p><p>滞后指标如下：</p><pre><code class="language-bash">kafka.server:type=FetcherLagMetrics,name=ConsumerLag,clientId=([-.\w]+),topic=([-.\w]+),partition=([0-9]+)</code></pre><p>如果发现指标没有降低，则应该放宽限速。</p><p>关于消费速度滞后监控，可以采取prometheus+grafana。</p>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>004-kafka用户配额</title>
      <link href="posts/14fdb3fd/"/>
      <url>posts/14fdb3fd/</url>
      
        <content type="html"><![CDATA[<h1>简要</h1><p>配额可以应用于认证用户（user）、客户端ID（<a href="http://client.id">client.id</a>）</p><p>配额信息存储在zk中，认证用户的配额信息位于<code>/config/users</code>，客户端ID的配额信息位于<code>/config/clients</code></p><p>主要说下网络限速。</p><h2 id="添加配额">添加配额</h2><ul><li>producer_byte_rate 生产速率，单位是字节/s</li><li>consumer_byte_rate 消费速率，单位是字节/s</li></ul><pre><code class="language-bash">bin/kafka-configs.sh --bootstrap-server kafka1:8123 --alter --add-config 'producer_byte_rate=1024,consumer_byte_rate=2048' --entity-type users --entity-name user1 --entity-type clients --entity-name clientA --command-config config/my.tools.properties</code></pre><p>通过<code>user1</code>认证，且符合<code>client.id=clientA</code>的客户端，在生产和消费的时候，速度均限制在1KB/s</p><h2 id="列出配额">列出配额</h2><pre><code class="language-bash">bin/kafka-configs.sh --bootstrap-server kafka1:8123 --describe --entity-type users --entity-name user1 --entity-type clients --command-config config/my.tools.properties</code></pre><h2 id="删除配额">删除配额</h2><pre><code class="language-bash">bin/kafka-configs.sh --bootstrap-server kafka1:8123 --alter --delete-config 'producer_byte_rate' --entity-type users --entity-name user1 --entity-type clients --entity-name clientA --command-config config/my.tools.properties</code></pre><p>删除<code>user-principal 'user1', client-id 'clientA'</code>配额规则中的生产网络限制<code>producer_byte_rate</code></p>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>003-kafka基本认证</title>
      <link href="posts/2fb0e26c/"/>
      <url>posts/2fb0e26c/</url>
      
        <content type="html"><![CDATA[<p>kafka的认证是通过java认证技术JAAS实现。</p><p><a href="http://kafka.apache.org/documentation/#security_sasl_plain">http://kafka.apache.org/documentation/#security_sasl_plain</a></p><h1>服务端</h1><h2 id="开启用户认证配置">开启用户认证配置</h2><h3 id="添加超级用户">添加超级用户</h3><ol><li>创建超级用户</li></ol><pre><code class="language-bash"># 创建超级用户bin/kafka-configs.sh --zookeeper data01.zyh.cool:2181 --alter --add-config 'SCRAM-SHA-256=[password=broker]' --entity-type users --entity-name broker</code></pre><p>✨通过<code>SCRAM</code>将用户信息存放在<code>zookeeper</code>中</p><ol start="2"><li>超级用户客户端配置</li></ol><p>✨超级用户用于brokers之间互联和管理用户</p><pre><code class="language-bash">vim config/my.tools.properties</code></pre><pre><code class="language-properties">security.protocol=SASL_PLAINTEXTsasl.mechanism=SCRAM-SHA-256sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \  username=&quot;broker&quot; \  password=&quot;broker&quot;;</code></pre><h3 id="追加认证配置">追加认证配置</h3><p><a href="https://kafka.apache.org/documentation/#security_sasl">https://kafka.apache.org/documentation/#security_sasl</a></p><p><code>config/server.properties</code></p><p>✨认证地址（认证+明文）：SASL_PLAINTEXT</p><p>✨非认证地址（明文）：PLAINTEXT</p><pre><code class="language-properties"># 监听地址的通信协议和认证算法listeners=SASL_PLAINTEXT://data01.zyh.cool:8123,PLAINTEXT://data01.zyh.cool:9092advertised.listeners=SASL_PLAINTEXT://data01.zyh.cool:8123,PLAINTEXT://data01.zyh.cool:9092## 开启访问broker的认证机制sasl，以及配置所用的机制算法sasl.enabled.mechanisms=SCRAM-SHA-256 # brokers之间的通信协议和认证算法## 配置broker之间的网络通信协议，这里采用认证+明文。security.inter.broker.protocol=SASL_PLAINTEXT## 开启broker之间认证通信机制sasl，以及配置所用的机制算法sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256## brokers 之间的认证方式：sasl.jaas.config### sasl.jaas.config的写法: listener.name.&#123;listenerName&#125;.&#123;saslMechanism&#125;.sasl.jaas.configlistener.name.sasl_plaintext.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \    username=&quot;broker&quot; \    password=&quot;broker&quot;;</code></pre><p>✨<a href="http://kafka.apache.org/documentation/#security_sasl_scram">SASL/SCRAM</a>算法将用户数据存放在zk，因此<a href="http://kafka.apache.org/documentation/#security_sasl_scram">SASL/SCRAM</a>算法可以动态的增删用户。</p><h3 id="重启服务">重启服务</h3><pre><code class="language-bash">./kafka stop./kafka start</code></pre><h3 id="创建普通用户">创建普通用户</h3><p>创建一个用户名<code>elk</code>，密码<code>123456</code></p><pre><code class="language-bash">bin/kafka-configs.sh --bootstrap-server data01.zyh.cool:8123  --command-config config/my.tools.properties --alter --add-config 'SCRAM-SHA-256=[password=123456]' --entity-type users --entity-name elk</code></pre><h3 id="列出用户信息">列出用户信息</h3><pre><code class="language-bash">bin/kafka-configs.sh --bootstrap-server data01.zyh.cool:8123  --command-config config/my.tools.properties --describe --entity-type users --entity-name elk</code></pre><h3 id="删除用户">删除用户</h3><pre><code class="language-bash">bin/kafka-configs.sh --bootstrap-server data01.zyh.cool:8123  --command-config config/my.tools.properties --alter --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name elk</code></pre><h2 id="资源授权规则">资源授权规则</h2><p>💥默认情况下，任何用户访问任何资源都不需要授权。因此，需要针对资源添加授权限制。</p><p><a href="http://kafka.apache.org/documentation/#security_authz">http://kafka.apache.org/documentation/#security_authz</a></p><ol><li><p>ACL规则是针对资源<code>topic</code>或者<code>消费者组</code>添加规则</p></li><li><p>规则由<code>principal</code>+<code>host</code>+<code>operation</code>+<code>permissionType</code>组成</p><ul><li><p><code>principal</code>就是User或者Group ✨ 这个属性决定了是否需要使用认证端口8123</p></li><li><p><code>host</code>是允许的主机</p></li><li><p><code>operation</code>是允许的操作</p></li><li><p><code>permissionType</code>允许或拒绝</p></li></ul></li><li><p>💥一个请求，必须满足<code>topic</code>或者<code>消费者组</code>两类下的列出规则。</p><ul><li><p>对于生产者，需要匹配<code>topic</code>下的规则。</p></li><li><p>对于消费者，需要同时满足<code>topic</code>和<code>消费者组</code>规则。</p></li></ul></li></ol><h3 id="追加授权配置">追加授权配置</h3><pre><code class="language-properties"># ACL默认规则# 设置授权方式，这里是ACL授权authorizer.class.name=kafka.security.authorizer.AclAuthorizer# 设置超级账号，如果是多个需要分号分割，例如：User:admin;User:rootsuper.users=User:broker# true 的意思是：如果某个topic没有ACL规则，则所有用户都可以通过非认证端口9092访问这个topic。allow.everyone.if.no.acl.found=true</code></pre><p>✨上述配置默认情况下仅允许<code>broker</code>用户是超级用户可以无视ACL规则。</p><p>✨其它用户发起的请求，不管是访问认证端口8123还是非认证端口9092，均没有权限。</p><h3 id="重启服务-2">重启服务</h3><pre><code class="language-bash">./kafka stop./kafka start</code></pre><h3 id="授权命令">授权命令</h3><p><a href="http://kafka.apache.org/documentation/#security_authz_cli">http://kafka.apache.org/documentation/#security_authz_cli</a></p><p><code>bin/kafka-acls.sh</code>用来授权的工具</p><p>常用的参数：</p><p>–bootstrap-server 指定broker列表</p><p>–add 添加</p><p>–remove 删除</p><p>–producer 指定生产者规则，在没有operation参数时，默认就是只写</p><p>–consumer 指定消费者规则，在没有operation参数时，默认就是只读</p><p>–operation 授权列表（READ、WRITE、DELETE、CREATE、ALTER、DESCRIBE、ALL）</p><p>–allow-principal User:[用户名] 允许指定用户</p><p>–deny-prinicipal User:[用户名] 拒绝指定用户</p><p>–allow-host [Host] 允许的主机网络</p><p>–deny-host [Host] 拒绝的主机网络</p><p>–group [group_name] 此规则仅能用于此消费者组</p><p>–resource-pattern-type 匹配类型，常用的有前缀匹配 prefixed</p><p>–topic 以字符串匹配的主题</p><p>–command-config 指定管理员配置文件</p><h3 id="绑定写入权限">绑定写入权限</h3><pre><code class="language-bash">bin/kafka-acls.sh \--bootstrap-server data01.zyh.cool:8123 \--command-config config/my.tools.properties \--topic zz.it.elk. \--resource-pattern-type prefixed \--add \--allow-principal User:elk \--producer</code></pre><p>针对访问资源是<code>zz.it.elk.</code>开头的 topic， 添加用户<code>elk</code>只写权限。</p><h3 id="绑定只读权限">绑定只读权限</h3><pre><code class="language-bash">bin/kafka-acls.sh \--bootstrap-server data01.zyh.cool:8123 \--command-config config/my.tools.properties \--group logstash \--topic zz.it.elk. \--resource-pattern-type prefixed \--add \--allow-principal User:* \--consumer</code></pre><p>针对访问资源是<code>zz.it.elk.</code>开头的 topic，且访问的消费者组是<code>logstash</code>，添加所有用户的只读权限。</p><blockquote><p>这条规则因允许所有用户，所以客户端可以使用非认证端口9092，但消费者组必须是<code>logstash</code>。</p></blockquote><p>✨–consumer参数必须同时使用–group和–topic</p><h3 id="列出ACL规则">列出ACL规则</h3><p>列出所有授权规则</p><blockquote><p>可以通过追加 --principal User:elk 列出某个认证用户使用的规则.</p></blockquote><pre><code class="language-bash">bin/kafka-acls.sh \--bootstrap-server data01.zyh.cool:8123 \--command-config config/my.tools.properties \--list Current ACLs for resource `ResourcePattern(resourceType=TOPIC, name=zz.it.elk., patternType=PREFIXED)`:        (principal=User:elk, host=*, operation=CREATE, permissionType=ALLOW)        (principal=User:elk, host=*, operation=WRITE, permissionType=ALLOW)        (principal=User:*, host=*, operation=READ, permissionType=ALLOW)        (principal=User:elk, host=*, operation=DESCRIBE, permissionType=ALLOW)        (principal=User:*, host=*, operation=DESCRIBE, permissionType=ALLOW)Current ACLs for resource `ResourcePattern(resourceType=GROUP, name=logstash, patternType=PREFIXED)`:        (principal=User:*, host=*, operation=READ, permissionType=ALLOW)</code></pre><p>💥resource 就是 ACL 绑定的对象。ACL是针对 resource 的，不是针对用户的。</p><h3 id="删除授权">删除授权</h3><p>针对 resource 删除 ACL 规则。</p><ul><li>删除<code>Current ACLs for resource ResourcePattern(resourceType=TOPIC, name=zz.it., patternType=PREFIXED)</code>下的ACL规则</li></ul><pre><code class="language-bash">bin/kafka-acls.sh --bootstrap-server data01.zyh.cool:8123 --command-config config/my.tools.properties --topic zz.it.elk. --resource-pattern-type prefixed --remove</code></pre><ul><li>删除<code>Current ACLs for resource ResourcePattern(resourceType=GROUP, name=logstash, patternType=PREFIXED)</code>下的ACL规则</li></ul><pre><code class="language-bash">bin/kafka-acls.sh --bootstrap-server data01.zyh.cool:8123 --command-config config/my.tools.properties --group logstash --resource-pattern-type prefixed --remove</code></pre><h1>客户端</h1><h2 id="kafka自带的客户端工具">kafka自带的客户端工具</h2><p>生产客户端配置<code>config/my.producer.properties</code></p><pre><code class="language-properties">security.protocol=SASL_PLAINTEXTsasl.mechanism=SCRAM-SHA-256sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \    username=&quot;elk&quot; \    password=&quot;123456&quot;;</code></pre><p>消费客户端配置<code>config/my.consumer.properties</code></p><pre><code class="language-properties">group.id=logstash#security.protocol=SASL_PLAINTEXT#sasl.mechanism=SCRAM-SHA-256#sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \#    username=&quot;elk&quot; \#    password=&quot;123456&quot;;</code></pre><h2 id="测试">测试</h2><pre><code class="language-bash"># 创建主题bin/kafka-topics.sh --bootstrap-server data01.zyh.cool:8123 --command-config config/my.tools.properties --create --replication-factor 3 --partitions 8 --topic zz.it.elk.syslog</code></pre><pre><code class="language-bash"># 生产连接brokerbin/kafka-console-producer.sh --bootstrap-server data01.zyh.cool:8123 --producer.config config/my.producer.properties  --topic zz.it.elk.syslog</code></pre><pre><code class="language-bash"># 消费者连接brokerbin/kafka-console-consumer.sh --bootstrap-server data01.zyh.cool:9092 --consumer.config config/my.consumer.properties  --topic zz.it.elk.syslog</code></pre>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>002-kafka基本安装</title>
      <link href="posts/bd4c8d1c/"/>
      <url>posts/bd4c8d1c/</url>
      
        <content type="html"><![CDATA[<h2 id="基本">基本</h2><p>3节点为例</p><p><code>/etc/hosts</code>添加</p><pre><code class="language-bash">10.200.16.51 data01.zyh.cool data0110.200.16.52 data02.zyh.cool data0210.200.16.53 data03.zyh.cool data03</code></pre><p>每个节点执行</p><pre><code class="language-bash">hostnamectl set-hostname &lt;hostName&gt;</code></pre><h2 id="安装">安装</h2><p><a href="http://kafka.apache.org/downloads">http://kafka.apache.org/downloads</a></p><p>下载二进制包</p><pre><code class="language-bash">cd /export &amp;&amp; mkdir src &amp;&amp; cd srccurl https://dlcdn.apache.org/kafka/3.0.0/kafka_2.13-3.0.0.tgz -o kafka.tgztar xf kafka.tgz -C ../ &amp;&amp; cd .. &amp;&amp; mv kafka_* kafka &amp;&amp; cd kafka</code></pre><h2 id="配置和启动">配置和启动</h2><h3 id="zk">zk</h3><p>一份集群（3节点）模式下的zk配置</p><pre><code class="language-properties">dataDir=/export/kafka/zk_dataclientPort=2181maxClientCnxns=0admin.enableServer=false# 心跳## ms单位，2秒一次检测tickTime=2000## 主从间初始化时候允许心跳检测失败的次数initLimit=10## 主从间同步时候允许心跳检测失败的次数syncLimit=5# 节点列表server.1=data01:2888:3888server.2=data02:2888:3888server.3=data03:2888:3888</code></pre><p>分别在三个节点上，创建zk的myid文件</p><pre><code class="language-bash"># node01mkdir -p /export/kafka/zk_dataecho '1' &gt; /export/kafka/zk_data/myid# node02mkdir -p /export/kafka/zk_dataecho '2' &gt; /export/kafka/zk_data/myid# node03mkdir -p /export/kafka/zk_dataecho '3' &gt; /export/kafka/zk_data/myid</code></pre><h4 id="启动脚本">启动脚本</h4><pre><code class="language-bash">vim zookeeper</code></pre><pre><code class="language-bash">#!/bin/bash#判断用户是否传参if [ $# -ne 1 ];then    echo &quot;无效参数，用法为: $0  &#123;start|stop|restart|status&#125;&quot;    exitfi#获取用户输入的命令cmd=$1#定义函数功能function Manger()&#123;    case $cmd in    start)        echo &quot;启动服务&quot;                remoteExec $cmd        ;;    stop)        echo &quot;停止服务&quot;        remoteExec $cmd        ;;    *)        echo &quot;无效参数，用法为: $0  &#123;start|stop&#125;&quot;        ;;    esac&#125;#定义执行的命令function remoteExec()&#123;    for i in `echo &#123;01..03&#125;`;do            tput setab 0; tput setaf 2; tput bold            echo ========== zookeeper: data$&#123;i&#125; $cmd ================            tput sgr0            if [[ $cmd == &quot;start&quot; ]];then                ssh data$&#123;i&#125;  &quot;source /etc/profile ; cd /export/kafka; ./bin/zookeeper-server-start.sh -daemon config/zookeeper.properties&quot;            elif [[ $cmd == &quot;stop&quot; ]];then                ssh data$&#123;i&#125;  &quot;source /etc/profile ; cd /export/kafka; ./bin/zookeeper-server-stop.sh&quot;            fi    done&#125;#调用函数Manger</code></pre><pre><code class="language-bash">chmod u+x zookeeper./zookeeper start</code></pre><p>✨日志位于 logs 目录</p><h3 id="kafka">kafka</h3><p>一份集群（3节点）模式下的kafka配置</p><pre><code class="language-properties"># broker id## 不同节点设置不同的值，例如三个节点，分别是1/2/3broker.id=1# kafka 参数num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600# 数据目录。存储主题分区日志，主题数据以文件夹方式程序，文件夹名称：&lt;主题名&gt;-&lt;分区ID&gt;log.dirs=/export/kafka/data# 多少vcpu，就设置多少num.partitions=8num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000group.initial.rebalance.delay.ms=0# kafka 链接 zk 的参数zookeeper.connect=data01.zyh.cool:2181,data02.zyh.cool:2181,data03.zyh.cool:2181zookeeper.connection.timeout.ms=18000# 监听地址## 不同节点设置不同的值listeners=SASL_PLAINTEXT://data01.zyh.cool:8123,PLAINTEXT://data01.zyh.cool:9092advertised.listeners=SASL_PLAINTEXT://data01.zyh.cool:8123,PLAINTEXT://data01.zyh.cool:9092</code></pre><h4 id="启动脚本-2">启动脚本</h4><p>一键启动脚本（需提前ssh互信）</p><pre><code class="language-bash">vi kafka</code></pre><pre><code class="language-bash">#!/bin/bash#判断用户是否传参if [ $# -ne 1 ];then    echo &quot;无效参数，用法为: $0  &#123;start|stop|restart|status&#125;&quot;    exitfi#获取用户输入的命令cmd=$1#定义函数功能function Manger()&#123;    case $cmd in    start)        echo &quot;启动服务&quot;                remoteExec $cmd        ;;    stop)        echo &quot;停止服务&quot;        remoteExec $cmd        ;;    *)        echo &quot;无效参数，用法为: $0  &#123;start|stop&#125;&quot;        ;;    esac&#125;#定义执行的命令function remoteExec()&#123;    for i in `echo &#123;01..03&#125;`;do            tput setab 0; tput setaf 2; tput bold            echo ========== kafka: data$&#123;i&#125; $cmd ================            tput sgr0            if [[ $cmd == &quot;start&quot; ]];then                ssh data$&#123;i&#125;  &quot;source /etc/profile ; cd /export/kafka; ./bin/kafka-server-start.sh -daemon config/server.properties&quot;            elif [[ $cmd == &quot;stop&quot; ]];then                ssh data$&#123;i&#125;  &quot;source /etc/profile ; cd /export/kafka; ./bin/kafka-server-stop.sh&quot;            fi    done&#125;#调用函数Manger</code></pre><pre><code class="language-bash">chmod u+x kafka./kafka start</code></pre><h2 id="创建主题">创建主题</h2><pre><code class="language-bash">bin/kafka-topics.sh --bootstrap-server data01.zyh.cool:9092 --create --replication-factor 3 --partitions 8 --topic zz.it.monit</code></pre><ul><li>–bootstrap-server 你可以写集群里的任意节点.</li><li>–replication-factor 复制因子数&lt;=broker数量.</li><li>–partitions 主题分区数，决定了主题下的日志分成多少份.</li></ul><p>✨</p><ol><li>主题一经创建，则不可重命名，只能通过新建主题并迁移数据。</li><li>主题名，应遵循一定的层次结构，以便于通过前缀来进行权控。例如：<ul><li><code>&lt;organization&gt;.&lt;team&gt;.&lt;dataset&gt;.&lt;event-name&gt;</code> 按照组织结构命名</li><li><code>&lt;project&gt;.&lt;product&gt;.&lt;event-name&gt;</code> 按照项目结构命名</li></ul></li><li>当复制因子数&lt;broker的时候，一个主题的副本分区将【无序（分区ID）】【尽量平均】的写入所有的broker中。</li><li>当复制因子数=broker的时候，一个主题的副本分区将【有序（分区ID）】【完全一致】的写入所有的broker中。</li></ol><h2 id="事件读写">事件读写</h2><p>生产者写入两条事件，事件将被永久存储</p><pre><code class="language-bash">bin/kafka-console-producer.sh --bootstrap-server data01.zyh.cool:9092 --topic zz.it.monit&gt;This is my first eventThis is my second event</code></pre><p>消费者读取两条事件，因事件被永久存储，故而可以多次消费</p><pre><code class="language-bash">bin/kafka-console-consumer.sh --bootstrap-server data01.zyh.cool:9092 --topic zz.it.monit --from-beginningThis is my first eventThis is my second event</code></pre><p>ℹ️生产者的数据可以立即在消费者端读取显示.</p><h2 id="第三方客户端">第三方客户端</h2><p><a href="https://github.com/edenhill/kcat">https://github.com/edenhill/kcat</a></p><p><code>kcat</code>是一个更加强大的客户端，包含了消费和生产，支持按照时间戳来消费，方便重新消费因某些原因导致丢失的数据。</p><p>ℹ️以前它叫<code>kafkacat</code>🙄</p><h3 id="查看topic列表元信息">查看topic列表元信息</h3><pre><code class="language-bash">docker run -it --rm --network=host edenhill/kcat:1.7.0 \-b data01.zyh.cool:8123 \-X security.protocol=&quot;SASL_PLAINTEXT&quot; \-X sasl.mechanism=SCRAM-SHA-256 \-X sasl.username=&quot;elk&quot; \-X sasl.password=&quot;123456&quot; \-L</code></pre><h3 id="获取某个时间范围内的事件">获取某个时间范围内的事件</h3><pre><code class="language-bash">docker run -it --rm --network=host edenhill/kcat:1.7.0 \-b data01.zyh.cool:8123 \-X security.protocol=&quot;SASL_PLAINTEXT&quot; \-X sasl.mechanism=SCRAM-SHA-256 \-X sasl.username=&quot;elk&quot; \-X sasl.password=&quot;123456&quot; \-G logstash \-t zz.it.elk.syslog.secure \-C \-o s@`date -d &quot;20211129 16:57&quot; +%s`000 \-o e@`date -d &quot;20211129 18:00&quot; +%s`000</code></pre><pre><code class="language-bash">-G 消费组-o 指定偏移s@&lt;value&gt; (timestamp in ms to start at)e@&lt;value&gt; (timestamp in ms to stop at (not included))</code></pre>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>001-kafka基本信息</title>
      <link href="posts/9bd7430f/"/>
      <url>posts/9bd7430f/</url>
      
        <content type="html"><![CDATA[<h2 id="基本结构">基本结构</h2><p>服务端：</p><ul><li>broker 存储数据，一个kafka实例节点代表一个 broker</li><li>kafka connect 连接其它系统（包括其它kafka集群），从而导入导出事件流</li><li>zookeeper zk集群用来存储kafka中broker和topic的信息</li></ul><p>客户端：</p><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">https://cwiki.apache.org/confluence/display/KAFKA/Clients</a></p><h2 id="基本概念对象">基本概念对象</h2><h3 id="事件">事件</h3><p>包含键（可选），值，时间戳，元数据标头（可选），存储在<code>主题</code>的<code>分区</code>中</p><h3 id="批次">批次</h3><p>kafka每次积攒事件，达到形成一个批次的条件后，按批次打包发送</p><h3 id="生产者">生产者</h3><p>将事件写入到服务端的客户端程序</p><h3 id="消费者">消费者</h3><p>从服务端获取事件的客户端程序</p><h3 id="消费者组">消费者组</h3><p>多个消费者共用一个唯一标识组ID</p><h3 id="主题">主题</h3><p>主题是存储事件的逻辑层。一个主题就如同一个管道，管道左边是多个生产者，管道右边是多个消费者。我们通过主题分类事件。</p><p>每个主题的事件存储周期，取决于配置。不管存储存多少，kafka的性能都是o(1)级别，也就是性能不会变。</p><p>主题散列在多个<code>broker</code>的多个<code>buckets</code>。</p><h3 id="分区">分区</h3><p>每一个主题包含多个分区，分区会自动尽量平均的分散在多个broker上.</p><p>多分区允许客户端并发访问主题中的数据.</p><p>同一个主题下的相同key的事件将始终写入同一个分区【通过hash(key)来落点】。</p><p>同一个主题下的没有定义key的事件遵循两个分发策略：轮询策略和粘性策略。粘性策略性能更好。</p><p>ℹ️分区内事件读取的时候遵循先入先出原则。</p><p>ℹ️添加新分区不会导致现有分区的数据发生改变。</p><p>为了防止重复消费：</p><ol><li><p>同一个分区里的事件同一时间不会被一个消费者组里的多个消费者消费。</p></li><li><p>同一个分区里的事件同一时间可以被多个消费者组里的一个消费者消费。</p></li><li><p>消费者消费完会触发位移提交，刷新主题下次消费的起始点。</p></li></ol><h3 id="副本因子">副本因子</h3><p>每一个分区有几个副本</p><h2 id="节点存活">节点存活</h2><ol><li>节点时刻与zk保持沟通。</li><li>追随节点的数据始终不会落后主节点太多。</li></ol><p>满足上面两个条件的节点，状态标记为<code>in sync</code>，也就是存活中。</p><h2 id="消息发送机制">消息发送机制</h2><p>当一个事件被分区所有副本写入后，kafka才会将事件发送给消费者。</p><h2 id="rebalance机制">rebalance机制</h2><p>这个概念是针对消费者组，意思是消费者组将一个topic的所有分区尽可能的分发给所有消费者。</p><p>另外，消费者出现消费不完，或者没有发送心跳，都会被踢出消费者组，导致消费者组成员发生变化，从而引发rebalance。</p><p>在上述情况中，被踢出的消费者又因为无法执行提交位移，这会导致相当于没有消费，最终产生重复消费问题。</p>]]></content>
      
      
      <categories>
          
          <category> 消息 </category>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞安装.</title>
      <link href="posts/b824edd8/"/>
      <url>posts/b824edd8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://hub.docker.com/_/mysql?tab=description">https://hub.docker.com/_/mysql?tab=description</a></p><pre><code class="language-bash">mkdir -p /export/docker-data-mysql/&#123;config,logs,mysql,mysql-files&#125;docker run --name mysql8 \-p 3306:3306 \--restart=always \--mount 'type=bind,src=/export/docker-data-mysql/config,dst=/etc/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/logs,dst=/var/log/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/mysql,dst=/var/lib/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/mysql-files,dst=/var/lib/mysql-files' \-e MYSQL_ROOT_PASSWORD=123456 \-d mysql:8 \--character-set-server=utf8mb4 \--collation-server=utf8mb4_general_ci</code></pre><p>mysql8 开启远程访问</p><pre><code class="language-sql">alter user 'root'@'指定IP' identified with mysql_native_password by '123456';</code></pre><p>mysql5.7 开启远程访问</p><pre><code class="language-sql">GRANT ALL PRIVILEGES ON *.* TO 'root'@'指定IP' IDENTIFIED BY '123456' WITH GRANT OPTION;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞rds-mysql参数</title>
      <link href="posts/438aa722/"/>
      <url>posts/438aa722/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># 下列是非默认值character_set_client: utf8character_set_connection: utf8character_set_database: utf8character_set_filesystem: utf8character_set_results: utf8character_set_server: utf8collation_connection: utf8_general_cislow_query_log: 1long_query_time: 10log_output: FILE# 下列是默认值explicit_defaults_for_timestamp: '1'innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'innodb_file_per_table: '1'innodb_flush_method: O_DIRECTbinlog_cache_size: '32768'binlog_format: MIXED</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞EC2国内外之间迁移</title>
      <link href="posts/2e304ad6/"/>
      <url>posts/2e304ad6/</url>
      
        <content type="html"><![CDATA[<h1>基本</h1><p>aws 国内和国外之间无法直接复制 AMI，所以经咨询 aws 技术人员，就有了这么一道手工饭。。。</p><h1>步骤</h1><ol><li>通过国外待迁移的AMI创建一个EC2，并挂载一个新EBS（/dev/xvdb），EBS大小略大于EC2的根分区，如10GB。</li><li>登录EC2，并执行下列命令。</li></ol><pre><code class="language-shell">mke2fs -t ext4 /dev/xvdbmount -t ext4 /dev/xvdb /mnt# 克隆根分区到 /mnt/root.imgdd if=/dev/xvda of=/mnt/root.img bs=1M# 将 /mnt/root.img 下载到国内</code></pre><ol start="3"><li>新建国内EC2，并挂载一个新EBS（/dev/xvdb），EBS大小10GB。</li><li>将国外的 root.img 上传到国内 EC2。</li><li>登录EC2，并执行下列命令。</li></ol><pre><code class="language-bash">dd if=/mnt/root.img of=/dev/xvdb bs=1M oflag=direct</code></pre><ol start="6"><li>对/dev/xvdb打快照，并针对这个快照生成AMI即可。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ec2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞利用cf服务初始化aws环境</title>
      <link href="posts/ddc4c969/"/>
      <url>posts/ddc4c969/</url>
      
        <content type="html"><![CDATA[<blockquote><p>因公司小项目多，账户多，每次构建aws初始化环境均很麻烦，且每个人操作的时候，想法都不一样，规则不统一，导致环境信息无法通用.</p><p>所以准备使用 cf 来构建一个通用模板，保持环境一致性.</p><p>之前一直没有使用，是因为 cf 本身编写不是太方便，然而我低估了 aws 的牛逼，仔细看了文档才知道， 有个 cf 本身有个堆栈，可以直接复刻当前环境.</p><p>这个工具就是 <code>cloudformer</code></p><p>官方文档是: <a href="https://docs.aws.amazon.com/zh_cn/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html#launch-cloudformer">https://docs.aws.amazon.com/zh_cn/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html#launch-cloudformer</a></p></blockquote><h2 id="那么步骤来了">那么步骤来了~</h2><ul><li><p>先通过 web 控制台生成一个适合本地化的通用环境，比如 vpc，子网，安全组，数据库子网组，参数组，s3以及相应的s3生命周期规则等等</p></li><li><p>构建 cloudformer 堆栈，并通过 cloudformer 服务生成当前环境模板</p></li><li><p>将当前环境模板的一些非通用内容，换成变量，并构建成 cf 参数或者脚本</p></li><li><p>最后通过执行脚本，并输入变量值，来生成新模板</p></li><li><p>在新环境里调用新模板来初始化环境</p></li></ul><h2 id="下面是我贴出的一个基础环境生成脚本">下面是我贴出的一个基础环境生成脚本</h2><ul><li><p>vpc: x.x.0.0/16</p></li><li><p>prod: x.x.128.0/17</p><ul><li>prod 子网默认有四个, 分别是 EC2 两个可用区, Database 两个可用区</li></ul></li><li><p>qa: x.x.0.0/19</p><ul><li>qa 子网默认有四个, 分别是 EC2 两个可用区, Database 两个可用区</li></ul></li><li><p>prod 和 qa 通过安全组来分割</p><ul><li><p>prod 规则示例如下(qa一样):</p></li><li><p>Prod-EC2-common:</p><table><thead><tr><th>规则名</th><th>协议</th><th>开放端口</th><th>源ip或者其它安全组ID</th><th></th></tr></thead><tbody><tr><td>所有流量</td><td>全部</td><td>全部</td><td>10.130.128.0/17</td><td></td></tr></tbody></table></li></ul><p>| SSH      | TCP  | 22       | ${src_allow_ssh_ip}  |      |</p><ul><li><p>Prod-Database-common:</p><table><thead><tr><th>规则名</th><th>协议</th><th>开放端口</th><th>源ip或者其它安全组ID</th><th></th></tr></thead><tbody><tr><td>自定义 TCP 规则</td><td>TCP</td><td>1025 - 65535</td><td>${src_allow_rds_ip}</td><td></td></tr><tr><td>自定义 TCP 规则</td><td>TCP</td><td>1025 - 65535</td><td>Prod-EC2-common-Group-ID</td><td></td></tr></tbody></table></li></ul></li><li><p>SNS 默认预警主题</p></li><li><p>mysql 子网组</p></li><li><p>redis 子网组</p></li><li><p>s3以及初始化生命周期规则</p></li><li><p>vpc 路由表不会添加默认路由, 还请自行添加(Internet 网关会自动生成)</p></li></ul><pre><code class="language-bash">#!/bin/bash# 本脚本用来生成一份基础环境的 cf 模板# 网络A和B段，例如 10.10 那么 vpc 就是 10.10.0.0/16VpcCidrBlockAB=# 大区区域, 例如 新加坡 那么区域就写 ap-southeast-1Region=# 项目名，用于区分资源以及添加 Team 标签 (只能是字母数字组合)TagTeamValue=# 允许访问EC2的22号端口的ip (指的是非vpc内网)，通常应该是堡垒机或者公司外网src_allow_ssh_ip=# 允许访问数据库端口的ip (指的是非vpc内网)，通常应该是堡垒机或者公司外网src_allow_rds_ip=# 默认 SNS 服务主体名 SNSThemeNameSNSThemeName=$&#123;TagTeamValue&#125;# 默认 SNS 服务信息接收用户邮箱, cf 模板执行完后，需要用户邮箱读取邮件自行确认消息SNSEmail=# 设置 vpc 任意两个可用区 字母标识, 比如 a bAvailabilityZoneOne=aAvailabilityZoneTwo=b######## 本模板, 不会添加 vpc 默认路由, 请自行加  #########cat &gt; cf-common.yml &lt;&lt; EOFAWSTemplateFormatVersion: 2010-09-09Resources:  vpc12345:    Type: 'AWS::EC2::VPC'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.0.0/16      InstanceTenancy: default      EnableDnsSupport: 'true'      EnableDnsHostnames: 'true'      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 84f85642-9390-4126-b347-95ca2ebf1c15  subnet0qa0database0$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.3.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-database-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 864ecf93-1ce6-4b4d-9b29-c3d89f07141b  subnet0prod0database0$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.131.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-database-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 7ef6daea-3bcd-4913-ab55-ba4f7bced319  subnet0qa0ec20$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.2.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-ec2-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: bc2b5a08-f7b8-4699-a7ba-5c9c6c021bd8  subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.4.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-database-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: b4738f0a-11a6-4d75-b9f0-a24072e53228  subnet0prod0ec20$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.129.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-ec2-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 2ffb24f6-49fd-4d3a-bdfe-69e90e228517  subnet0prod0ec20$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.128.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-ec2-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 9a2789fc-b7d2-4532-811b-be3455ab7d7d  subnet0qa0ec20$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.1.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-ec2-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 5c6c2638-a7b5-4185-a7a3-08d5901c252a  subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.132.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-database-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 00a78a8b-fd72-4613-bf09-8319b876ba2b  igw088b65cb430bc5ca0:    Type: 'AWS::EC2::InternetGateway'    Properties:      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-internet-gw    Metadata:      'AWS::CloudFormation::Designer':        id: e60ac13b-274d-46db-ac2a-5afaf975e1eb  dopt8128ece5:    Type: 'AWS::EC2::DHCPOptions'    Properties:      DomainName: $&#123;Region&#125;.compute.internal      DomainNameServers:        - AmazonProvidedDNS    Metadata:      'AWS::CloudFormation::Designer':        id: 909e6481-ccca-40fe-a1b1-4abe15a2ba18  acl03e81a4f65756520d:    Type: 'AWS::EC2::NetworkAcl'    Properties:      VpcId: !Ref vpc12345    Metadata:      'AWS::CloudFormation::Designer':        id: 93e06538-fe9e-49a7-8549-e2efb97dfab7  dbsubnet$&#123;TagTeamValue&#125;prod:    Type: 'AWS::RDS::DBSubnetGroup'    Properties:      DBSubnetGroupDescription: $&#123;TagTeamValue&#125;-prod      SubnetIds:        - !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 16096ba6-c5de-4a73-a083-07b2c0573362  dbsubnet$&#123;TagTeamValue&#125;qa:    Type: 'AWS::RDS::DBSubnetGroup'    Properties:      DBSubnetGroupDescription: $&#123;TagTeamValue&#125;-qa      SubnetIds:        - !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 186893f8-59ef-4d56-880f-d62294d25780  dbpg$&#123;TagTeamValue&#125;prod:    Type: 'AWS::RDS::DBParameterGroup'    Properties:      Description: $&#123;TagTeamValue&#125;-prod-mysql5.6      Family: mysql8.0      Parameters:        binlog_cache_size: '32768'        binlog_format: MIXED        character_set_client: utf8mb4        character_set_connection: utf8mb4        character_set_database: utf8mb4        character_set_filesystem: utf8mb4        character_set_results: utf8mb4        character_set_server: utf8mb4        collation_connection: utf8mb4_general_ci        explicit_defaults_for_timestamp: '1'        innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'        innodb_file_per_table: '1'        innodb_flush_method: O_DIRECT    Metadata:      'AWS::CloudFormation::Designer':        id: 87a954ed-d1b2-4626-87fe-f2515f4479b4  dbpg$&#123;TagTeamValue&#125;qa:    Type: 'AWS::RDS::DBParameterGroup'    Properties:      Description: $&#123;TagTeamValue&#125;-qa-mysql5.6      Family: mysql8.0      Parameters:        binlog_cache_size: '32768'        binlog_format: MIXED        character_set_client: utf8        character_set_connection: utf8        character_set_database: utf8        character_set_filesystem: utf8        character_set_results: utf8        character_set_server: utf8        collation_connection: utf8_general_ci        explicit_defaults_for_timestamp: '1'        innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'        innodb_file_per_table: '1'        innodb_flush_method: O_DIRECT    Metadata:      'AWS::CloudFormation::Designer':        id: 6c4c2e1e-ec99-4e7f-b254-0f09145a0d87  cachesubnet$&#123;TagTeamValue&#125;databaseprod:    Type: 'AWS::ElastiCache::SubnetGroup'    Properties:      Description: '$&#123;TagTeamValue&#125;_prod'      SubnetIds:        - !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: bdfb3d49-78d8-40df-84b6-63bf65616f5c  cachesubnet$&#123;TagTeamValue&#125;databaseqa:    Type: 'AWS::ElastiCache::SubnetGroup'    Properties:      Description: '$&#123;TagTeamValue&#125;_qa'      SubnetIds:        - !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;        - !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 70367616-0a11-4bc9-9373-a23be1690a59  topicIT:    Type: 'AWS::SNS::Topic'    Properties:      DisplayName: $&#123;SNSThemeName&#125;      Subscription:        - Endpoint: $&#123;SNSEmail&#125;          Protocol: email    Metadata:      'AWS::CloudFormation::Designer':        id: 343cc3f4-4380-47e5-850e-cc3fb560f22a  $&#123;TagTeamValue&#125;qacommon:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-qa-common      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  QA-EC2-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: d31db020-e702-41e5-8ccc-d71a5a26f565  $&#123;TagTeamValue&#125;prodcommon:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-prod-common      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  Prod-EC2-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: f54456ef-e984-4b00-b04d-64b16a2057ea  ITLinshi:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: IT-Linshi      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  I'm god    Metadata:      'AWS::CloudFormation::Designer':        id: 182b7e60-f895-4519-8845-4d32a8591340  $&#123;TagTeamValue&#125;proddatabase:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-prod-database      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  Prod-database-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: 44f0485a-2575-4eab-82b3-bc7507cd5820  $&#123;TagTeamValue&#125;qadatabase:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-qa-database      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  QA-database-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: f903102a-c394-4301-a08d-5625293ac23f  snspolicyIT:    Type: 'AWS::SNS::TopicPolicy'    Properties:      Topics:        - !Ref topicIT      PolicyDocument:        Version: 2008-10-17        Id: __default_policy_ID        Statement:          - Sid: __default_statement_ID            Effect: Allow            Principal:              AWS: '*'            Action:              - 'SNS:GetTopicAttributes'              - 'SNS:SetTopicAttributes'              - 'SNS:AddPermission'              - 'SNS:RemovePermission'              - 'SNS:DeleteTopic'              - 'SNS:Subscribe'              - 'SNS:ListSubscriptionsByTopic'              - 'SNS:Publish'              - 'SNS:Receive'            Resource: !Ref topicIT            Condition:              StringEquals:                'AWS:SourceOwner': '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 66a45d1a-3c99-4ef8-9c4e-db9ba717e17e  acl1:    Type: 'AWS::EC2::NetworkAclEntry'    Properties:      CidrBlock: 0.0.0.0/0      Egress: 'true'      Protocol: '-1'      RuleAction: allow      RuleNumber: '100'      NetworkAclId: !Ref acl03e81a4f65756520d    Metadata:      'AWS::CloudFormation::Designer':        id: de76822e-cddc-40d9-ba6f-4de20d1188ec  acl2:    Type: 'AWS::EC2::NetworkAclEntry'    Properties:      CidrBlock: 0.0.0.0/0      Protocol: '-1'      RuleAction: allow      RuleNumber: '100'      NetworkAclId: !Ref acl03e81a4f65756520d    Metadata:      'AWS::CloudFormation::Designer':        id: eb6e0bf9-3c84-4991-a037-64e2a368650b  subnetacl1:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0ec20$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 2801f8a7-901b-43b6-b4d9-9275d5639b86  subnetacl2:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0ec20$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 67641cac-39f6-4ee1-8410-60eaba963c27  subnetacl3:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0ec20$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 4b480f9d-e7b8-4034-b253-1c254fd83758  subnetacl4:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 1792de63-b1da-4c31-bab9-9925de98de6c  subnetacl5:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: e62f3e81-63f0-4f5e-b3e0-aa733ba6f323  subnetacl6:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0ec20$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 28228fcf-5649-4e5e-bf03-d636ca28bffe  subnetacl8:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 00ab0d37-3e37-427e-ac06-e94663719fcf  subnetacl9:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: ee93485d-3df4-416d-ab61-4f77c46441f1  subnetacl10:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: a4208016-e7cf-4acf-a718-104a25fef4c3  gw1:    Type: 'AWS::EC2::VPCGatewayAttachment'    Properties:      VpcId: !Ref vpc12345      InternetGatewayId: !Ref igw088b65cb430bc5ca0    Metadata:      'AWS::CloudFormation::Designer':        id: 3071b8a9-f4ab-4d61-83d8-d2792481e135  dchpassoc1:    Type: 'AWS::EC2::VPCDHCPOptionsAssociation'    Properties:      VpcId: !Ref vpc12345      DhcpOptionsId: !Ref dopt8128ece5    Metadata:      'AWS::CloudFormation::Designer':        id: 69a63971-39fa-45c9-8a17-cc730730083d  ingress1:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: '-1'      CidrIp: $&#123;VpcCidrBlockAB&#125;.0.0/19  ingress2:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: '-1'      CidrIp: $&#123;VpcCidrBlockAB&#125;.128.0/17  ingress3:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      SourceSecurityGroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      SourceSecurityGroupOwnerId: '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 8972689f-9301-4538-b493-bd1956559ecd  ingress4:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      SourceSecurityGroupId: !Ref $&#123;TagTeamValue&#125;qacommon      SourceSecurityGroupOwnerId: '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 236a8060-da9c-4be7-af29-fb6758a764a9  ingress5:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref ITLinshi      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  ingress6:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: tcp      FromPort: '22'      ToPort: '22'      CidrIp: $&#123;src_allow_ssh_ip&#125;  ingress8:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: tcp      FromPort: '22'      ToPort: '22'      CidrIp: $&#123;src_allow_ssh_ip&#125;  ingress11:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      CidrIp: $&#123;src_allow_rds_ip&#125;  ingress14:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      CidrIp: $&#123;src_allow_rds_ip&#125;  egress1:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress2:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress3:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref ITLinshi      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress4:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress5:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: '-1'      CidrIp: 0.0.0.0/0Description: ''EOF</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ec2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab☞批量修改用户密码</title>
      <link href="posts/5edc3545/"/>
      <url>posts/5edc3545/</url>
      
        <content type="html"><![CDATA[<pre><code>export OldToken=export OldDomain=# 导出 用户ID 用户名 用户状态 用户邮箱curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=1&quot;  | jq '.[] | .id,.username,.state,.email' | sed 'N;N;N;s#\n# #g;s#&quot;##g' | grep 'active' &gt;&gt; user.id;curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=2&quot;  | jq '.[] | .id,.username,.state,.email' | sed 'N;N;N;s#\n# #g;s#&quot;##g' | grep 'active' &gt;&gt; user.id;# 修改用户密码, 并生成密码记录文件 user.passwordwhile read userid username state email;do    Pwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`    curl --request  PUT --header &quot;PRIVATE-TOKEN:$&#123;OldToken&#125;&quot; --data &quot;password=$&#123;username&#125;@$&#123;Pwd&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users/$&#123;userid&#125;&quot;    echo &quot;$&#123;email&#125; $&#123;username&#125; $&#123;username&#125;@$&#123;Pwd&#125;&quot; &gt;&gt; user.passworddone &lt; user.id</code></pre><pre><code>#!/usr/bin/python #coding:utf-8   import smtplib from email.mime.text import MIMEText import sys  mail_host = 'smtp.gmail.com:587'mail_user = 'it-ops@altamob.com'mail_pass = 'hoqmefjrmkvkukth'def send_mail(to_list,subject,content):     me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;     msg = MIMEText(content)     msg['Subject'] = subject     msg['From'] = me     msg['to'] = to_list          try:     print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)    print &quot;connect mail server suesscc&quot;     s.starttls()        s.login(mail_user,mail_pass)     print &quot;login mail server suesscc&quot;        s.sendmail(me,to_list,msg.as_string())         s.close()         return True     except Exception,e:         print str(e)         return False      if __name__ == &quot;__main__&quot;:     send_mail(sys.argv[1], sys.argv[2], sys.argv[3])</code></pre><pre><code>while read email username pwd;do python sendmail.py &quot;$&#123;email&#125;&quot; &quot;gitlab $&#123;username&#125;新密码&quot; &quot;$&#123;pwd&#125;&quot;;done &lt; user.password</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞泛域名_变量截取</title>
      <link href="posts/78ceb386/"/>
      <url>posts/78ceb386/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">server &#123;    listen       80;    listen       [::]:80;    server_name ~^(?&lt;userName&gt;.*)\.apple\.com\.cn$;    root /export/webapps/apple.com/$userName;    access_log  /var/log/nginx/access.log  main;    #开启浏览器静态文件缓存    location ~ .*\.(html|htm|gif|jpg|jpeg|bmp|png|ico|txt|js|css)$ &#123;        expires      3h;    &#125;        # https://&lt;username&gt;.apple.com/api -&gt; /export/webapps/apple.com/api.php #####    location ~* ^/(api|event_api)$ &#123;        root /export/webapps/apple.com;        rewrite ^/(.*)$ /$1.php break;        fastcgi_pass     127.0.0.1:9001;        fastcgi_index    index.php;        include      fastcgi.conf;        fastcgi_connect_timeout    600s;        fastcgi_send_timeout       600s;        fastcgi_read_timeout       600s;        fastcgi_buffers 8 256k;        fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;        # https://&lt;username&gt;.apple.com/&lt;uri&gt; -&gt; /export/webapps/apple.com/&lt;username&gt;/&lt;uri&gt;.php    location ~* ^/[0-9a-zA-Z]+$ &#123;        rewrite ^/(.*)$ /$1.php break;        fastcgi_pass     127.0.0.1:9001;        fastcgi_index    index.php;        include      fastcgi.conf;        fastcgi_connect_timeout    600s;        fastcgi_send_timeout       600s;        fastcgi_read_timeout       600s;        fastcgi_buffers 8 256k;        fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;&#125;</code></pre><blockquote><p>location 优先级从上往下依次递减：<br>location =      仅匹配字符串自身<br>location ^~    匹配某个字符串开头的uri<br>location ~      正则匹配，区分大小写<br>location ~*    正则匹配，不区分大小写<br>location /      表示匹配“域名/之后的uri”，再比如localtion /images，表示匹配“域名/images之后的uri</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab☞大版本迁移</title>
      <link href="posts/a38d0013/"/>
      <url>posts/a38d0013/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><ul><li>现在 gitlab 官方包升级很容易，直接 yum 之类的就可以，之所以有这篇文章，是因为我要从 v7 升级到 v11, 数据库也从 mysql 转变为了 pgsql;而且 v7 当时用的是 bitnami 安装的。所以无奈之下，就准备用 api 进行迁移，因而遗漏之处是肯定有的。</li></ul><h1>正文</h1><ul><li><p>Token变量定义[Token不同版本获取方式不一样，但是总归都是从web用户界面能直接拿到的], Token需要是最大权限用户，且拥有所有的项目权限</p><pre><code>export OldToken=su2GwpNeVQexport NewToken=yTVZTPeussexport OldDomain=git.a.comexport NewDomain=git.b.com</code></pre><blockquote><p>gitlab 不同版本的 api 版本不一样，本文档是从 7 迁移到 11，所以 api 版本也是从 v3 到 v4<br>gitlab 的 api 默认一次最多获取100条信息，所以需要进行分页获取<br>per_page 每页信息条数 page 页数<br>所以信息太多的自行分页即可</p></blockquote></li><li><p>迁移用户</p><blockquote><p>下面代码，并不迁移用户的公钥信息</p></blockquote><pre><code>mkdir create_user &amp;&amp; cd create_user;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=$i&quot; | jq '.[] |.name, .username, .state, .email, .can_create_group, .can_create_project' | sed 'N;N;N;N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; users.list;while read name username state email can_create_group can_create_project;do    curl --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; --data &quot;name=$name&amp;username=$username&amp;password=$&#123;username&#125;@123456&amp;state=$state&amp;email=$email&amp;can_create_group=$can_create_group&amp;can_create_project=$can_create_project&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users&quot;done &lt; users.listcd ..</code></pre></li><li><p>创建项目组</p><pre><code>mkdir create_group &amp;&amp; cd create_group;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/groups?per_page=100&amp;page=$i&quot; | jq '.[] |.name, .path' | sed 'N;s#\n# #;s#&quot;##g';done &gt; group.list;echo 'repo repo' &gt;&gt; group.list;cat group.list | while read name path;do curl --request POST --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; --data &quot;name=$name&amp;path=$path&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups&quot;;donecd ..</code></pre></li><li><p>创建项目到对应组</p><pre><code>mkdir create_project &amp;&amp; cd create_project;</code></pre><pre><code>for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.name, .namespace.name, .description' | sed 'N;N;s#\n#\t#g;s# #-#g;s#&quot;##g';done  &gt;&gt; project.list;mv  project.list project.list.tmp;cat project.list.tmp | sort | uniq &gt; project.list;rm -rf project.list.tmp;</code></pre><pre><code># 这里 pre_page 不起作用for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups?page=$i&quot; | jq '.[] | .id, .name' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; group.id</code></pre><pre><code>cat project.list | while read projectname groupname des;do    newgroupid=`egrep &quot;\b$&#123;groupname&#125;\b&quot; ./group.id | awk '&#123;print $1&#125;'`    curl --request POST --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot;  --data &quot;name=$&#123;projectname&#125;&amp;namespace_id=$&#123;newgroupid&#125;&amp;description=$des&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/projects&quot;donecd ..</code></pre></li><li><p>添加用户到组</p><pre><code>mkdir add_user_group &amp;&amp; cd add_user_group;</code></pre><pre><code>for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/groups?page=$i&quot; | jq '.[] | .id, .name' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; group.old.id;cp ../create_project/group.id ./group.new.id;#curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups&amp;page=1&quot; | jq '.[] | .id, .name' | sed 'N;s#\n# #g;s#&quot;##g' &gt;&gt; group.new.id;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot;  | jq '.[] | .id,.username' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; new_user.id;</code></pre><pre><code>cat ./group.old.id | while read groupid groupname;do    curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; http://$&#123;OldDomain&#125;/api/v3/groups/$&#123;groupid&#125;/members | jq '.[] | .id, .username, .access_level' |sed 'N;N;s#\n# #g;s#&quot;##g' | while read nameid username level;do        newid=`egrep &quot;\b$&#123;username&#125;\b&quot; ./new_user.id| awk '&#123;print $1&#125;'`        newgid=`egrep &quot;\b$&#123;groupname&#125;\b&quot; ./group.new.id|awk '&#123;print $1&#125;'`curl --request POST --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; --data &quot;user_id=$&#123;newid&#125;&amp;access_level=$&#123;level&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/groups/$&#123;newgid&#125;/members    donedonecd ..</code></pre></li><li><p>添加用户到项目</p><pre><code>mkdir add_user_project &amp;&amp; cd add_user_project;</code></pre><pre><code>for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.id, .name, .namespace.name' | sed 'N;N;s#\n# #g;s#&quot;##g';done  &gt;&gt; project.old.id;mv  project.old.id project.old.id.tmp;cat project.old.id.tmp | sort | uniq &gt; project.old.id;rm -rf project.old.id.tmp;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.id, .name, .namespace.name' | sed 'N;N;s#\n# #g;s#&quot;##g';done  &gt;&gt; project.new.id;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot;  | jq '.[] | .id,.username' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; new_user.id;</code></pre><pre><code>cat ./project.old.id | while read projectid projectname groupname;do    curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; http://$&#123;OldDomain&#125;/api/v3/projects/$&#123;projectid&#125;/members | jq '.[] | .id, .username, .access_level' |sed 'N;N;s#\n# #g;s#&quot;##g' | while read nameid username level;do        newid=`egrep &quot;\b$&#123;username&#125;\b&quot; ./new_user.id| awk '&#123;print $1&#125;'`        newpid=`egrep &quot;\b$&#123;projectname&#125; $&#123;groupname&#125;\b&quot; ./project.new.id|awk '&#123;print $1&#125;'`        #echo &quot;$projectid $projectname $groupname $username &gt; $newpid $projectname $groupname&quot;curl --request POST --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; --data &quot;user_id=$&#123;newid&#125;&amp;access_level=$&#123;level&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/projects/$&#123;newpid&#125;/members    donedonecd ..</code></pre></li><li><p>代码迁移</p><pre><code>mkdir sync_code &amp;&amp; cd sync_code;#for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.name, .namespace.name, .description' | sed 'N;N;s#\n#\t#g;s# #-#g;s#&quot;##g';done  &gt;&gt; project.list;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot; | jq '.[] |.path_with_namespace' | sed 's#&quot;##g';done &gt;&gt; project.path;</code></pre><pre><code># 将本代码框的代码保存到 start.sh 并执行(确保sync_code_base变量路径准确)num=1sync_code_base=/export/sync/sync_codenohup while read line;do     echo &quot;$num: -------------------------$line-------------------------&quot; &gt;&gt; $&#123;sync_code_base&#125;/sync.log    [[ -d $&#123;line%%/*&#125; ]] || mkdir $&#123;line%%/*&#125;    cd $&#123;line%%/*&#125;    git clone --mirror git@$&#123;OldDomain&#125;:$&#123;line&#125;.git &gt;&gt; $&#123;sync_code_base&#125;/sync.log 2&gt;&amp;1 || echo &quot;clone:$line&quot; &gt;&gt; $&#123;sync_code_base&#125;/clone_error.log    cd $&#123;line##*/&#125;.git    git remote set-url origin git@$&#123;NewDomain&#125;:$&#123;line&#125;.git    git push -f origin &gt;&gt; $&#123;sync_code_base&#125;/sync.log 2&gt;&amp;1 || echo &quot;sync:$line&quot; &gt;&gt; $&#123;sync_code_base&#125;/sync_error.log    cd /export/sync/sync_code    let num++done &lt; $&#123;sync_code_base&#125;/project.path &amp;</code></pre><pre><code>cd ..</code></pre></li><li><p>初始化新用户密码</p><pre><code>mkdir init_password; cd init_password;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot; |  jq '.[] |.id, .name, .username' |  sed 'N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; user.id;cat user.id | while read id name nameuser;do  curl --request PUT --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; --data &quot;password=$&#123;nameuser&#125;@123&quot; http://$&#123;NewDomain&#125;/api/v4/users/$&#123;id&#125;;donecd ..</code></pre></li><li><p>同步用户公钥信息</p><pre><code>mkdir sync_user_pub &amp;&amp; cd sync_user_pub;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=$i&quot;  |  jq '.[] |.id, .name, .username' |  sed 'N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; old_users.id;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot;  |  jq '.[] |.id, .name, .username' |  sed 'N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; new_users.id;cat old_users.id | while read old_current_id name username;do    new_current_id=`grep $username new_users.id | awk '&#123;print $1&#125;'`    curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; http://$&#123;OldDomain&#125;/api/v3/users/$&#123;old_current_id&#125;/keys | jq '.[]|.title,.key' | sed 'N;s/\n/\t/;s/&quot;//g' | while read  current_title current_key;do        curl --request POST --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; --data-urlencode &quot;title=$&#123;current_title&#125;&quot; --data-urlencode &quot;key=$&#123;current_key&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users/$&#123;new_current_id&#125;/keys&quot;    donedone</code></pre></li><li><p>如果出现push故障，则删除新服务器的项目, 并重新push</p><pre><code>sync_code_base=/export/sync/sync_codecd $&#123;sync_code_base&#125;# 准备好 delete.path 文件，内部一行一个 project.path_with_namespacefor i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/projects?per_page=100&amp;page=$i&quot; | jq '.[]|.id, .path_with_namespace'| sed 'N;s#\n#\t#g;s# #-#g;s#&quot;##g';done &gt; project-path-id.allwhile read line;do grep $line project-path-id.all ;done &lt; delete.path &gt; delete.path.idwhile read id path;do curl -s --request DELETE --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/projects/$&#123;id&#125;;sleep 5;cd $&#123;path&#125;.git;git push -f origin ;cd $&#123;sync_code_base&#125;;done &lt; delete.path.id</code></pre></li><li><p>删除新服务器的项目组</p><pre><code>mkdir delete_group &amp;&amp; cd delete_group;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups?per_page=100&amp;page=$i&quot; | jq '.[] | .id';done &gt;&gt; group.id;cat group.id | while read id;do curl -s --request DELETE --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/groups/$&#123;id&#125;;done;</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞安装-编译方式</title>
      <link href="posts/3321a2dc/"/>
      <url>posts/3321a2dc/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>用于编译安装php7.1</p><h4 id="目录结构">目录结构</h4><p>根目录：/usr/local/php<br>日志目录：/usr/local/php/var/log -&gt; /export/logs/php</p><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bash# 脚本, 适用于 php 7.1basedir=/usr/local/srccd $basedirrunuser=`whoami`[[ $runuser == 'root' ]] || &#123;        echo &quot;ERROR:执行用户不是$runuser&quot; &amp;&amp; exit&#125;#初始化服务器环境[[ -d /export/logs/php ]] || &#123;        echo &quot;/export/logs/php目录不存在&quot; &amp;&amp; exit&#125;yum install libmcrypt-devel ncurses-devel recode-devel aspell-devel curl-devel readline-devel openldap-devel enchant-devel pcre-devel net-snmp-devel libicu-devel libtool-ltdl-devel libjpeg-devel libpng-devel libxml2-devel bzip2-devel freetype-devel gcc-c++ mysql-develcp -p /usr/lib64/libldap* /usr/libln -s /usr/lib64/mysql /usr/lib/mysqlwget http://sg2.php.net/distributions/php-7.1.25.tar.gz -O php.tar.gzrm -rf php &amp;&amp; mkdir phptar xf php.tar.gz --strip-components 1 -C phpcd php &amp;&amp; ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-config-file-scan-dir=/usr/local/php/etc/php.d --with-curl --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-openssl --enable-mbstring --with-freetype-dir --with-jpeg-dir --with-png-dir --with-gd --with-libxml-dir --with-zlib --with-mcrypt --with-bz2 --enable-sysvshm --enable-sysvsem --enable-soap --with-recode --with-snmp --with-readline --enable-intl --enable-dba --enable-bcmath --with-enchant --with-pspell --enable-xml --enable-sockets --enable-exif --enable-inline-optimization --enable-fpm || exitmake &amp;&amp; make install || exitcp sapi/fpm/init.d.php-fpm /etc/rc.d/init.d/php-fpmchkconfig --add php-fpmchkconfig php-fpm offchmod u+x /etc/rc.d/init.d/php-fpmcp php.ini-production /usr/local/php/etc/php.inicd /usr/local/php/var &amp;&amp; rm -rf logln -s /export/logs/php logcat&gt;&gt;/usr/local/php/etc/php.ini&lt;&lt;EOF; 关闭php无用日志信息error_reporting = E_COMPILE_ERROR|E_RECOVERABLE_ERROR|E_ERROR|E_CORE_ERROR; 开启php opcache 缓存功能[opcache]zend_extension=opcache.so; 启动操作码缓存opcache.enable=1; 针对支持CLI版本PHP启动操作码缓存 一般被用来测试和调试opcache.enable_cli=0; 共享内存大小，单位为MBopcache.memory_consumption=128; 存储临时字符串缓存大小，单位为MB，PHP5.3.0以前会忽略此项配置opcache.interned_strings_buffer=8; 缓存文件数最大限制，命中率不到100%，可以试着提高这个值opcache.max_accelerated_files=4000; 一定时间内检查文件的修改时间, 这里设置检查的时间周期, 默认为 2, 单位为秒opcache.revalidate_freq=60; 开启快速停止续发事件，依赖于Zend引擎的内存管理模块，一次释放全部请求变量的内存，而不是依次释放内存块opcache.fast_shutdown=1;启用检查 PHP 脚本存在性和可读性的功能，无论文件是否已经被缓存，都会检查操作码缓存,可以提升性能。 但是如果禁用了 opcache.validate_timestamps选项， 可能存在返回过时数据的风险。opcache.enable_file_override=1EOFcat&gt;&gt;/usr/local/php/etc/php-fpm.conf&lt;&lt;EOF[global]log_level =errordaemonize = yesevents.mechanism = epollrlimit_files = 10240emergency_restart_threshold = 60emergency_restart_interval = 60s[fcgi]user = webappsgroup = webappslisten = 0.0.0.0:9000pm = staticpm.max_children = 100pm.max_requests = 1024request_slowlog_timeout = 1sslowlog = /usr/local/php/var/log/php-slow.logpm.status_path = /php-fpm_statusEOF</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab☞安装及备份</title>
      <link href="posts/3f38f454/"/>
      <url>posts/3f38f454/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://docs.gitlab.com/omnibus/docker/">https://docs.gitlab.com/omnibus/docker/</a></p><p><a href="https://hub.docker.com/r/gitlab/gitlab-ce">https://hub.docker.com/r/gitlab/gitlab-ce</a></p><p><a href="https://docs.gitlab.com/14.0/ee/update/index.html#community-to-enterprise-edition">Upgrading GitLab | GitLab</a></p></blockquote><h2 id="安装">安装</h2><pre><code class="language-bash"># 确保 /export 存在mkdir -p /export/docker-data-gitlab/&#123;config, logs, data&#125;gitlabtag=# createdomainName=docker pull gitlab/gitlab-ce:$&#123;gitlabtag&#125;docker run --detach   --hostname $&#123;domainName&#125;   --publish 443:443 --publish 80:80 --publish 2222:22   --name gitlab   --restart always   --volume /export/docker-data-gitlab/config:/etc/gitlab   --volume /export/docker-data-gitlab/logs:/var/log/gitlab   --volume /export/docker-data-gitlab/data:/var/opt/gitlab   gitlab/gitlab-ce:$&#123;gitlabtag&#125;# startdocker start gitlab# stopdocker stop gitlab</code></pre><h2 id="基本配置">基本配置</h2><pre><code class="language-bash"># configure# https://docs.gitlab.com/omnibus/settings/README.htmlcp /export/docker-data-gitlab/config/gitlab.rb /export/docker-data-gitlab/config/gitlab.rb.bak</code></pre><pre><code class="language-bash">## /export/docker-data-gitlab/config/gitlab.rb# 决定各个位置 url 链接内容external_url 'http://$&#123;domainName&#125;'# 决定各个位置 ssh 链接内容gitlab_rails['gitlab_shell_ssh_port'] = 2222gitlab_rails['gitlab_email_enabled'] = truegitlab_rails['gitlab_email_from'] = 'ew@xxx.com'gitlab_rails['gitlab_email_display_name'] = 'GitLab Admin'gitlab_rails['gitlab_email_reply_to'] = 'ew@xxx.com'gitlab_rails['gitlab_email_subject_suffix'] = 'GitLab'gitlab_rails['smtp_enable'] = truegitlab_rails['smtp_address'] = &quot;smtp.gmail.com&quot;gitlab_rails['smtp_port'] = 587gitlab_rails['smtp_user_name'] = &quot;ew@xxx.com&quot;gitlab_rails['smtp_password'] = &quot;&quot;gitlab_rails['smtp_domain'] = &quot;smtp.gmail.com&quot;gitlab_rails['smtp_authentication'] = &quot;login&quot;gitlab_rails['smtp_enable_starttls_auto'] = truegitlab_rails['smtp_tls'] = falsegitlab_rails['smtp_openssl_verify_mode'] = 'peer'</code></pre><h2 id="备份配置">备份配置</h2><p>备份到AWS-S3，role 授权</p><pre><code class="language-bash">## /export/docker-data-gitlab/config/gitlab.rbgitlab_rails['backup_upload_connection'] = &#123;  'provider' =&gt; 'AWS',  'region' =&gt; '&lt;region-id&gt;',  #'aws_access_key_id' =&gt; 'AKIAKIAKI',  #'aws_secret_access_key' =&gt; 'secret123'  # If using an IAM Profile, don't configure aws_access_key_id &amp; aws_secret_access_key  'use_iam_profile' =&gt; true&#125;# 备份到S3上的根路径 bucket/Path gitlab_rails['backup_upload_remote_directory'] = '&lt;桶名&gt;/backup/gitlab'</code></pre><pre><code class="language-json"># s3策略&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:PutObject&quot;,                &quot;s3:GetObject&quot;,                &quot;s3:ListBucket&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::&lt;桶名&gt;&quot;,                &quot;arn:aws:s3:::&lt;桶名&gt;/backup/gitlab/*&quot;            ]        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListAllMyBuckets&quot;,            &quot;Resource&quot;: &quot;*&quot;        &#125;    ]&#125;</code></pre><p>备份到阿里云-OSS，AK授权，暂不支持 role 授权</p><pre><code class="language-bash">## /export/docker-data-gitlab/config/gitlab.rbgitlab_rails['backup_upload_connection'] = &#123;'provider' =&gt; 'aliyun','aliyun_accesskey_id' =&gt; 'AK123','aliyun_accesskey_secret' =&gt; 'secret123','aliyun_oss_bucket' =&gt; '&lt;桶名&gt;','aliyun_region_id' =&gt; '&lt;region-id&gt;','aliyun_oss_endpoint' =&gt; 'http://oss-&lt;region-id&gt;-internal.aliyuncs.com'&#125;gitlab_rails['backup_upload_remote_directory'] = 'backup/gitlab'</code></pre><p>⚠️</p><blockquote><h3 id="需要注意的是，截止-fog-aliyun-0-3-19-版本，aliyun-oss-endpoint-指定内网地址的时候，依然走的是公网的-endpoint，会消耗公网流量">需要注意的是，截止 fog-aliyun: 0.3.19 版本，aliyun_oss_endpoint 指定内网地址的时候，依然走的是公网的 endpoint，会消耗公网流量</h3><p><a href="https://gitlab.com/gitlab-org/gitlab/-/blob/da46c9655962df7d49caef0e2b9f6bbe88462a02/Gemfile#L122">https://gitlab.com/gitlab-org/gitlab/-/blob/da46c9655962df7d49caef0e2b9f6bbe88462a02/Gemfile#L122</a><br><a href="https://rubygems.org/gems/fog-aliyun">https://rubygems.org/gems/fog-aliyun</a></p></blockquote><pre><code class="language-json">&#123;    &quot;Version&quot;: &quot;1&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;oss:*&quot;            ],            &quot;Resource&quot;: [                &quot;acs:oss:*:*:&lt;桶名&gt;/backup/gitlab/*&quot;            ]        &#125;,        &#123;            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;oss:Get*&quot;,                &quot;oss:List*&quot;            ],            &quot;Resource&quot;: &quot;*&quot;        &#125;    ]&#125;</code></pre><h2 id="备份计划，触发备份">备份计划，触发备份</h2><ol><li>阿里云 （role授权）</li></ol><pre><code class="language-bash"># 配置 role 到服务器上cat &gt; /etc/profile.d/ecs_role.sh &lt;&lt; EOFRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;EOF</code></pre><p>添加脚本到crontab</p><pre><code class="language-bash">#!/bin/bashsource /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $basedirTime=`date +%Y%m%d`docker exec -t gitlab gitlab-backup createtar zcf $&#123;Time&#125;.config.tar.gz /export/docker-data-gitlab/configaliyun oss cp $&#123;Time&#125;.config.tar.gz oss://桶名/backup/gitlab/ -e $&#123;Endpoint&#125; --force</code></pre><ol start="2"><li><p>aws (role授权)</p><p>添加脚本到crontab</p></li></ol><pre><code class="language-bash">#!/bin/bashbasedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $basedirTime=`date +%Y%m%d`docker exec -t gitlab gitlab-backup createtar zcf $&#123;Time&#125;.config.tar.gz /export/docker-data-gitlab/configaws s3 cp $&#123;Time&#125;.config.tar.gz s3://桶名/backup/gitlab/</code></pre><h2 id="重载配置服务">重载配置服务</h2><p>docker exec -it gitlab gitlab-ctl reconfigure</p><h2 id="升级">升级</h2><blockquote><p>大版本升级可能失败，特别是数据库也升级的情况下</p></blockquote><pre><code class="language-bash">docker stop gitlabdocker rm gitlabgitlabtag=domainName=docker pull gitlab/gitlab-ce:$&#123;gitlabtag&#125;docker run --detach --hostname $&#123;domainName&#125;  --publish 443:443 --publish 80:80 --publish 2222:22   --name gitlab   --restart always   --volume /export/docker-data-gitlab/config:/etc/gitlab   --volume /export/docker-data-gitlab/logs:/var/log/gitlab   --volume /export/docker-data-gitlab/data:/var/opt/gitlab   gitlab/gitlab-ce:$&#123;gitlabtag&#125;</code></pre><h2 id="Oauth">Oauth</h2><p><a href="https://docs.gitlab.com/ee/administration/auth/README.html">GitLab authentication and authorization | GitLab</a></p><h2 id="大问题">大问题</h2><p>暂无</p><h2 id="其它">其它</h2><p>若没有直接采用oss作为存储路径，即本地压缩后，再用脚本上传</p><pre><code class="language-bash">#!/bin/bashbasedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`# 确定gitlab备份文件地址mv /path/to/backup/*backup.tar $&#123;basedir&#125;/ls $&#123;basedir&#125;/*.tar | awk -F '/|_' '&#123;print $4&#125;' | while read line;do        Old7day=`date -d &quot;-7 days&quot; &quot;+%s&quot;`        if [ $line -lt $Old7day ];then              rm -rf $&#123;basedir&#125;/$&#123;line&#125;*.tar &amp;&amp; echo &quot;Delete old7day file $&#123;basedir&#125;/$&#123;line&#125;*.tar&quot;        fidonecd ~ &amp;&amp; source $&#123;basedir&#125;/role.confRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;BucketName=&lt;存储桶&gt;echo &quot;Start: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; oss://$&#123;BucketName&#125;/backup/&quot;aliyun oss sync $&#123;basedir&#125;/ oss://$&#123;BucketName&#125;/backup/ --exclude='*.log' --update --delete --force -e $&#123;Endpoint&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_outputecho &quot;End: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; oss://$&#123;BucketName&#125;/backup/&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis-006获取没有设置TTL的key</title>
      <link href="posts/5067cdc/"/>
      <url>posts/5067cdc/</url>
      
        <content type="html"><![CDATA[<p>redis 内存占用爆炸. 所以网上找了一个获取所有没有设置ttl的key脚本</p><pre><code class="language-python"># encoding: utf-8&quot;&quot;&quot;author: yangyi@youzan.comtime: 2018/4/26 下午4:34func: 获取数据库中没有设置ttl的 key&quot;&quot;&quot;import redisimport argparseimport timeimport sysclass ShowProcess:    &quot;&quot;&quot;    显示处理进度的类    调用该类相关函数即可实现处理进度的显示    &quot;&quot;&quot;    i = 0 # 当前的处理进度    max_steps = 0 # 总共需要处理的次数    max_arrow = 100 # 进度条的长度    # 初始化函数，需要知道总共的处理次数    def __init__(self, max_steps):        self.max_steps = max_steps        self.i = 0    # 显示函数，根据当前的处理进度i显示进度    # 效果为[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;]100.00%    def show_process(self, i = None):        if i is not None:            self.i = i        else:            self.i += 1        num_arrow = int(self.i * self.max_arrow / self.max_steps) # 计算显示多少个'&gt;'        num_line = self.max_arrow - num_arrow # 计算显示多少个'-'        percent = self.i * 100.0 / self.max_steps # 计算完成进度，格式为xx.xx%        process_bar = '[' + '&gt;' * num_arrow + ' ' * num_line + ']'\                      + '%.2f' % percent + '%' + '\r' # 带输出的字符串，'\r'表示不换行回到最左边        sys.stdout.write(process_bar) # 这两句打印字符到终端        sys.stdout.flush()    def close(self, words='done'):        print ''        print words        self.i = 0def check_ttl(redis_conn, no_ttl_file, dbindex):    start_time = time.time()    no_ttl_num = 0    keys_num = redis_conn.dbsize()    print &quot;there are &#123;num&#125; keys in db &#123;index&#125; &quot;.format(num=keys_num, index=dbindex)    process_bar = ShowProcess(keys_num)    with open(no_ttl_file, 'a') as f:        for key in redis_conn.scan_iter(count=1000):            process_bar.show_process()            if redis_conn.ttl(key) == -1:                no_ttl_num += 1    #            if no_ttl_num &lt; 1000:                f.write(key+'\n')            else:                continue    process_bar.close()    print &quot;cost time(s):&quot;, time.time() - start_time    print &quot;no ttl keys number:&quot;, no_ttl_num    print &quot;we write keys with no ttl to the file: %s&quot; % no_ttl_filedef main():    parser = argparse.ArgumentParser()    parser.add_argument('-p', type=int, dest='port', action='store', help='port of redis ')    parser.add_argument('-d', type=str, dest='db_list', action='store', default=0,                        help='ex : -d all / -d 1,2,3,4 ')    args = parser.parse_args()    port = args.port    if args.db_list == 'all':        db_list = [i for i in xrange(0, 16)]    else:        db_list = [int(i) for i in args.db_list.split(',')]    for index in db_list:        try:            pool = redis.ConnectionPool(host='127.0.0.1', port=port, db=index)            r = redis.StrictRedis(connection_pool=pool)        except redis.exceptions.ConnectionError as e:            print e        else:            no_ttl_keys_file = &quot;/tmp/&#123;port&#125;_&#123;db&#125;_no_ttl_keys.txt&quot;.format(port=port, db=index)            check_ttl(r, no_ttl_keys_file, index)if __name__ == '__main__':    main()</code></pre><ol><li>请勿在redis服务器上执行</li><li>修改脚本中 127.0.0.1 为 redis 服务器地址</li><li>脚本执行命令: <code>python nottlkey.py -d all -p 6379  </code></li><li>添加ttl命令：<code>cat no_ttl_keys.txt  |  xargs -i -t ./redis-cli -h &lt;redis_ip&gt; -n &lt;database_num&gt; expire &lt;ttl_time&gt;</code></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞EC2-GPU实例安装显卡驱动</title>
      <link href="posts/2e07203a/"/>
      <url>posts/2e07203a/</url>
      
        <content type="html"><![CDATA[<blockquote><p>以下命令均在 ubuntu 用户中执行</p></blockquote><h2 id="安装-awscli-到-ubuntu-用户下">安装 awscli 到 ubuntu 用户下</h2><pre><code class="language-bash:">sudo apt-get update -ysudo apt-get install python3-pip -ypip3 install awscli --upgrade --user</code></pre><h2 id="更新系统内核和gcc，以及禁用开源驱动">更新系统内核和gcc，以及禁用开源驱动</h2><pre><code class="language-bash:">sudo apt-get upgrade -y linux-awssudo rebootsudo apt-get install -y gcc make linux-headers-$(uname -r)cat &lt;&lt; EOF | sudo tee --append /etc/modprobe.d/blacklist.confblacklist vga16fbblacklist nouveaublacklist rivafbblacklist nvidiafbblacklist rivatvEOFsudo vi /etc/default/grub     GRUB_CMDLINE_LINUX=&quot;rdblacklist=nouveau&quot;sudo update-grubsudo reboot</code></pre><h2 id="配置-aws-key-拉取-G3-系列驱动">配置 aws key 拉取 G3 系列驱动</h2><pre><code class="language-bash:">aws configure# 根据提示输入相关数据，这里 aws key 可以用任意 iam 用户的</code></pre><h2 id="下载-G3-驱动">下载 G3 驱动</h2><pre><code class="language-bash:"># G3实例直接从aws库里下载，其它需要自行去官网下载https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/install-nvidia-driver.html#obtain-nvidia-GRID-driver-linuxaws s3 cp --recursive s3://ec2-linux-nvidia-drivers/latest/ .</code></pre><h2 id="安装驱动">安装驱动</h2><pre><code class="language-bash:">sudo /bin/sh ./NVIDIA-Linux-x86_64*.run# 可能会出现 gcc 版本检测，忽略掉版本检测，直接安装sudo reboot</code></pre><h2 id="启用-GRUD-虚拟应用程序，默认启用-GRUD-虚拟工作站">启用 GRUD 虚拟应用程序，默认启用 GRUD 虚拟工作站</h2><pre><code class="language-bash:"># 具体步骤看文档https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/activate_grid.html</code></pre><h2 id="优化G3，开启全功率，关闭autoboost-（P3-实例上的-GPU-不支持-autoboost-功能。）">优化G3，开启全功率，关闭autoboost （P3 实例上的 GPU 不支持 autoboost 功能。）</h2><pre><code class="language-bash:">sudo nvidia-persistencedsudo nvidia-smi --auto-boost-default=0# P2 实例：sudo nvidia-smi -ac 2505,875# P3 实例：sudo nvidia-smi -ac 877,1530# G3 实例：sudo nvidia-smi -ac 2505,1177</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ubuntu </tag>
            
            <tag> gpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞静态配置</title>
      <link href="posts/d1d65c30/"/>
      <url>posts/d1d65c30/</url>
      
        <content type="html"><![CDATA[<blockquote><p>QPS 800-1000</p><p>2*4 机器</p></blockquote><pre><code class="language-ini">[www01]user = webappsgroup = webappslisten = /usr/local/php/var/log/php-fpm-www01.socklisten.owner = webappslisten.group = webappslisten.backlog = 10240listen.mode = 0666listen.allowed_clients = 127.0.0.1pm = staticpm.max_children = 300pm.max_requests = 1024request_slowlog_timeout = 1request_terminate_timeout = 1slowlog = /usr/local/php/var/log/php-slow.logpm.status_path = /php-fpm_status</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> php-fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞问题集</title>
      <link href="posts/6332ca50/"/>
      <url>posts/6332ca50/</url>
      
        <content type="html"><![CDATA[<h2 id="💥upstream-prematurely-closed-connection-while-reading-response-header-from-upstream">💥upstream prematurely closed connection while reading response header from upstream</h2><p>汉译：当从上游读取响应头时，上游提前关闭连接</p><p>经确认，此错误一般是开启长链接后，长链接超时关闭时，恰好有数据发生</p><p>因此，解决方案有两个：</p><ol><li>nginx 将长链接的超时设置为 0 ，即关闭长链接</li></ol><pre><code class="language-bash">keepalive_timeout 0; # 此值默认为0，默认位于nginx主配置</code></pre><ol start="2"><li>关闭 nginx 长链接超时主动关闭的功能</li></ol><pre><code class="language-bash">proxy_http_version 1.1; # 支持长链接proxy_set_header Connection &quot;&quot;; # 此值默认是 close，将其设置为空从而使nginx不会主动关闭长连接；</code></pre><blockquote><p>nginx 添加自定义 header 后，就会覆盖掉相应的默认值</p><p><a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header">Module ngx_http_proxy_module (nginx.org)</a></p><table><thead><tr><th style="text-align:left">Syntax:</th><th><code>proxy_set_header field value;</code></th></tr></thead><tbody><tr><td style="text-align:left">Default:</td><td><code>proxy_set_header Host $proxy_host; proxy_set_header Connection close;</code></td></tr><tr><td style="text-align:left">Context:</td><td><code>http</code>, <code>server</code>, <code>location</code></td></tr></tbody></table></blockquote><h2 id="💥get参数经过-proxy-pass-后，上游后端程序无法获取">💥get参数经过 proxy_pass 后，上游后端程序无法获取</h2><pre><code class="language-bash">    location ~* ^/v2/(.*) &#123;        proxy_pass http://127.0.0.1:8083/$1?$args;        proxy_connect_timeout 10s;        proxy_send_timeout 10s;        proxy_read_timeout 10s;        proxy_max_temp_file_size 1024m;     # 若设置为0，表示不启用缓存        proxy_set_header   Host         $host;        proxy_set_header   X-Real-IP    $remote_addr;        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;        proxy_buffers 256 4k;        proxy_intercept_errors on;        #开启错误页支持    &#125;</code></pre><p>✨通过 $args 传递。</p>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>openvpn☞安装</title>
      <link href="posts/7c1abbe1/"/>
      <url>posts/7c1abbe1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>openvpn 一般用于小公司远程连接公司办公网络环境，此脚本需要一个linux系统的主机。<br>至于土豪公司，可以无视。<br>文档包含安装脚本，创建用户脚本，删除用户脚本。使用的时候，安装脚本和创建用户脚本，需要自行修改一些变量。</p><blockquote><p>需要注意的是，发现移动网络连接其它运营商网络的时候，不稳定。<br>比如我这边，移动网络连接电信网络（openvpn所在网络），就容易丢包。</p></blockquote><h2 id="安装">安装</h2><p>💥以下命令需逐行执行，不能复制批量执行，因为中间需要自行添加变量，以及手动输入一些信息</p><pre><code class="language-bash:">yum install dockerdocker pull kylemanna/openvpnOVPN_DATA=&quot;/root/ovpn-data&quot;IP=&quot;服务器主网卡ip&quot;  # 自行修改PORT=  # 容器映射的宿主机端口mkdir $&#123;OVPN_DATA&#125;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_genconfig -u tcp://$&#123;IP&#125;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn ovpn_initpki# 输入组织机构名以及密码</code></pre><pre><code class="language-bash">docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn easyrsa build-client-full CLIENTNAME nopass</code></pre><pre><code class="language-bash">echo 'net.ipv4.ip_forward = 1' &gt;&gt; /etc/sysctl.d/99-sysctl.conf &amp;&amp; sysctl -pdocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient CLIENTNAME &gt; $&#123;OVPN_DATA&#125;/CLIENTNAME.ovpndocker run --name openvpn -v $&#123;OVPN_DATA&#125;:/etc/openvpn -d -p $&#123;PORT&#125;:1194 --privileged kylemanna/openvpn</code></pre><h2 id="容器生成的服务器主配置">容器生成的服务器主配置</h2><p>/root/ovpn-data/openvpn.conf</p><p>下面是一个改后的示例配置</p><pre><code class="language-bash">server 192.168.255.0 255.255.255.0verb 3# 变更为容器里默认生成的key /etc/openvpn/pki/private/&lt;修改我&gt;.keyca /etc/openvpn/pki/ca.crt# 变更为容器里默认生成的cert /etc/openvpn/pki/issued/&lt;修改我&gt;.crtdh /etc/openvpn/pki/dh.pemtls-auth /etc/openvpn/pki/ta.keykey-direction 0keepalive 10 60persist-keypersist-tunproto tcp# Rely on Docker to do port mapping, internally always 1194port 1194dev tun0status /tmp/openvpn-status.loguser nobodygroup nogroupcomp-lzo no### Route Configurations Belowroute 192.168.254.0 255.255.255.0### Push Configurations Belowpush &quot;block-outside-dns&quot;push &quot;comp-lzo no&quot;### 开启 DNS 推送push &quot;dhcp-option DNS 223.5.5.5&quot;push &quot;dhcp-option DNS 114.114.114.114&quot;### 添加 DNS 的路由push &quot;route 223.5.5.5 255.255.255.255&quot;push &quot;route 114.114.114.114 255.255.255.255&quot;### 添加客户端需要通过openvpn访问的网络push &quot;route 10.100.0.0 255.255.0.0&quot;</code></pre><h3 id="服务端dns推送与客户端全局路由的关系">服务端dns推送与客户端全局路由的关系</h3><p>如果客户端删除了全局路由 redirect-gateway def1；则服务端要么关闭 dns 推送，要么开启 dns 推送的同时，开启 dns 的相关路由。</p><p>如果客户端开启了全局路由 redirect-gateway def1；则服务端最好开启 dns 推送，但无需开启 dns 路由。</p><h3 id="示例，客户仅部分网络需要走openvpn，其余网络走本地，且需要通过公司dns解析域名。">示例，客户仅部分网络需要走openvpn，其余网络走本地，且需要通过公司dns解析域名。</h3><p>服务端配置：开启 dns 推送和 dns 路由</p><pre><code class="language-bash">push &quot;dhcp-option DNS 公司dns地址&quot;push &quot;dhcp-option DNS 114.114.114.114&quot;push &quot;route 公司dns地址 255.255.255.255&quot;push &quot;route 114.114.114.114 255.255.255.255&quot;push &quot;route 需要走openvpn的网段和掩码&quot;</code></pre><p>客户端配置：关闭全局路由</p><pre><code class="language-bash"># redirect-gateway def1</code></pre><h2 id="创建用户脚本">创建用户脚本</h2><p>👙需要注意的是，下列脚本中，有两个 sed 命令是需要自行修改ip的。</p><pre><code class="language-bash">sed -i &quot;s/1194/&lt;外网端口&gt;/&quot; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn # 修改我sed -i &quot;s/&lt;服务器内网IP&gt;/&lt;客户端连接的外网IP&gt;/&quot; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn  # 修改我</code></pre><pre><code class="language-bash:">#!/bin/bash# 如果是nat后的内网ip，则需要修改配置文件里的ip为外网ip# bash xxx.sh &lt;用户名&gt;[[ -z $1 ]] &amp;&amp; exitCLIENTNAME=$1OVPN_DATA=&quot;/root/ovpn-data&quot;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn easyrsa build-client-full $&#123;CLIENTNAME&#125; nopassdocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient $CLIENTNAME &gt; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '1a comp-lzo' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '2a tun-mtu 1500' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '3a auth-nocache' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i &quot;s/1194/&lt;外网端口&gt;/&quot; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn                   # 修改我sed -i &quot;s/&lt;服务器内网IP&gt;/&lt;客户端连接的外网IP&gt;/&quot; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn  # 修改我read -p '是否关闭全局路由，（y）关闭全局路由，仅当客户端匹配下发路由时才走openvpn.[确保服务端没有开启DNS推送，若开启则添加对应的DNS路由]；（n）开启&gt;全局路由，客户端所有流量均走openvpn.[确保服务端开启了DNS推送]：' yn[[ $yn == 'y' ]] &amp;&amp; sed -i '/redirect-gateway def1/d' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnmkdir -pv $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv /root/ovpn-data/pki/private/$&#123;CLIENTNAME&#125;.key $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv /root/ovpn-data/pki/issued/$&#123;CLIENTNAME&#125;.crt $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;cd $&#123;OVPN_DATA&#125;/users/tar zcf $&#123;CLIENTNAME&#125;.tar.gz $&#123;CLIENTNAME&#125;echo &quot;`pwd`/$&#123;CLIENTNAME&#125;.tar.gz&quot;echo &quot;################################################&quot;cat $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;/$&#123;CLIENTNAME&#125;.ovpn#sz $&#123;CLIENTNAME&#125;.tar.gz</code></pre><h2 id="删除用户脚本">删除用户脚本</h2><pre><code class="language-bash:">#!/bin/bash# bash xxx.sh &lt;用户名&gt;[[ -z $1 ]] &amp;&amp; exitCLIENTNAME=$1#read -p '输入用户名（字母组成）:' CLIENTNAMEOVPN_DATA=&quot;/root/ovpn-data&quot;rm -rf /root/ovpn-data/pki/private/$&#123;CLIENTNAME&#125;.key*rm -rf /root/ovpn-data/pki/reqs/$&#123;CLIENTNAME&#125;.reqrm -rf /root/ovpn-data/pki/issued/$&#123;CLIENTNAME&#125;.crtsed -i &quot;/$&#123;CLIENTNAME&#125;/d&quot; /root/ovpn-data/pki/index.txtcd /root/ovpn-data/usersrm -rf $&#123;CLIENTNAME&#125;rm -f $&#123;CLIENTNAME&#125;.tar.gz</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openvpn </tag>
            
            <tag> 安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis-005主从复制和哨兵配置</title>
      <link href="posts/847734cb/"/>
      <url>posts/847734cb/</url>
      
        <content type="html"><![CDATA[<h2 id="基本配置">基本配置</h2><p>构建三个节点，一主两从。</p><p>之所以是三节点，是因为哨兵模式（一种高可用）需要最少三个才稳健。</p><p>✨这里的三个指的是哨兵进程，而不是redis进程。redis可以只有主从两个。</p><table><thead><tr><th>服务类型</th><th>是否是主服务器</th><th>IP地址</th><th>端口</th></tr></thead><tbody><tr><td>Redis</td><td>是</td><td>10.200.16.51</td><td>6379</td></tr><tr><td>Redis</td><td>否</td><td>10.200.16.52</td><td>6379</td></tr><tr><td>Redis</td><td>否</td><td>10.200.16.53</td><td>6379</td></tr><tr><td>Sentinel</td><td>-</td><td>10.200.16.51</td><td>26379</td></tr><tr><td>Sentinel</td><td>-</td><td>10.200.16.52</td><td>26379</td></tr><tr><td>Sentinel</td><td>-</td><td>10.200.16.53</td><td>26379</td></tr></tbody></table><p>哨兵模式下，不应该使用的东西：</p><p>💥不应该使用主机名，因为当前哨兵模式下支持度不行（6.2版本有提到相关配置，但默认是禁止）。</p><p>💥不应该使用docker部署，除非你采用主机网络模式。原因在于：</p><ul><li>哨兵进程基于&quot;hello&quot;信息相互发现，而&quot;hello&quot;信息里包含的IP和端口均为容器内端口，而不是映射后的端口信息。</li><li>哨兵进程通过master的info信息检测副本，而info信息依然不是映射后的端口。这就导致哨兵不知道副本信息，也就没法进行故障转移。</li></ul><p>✨你可以使用docker主机网络模式，因为这种网络模式下不存在端口映射。</p><h2 id="主配置">主配置</h2><pre><code class="language-bash"># Basebind 0.0.0.0protected-mode yesport 6379daemonize yespidfile &quot;/export/redis_6379/redis.pid&quot;loglevel warninglogfile &quot;/export/redis_6379/logs/redis.log&quot;dir &quot;/export/redis_6379/data/&quot;# Authrequirepass 123456# Replica master## 必须有一个从服务，才允许写入min-replicas-to-write 1## 从服务有10秒时间来发送异步确认，若超过则认为从不可用，进而因为 min-replicas-to-write 1 导致不可写入min-replicas-max-lag 10# Replica slave## master 认证#masterauth &lt;master-password&gt;## 指定 master 地址#replicaof &lt;masterip&gt; &lt;masterport&gt;replica-read-only yesreplica-priority 100# Memorymaxmemory 1gmaxmemory-policy allkeys-lru# RDBsave 900 1save 300 10save 60 10000dbfilename redis_6379.rdb# AOFappendonly yesappendfilename appendonly_6379.aofappendfsync everysecno-appendfsync-on-rewrite noaof-use-rdb-preamble yes# rewrite commandrename-command FLUSHDB GOD_FLUSHDBrename-command FLUSHALL GOD_FLUSHALL#rename-command CONFIG GOD_CONFIGrename-command KEYS GOD_KEYS</code></pre><h2 id="从节点">从节点</h2><p>将主配置里略加修改即可</p><pre><code class="language-bash">## master 认证masterauth &lt;master-password&gt;## 指定 master 地址replicaof &lt;masterip&gt; &lt;masterport&gt;</code></pre><p>命令行方式：</p><pre><code class="language-bash">-&gt; bin/redis-cli -h 127.0.0.1 -p 6379 -a 123456 replicaof &lt;master_ip&gt; 6379</code></pre><h2 id="链接状态">链接状态</h2><p>主节点：</p><pre><code class="language-bash">127.0.0.1:6379&gt; INFO replication# Replicationrole:masterconnected_slaves:1slave0:ip=10.200.16.52,port=6379,state=online,offset=309,lag=1……</code></pre><p>从节点：</p><pre><code class="language-bash">127.0.0.1:6379&gt; INFO replication# Replicationrole:slavemaster_host:10.200.16.51master_port:6379master_link_status:upmaster_last_io_seconds_ago:7master_sync_in_progress:0slave_repl_offset:365slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0</code></pre><h2 id="从升级主">从升级主</h2><p>当主不可用的时候，从执行命令可升级为主</p><pre><code>-&gt; bin/redis-cli -h 127.0.0.1 -p 6379 -a 123456 SLAVEOF NO ONEOK</code></pre><h2 id="健康检测">健康检测</h2><pre><code class="language-bash">-&gt; bin/redis-cli -h 127.0.0.1 -p 6379 -a 123456 monitorOK1448515184.249169 [0 10.86.255.166:6379] &quot;PING&quot;</code></pre><p>可以通过<code>ping</code>指令来确认节点是否正常。</p><pre><code class="language-bash">-&gt; bin/redis-cli -h 127.0.0.1 -p 6379 -a 123456 pingPONG</code></pre><h2 id="如何实现主从自动切换">如何实现主从自动切换</h2><ol><li>通过编写keepalived脚本来实现健康检测，从而实现VIP漂移。<ol><li>keepalived方式不适合云环境，因为没法实现vip。</li></ol></li><li>通过官方sentinel哨兵来进行检测。<ol><li>哨兵方式因为没有vip，所以需要客户端库实现及时获取正确的主节点</li></ol></li></ol><h2 id="哨兵模式">哨兵模式</h2><p><a href="https://redis.io/topics/sentinel">https://redis.io/topics/sentinel</a></p><p>哨兵模式，通过哨兵来监控master节点，并通过调用master节点info命令中发现slave节点。</p><p>当故障切换的时候，哨兵需要执行config指令来修改配置信息。</p><p>💥因此，info和config两个指令不可自定义别名。</p><p>哨兵分为主观下线（sdown）和客观下线（odown）。主观下线就是自己监控master down掉，但此时还不会切换，客观下线就是其它哨兵也发现有问题，则此时满足客观下线条件。</p><p>当客观下线也满足的时候，就会开始选出执行故障转移的哨兵领导者（一个哨兵不会在两次连续的故障中被选为领导者）。</p><p>领导者选出一个从redis，并发送slaveof no one指令将其转为主。</p><p>之后，发送修改配置指令给其它节点，变更配置指向新主。</p><p>最后执行主切换指令。在此之后，通过哨兵查询主节点就会返回新的主地址给客户端。</p><h3 id="配置">配置</h3><p>redis-sentinel.conf</p><pre><code>protected-mode noport 26379daemonize yespidfile &quot;/export/redis_6379/redis-sentinel.pid&quot;logfile &quot;/export/redis_6379/logs/redis-sentinel.log&quot;dir &quot;/export/redis_6379/data&quot;# 配置监听的主服务器## mymaster 自定义一个主服务器的昵称## 代表监控的主服务器## 6379代表端口## 2代表只有两个或两个以上的哨兵认为主服务器不可用的时候，才会进行failover操作。sentinel monitor mymaster &lt;主redis的ip&gt; 6379 2# 访问master所需的认证账户密码## mymaster 自定义的主服务器昵称## 123456 master 配置的密码sentinel auth-pass mymaster &lt;主redis的密码&gt;# 客户端访问哨兵所需的auth认证requirepass &lt;哨兵的密码&gt;sentinel down-after-milliseconds mymaster 10000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1</code></pre><p>✨这个配置里的信息会在哨兵启动后自动变更，不要在随意修改:</p><ul><li>自动写入自己哨兵id</li><li>自动写入已经发现的其它哨兵信息</li><li>自动写入已经发现的从redis信息</li></ul><h3 id="启动">启动</h3><p>先启动主服务，然后启动2个从服务器，最后启动三个哨兵</p><pre><code class="language-bash"># 启动Redis服务器进程## 先启动主服务，然后启动从服务器systemctl start redis# 启动哨兵进程## 最后启动哨兵systemctl start redis-sentinel</code></pre><h3 id="状态">状态</h3><pre><code class="language-bash">$ redis-cli -p 26379127.0.0.1:26379&gt; sentinel master mymaster 1) &quot;name&quot; 2) &quot;mymaster&quot; 3) &quot;ip&quot; 4) &quot;127.0.0.1&quot; 5) &quot;port&quot; 6) &quot;6379&quot; 7) &quot;runid&quot; 8) &quot;953ae6a589449c13ddefaee3538d356d287f509b&quot; 9) &quot;flags&quot;          # 如果master没了，这里会变成 s_down(主观认为) 或者 o_down（客观认为）10) &quot;master&quot;          11) &quot;link-pending-commands&quot;12) &quot;0&quot;13) &quot;link-refcount&quot;14) &quot;1&quot;15) &quot;last-ping-sent&quot;16) &quot;0&quot;17) &quot;last-ok-ping-reply&quot;18) &quot;735&quot;19) &quot;last-ping-reply&quot;20) &quot;735&quot;21) &quot;down-after-milliseconds&quot;22) &quot;5000&quot;23) &quot;info-refresh&quot;24) &quot;126&quot;25) &quot;role-reported&quot;26) &quot;master&quot;27) &quot;role-reported-time&quot;28) &quot;532439&quot;29) &quot;config-epoch&quot;30) &quot;1&quot;31) &quot;num-slaves&quot;        # 通过master发现的副本数32) &quot;1&quot;33) &quot;num-other-sentinels&quot;  # 发现的其它哨兵数34) &quot;2&quot;35) &quot;quorum&quot;36) &quot;2&quot;37) &quot;failover-timeout&quot;38) &quot;60000&quot;39) &quot;parallel-syncs&quot;40) &quot;1&quot;</code></pre><p>其它状态命令</p><pre><code class="language-bash">SENTINEL replicas mymaster  # 获取副本信息SENTINEL sentinels mymaster # 获取其它哨兵信息</code></pre><p>客户端需要实时的获取master的地址，否则无法使用，哨兵有API可以拿到。命令行里获取master地址</p><pre><code class="language-bash">SENTINEL get-master-addr-by-name mymaster</code></pre><h3 id="模拟主故障">模拟主故障</h3><pre><code class="language-bash">bin/redis-cli -p 6379 -a 123456 DEBUG sleep 120</code></pre><h3 id="故障信息日志">故障信息日志</h3><p>被选为执行故障转移的领导者哨兵日志如下：</p><pre><code class="language-log">30977:X 15 Mar 2022 18:25:46.361 # +sdown master mymaster 10.200.16.51 6379    # 10.200.16.51 主观下线30977:X 15 Mar 2022 18:25:46.419 # +odown master mymaster 10.200.16.51 6379 #quorum 2/2  # 10.200.16.51 满足客观下线30977:X 15 Mar 2022 18:25:46.420 # +new-epoch 3  # 第三次出现问题30977:X 15 Mar 2022 18:25:46.420 # +try-failover master mymaster 10.200.16.51 6379   30977:X 15 Mar 2022 18:25:46.422 # +vote-for-leader c4f6768af1f0f82e2b094d10e0cdaeba030a5412 3   # 选出哨兵领导，字符串是哨兵ID30977:X 15 Mar 2022 18:25:46.424 # 2c0702024e54b96dfc750455d4fb927add2ca3fe voted for c4f6768af1f0f82e2b094d10e0cdaeba030a5412 330977:X 15 Mar 2022 18:25:46.427 # 52ac0f8bc13ff4ddd3a0557008664742a06db10f voted for c4f6768af1f0f82e2b094d10e0cdaeba030a5412 330977:X 15 Mar 2022 18:25:46.523 # +elected-leader master mymaster 10.200.16.51 637930977:X 15 Mar 2022 18:25:46.523 # +failover-state-select-slave master mymaster 10.200.16.51 637930977:X 15 Mar 2022 18:25:46.585 # +selected-slave slave 10.200.16.52:6379 10.200.16.52 6379 @ mymaster 10.200.16.51 6379 # 选定 10.200.16.52 从作为备选主30977:X 15 Mar 2022 18:25:46.585 * +failover-state-send-slaveof-noone slave 10.200.16.52:6379 10.200.16.52 6379 @ mymaster 10.200.16.51 6379 # 发送 slaveof no one ，将 10.200.16.52 升级为主30977:X 15 Mar 2022 18:25:46.675 * +failover-state-wait-promotion slave 10.200.16.52:6379 10.200.16.52 6379 @ mymaster 10.200.16.51 637930977:X 15 Mar 2022 18:25:47.486 # +promoted-slave slave 10.200.16.52:6379 10.200.16.52 6379 @ mymaster 10.200.16.51 637930977:X 15 Mar 2022 18:25:47.486 # +failover-state-reconf-slaves master mymaster 10.200.16.51 6379  # 重写 10.200.16.51 配置为从30977:X 15 Mar 2022 18:25:47.540 * +slave-reconf-sent slave 10.200.16.53:6379 10.200.16.53 6379 @ mymaster 10.200.16.51 6379 # 重写 10.200.16.53 从配置30977:X 15 Mar 2022 18:25:48.490 * +slave-reconf-inprog slave 10.200.16.53:6379 10.200.16.53 6379 @ mymaster 10.200.16.51 637930977:X 15 Mar 2022 18:25:48.491 * +slave-reconf-done slave 10.200.16.53:6379 10.200.16.53 6379 @ mymaster 10.200.16.51 637930977:X 15 Mar 2022 18:25:48.542 # -odown master mymaster 10.200.16.51 6379 # 10.200.16.51 客观下线事件结束30977:X 15 Mar 2022 18:25:48.542 # +failover-end master mymaster 10.200.16.51 6379  # 10.200.16.51 故障转移结束30977:X 15 Mar 2022 18:25:48.542 # +switch-master mymaster 10.200.16.51 6379 10.200.16.52 6379  # 切换主30977:X 15 Mar 2022 18:25:48.543 * +slave slave 10.200.16.53:6379 10.200.16.53 6379 @ mymaster 10.200.16.52 637930977:X 15 Mar 2022 18:25:48.543 * +slave slave 10.200.16.51:6379 10.200.16.51 6379 @ mymaster 10.200.16.52 6379 # 51成为从30977:X 15 Mar 2022 18:25:58.554 # +sdown slave 10.200.16.51:6379 10.200.16.51 6379 @ mymaster 10.200.16.52 6379 # 51从主观下线(因为debug指令还未结束)30977:X 15 Mar 2022 18:30:36.203 # -sdown slave 10.200.16.51:6379 10.200.16.51 6379 @ mymaster 10.200.16.52 6379 # 51从恢复(debug指令结束)30977:X 15 Mar 2022 18:30:46.159 * +convert-to-slave slave 10.200.16.51:6379 10.200.16.51 6379 @ mymaster 10.200.16.52 6379</code></pre><h3 id="监控主节点">监控主节点</h3><pre><code class="language-bash">-&gt; watch bin/redis-cli -h 127.0.0.1 -a 123456 -p 26379 SENTINEL get-master-addr-by-name mymaster===10.200.16.526379</code></pre><h3 id="添加一个哨兵">添加一个哨兵</h3><p>只需要正常配置一个哨兵并启动即可</p><h3 id="删除一个哨兵">删除一个哨兵</h3><ol><li>停止要删除的哨兵</li><li>在所有剩余哨兵里执行重置命令</li></ol><pre><code class="language-bash">SENTINEL RESET &lt;master昵称&gt;</code></pre><ol start="3"><li>检查哨兵，确认其它哨兵数量是否一致</li></ol><pre><code class="language-bash">SENTINEL MASTER &lt;master昵称&gt;</code></pre><h3 id="删除一个节点">删除一个节点</h3><p>在你删除一个旧的主节点或者不可用的节点后，应在所有哨兵里执行重置命令</p><pre><code class="language-bash">SENTINEL RESET &lt;master昵称&gt;</code></pre><h2 id="客户端-JAVA">客户端(JAVA)</h2><p>网上找的一个JAVA例子=。=</p><p>jedis通过配置哨兵列表从而满足从哨兵中动态的获取主节点信息。</p><p>💥如果哨兵里配置了requirepass “your_password_here”，则需要jedis支持。</p><pre><code class="language-java">import java.util.HashSet;import java.util.Random;import java.util.Set;import java.util.concurrent.TimeUnit;import org.apache.log4j.Logger;import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisSentinelPool;public class RedisSentinel &#123;    private static Logger log = Logger.getLogger(RedisSentinel.class);        public static void redisSentinel() &#123;        Set&lt;String&gt; sentinels = new HashSet&lt;String&gt;();        sentinels.add(&quot;192.168.170.128:26379&quot;);//192.168.170.11        sentinels.add(&quot;192.168.170.129:26379&quot;);//192.168.170.12        sentinels.add(&quot;192.168.170.130:26379&quot;);//192.168.170.13        JedisSentinelPool jedisSentinelPool = new JedisSentinelPool(&quot;mymaster&quot;, sentinels);        Jedis jedis = jedisSentinelPool.getResource();        String a = jedis.get(&quot;a&quot;);        String b = jedis.get(&quot;b&quot;);        System.out.println(&quot;a=&quot; + a);        System.out.println(&quot;b=&quot; + b);        jedis.close();        jedisSentinelPool.close();    &#125;            /**     * 测试哨兵转移     */    public static void redisSentinelTransfer() &#123;        Set&lt;String&gt; sentinels = new HashSet&lt;String&gt;();        sentinels.add(&quot;192.168.170.11:26379&quot;);        sentinels.add(&quot;192.168.170.12:26379&quot;);        sentinels.add(&quot;192.168.170.13:26379&quot;);        @SuppressWarnings(&quot;resource&quot;)        JedisSentinelPool jedisSentinelPool = new JedisSentinelPool(&quot;mymaster&quot;, sentinels);        Jedis jedis = null;                while(true) &#123;            try &#123;                //不能放在while里面，这样连接池在8次后会被用完，不执行redis方法，也不报错。                //如果放在while，必须回收。                jedis = jedisSentinelPool.getResource();                int randInt = new Random().nextInt(10000);                String key  = &quot;k_&quot; + randInt;                String value  = &quot;v_&quot; + randInt;                jedis.set(key, value);                                log.info(key + &quot; = &quot; + value);                TimeUnit.SECONDS.sleep(2);                            &#125; catch (Exception e) &#123;                log.info(e);                            &#125; finally &#123;                if(jedis != null) &#123;                    jedis.close();                &#125;            &#125;                    &#125;            &#125;            public static void main(String[] args) &#123;        //redisSentinel();        redisSentinelTransfer();    &#125;  &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis-004主从复制知识</title>
      <link href="posts/783b242a/"/>
      <url>posts/783b242a/</url>
      
        <content type="html"><![CDATA[<h2 id="主从复制">主从复制</h2><p>Redis主从复制分为全量同步和增量同步</p><h2 id="全量复制">全量复制</h2><p>1）从服务器连接主服务器，发送SYNC命令；</p><p>2）主服务器接收到SYNC命令后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； 这个过程比较消耗CPU、内存和硬盘IO。</p><p>3）主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令；</p><p>4）从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 这个过程是阻塞的，客户端无法访问从服务器。</p><p>5）主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令；</p><p>6）从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令；即部分复制。</p><h2 id="部分复制">部分复制</h2><ol><li><p>复制偏移量：主从节点分别维护一个复制偏移量。</p></li><li><p>复制积压缓冲区：主节点内部维护一个队列，用于在从节点两次复制间隔期内存放新的命令。从而实现从redis每次只需复制缓冲区里的命令。</p><p>💥如果从节点同步的时候，【主节点的偏移量】-【从节点的偏移量】&gt;【缓冲区长度】，则需要重新全量复制。</p></li><li><p>服务器运行ID：每一个redis运行的时候，都有一个运行ID，表明自己的身份。当主从同步的时候，主redis会将自己的运行id发给从redis。</p><p>它的作用是，当从因某些原因导致主从断开，则断开期间：</p><ul><li>若主redis变了，则从redis重连后，需要全量复制。</li><li>若主redis没变，则从redis重连后，则采取2号步骤。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis-003内存紧张bgsave失败</title>
      <link href="posts/28361833/"/>
      <url>posts/28361833/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>之前安装完redis，启动redis，总会告诉你让你默认修改一些参数，不知其原因，但每次都照办。<br>后来遇到一些redis内存使用紧张的时候，从而导致 redis-bgsave 失败，再一次想到了这个问题。<br>经过查阅资料，发现官方文档里告诉了原因，在此记录一下。如有不对之处，还请指出。</p><h4 id="问题">问题</h4><p>redis-bgsave与overcommit_memory的关系。<br>当剩余物理内存低于当前redis所用内存的时候，overcommit_memory=1的意义</p><h4 id="官方解释在此">官方解释在此</h4><blockquote><p><a href="https://redis.io/topics/faq">https://redis.io/topics/faq</a></p></blockquote><h2 id="Background-saving-fails-with-a-fork-error-under-Linux-even-if-I-have-a-lot-of-free-RAM">Background saving fails with a fork() error under Linux even if I have a lot of free RAM!</h2><p>Short answer: : <code>echo 1 &gt; /proc/sys/vm/overcommit_memory</code></p><p>And now the long one:</p><p>Redis background saving schema relies on the copy-on-write semantic of fork in modern operating systems: Redis forks (creates a child process) that is an exact copy of the parent. The child process dumps the DB on disk and finally exits. In theory the child should use as much memory as the parent being a copy, but actually thanks to the copy-on-write semantic implemented by most modern operating systems the parent and child process will <em>share</em> the common memory pages. A page will be duplicated only when it changes in the child or in the parent. Since in theory all the pages may change while the child process is saving, Linux can’t tell in advance how much memory the child will take, so if the setting is set to zero fork will fail unless there is as much free RAM as required to really duplicate all the parent memory pages, with the result that if you have a Redis dataset of 3 GB and just 2 GB of free memory it will fail.<code>overcommit_memory</code></p><p>Setting to 1 tells Linux to relax and perform the fork in a more optimistic allocation fashion, and this is indeed what you want for Redis.<code>overcommit_memory</code></p><p>A good source to understand how Linux Virtual Memory works and other alternatives for and is this classic from Red Hat Magazine, <a href="https://people.redhat.com/nhorman/papers/rhel3_vm.pdf">“Understanding Virtual Memory”</a>. You can also refer to the <a href="http://man7.org/linux/man-pages/man5/proc.5.html">proc(5)</a> man page for explanations of the available values.<code>overcommit_memory``overcommit_ratio</code></p><h4 id="翻译后的大致意思">翻译后的大致意思</h4><p>官方的FAQ，给人一种这么个意思。 如果你不设置overcommit_memory=1，那么COW机制将无法使用，所以当空余内存小于当前redis占用内存时，redis-bgsave 因为无法申请到足够的内存，将导致分配内存失败。<br>而COW机制的意思就是：父子进程公用内存页，因此只会copy变化的数据。因此，在COW机制下，redis-bgsave只需要申请到支撑变化数据的内存即可。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis-002备份</title>
      <link href="posts/2c2d50a0/"/>
      <url>posts/2c2d50a0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>本文主要记录 redis 的两种数据磁盘固化方式。<br>涉及到相关参数，简单的命令操作等。</p><h2 id="rdb">rdb</h2><blockquote><p>通过save或者bgsave存储某一时刻redis数据。rdb持久化是默认方式。</p><p>特点：</p><ul><li>小幅度丢失数据(取决于save或者bgsave命令的执行周期)。<ul><li>save命令会阻塞其它客户端访问，基本不会用。</li><li>bgsave会fork一个子进程来处理，子进程不会阻塞，不过会占用额外的内存。✨fork子进程这个操作会阻塞，一般这个操作很快。</li></ul></li><li>恢复速度快，因为它就是redis内存数据的快照，只需要将rdb文件直接加到内存里。</li></ul></blockquote><pre><code class="language-bash"># 压缩rdb文件rdbcompression yes# rdb 文件名称dbfilename redis-6379.rdb# rdb文件保存目录dir /redis/data/</code></pre><ul><li>自动执行策略 (满足下列规则，就执行bgsave)</li></ul><pre><code class="language-bash"># 900s内至少达到一条写命令save 900 1# 300s内至少达至10条写命令save 300 10# 60s内至少达到10000条写命令save 60 10000</code></pre><ul><li>人工执行策略 (每10分钟计划任务调用一次bgsave)</li></ul><pre><code class="language-bash">*/10 * * * * root /export/redis/bin/redis-cli -h 127.0.0.1 bgsave &gt;&gt; /export/redis/bgsave.log 2&gt;&amp;1  </code></pre><blockquote><p>bgsave 因需要fork子进程，所以需要额外预留空闲的物理内存，在overcommit_memory=1开启的情况下，预留内存大小 &gt; 周期变化数据大小</p></blockquote><h2 id="aof">aof</h2><blockquote><p>记录的是redis每一次的写入操作记录</p><p>特点：</p><ul><li>恢复速度慢，因为aof记录的不是内存数据快照，而是执行过的命令，恢复的时候需要从新执行。</li><li>丢失数据小。默认是1秒同步一次执行命令到aof文件内。</li></ul></blockquote><pre><code class="language-bash"># 开启aof机制appendonly yes# aof文件名appendfilename &quot;appendonly.aof&quot;# 写入策略## always 表示每个写操作在写入aof_buffer后，都立即执行fsync同步到磁盘。## everysec 表示每个写操作在写入aof_buffer后，每秒执行一次fsync同步到磁盘。这是默认值## no 表示每个写操作在写入aof_buffer后，由操作系统自身策略执行一次fsync同步到磁盘。（一般默认操作系统是30秒）。appendfsync everysec# 重写（压缩）AOF文件的时候不立即执行fsync## no 意思就是压缩整合aof文件后立即调用fsync。## yes 意思就是重写aof文件后，将其放置缓冲区。缓解io压力，但可能会因为down机丢数据。no-appendfsync-on-rewrite yes# 保存目录dir /redis/data/</code></pre><ul><li>人工计划任务重写（压缩）AOF文件</li></ul><pre><code class="language-bash">*/10 * * * * root /export/redis/bin/redis-cli -h 127.0.0.1 bgrewriteaof &gt;&gt; /export/redis/bgrewriteaof.log 2&gt;&amp;1</code></pre><ul><li>aof 因服务器挂掉损坏可以修复</li></ul><pre><code class="language-bash">cp aof aof.bakredis-check-aof -fix file.aof</code></pre><h3 id="AOF重写">AOF重写</h3><p>AOF重写意思：将AOF文件中冗余的命令删除，例如多个set key1指令，只需要保留最后一个set key1指令即可。</p><p>AOF重写原理：AOF 重写和 RDB 创建快照一样，都巧妙地利用了写时复制机制</p><ul><li>Redis 执行 fork() ，现在同时拥有父进程和子进程。</li><li>子进程开始将 AOF 文件的内容写入到临时文件。</li><li>对于所有新执行的写入命令，父进程一边将它们累积到一个内存缓存中，一边将这些改动追加到现有 AOF 文件的末尾,这样样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。</li><li>当子进程完成重写工作时，它给父进程发送一个信号，父进程在接收到信号之后，将内存缓存中的所有数据追加到新 AOF 文件的末尾。</li><li>搞定！现在 Redis 原子地用新文件替换旧文件，之后所有命令都会直接追加到新 AOF 文件的末尾。</li></ul><p>✨AOF重写和bgsave不会同时进行。如果bgsave的过程中，发起aof重写，则这个操作是异步。redis会立即返回OK，并在bgsave执行完后执行。</p><p>💥 rdb的save操作和aof的文件重写，都会消耗磁盘性能。</p><h2 id="结论">结论</h2><p>官方建议是两者都开启，具体根据业务情况确定是否需要，但是不管是哪一种，都可以在人工计划任务之后，复刻一份备份文件到云端对象存储中。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 备份 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis-001安装</title>
      <link href="posts/65ab632a/"/>
      <url>posts/65ab632a/</url>
      
        <content type="html"><![CDATA[<h4 id="docker-安装">docker 安装</h4><p><a href="https://registry.hub.docker.com/_/redis">Docker Hub</a></p><pre><code class="language-bash">redisName=redisversionTag=6dataPath=/export/docker-data-redismkdir -p $&#123;dataPath&#125;# redis.conf 请参考官方默认配置文档，默认配置文档地址： https://redis.io/topics/configwget -P $&#123;dataPath&#125; https://raw.githubusercontent.com/redis/redis/$&#123;versionTag&#125;.0/redis.confdocker run --name $&#123;redisName&#125;_$&#123;versionTag&#125; \--mount &quot;type=bind,src=$&#123;dataPath&#125;,dst=/data&quot; \-p 6379:6379 \--restart always \-d redis:$&#123;versionTag&#125; redis-server /data/redis.conf</code></pre><p>ℹ️配置改动：<code>bind 0.0.0.0</code></p><h4 id="编译安装">编译安装</h4><p>💥redis6需要gcc版本9。通过下列命令进入特定版本的环境来编译安装</p><pre><code class="language-bash"># 安装包yum install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils -y# 进入特定shell环境scl enable devtoolset-9 bash</code></pre><pre><code class="language-bash">vim /usr/local/src/redis.sh</code></pre><pre><code class="language-bash">#!/bin/bash# by zyh# DownUrl: redis源码包# RedisMaxMem：redis内存限制# RedisPort: redis端口# RedisSentinelPort: redis哨兵端口# AUTHPWD: redis认证密码和哨兵认证密码# RedisBaseDir： redis安装路径# 安装完毕后，会输出# 1. redis信息# 2. systemd redis 配置# 3. systemd redis-sentinel 配置# redis.conf 中从配置已注释，若变更为从，开启注释# redis-sentinel.conf 中默认监控的是 localhost，需改成主redis的ipBaseDir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`DownUrl=http://download.redis.io/releases/redis-stable.tar.gzRedisMaxMem=1gRedisPort=6379RedisSentinelPort=26379AUTHPWD=123456RedisBaseDir=/export/redis_$&#123;RedisPort&#125;RedisBaseName=$&#123;RedisBaseDir##*/&#125;export TOP_PID=$$trap 'exit 1' TERMexit_script()&#123;    kill -s TERM $TOP_PID&#125;yum install gcc-c++ make -y[[ -d $&#123;RedisBaseDir&#125; ]] &amp;&amp; echo &quot;$&#123;RedisBaseDir&#125;已存在&quot; &amp;&amp; exit_script || mkdir -p $&#123;RedisBaseDir&#125;ss -tnalp | grep redis | awk '&#123;print $4&#125;' | awk -F':' '&#123;print $2&#125;' | while read line;do    [[ $line -eq $&#123;RedisPort&#125; ]] &amp;&amp; echo &quot;$&#123;RedisPort&#125;已被占用&quot; &amp;&amp; exit_scriptdonecd $&#123;BaseDir&#125; &amp;&amp; mkdir rediswget $&#123;DownUrl&#125; -O redis.tar.gztar xf redis.tar.gz --strip-components 1 -C rediscd redismake PREFIX=$&#123;RedisBaseDir&#125; install || exit_scriptmkdir $&#123;RedisBaseDir&#125;/&#123;etc,data,logs&#125;cat&gt;$&#123;RedisBaseDir&#125;/etc/redis.conf &lt;&lt;EOF# Basebind 0.0.0.0protected-mode yesport $&#123;RedisPort&#125;daemonize yespidfile &quot;$&#123;RedisBaseDir&#125;/redis.pid&quot;loglevel warninglogfile &quot;$&#123;RedisBaseDir&#125;/logs/redis.log&quot;dir &quot;$&#123;RedisBaseDir&#125;/data/&quot;# Authrequirepass $&#123;AUTHPWD&#125;# Replica master## 必须有一个从服务，才允许写入min-replicas-to-write 1## 从服务有10秒时间来发送异步确认，若超过则认为从不可用，进而因为 min-replicas-to-write 1 导致不可写入min-replicas-max-lag 10# Replica slave## master 认证#masterauth &lt;master-password&gt;## 指定 master 地址#replicaof &lt;masterip&gt; &lt;masterport&gt;replica-read-only yesreplica-priority 100# Memorymaxmemory $&#123;RedisMaxMem&#125;maxmemory-policy allkeys-lru# RDBsave 900 1save 300 10save 60 10000dbfilename redis_$&#123;RedisPort&#125;.rdb# AOFappendonly yesappendfilename appendonly_$&#123;RedisPort&#125;.aofappendfsync everysecno-appendfsync-on-rewrite noaof-use-rdb-preamble yes# rewrite commandrename-command FLUSHDB GOD_FLUSHDBrename-command FLUSHALL GOD_FLUSHALL#rename-command CONFIG GOD_CONFIGrename-command KEYS GOD_KEYSEOFcat&gt;$&#123;RedisBaseDir&#125;/etc/redis-sentinel.conf &lt;&lt;EOFprotected-mode noport $&#123;RedisSentinelPort&#125;daemonize yespidfile &quot;$&#123;RedisBaseDir&#125;/redis-sentinel.pid&quot;logfile &quot;$&#123;RedisBaseDir&#125;/logs/redis-sentinel.log&quot;dir &quot;$&#123;RedisBaseDir&#125;/data&quot;# 配置监听的主服务器## mymaster 自定义一个主服务器的昵称## 代表监控的主服务器## 6379代表端口## 2代表只有两个或两个以上的哨兵认为主服务器不可用的时候，才会进行failover操作。sentinel monitor mymaster localhost $&#123;RedisPort&#125; 2# 访问master所需的认证账户密码## mymaster 自定义的主服务器昵称## 123456 master 配置的密码sentinel auth-pass mymaster $&#123;AUTHPWD&#125;# 客户端访问哨兵所需的auth认证requirepass $&#123;AUTHPWD&#125;sentinel down-after-milliseconds mymaster 60000sentinel failover-timeout mymaster 180000sentinel parallel-syncs mymaster 1EOFcat&gt;$&#123;RedisBaseDir&#125;/redis.sh &lt;&lt; EOF#!/bin/bash# Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.BASEDIR=$&#123;RedisBaseDir&#125;REDISPORT=$&#123;RedisPort&#125;EXEC=\$BASEDIR/bin/redis-serverCLIEXEC=\$BASEDIR/bin/redis-cliPIDFILE=\$BASEDIR/redis.pidCONF=&quot;\$BASEDIR/etc/redis.conf&quot;AUTH=&quot;$AUTHPWD&quot;case &quot;\$1&quot; in    start)        [[ -f \$PIDFILE ]] &amp;&amp; kill -0 \`cat \$PIDFILE\` 2&gt;&gt;\$BASEDIR/crash.log &amp;&amp; echo &quot;\$PIDFILE exists, process is already running or crashed&quot; || &#123;                echo &quot;Starting Redis server...&quot;                \$EXEC \$CONF        &#125;        ;;    stop)        if [ ! -f \$PIDFILE ]        then                echo &quot;\$PIDFILE does not exist, process is not running&quot;        else                PID=\$(cat \$PIDFILE)                echo &quot;Stopping ...&quot;                [[ -z \$AUTH ]] &amp;&amp; \$CLIEXEC -p \$REDISPORT shutdown || \$CLIEXEC -p \$REDISPORT -a \$AUTH shutdown                while [ -x /proc/\$&#123;PID&#125; ]                do                    echo &quot;Waiting for Redis to shutdown ...&quot;                    sleep 1                done                echo &quot;Redis stopped&quot;        fi        ;;    *)        echo &quot;Please use start or stop as first argument&quot;        ;;esacEOFcat&gt;$&#123;RedisBaseDir&#125;/redis-sentinel.sh &lt;&lt; EOF#!/bin/bash# Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.BASEDIR=$&#123;RedisBaseDir&#125;REDISPORT=$&#123;RedisSentinelPort&#125;EXEC=\$BASEDIR/bin/redis-sentinelCLIEXEC=\$BASEDIR/bin/redis-cliPIDFILE=\$BASEDIR/redis-sentinel.pidCONF=&quot;\$BASEDIR/etc/redis-sentinel.conf&quot;AUTH=&quot;$AUTHPWD&quot;case &quot;\$1&quot; in    start)        [[ -f \$PIDFILE ]] &amp;&amp; kill -0 \`cat \$PIDFILE\` 2&gt;&gt;\$BASEDIR/crash-sentinel.log &amp;&amp; echo &quot;\$PIDFILE exists, process is already running or crashed&quot; || &#123;                echo &quot;Starting Redis Sentinel server...&quot;                \$EXEC \$CONF        &#125;        ;;    stop)        if [ ! -f \$PIDFILE ]        then                echo &quot;\$PIDFILE does not exist, process is not running&quot;        else                PID=\$(cat \$PIDFILE)                echo &quot;Stopping ...&quot;                [[ -z \$AUTH ]] &amp;&amp; \$CLIEXEC -p \$REDISPORT shutdown || \$CLIEXEC -p \$REDISPORT -a \$AUTH shutdown                while [ -x /proc/\$&#123;PID&#125; ]                do                    echo &quot;Waiting for Redis Sentinel to shutdown ...&quot;                    sleep 1                done                echo &quot;Redis Sentinel stopped&quot;        fi        ;;    *)        echo &quot;Please use start or stop as first argument&quot;        ;;esacEOFchmod u+x $&#123;RedisBaseDir&#125;/*.sh#修改内核参数grep -q net.core.somaxconn /etc/sysctl.conf || echo &quot;net.core.somaxconn = 511&quot; &gt;&gt; /etc/sysctl.confgrep -q vm.overcommit_memory /etc/sysctl.conf || &#123;    echo &quot;vm.overcommit_memory = 1&quot; &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -p&#125;grep -q '/sys/kernel/mm/transparent_hugepage/enabled' /etc/rc.local || &#123;    echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled    echo 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' &gt;&gt; /etc/rc.local&#125;echo '-------------------------------------------------- redis base info-----------------------------------------------------------'echo 'redis相关信息如下：'echo &quot;根路径：$&#123;RedisBaseDir&#125;启动脚本：$&#123;RedisBaseDir&#125;/redis.sh启动脚本：$&#123;RedisBaseDir&#125;/redis-sentinel.sh&quot;echo '-------------------------------------------------- redis systemd info-----------------------------------------------------------'echo &quot;cat &gt;/etc/systemd/system/redis.service &lt;&lt; EOF[Unit]# 描述服务Description=Redis# 描述服务类别After=network.target# 服务运行参数的设置[Service]# 后台运行的形式Type=forking# 运行命令，路径必须是绝对路径ExecStart=$&#123;RedisBaseDir&#125;/redis.sh start# 停止命令，路径必须是绝对路径ExecStop=$&#123;RedisBaseDir&#125;/redis.sh stop# 表示给服务分配独立的临时空间PrivateTmp=true# 运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3[Install]WantedBy=multi-user.targetEOF&quot;echo '-------------------------------------------------- redis-sentinel systemd info-----------------------------------------------------------'echo &quot;cat&gt;/etc/systemd/system/redis-sentinel.service &lt;&lt; EOF[Unit]# 描述服务Description=RedisSentinel# 描述服务类别After=network.target# 服务运行参数的设置[Service]# 后台运行的形式Type=forking# 运行命令，路径必须是绝对路径ExecStart=$&#123;RedisBaseDir&#125;/redis-sentinel.sh start# 停止命令，路径必须是绝对路径ExecStop=$&#123;RedisBaseDir&#125;/redis-sentinel.sh stop# 表示给服务分配独立的临时空间PrivateTmp=true# 运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3[Install]WantedBy=multi-user.targetEOF&quot;</code></pre><hr><p>以下是废弃的。</p><pre><code class="language-shell">#!/bin/bash# by zyh# 2018-06-21# DownUrl: redis源码包# RedisBaseDir： redis安装路径# RedisPort: redis端口# RedisMaxMem：redis内存限制# ZabbixBase: zabbix 根路径# 安装完毕后，会输出# 1. redis信息# 2. zabbix需要额外手动添加的命令， 并在zabbix_server_web里，给机器关联上 &lt;Template Redis Auto Discovert Active mode&gt; 模板# 3. monit需要额外手动添加的配置BaseDir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`DownUrl=http://download.redis.io/releases/redis-stable.tar.gzRedisMaxMem=1gRedisPort=6379RedisBaseDir=/export/redis_$&#123;RedisPort&#125;ZabbixBase=/etc/zabbixif [[ $&#123;ZabbixBase&#125; == '/usr/local/zabbix' ]];then    ZabbixShell=$&#123;ZabbixBase&#125;/shell    ZabbixEtc=$&#123;ZabbixBase&#125;/etc/zabbix_agentd.conf.d/redis.confelse    ZabbixShell=/etc/zabbix/shell    ZabbixEtc=/etc/zabbix/zabbix_agentd.d/redis.conffiRedisBaseName=$&#123;RedisBaseDir##*/&#125;export TOP_PID=$$trap 'exit 1' TERMexit_script()&#123;    kill -s TERM $TOP_PID&#125;yum install gcc-c++ -y[[ -d $&#123;RedisBaseDir&#125; ]] &amp;&amp; echo &quot;$&#123;RedisBaseDir&#125;已存在&quot; &amp;&amp; exit_scriptss -tnalp | grep redis | awk '&#123;print $4&#125;' | awk -F':' '&#123;print $2&#125;' | while read line;do    [[ $line -eq $&#123;RedisPort&#125; ]] &amp;&amp; echo &quot;$&#123;RedisPort&#125;已被占用&quot; &amp;&amp; exit_scriptdonecd $&#123;BaseDir&#125; &amp;&amp; mkdir rediswget $&#123;DownUrl&#125; -O redis.tar.gztar xf redis.tar.gz --strip-components 1 -C rediscd redismake PREFIX=$&#123;RedisBaseDir&#125; installmkdir $&#123;RedisBaseDir&#125;/&#123;etc,data,logs&#125;cat&gt;$&#123;RedisBaseDir&#125;/etc/redis.conf &lt;&lt;EOFbind 0.0.0.0protected-mode yesport $&#123;RedisPort&#125;tcp-backlog 511timeout 0tcp-keepalive 300daemonize yessupervised nopidfile $&#123;RedisBaseDir&#125;/redis.pidloglevel warninglogfile &quot;$&#123;RedisBaseDir&#125;/logs/redis.log&quot;databases 16always-show-logo yessave 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename redis_$&#123;RedisPort&#125;.rdbdir $&#123;RedisBaseDir&#125;/data/slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay noslave-priority 100rename-command FLUSHDB GOD_FLUSHDBrename-command FLUSHALL GOD_FLUSHALLrename-command CONFIG GOD_CONFIGrename-command KEYS GOD_KEYSmaxmemory $&#123;RedisMaxMem&#125;maxmemory-policy allkeys-lrulazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush noappendonly noappendfilename &quot;appendonly.aof&quot;appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yesaof-use-rdb-preamble nolua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events &quot;&quot;hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-size -2list-compress-depth 0set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yesEOFcat&gt;$&#123;RedisBaseDir&#125;/redis.sh &lt;&lt; EOF#!/bin/bash# Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.BASEDIR=$&#123;RedisBaseDir&#125;REDISPORT=$&#123;RedisPort&#125;EXEC=\$BASEDIR/bin/redis-serverCLIEXEC=\$BASEDIR/bin/redis-cliPIDFILE=\$BASEDIR/redis.pidCONF=&quot;\$BASEDIR/etc/redis.conf&quot;case &quot;\$1&quot; in    start)        [[ -f \$PIDFILE ]] &amp;&amp; kill -0 \`cat \$PIDFILE\` 2&gt;&gt;\$BASEDIR/crash.log &amp;&amp; echo &quot;\$PIDFILE exists, process is already running or crashed&quot; || &#123;                echo &quot;Starting Redis server...&quot;                \$EXEC \$CONF        &#125;        ;;    stop)        if [ ! -f \$PIDFILE ]        then                echo &quot;\$PIDFILE does not exist, process is not running&quot;        else                PID=\$(cat \$PIDFILE)                echo &quot;Stopping ...&quot;                \$CLIEXEC -p \$REDISPORT shutdown                while [ -x /proc/\$&#123;PID&#125; ]                do                    echo &quot;Waiting for Redis to shutdown ...&quot;                    sleep 1                done                echo &quot;Redis stopped&quot;        fi        ;;    *)        echo &quot;Please use start or stop as first argument&quot;        ;;esacEOFchmod u+x $&#123;RedisBaseDir&#125;/redis.sh#修改内核参数grep -q net.core.somaxconn /etc/sysctl.conf || echo &quot;net.core.somaxconn = 511&quot; &gt;&gt; /etc/sysctl.confgrep -q vm.overcommit_memory /etc/sysctl.conf || &#123;    echo &quot;vm.overcommit_memory = 1&quot; &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -p&#125;grep -q '/sys/kernel/mm/transparent_hugepage/enabled' /etc/rc.local || &#123;    echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled    echo 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' &gt;&gt; /etc/rc.local&#125;#修改zabbix监控cat&gt;$&#123;ZabbixEtc&#125;&lt;&lt;EOF##redis monitorUserParameter=redis_info[*],$&#123;ZabbixShell&#125;/redis_info.sh \$1 \$2 \$3 \$4UserParameter=ip_port_discovery[*],$&#123;ZabbixShell&#125;/ip_port_discovery.sh \$1EOFmkdir $&#123;ZabbixShell&#125;cat&gt;$&#123;ZabbixShell&#125;/redis_info.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashREDISPATH=&quot;/export/redis/bin/redis-cli&quot;HOST=$1PORT=$2REDIS_INFO=&quot;$REDISPATH -h $HOST -p $PORT info&quot;[[ $# -ne 3 ]] &amp;&amp; [[ $# -ne 4 ]] &amp;&amp; &#123; exit&#125;if [[ $# -eq 3 ]];thencase $3 inexist) result=`$REDISPATH -h $HOST -p $PORT ping 2&gt;/dev/null |grep -c PONG` echo $result;;cluster)        result=`$REDIS_INFO|/bin/grep cluster|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;uptime_in_seconds)        result=`$REDIS_INFO|/bin/grep uptime_in_seconds|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;connected_clients)        result=`$REDIS_INFO|/bin/grep connected_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_longest_output_list)        result=`$REDIS_INFO|/bin/grep client_longest_output_list|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_biggest_input_buf)        result=`$REDIS_INFO|/bin/grep client_biggest_input_buf|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;blocked_clients)        result=`$REDIS_INFO|/bin/grep blocked_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#内存maxmemory)        result=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory)        result=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_rss)        result=`$REDIS_INFO|/bin/grep -w used_memory_rss|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk -F'k' '&#123;print $1&#125;'`        echo $result;;used_memory_pct) A=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` B=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` result=`echo $A $B | awk '&#123;printf&quot;%0.2f&quot;,$1/$2&#125;'`        echo $result;;used_memory_peak)        result=`$REDIS_INFO|/bin/grep used_memory_peak|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_lua)        result=`$REDIS_INFO|/bin/grep used_memory_lua|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;mem_fragmentation_ratio)        result=`$REDIS_INFO|/bin/grep mem_fragmentation_ratio|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#rdbrdb_changes_since_last_save)        result=`$REDIS_INFO|/bin/grep rdb_changes_since_last_save|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_bgsave_in_progress)        result=`$REDIS_INFO|/bin/grep rdb_bgsave_in_progress|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_save_time)        result=`$REDIS_INFO|/bin/grep rdb_last_save_time|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_bgsave_status)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_last_bgsave_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;rdb_current_bgsave_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_current_bgsave_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#rdbinfoaof_enabled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_enabled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_scheduled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_scheduled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_last_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_current_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_last_bgrewrite_status)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_bgrewrite_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;#aofinfoaof_current_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_base_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_base_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_rewrite)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_rewrite&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_bio_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_bio_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_delayed_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_delayed_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#statstotal_connections_received)        result=`$REDIS_INFO|/bin/grep -w &quot;total_connections_received&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;total_commands_processed)        result=`$REDIS_INFO|/bin/grep -w &quot;total_commands_processed&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;instantaneous_ops_per_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;instantaneous_ops_per_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;rejected_connections)        result=`$REDIS_INFO|/bin/grep -w &quot;rejected_connections&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;expired_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;expired_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;evicted_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;evicted_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_hits)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_hits&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_misses)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_misses&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_patterns)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_patterns&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;latest_fork_usec)        result=`$REDIS_INFO|/bin/grep -w &quot;latest_fork_usec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;connected_slaves)        result=`$REDIS_INFO|/bin/grep -w &quot;connected_slaves&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;master_link_status)        result=`$REDIS_INFO|/bin/grep -w &quot;master_link_status&quot;|awk -F':' '&#123;print $2&#125;'|/bin/grep -c up`        echo $result;;master_last_io_seconds_ago)        result=`$REDIS_INFO|/bin/grep -w &quot;master_last_io_seconds_ago&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;master_sync_in_progress)        result=`$REDIS_INFO|/bin/grep -w &quot;master_sync_in_progress&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;slave_priority)        result=`$REDIS_INFO|/bin/grep -w &quot;slave_priority&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;#cpuused_cpu_sys)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_sys_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;*) echo &quot;argu error&quot;;;esac#db0:key   elif [[ $# -eq 4 ]];thencase $4 inkeys)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;keys&quot; | awk -F'=|,' '&#123;print $2&#125;'`        echo $result;;expires)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;expires&quot; | awk -F'=|,' '&#123;print $4&#125;'`        echo $result;;avg_ttl)        result=`$REDIS_INFO|/bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;avg_ttl&quot; | awk -F'=|,' '&#123;print $6&#125;'`        echo $result;;*)     echo &quot;argu error&quot; ;;esacfiEOFcat&gt;$&#123;ZabbixShell&#125;/ip_port_discovery.sh&lt;&lt;&quot;EOF&quot;#!/bin/bash#$1是要发现的进程部分词汇portarray=(`sudo ss -tnlp|egrep -i &quot;$1&quot;|awk &#123;'print $4'&#125;|awk -F':' '&#123;if ($NF~/^[0-9]*$/) print $NF&#125;'|sort|uniq`)iparray=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`length=$&#123;#portarray[@]&#125;printf &quot;&#123;\n&quot;printf '\t'&quot;\&quot;data\&quot;:[&quot;for ((i=0;i&lt;$length;i++))  do     printf '\n\t\t&#123;'     printf &quot;\&quot;&#123;#IP&#125;\&quot;:\&quot;$&#123;iparray&#125;\&quot;,\&quot;&#123;#TCP_PORT&#125;\&quot;:\&quot;$&#123;portarray[$i]&#125;\&quot;&#125;&quot;     if [ $i -lt $[$length-1] ];then                printf ','     fi  doneprintf &quot;\n\t]\n&quot;printf &quot;&#125;\n&quot;EOFchmod a+x $&#123;ZabbixShell&#125;/redis_info.shchmod a+x $&#123;ZabbixShell&#125;/ip_port_discovery.shsed -i '2s#/export/redis#'&quot;$&#123;RedisBaseDir&#125;&quot;'#' $&#123;ZabbixShell&#125;/redis_info.shecho '--------------------------------------------------我是 redis 信息-----------------------------------------------------------'echo 'redis相关信息如下：'echo &quot;根路径：$&#123;RedisBaseDir&#125;启动脚本：$&#123;RedisBaseDir&#125;/redis.sh&quot;echo '--------------------------------------------------我是 zabbix 监控信息----------------------------------------------------------'echo '请先安装 zabbix.'echo 'zabbix监控因使用了ss命令，故而需要开启sudo相关信息'echo &quot;#zabbix用户可以以sudo执行sszabbix ALL = NOPASSWD: /usr/sbin/ss#zabbix用户使用sudo无需ttyDefaults:zabbix    !requiretty&quot;echo '---------------------------------------------------我是 monit 监控信息----------------------------------------------------------'echo '请先安装 monit.'echo 'monit配置文件如下:'echo &quot;check process $&#123;RedisBaseName&#125; with pidfile $&#123;RedisBaseDir&#125;/redis.pid  start program = \&quot;$&#123;RedisBaseDir&#125;/redis.sh start\&quot;  stop program = \&quot;$&#123;RedisBaseDir&#125;/redis.sh stop\&quot;if changed pid then alert&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞s3跨账户访问</title>
      <link href="posts/7cac5d21/"/>
      <url>posts/7cac5d21/</url>
      
        <content type="html"><![CDATA[<ul><li>规则如下：</li></ul><pre><code class="language-:">存储桶策略,授权其它账户的某个 iam 资源访问此存储桶arn:aws:iam::&lt;aws_account_id&gt;:&lt;type&gt;/&lt;name&gt;</code></pre><ul><li>示例配置如下：</li></ul><pre><code class="language-json:">&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Id&quot;: &quot;Policy1564021125924&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;object1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Principal&quot;: &#123;                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;role-name&gt;&quot;            &#125;,            &quot;Action&quot;: [                &quot;s3:Get*&quot;,                &quot;s3:List*&quot;            ],            &quot;Resource&quot;: &quot;arn:aws:s3:::&lt;open-bucket&gt;/&lt;open-path&gt;/*&quot;        &#125;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cloudformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞rds-mysql日志</title>
      <link href="posts/604c5952/"/>
      <url>posts/604c5952/</url>
      
        <content type="html"><![CDATA[<h2 id="常规-错误-慢-日志">常规/错误/慢/日志</h2><pre><code class="language-bash"># 修改参数组slow_query_log：要创建慢速查询日志，请设置为 1。默认值为 0。general_log(会产生大量日志)：要创建常规日志，请设置为 1。默认值为 0。</code></pre><p>long_query_time：要防止在慢速查询日志中记录快速运行的查询，请指定需要记录的最短查询执行时间值，以秒为单位。默认值为 10 秒；最小值为 0。<br>如果 log_output = FILE，则可以指定精确到微秒的浮点值, 即可记录 0.1s<br>如果 log_output = TABLE，则必须指定精确到秒的整数值。<br>系统只记录执行时间超过 long_query_time 值的查询。</p><p>log_queries_not_using_indexes(会产生大量日志)：要将所有不使用索引的查询记录到慢速查询日志，请设置为 1。默认值为 0。将记录不使用索引的查询，即使它们的执行时间小于 long_query_time 参数的值。</p><p>log_output option：您可为 log_output 参数指定下列选项之一。</p><p>TABLE（默认, 不建议, 影响数据库本身性能）– 将一般查询写入 mysql.general_log 表，将慢速查询写入 mysql.slow_log 表。</p><p>FILE(推荐,也是必须,否则无法推送到cloudwatch)– 将一般查询日志和慢速查询日志写入文件系统。日志文件每小时轮换一次。默认记录24小时.</p><p>NONE– 禁用日志记录</p><h2 id="审计日志">审计日志</h2><pre><code># 修改选项组, 添加额外选项MARIADB_AUDIT_PLUGIN </code></pre><h2 id="二进制日志">二进制日志</h2><blockquote><p>二进制日志一般我们用来恢复数据, 默认 rds 会尽量销毁二进制日志, 来保留磁盘可用空间.</p><p>二进制日志默认格式是   mixed</p></blockquote><p>配置二进制日志的方法:</p><pre><code class="language-bash"># 将二进制日志保留24小时call mysql.rds_set_configuration('binlog retention hours', 24);</code></pre><p>访问二进制日志的方法如下:</p><pre><code class="language-bash">mysqlbinlog \    --read-from-remote-server \    --host=&lt;mysql_rds_instance_address&gt; \    --port=&lt;mysql_port&gt; \    --user ReplUser \    --password \    --raw \    --result-file=/tmp/ \    binlog.00098</code></pre><blockquote><ul><li><p>指定  <code>--read-from-remote-server</code>  选项。</p></li><li><p><code>--host</code>：指定该实例所在的终端节点中的 DNS 名称。</p></li><li><p><code>--port</code>：指定该实例使用的端口。</p></li><li><p><code>--user</code>：指定已授予了复制从属实例权限的 MySQL 用户。</p></li><li><p><code>--password</code>：指定用户的密码，或忽略密码值以让实用程序提示您输入密码。</p></li><li><p>要按二进制格式下载文件，请指定  <code>--raw</code>  选项。</p></li><li><p><code>--result-file</code>：指定用于接收原始输出的本地文件。</p></li><li><p>指定一个或多个二进制日志文件的名称。要获取可用日志的列表，请使用 SQL 命令 SHOW BINARY LOGS。</p></li><li><p>要流式传输二进制日志文件，请指定  <code>--stop-never</code>  选项。</p></li></ul></blockquote><hr><h2 id="参考文档">参考文档:</h2><ol><li><a href="https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html#Appendix.MySQL.CommonDBATasks.Logs">https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html#Appendix.MySQL.CommonDBATasks.Logs</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞centos7重置root密码</title>
      <link href="posts/13364d65/"/>
      <url>posts/13364d65/</url>
      
        <content type="html"><![CDATA[<ol><li>开机按ECS，进入启动项</li></ol><p><img src="/posts/13364d65/image-20210706143953672.png" alt="image-20210706143953672"></p><ol start="2"><li><p>按e，编辑第一条</p></li><li><p>将 root 挂载，从 ro 变更为 rw，并在最后添加 init=/bin/sh</p></li></ol><p><img src="/posts/13364d65/image-20210706144107672.png" alt="image-20210706144107672"></p><ol start="4"><li>按 ctrl+x，进入系统，并执行 <code>passwd</code> 命令进行修改</li></ol>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞系统内核优化</title>
      <link href="posts/f41d0763/"/>
      <url>posts/f41d0763/</url>
      
        <content type="html"><![CDATA[<h2 id="内核优化">内核优化</h2><pre><code class="language-bash"># 内核参数# 系统级别上限， 即整个系统所有进程单位时间可打开的文件描述符数量fs.file-max = 6553500# 三次握手请求频次net.ipv4.tcp_syn_retries = 5# 放弃回应一个TCP请求之前，需要尝试多少次net.ipv4.tcp_retries1 = 3# 三次握手应答频次net.ipv4.tcp_synack_retries = 2# 三次握手完毕， 没有数据沟通的情况下， 空连接存活时间net.ipv4.tcp_keepalive_time = 60# 探测消息发送次数net.ipv4.tcp_keepalive_probes = 3# 探测消息发送间隔时间net.ipv4.tcp_keepalive_intvl = 15net.ipv4.tcp_retries2 = 5net.ipv4.tcp_fin_timeout = 5# 系统处理不属于任何进程的TCP链接net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_max_orphans = 327680# 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。net.core.netdev_max_backlog = 10240# 对于还未获得对方确认的连接请求，可保存在队列中的最大数目net.ipv4.tcp_max_syn_backlog = 10240# 定义了系统中每一个端口最大的监听队列的长度 net.core.somaxconn=10240#最大timewait数net.ipv4.tcp_max_tw_buckets = 20000net.ipv4.ip_local_port_range=1024 65500# 开启时间戳net.ipv4.tcp_timestamps=1# 针对客户端有效，必须在开启时间戳的前提下net.ipv4.tcp_tw_reuse = 1# 开启 iptables 后， 链路追踪上限和超时时间, 若没有使用 iptables，则无效net.netfilter.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_tcp_timeout_established = 150## tcp栈内存使用， 单位是内存页， 一页=4KB#net.ipv4.tcp_mem = 524288 786432 1310720## socket读写缓冲区大小，单位是字节#net.ipv4.tcp_rmem = 4096 4096 16777216#net.ipv4.tcp_wmem = 4096 4096 16777216##最低内存和缓冲区回收倾向（此参数有一定风险）#vm.min_free_kbytes=409600#vm.vfs_cache_pressure=200</code></pre><pre><code class="language-bash">#修改/etc/security/limits.conf# 单会话级别，可打开的所有文件描述符上限* soft nofile 655350* hard nofile 655350# 单会话级别， 可打开的所有进程上限* soft nproc 10240* hard nproc 10240</code></pre><h2 id="开启-iptables-后">开启 iptables 后</h2><h3 id="查看当前链路表数量命令">查看当前链路表数量命令</h3><blockquote><p>$ sysctl net.netfilter.nf_conntrack_count</p><p>net.netfilter.nf_conntrack_count = 601032</p></blockquote><h3 id="查看连接数最高的10个IP：可以查封某个ip-或者判断是谁导致的">查看连接数最高的10个IP：可以查封某个ip, 或者判断是谁导致的</h3><p>$ awk -F’=’ ‘{c[$2]++}END{for ( i in c) print i,c[i]}’ /proc/net/nf_conntrack | head -10</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx-常用变量</title>
      <link href="posts/2722921e/"/>
      <url>posts/2722921e/</url>
      
        <content type="html"><![CDATA[<h2 id="常用的全局变量">常用的全局变量</h2><pre><code class="language-bash">arg_PARAMETER    #这个变量包含GET请求中，如果有变量PARAMETER时的值。args                    #这个变量等于请求行中(GET请求)的参数，如：foo=123&amp;bar=blahblah;binary_remote_addr #二进制的客户地址。body_bytes_sent    #响应时送出的body字节数数量。即使连接中断，这个数据也是精确的。content_length    #请求头中的Content-length字段。content_type      #请求头中的Content-Type字段。cookie_COOKIE    #cookie COOKIE变量的值document_root    #当前请求在root指令中指定的值。document_uri      #与uri相同。host                #请求主机头字段，否则为服务器名称。hostname          #Set to themachine’s hostname as returned by gethostnamehttp_HEADERis_args              #如果有args参数，这个变量等于”?”，否则等于”&quot;，空值。http_user_agent    #客户端agent信息http_cookie          #客户端cookie信息limit_rate            #这个变量可以限制连接速率。query_string          #与args相同。request_body_file  #客户端请求主体信息的临时文件名。request_method    #客户端请求的动作，通常为GET或POST。remote_addr          #客户端的IP地址。remote_port          #客户端的端口。remote_user          #已经经过Auth Basic Module验证的用户名。request_completion #如果请求结束，设置为OK. 当请求未结束或如果该请求不是请求链串的最后一个时，为空(Empty)。request_method    #GET或POSTrequest_filename  #当前请求的文件路径，由root或alias指令与URI请求生成。request_uri          #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。不能修改。scheme                #HTTP方法（如http，https）。server_protocol      #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。server_addr          #服务器地址，在完成一次系统调用后可以确定这个值。server_name        #服务器名称。server_port          #请求到达服务器的端口号。</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell-[[ ]]和(( ))</title>
      <link href="posts/f2de1fb/"/>
      <url>posts/f2de1fb/</url>
      
        <content type="html"><![CDATA[<h2 id>(( ))</h2><p>整数表达式运算用 (( )) ，(( a+b*c-d )) ，a、b、c、d为变量，可不写$，运算符 &gt;、&gt;=、&lt;、&lt;=、==、!=</p><p>举例：</p><pre><code class="language-bash">➜   a=1;b=2;(( a=a+b ));echo $a2</code></pre><p>💥使用 (( )) 时，不需要空格分隔各值和运算符，当然空格分割也不会错</p><h2 id="-2">[[ ]]</h2><p>字符表达式的比较使用 [[ ]] ，其运算符 =、!=、-n、-z</p><p>文件表达式的测试使用 [[ ]] ，其运算符 -r、-l、-w、-x、-f、-d、-s、-nt、-ot 等</p><p>逻辑表达式的测试使用 [[ ]] ，其运算符 !、&amp;&amp;、||</p><p>数字比较、字符比较、逻辑测试可以组合，如$ [[ “a” != “b” &amp;&amp; 4 -gt 3 ]]</p><p>组合使用的时候，数字部分应该用 -eq、-ne、-le、-lt、-gt、-ge</p><p>支持bash中的通配符扩展，如：[[ hest = h??t ]] 、[[ hest = h*t ]]</p><p>💥使用 [[ ]] 时需要用空格分隔各值和运算符。</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell☞if判定</title>
      <link href="posts/241ce96c/"/>
      <url>posts/241ce96c/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">[ -e FILE ] 如果 FILE 存在则为真。[ -d DIR ]  如果 FILE 存在且是一个目录则为真。[ -b FILE ] 如果 FILE 存在且是一个块特殊文件则为真。[ -c FILE ] 如果 FILE 存在且是一个字符特殊文件则为真。[ -f FILE ] 如果 FILE 存在且是一个普通文件则为真。[ -L FILE ] 如果 FILE 存在且是一个符号连接则为真。[ -p FILE ] 如果 FILE 存在且是一个命名管道(F如果O)则为真。[ -S FILE ] 如果 FILE 存在且是一个套接字则为真。[ -t FD ]   如果文件描述符 FD 打开且指向一个终端则为真。[ -g FILE ] 如果 FILE 存在且已经设置了SGID则为真。[ -k FILE ] 如果 FILE 存在且已经设置了粘制位则为真。[ -r FILE ] 如果 FILE 存在且是可读的则为真。[ -w FILE ] 如果 FILE 存在且是可写的则为真。[ -x FILE ] 如果 FILE 存在且是可执行的则为真。[ -s FILE ] 如果 FILE 存在且大小不为0则为真。[ -u FILE ] 如果 FILE 存在且设置了 SUID (set user ID)则为真。[ -O FILE ] 如果 FILE 存在且属有效用户ID则为真。[ -G FILE ] 如果 FILE 存在且属有效用户组则为真。[ -N FILE ] 如果 FILE 存在，且距离上次访问后有修改则为真。[ FILE1 -nt FILE2 ] 如果 FILE1 比 FILE2 要新, 或者 FILE1 存在且 FILE2 不存在则为真。[ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 要老, 或者 FILE1 不存在且 FILE2 存在则为真。[ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则为真。</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞df与du数据不一致的原因</title>
      <link href="posts/a3dfc0e8/"/>
      <url>posts/a3dfc0e8/</url>
      
        <content type="html"><![CDATA[<h2 id="linux-☞-df与du数据不一致的原因">linux ☞ df与du数据不一致的原因</h2><ul><li><h3 id="问题表现：">问题表现：</h3></li></ul><p>df 数据比 du 数据小，而且是发生在删除文件之后。</p><hr><p>这个问题牵扯到 linux 系统是将如何确认和回收数据的.</p><ul><li><h3 id="空间是如何被判定占用的">空间是如何被判定占用的</h3></li></ul><p>linux文件系统判定空间是否被占用，是查看空间的 imap 是否为1，而 imap 是否为1，则取决于占用空间的文件的 inode 节点的 Links 是否为0。而每当程序调用文件且没有关闭，那么此文件的 inode 的 Links 就会加 1。</p><p>如果 Links 不为0，则 imap 就不会是 0 ，则这份空间就无法被再次调用.<br>另外，每一个文件，默认 inode  Links 是1</p><ul><li><h3 id="系统是如何删除一个文件的">系统是如何删除一个文件的</h3></li></ul><p>文件系统首先在父目录文件里找到所要删除的文件名，将此文件信息从父目录里清除掉，如若清除后，Links 为0 ，则删除 inode 节点，将 imap 就置为0，空间可以再次被利用。但是，如果父目录文件信息清除后，有程序在调用文件，则 Links 不为0，那么 inode 无法被删除， imap 也无法置为0，结果就是文件看着没了，但是空间还是没有释放。</p><hr><p>df 和 du 的最大区别就是：</p><p>df 是根据 inode 的 Links 来确认空间是否被占用，并进而统计</p><p>du 是根据目录的文件信息来确认空间是否被占用，并进而统计</p><ul><li><h3 id="如何释放被占用的空间">如何释放被占用的空间</h3></li></ul><p>从上面可知，我们只需要将被删除文件的 inode Links 降成 0 即可，也就是关闭此文件的调用程序.</p><p>如何查找调用程序？</p><pre><code class="language-bash">lsof -n | grep rm_file_name # 获取到程序 pid，并将其杀死即可</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql-主从同步</title>
      <link href="posts/a1d2702d/"/>
      <url>posts/a1d2702d/</url>
      
        <content type="html"><![CDATA[<h2 id="基本原理">基本原理</h2><p>mysql的主从复制默认基于异步复制，主要有三个进程参与，分别是主mysql上mysqldump进程，从mysql的i/o进程和sql进程。</p><p>执行过程：</p><ol><li>从mysql的I/O进程向主mysql上mysqldump进程发起请求，读取主服务的本地binlog，主服务将数据通过3306端口发送给从mysql。</li><li>从mysql的I/O进程将数据存放到中继日志中，同时在master-info文件中记录所获取的binlog数据对应的日志文件名和日志读取点位置，便于告知主mysql下次发送二进制日志数据的起始点。</li><li>从mysql的sql进程检测到中继日志有数据后，在从mysql本地执行新加的语句，并将执行的语句的结果，存入数据库。</li></ol><h2 id="优点">优点</h2><p>MySQL的主服务器上的I/O线程，将变更数据按照事务提交的顺序写到binlong中后，就直接返回给客户端数据更新成功，主服务不会考虑从服务是否发起同步请求。所以不管从同步还是不同步，都不会影响到主库的数据写入。</p><h2 id="缺点">缺点</h2><p>如果主挂了，主的二进制日志还没来得及发给从，又没有其它措施保存二进制日志，那数据就丢了。</p><h2 id="搭建步骤">搭建步骤</h2><h3 id="master">master</h3><ol><li>创建监控主从状态的用户</li></ol><pre><code class="language-sql">grant replication slave,replication client on *.* to myreplic@'&lt;slave_ip&gt;' identified by '123456';flush privileges;</code></pre><p>💥做主从服务器的原则是，MYSQL版本要相同，如果不能满足，最起码从服务器的MYSQL的版本必须高于主服务器的MYSQL版本</p><ol start="2"><li>主关键配置</li></ol><pre><code class="language-shell:">[mysqld] server-id=100log-bin=mysql-bin binlog-ignore-db=mysqlbinlog_format=rowbinlog_row_image=FULLmax_allowed_packet=128M</code></pre><p>💥最好是只设置binlog-ignore-db。在5.5的版本中，同时设置了binlog-do-db和binlog-ignore-db会导致日志不再记录。</p><p>✨<code>binlog-do-db</code>可以只记录某个库的二进制日志。</p><ol start="3"><li>锁表、备份、二进制日志信息</li></ol><p>导出包括锁表语句以及master的二进制日志语句，均可以用下列一条命令处理</p><pre><code class="language-bash">mysqldump --all-databases --lock-all-tables --master-data=2 -p&lt;密码&gt; &gt; slave.sql</code></pre><h3 id="slave">slave</h3><ol><li>从关键配置</li></ol><pre><code>server-id=200replicate-ignore-db=mysqlrelay-log=relay-binrelay_log_index=relay-bin.indexmax_allowed_packet=128M</code></pre><ol start="2"><li>导入全量数据</li></ol><pre><code>mysql –u root –p&lt;密码&gt; &lt; slave.sql</code></pre><ol start="3"><li>链接主，并告知主从哪开始发日志数据</li></ol><p>语句可以从 slave.sql 里获取</p><pre><code class="language-sql">mysql&gt; change master to MASTER_HOST='&lt;master_ip&gt;', MASTER_USER='myreplic', MASTER_PASSWORD='123456', MASTER_LOG_FILE='master-bin.xxxxx', MASTER_LOG_POS=xxx;</code></pre><p>查看主从服务器的状态</p><pre><code class="language-sql">mysql&gt; show slave status\G;                  Master_Host: 10.200.10.10         -- master服务器IP                  Master_User: myreplic                 -- 主从同步用户                  Master_Port: 3306                 -- 主库端口                Connect_Retry: 60                   -- 建立连接尝试次数              Master_Log_File: mysql-bin.000010     -- master的binlog日志，在master上执行show master status查看          Read_Master_Log_Pos: 33101249             -- 中继器读到的master binlog的位置               Relay_Log_File: relay-bin.000010     -- 从库中继器的日志文件                Relay_Log_Pos: 367                  -- 中继日志读到的位置        Relay_Master_Log_File: mysql-bin.000010     -- 当前中继读到的master的binlog文件             Slave_IO_Running: Yes  -- I/O线程状态            Slave_SQL_Running: Yes  -- SQL线程状态              Replicate_Do_DB:           Replicate_Ignore_DB: mysql,information_schema,performance_schema,sys -- 主从同步忽略的库           Replicate_Do_Table:        Replicate_Ignore_Table:       Replicate_Wild_Do_Table:   Replicate_Wild_Ignore_Table:                    Last_Errno: 0                   Last_Error:                  Skip_Counter: 0          Exec_Master_Log_Pos: 33101249              -- SQL线程执行到的Relay_Master_Log_File文件的位置              Relay_Log_Space: 741              Until_Condition: None               Until_Log_File:                 Until_Log_Pos: 0           Master_SSL_Allowed: No           Master_SSL_CA_File:            Master_SSL_CA_Path:               Master_SSL_Cert:             Master_SSL_Cipher:                Master_SSL_Key:         Seconds_Behind_Master: 0                     -- 主从数据同步间隔时间，即从库Master_SSL_Verify_Server_Cert: No                Last_IO_Errno: 0                Last_IO_Error:                Last_SQL_Errno: 0               Last_SQL_Error:   Replicate_Ignore_Server_Ids:              Master_Server_Id: 2                        -- master库的serverId，在my.cnf中配置的，主从不能设置一样                  Master_UUID:              Master_Info_File: mysql.slave_master_info  -- master的信息，这里根据master_info_repository配置决定                    SQL_Delay: 0          SQL_Remaining_Delay: NULL      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates           Master_Retry_Count: 86400                  Master_Bind:       Last_IO_Error_Timestamp:      Last_SQL_Error_Timestamp:                Master_SSL_Crl:            Master_SSL_Crlpath:            Retrieved_Gtid_Set:             Executed_Gtid_Set:                 Auto_Position: 0         Replicate_Rewrite_DB:                  Channel_Name:            Master_TLS_Version:</code></pre><h2 id="半同步模式">半同步模式</h2><h3 id="原理">原理</h3><p>半同步模式的出现是为了解决异步复制的丢数据问题。半同步模式分两种<code>AFTER_COMMIT</code>和<code>AFTER_SYNC</code>。</p><p><code>AFTER_COMMIT</code>模式下：</p><p>主服务器提交事务之后，并不会返回OK给客户端，而是等待从服务器发送中继日志接收完毕的ACK确认。</p><p>因事务已提交，所以其它客户端可以看到事务信息。</p><p>如果从服务器接收中继日志期间，主服务器挂了，则故障转移后，客户端在老主和新主上看到的信息就不一致了。</p><p>5.7.2版本之后的【AFTER_SYNC】大概原理：可以保证的是不丢数据，不过会多数据</p><p>就是在【主】将事务写入二进制日志之后，就会等待【从】接收二进制日志，在【从】将日志写入到中继日志后并返回【ACK】给【主】，主才会将事务提交给存储引擎执行并返回ok给客户端。</p><p>那么，当【主】挂的时候，且客户端没有收到【主】响应，则有两种情况：</p><ol><li>若【从】还未接收二进制日志到中继日志，那么故障转移后，【旧主】还未提交事务 + 【新主（旧从）】中继日志里没有事务，所以客户端在【新主（旧从）】上重新发起提交事务，此时【新主】数据既没有少，也没有多。</li><li>若【从】已经接收二进制日志中继日志，那么故障转移后，【旧主】还未提交事务 + 【新主（旧从）】中继日志里有事务，所以客户端在【新主（旧从）】上重新发起提交事务，此时【新主（旧从）】数据会出现重复。</li></ol><h3 id="配置方法">配置方法</h3><ul><li>命令方式（临时）</li></ul><pre><code class="language-sql"># 主服务器安装插件：mysql&gt; INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';启动模块：mysql&gt; SET GLOBAL rpl_semi_sync_master_enabled = 1;</code></pre><pre><code class="language-sql"># 从服务器安装插件：msyql&gt; INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';启动模块：mysql&gt; SET GLOBAL rpl_semi_sync_slave_enabled = 1;重启从的IO线程: mysql&gt; STOP SLAVE IO_THREAD; START SLAVE IO_THREAD;</code></pre><ul><li><p>配置方式（永久）</p><p>在Master和Slave的my.cnf中编辑</p></li></ul><pre><code class="language-ini"># On Master[mysqld]plugin-load=&quot;rpl_semi_sync_master=semisync_master.so&quot;rpl_semi_sync_master_enabled=1rpl_semi_sync_master_timeout=10000   # 此单位是毫秒，主10秒内没有收到从的ACK，就会降级为异步复制rpl_semi_sync_master_wait_for_slave_count=1 # 主在rpl_semi_sync_master_timeout内只需要收到一个从的ACK，就可以返回响应给客户端。若没达成条件，就会转为异步。rpl_semi_sync_master_wait_no_slave=ON # ON表示仅当事务期间，Rpl_semi_sync_master_clients &lt; rpl_semi_sync_master_wait_for_slave_count ，才会转为异步。OFF表示，哪怕空闲的时候，也会转为异步。rpl_semi_sync_master_wait_point=AFTER_SYNC # 主等待从同步完事务后才提交事务到存储引擎# On Slave[mysqld]plugin-load=&quot;rpl_semi_sync_slave=semisync_slave.so&quot;rpl_semi_sync_slave_enabled=1</code></pre><p>✨rpl_semi_sync_master_wait_point 参数若设置为 AFTER_COMMIT，则表示主提交事务到存储引擎后才会同步事务到从。</p><h3 id="配置参数">配置参数</h3><pre><code class="language-sql">mysql&gt; show variables like '%Rpl%';</code></pre><h3 id="状态参数">状态参数</h3><p>半同步模式开启后，通过主可以看到半同步的一些状态</p><pre><code class="language-sql">msyql&gt; show status like '%Rpl_semi%';+--------------------------------------------+-------+| Variable_name                              | Value |+--------------------------------------------+-------+| Rpl_semi_sync_master_clients               | 1     | # 当前处于半同步状态的从服务个数| Rpl_semi_sync_master_net_avg_wait_time     | 0     || Rpl_semi_sync_master_net_wait_time         | 0     || Rpl_semi_sync_master_net_waits             | 0     || Rpl_semi_sync_master_no_times              | 0     || Rpl_semi_sync_master_no_tx                 | 0     || Rpl_semi_sync_master_status                | ON    | # OFF表示出现异常转为异步模式| Rpl_semi_sync_master_timefunc_failures     | 0     || Rpl_semi_sync_master_tx_avg_wait_time      | 0     || Rpl_semi_sync_master_tx_wait_time          | 0     || Rpl_semi_sync_master_tx_waits              | 0     || Rpl_semi_sync_master_wait_pos_backtraverse | 0     || Rpl_semi_sync_master_wait_sessions         | 0     || Rpl_semi_sync_master_yes_tx                | 0     |+--------------------------------------------+-------+14 rows in set (0.00 sec)</code></pre><h2 id="同步错误">同步错误</h2><p>当同步出错的时候，你可以通过<code>show slave status\G</code>看到错误信息，例如：</p><pre><code class="language-bash">1032：记录不存在，当更新或删除一个条不存在的记录时，我们可以忽略此异常1050：数据表已经存在</code></pre><p>上述几个错误，根据情况，一般都可以直接通过<code>slave-skip-errors=&lt;错误码&gt;</code>跳过错误，从而恢复同步。</p><pre><code class="language-bash">1236: Got fatal error 1236 from master when reading data from binary log: 'Client requested master to startreplicationfromposition &gt; filesize'主库异常断电，导致buffer中的事务来不及写入二进制日志，使从库I/O线程读取到比二进制日志中end pos还大的pos。</code></pre><p>通过mysqlbinlog查看Relay_Master_Log_File文件，找到对应Exec_Master_Log_Pos相近的pos.</p><p>这里有两种情况：</p><p>一种是有相近的pos值，然后从库通过执行select master_pos_wait方式，查看已经同步的pos值，找到未同步的，然后从库执行<code>change master to master_log_pos=xxx;</code><br>另外一种情况就是Relay_Master_Log_File没有相近的pos值，这时候一般是本binlog已经完成同步，将master_log_file指向下一个文件，pos值改成下个文件的最小值即可。</p><pre><code class="language-bash">1236: Got fatal error 1236 from master when reading data from binary log</code></pre><p>需要重建slave。常见于主库二进制日志被提前删除。加大<code>expire_logs_days</code>的时间。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql-联合索引</title>
      <link href="posts/d73c1236/"/>
      <url>posts/d73c1236/</url>
      
        <content type="html"><![CDATA[<h2 id="联合索引的本质：最左匹配">联合索引的本质：最左匹配</h2><p>联合索引又叫联合索引。对于联合索引：Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。</p><h2 id="mysql联合索引原理及失效条件">mysql联合索引原理及失效条件</h2><p>当创建(col1,col2,col3)联合索引时，相当于创建了**(col)单列索引**，<strong>(clo1,clo2)联合索引</strong>以及**(col1,col2,col3)联合索引**。</p><p>想要索引生效，只能使用<strong>col1</strong>和<strong>col1,col2</strong>和<strong>col1,col2,col3</strong>三种组合。当然，<strong>col1,col3组合也可以，但实际上只用到了col1的索引，col3并没有用到</strong>。</p><p>例如：</p><p>索引是key index (a,b,c)。可以支持a | a,b| a,b,c 3种组合进行查找，但不支持 b,c进行查找 。当最左侧字段是常量引用时，索引就十分有效。</p><h2 id="图解">图解</h2><p><img src="/posts/d73c1236/image-20220517145349119.png" alt="image-20220517145349119"></p><h2 id="以实际例子说明">以实际例子说明</h2><p>联合索引的结构与电话簿类似，人名由姓和名构成，电话簿首先按姓氏对进行排序，然后按名字对有相同姓氏的人进行排序。如果您知 道姓，电话簿将非常有用；如果您知道姓和名，电话簿则更为有用，但如果您只知道名不姓，电话簿将没有用处。</p><p>ℹ️即：依然满足最左原则，任何查找，从最左边索引列开始。</p><p>所以说创建联合索引时，应该仔细考虑列的顺序。对索引中的所有列执行搜索或仅对前几列执行搜索时，联合索引非常有用；仅对后面的任意列执行搜索时，联合索引则没有用处。</p><pre><code class="language-sql">1、只命中 col1，col2SELECT * FROM `table_name` WHERE `col1`='XX'; 2、命中col1，col2。col1，col2的顺序可以颠倒SELECT * FROM `table_name` WHERE `clo1`='XX' AND `clo2`='XXX'; SELECT * FROM `table_name` WHERE `clo2`='XXX' AND `clo1`='XX';   3、命中col1,col2,col3，同理，三个列的顺可以颠倒SELECT * FROM `table_name` WHERE `col1`='X' AND `col2`='XX' AND `col3`='XXX';SELECT * FROM `table_name` WHERE `col1`='X' AND `col3`='XX' AND `col2`='XXX';SELECT * FROM `table_name` WHERE `col2`='X' AND `col3`='XX' AND `col1`='XXX';</code></pre><h2 id="命令">命令</h2><pre><code class="language-bash"># 创建ALTER TABLE `table_name` ADD INDEX (`col1`,`col2`,`col3`);# 删除drop index index_name on table_name ;</code></pre><p>联合索引的索引体积比单独索引的体积要小，而且只是一个索引树，相比单独列的索引要更加的节省时间复杂度和空间复杂度。</p><h2 id="总结">总结</h2><p>命名规则：表名_字段名</p><p>1、需要加索引的字段，要在where条件中</p><p>2、数据量少的字段不需要加索引</p><p>3、如果where条件中是OR关系，加索引不起作用</p><p>4、符合最左原则</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-添加ssh登陆限制</title>
      <link href="posts/c3b69c28/"/>
      <url>posts/c3b69c28/</url>
      
        <content type="html"><![CDATA[<ol><li>开启ssh的UsePAM</li></ol><pre><code class="language-bash"># /etc/ssh/sshd_configUsePAM yes</code></pre><ol start="2"><li>添加规则</li></ol><pre><code class="language-bash"># /etc/pam.d/sshd#%PAM-1.0auth required pam_tally2.so deny=3 unlock_time=600 even_deny_root root_unlock_time=600</code></pre><p>任何用户每600秒只有三次试错。</p><ol start="3"><li>解封用户</li></ol><pre><code class="language-bash">pam_tally2 -u &lt;用户名&gt; -r</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-中文乱码</title>
      <link href="posts/3dd05018/"/>
      <url>posts/3dd05018/</url>
      
        <content type="html"><![CDATA[<p>~/.vimrc 添加</p><pre><code class="language-bash">set fileencodings=ucs-bom,utf-8,cp936,gb18030,big5,euc-jp,euc-kr,latin1set encoding=utf8set termencoding=utf-8</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="posts/0/"/>
      <url>posts/0/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-go">package mainimport (      &quot;fmt&quot;    &quot;sync&quot;)func main() &#123;      var wg sync.WaitGroup  // 1.等待组    done := make(chan struct&#123;&#125;) // 2.通道（仅充当标识完成与否）    wq := make(chan interface&#123;&#125;) // 3.通道（存储消费数据）    workerCount := 2 // 4.消费者    for i := 0; i &lt; workerCount; i++ &#123;        wg.Add(1)  // 5.添加等待组计数，一个消费者对应一个        go doit(i,wq,done,&amp;wg) // 6.并发执行消费函数    &#125;     for i := 0; i &lt; workerCount; i++ &#123; // 9.写入0,1到消费通道wq        wq &lt;- i    &#125;    close(done) // 11. 关闭完成通道，使消费者可以从关闭通道里获取空值，从而推出监听for循环    wg.Wait() // 15. 等待组在计数为0之前，阻塞进程    fmt.Println(&quot;all done!&quot;)&#125;func doit(workerId int, wq &lt;-chan interface&#123;&#125;,done &lt;-chan struct&#123;&#125;,wg *sync.WaitGroup) &#123;  // 消费者ID，消费通道（仅可只读），完成通道（仅可只读），等待组指针    fmt.Printf(&quot;worker [%v] is running\n&quot;,workerId) // 7. 打印消费者状态    defer wg.Done() // 14. 一个消费者完成消费函数，等待组计数减1    for &#123;      // 8.每一个消费者内部循环监听消费通道wq        select &#123;        case m := &lt;- wq: // 10.一旦消费通道出现数据，则消费者就从消费通道里获取数据，并打印            fmt.Printf(&quot;worker [%v] consumed %v\n&quot;,workerId,m)        case &lt;- done:  // 12. 从完成通道里获取空值，从而达成执行条件            fmt.Printf(&quot;worker [%v] is done\n&quot;,workerId)            return  // 13. 退出循环监听        default:            time.Sleep(1 * time.Second) // 8-1. 在没有消费数据之前，每一秒监听一次            fmt.Println(&quot;No Event!&quot;)        &#125;    &#125;&#125;</code></pre><p>11执行之后，12如何确保在10之后运行。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-lookup插件</title>
      <link href="posts/25732c33/"/>
      <url>posts/25732c33/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>lookup 插件可以配合 loop 关键词循环实现 with_x 循环</p><h2 id="lookup-查询插件">lookup 查询插件</h2><p>通过不同的参数，返回结果集，结果集是一个列表。</p><blockquote><p>query 插件与 lookup 插件一样，区别在于 query 默认情况下，启用了wantlist=true。<br>wantlist 的意思是将返回的字符串构成一个列表<br>另外，query 可以简写为q</p></blockquote><h2 id="loop-关键词">loop 关键词</h2><p>loop 作用是将一个列表循环输出，每次循环时，将列表子元素赋值给 item 变量. 官方用来替代 with_xxx</p><h2 id="lookup和loop结合">lookup和loop结合</h2><p>loop用来遍历 lookup 结果集。</p><blockquote><p>自定义loop循环中的子元素变量为 xxx，代替item</p><pre><code class="language-yaml">loop_control:  loop_var: xxx</code></pre><p>loop_control 还有其它控制，比如</p><ul><li>index_var: var_name 循环索引.</li><li>pause: time 循环间隔时间</li><li>label: var_name 去掉 var_name 之外的不相干信息输出</li></ul></blockquote><h2 id="lookup常用参数示例">lookup常用参数示例</h2><h4 id="file-参数">file 参数</h4><blockquote><p>多个文件内容合并在一起，或以字符串逗号分隔输出，或以列表形式输出</p></blockquote><h4 id="ini-参数">ini 参数</h4><blockquote><p>获取 ini 配置信息</p></blockquote><pre><code class="language-ini"># ~/test.ini[testA]a1=zhangsana2=lisi[testB]b1=wangwu</code></pre><pre><code class="language-yaml">---- hosts: localhost  gather_facts: no  remote_user: zyh  tasks:    - name: get ini      debug:        msg: &quot;&#123;&#123; q('ini', 'a1 section=testA file=~/test.ini') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;zhangsan&quot;    ]&#125;</code></pre><blockquote><p>当配置是 properties，可以追加 type=properties</p></blockquote><h4 id="dict参数">dict参数</h4><p>生成一个字典列表</p><pre><code class="language-bash"># 结果集 [&#123;key: xxx, value: xxx&#125;, &#123;key: xxx, value: xxx&#125;]</code></pre><pre><code class="language-yaml">---- hosts: local  gather_facts: no  connection: local  vars:    users:      male: Bob      female: Maris  tasks:    - name: test vars 1      loop: &quot;&#123;&#123; lookup('dict', users) &#125;&#125;&quot;      debug:        msg: &quot;&#123;&#123; item.key &#125;&#125;: &#123;&#123; item.value &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=&#123;'key': u'male', 'value': u'Bob'&#125;) =&gt; &#123;    &quot;msg&quot;: &quot;male: Bob&quot;&#125;ok: [localhost] =&gt; (item=&#123;'key': u'female', 'value': u'Maris'&#125;) =&gt; &#123;    &quot;msg&quot;: &quot;female: Maris&quot;&#125;</code></pre><h4 id="subelements参数">subelements参数</h4><pre><code class="language-bash"># 写法：&#123;&#123; lookup('subelements',list,'content') &#125;&#125;# 一个由相同结构字典组成的列表list，将字典中某一个元素key（值必须是列表）与字典剩余的元素（剩余的元素作为一个整体新字典），构建笛卡尔积。从而形成 item。每一个字典拆分组合后的 item 构建结果集 items# 结果集items=[[&#123;list.0.剩余kv&#125;,  list.0.key.0], [&#123;list.0.剩余kv&#125;,  list.0.key.1], [&#123;list.1.剩余kv&#125;,  list.1.key.0], [&#123;list.1.剩余kv&#125;,  list.1.key.1], ]</code></pre><pre><code class="language-yaml">---- hosts: local  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                    age: 18                    content:                            - eating                            - sleeping                            - play ogre                  - name: Maris                    gender: female                    age: 20                    content:                            - eating                            - sleeping                            - shopping  tasks:          - name: test vars 2            debug:                    msg: &quot;&#123;&#123; item.0.name &#125;&#125; - &#123;&#123; item.0.gender &#125;&#125; - &#123;&#123; item.1 &#125;&#125;&quot;            loop: &quot;&#123;&#123; lookup('subelements',users,'content') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'eating']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - eating&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'sleeping']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - sleeping&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'play ogre']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - play ogre&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'eating']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - eating&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'sleeping']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - sleeping&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'shopping']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - shopping&quot;&#125;</code></pre><blockquote><p>若有多个key需要附加，需要 nested 与  include_tasks 的组合实现</p></blockquote><h4 id="nested参数">nested参数</h4><blockquote><p>将多个列表进行笛卡尔积运算</p><ol><li>test3.yml 中拿到 users循环单体 user</li><li>针对循环单体 user引入附加任务 test3_1.yml</li><li>通过lookup插件nested，将 user字典中各key的value相互遍历，构建新列表 item</li></ol></blockquote><pre><code class="language-yaml">################ test3.yml---- hosts: local  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                           age: 18                    content:                            - eating                               - sleeping                             - play ogre                    specialty:                            - english                              - game                       - name: Maris                            gender: female                         age: 20                    content:                            - eating                               - sleeping                             - shopping                     specialty:                            - history                              - cooking    tasks:          - include_tasks: test3_1.yml             loop: &quot;&#123;&#123; users &#125;&#125;&quot;            loop_control:                    loop_var: user ################ test3_1.yml- loop: &quot;&#123;&#123; lookup('nested',user.name, user.age, user.content, user.specialty) &#125;&#125;&quot;  debug:    msg: &quot;name:&#123;&#123; item.0 &#125;&#125;, age:&#123;&#123; item.1 &#125;&#125;, &#123;&#123; item.2 &#125;&#125;, &#123;&#123; item.3 &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=[u'Bob', 18, u'eating', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, eating, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'eating', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, eating, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'sleeping', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, sleeping, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'sleeping', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, sleeping, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'play ogre', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, play ogre, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'play ogre', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, play ogre, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'eating', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, eating, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'eating', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, eating, cooking&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'sleeping', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, sleeping, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'sleeping', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, sleeping, cooking&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'shopping', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, shopping, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'shopping', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, shopping, cooking&quot;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> lookup </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

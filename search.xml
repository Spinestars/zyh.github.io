<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>rabbitmq☞安装</title>
      <link href="posts/b43013ef/"/>
      <url>posts/b43013ef/</url>
      
        <content type="html"><![CDATA[<h2 id="安装rabbitmq-server">安装rabbitmq server</h2><h3 id="docker方式">docker方式</h3><pre><code class="language-bash"># https://hub.docker.com/_/rabbitmq, 然后搜索 management 版本 [docker pull rabbitmq:management]mkdir /export/docker-data-rabbitmqpwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`echo mq@$&#123;pwd&#125;;container_name=rabbitmqnetwork=cmsdocker run --name $&#123;container_name&#125; \--hostname $&#123;container_name&#125; \--network $&#123;network&#125; \--restart always \-p 5672:5672 -p 15672:15672 \--cpus=1 \--memory=1G --memory-swap=1G \--ulimit nofile=204800 \--mount &quot;type=bind,src=/export/docker-data-rabbitmq,dst=/var/lib/rabbitmq&quot; \-e RABBITMQ_DEFAULT_VHOST=rabbitmq \-e RABBITMQ_DEFAULT_USER=admin \-e RABBITMQ_DEFAULT_PASS=mq@$&#123;pwd&#125; \-d rabbitmq:3.8.13-management</code></pre><h3 id="rpm方式">rpm方式</h3><pre><code># 安装 erlang 的 rabbitmq 兼容版方式 (卸载命令 yum erase erlang-*)yum install -y https://github.com/rabbitmq/erlang-rpm/releases/download/v22.0.7/erlang-22.0.7-1.el6.x86_64.rpm# 安装 rabbitmq-serveryum install -y https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.15/rabbitmq-server-3.7.15-1.el6.noarch.rpm;## 开启管理rabbitmq-plugins enable rabbitmq_management;# 修改一些小配置vim /etc/rabbitmq/rabbitmq.conflog.file.level = warninglog.file = rabbitmq.logvim /etc/rabbitmq/rabbitmq-env.confRABBITMQ_LOG_BASE=/export/rabbitmq/logsmkdir -p /export/rabbitmq/logs &amp;&amp; chown rabbitmq:rabbitmq /export/rabbitmq/logschown rabbitmq.rabbitmq /var/lib/rabbitmq/.erlang.cookie;echo '* soft nofile 202400' &gt;&gt; /etc/security/limits.confecho '* hard nofile 202400' &gt;&gt; /etc/security/limits.conf/etc/init.d/rabbitmq-server restart;</code></pre><h3 id="添加用户和虚拟主机">添加用户和虚拟主机</h3><pre><code class="language-bash">#输入http://ip:15672可以登录管理界面,# rpm 安装后，只有默认账户guest/guest.# docker 安装后，有 admin/admin 用户# 添加一个 vhostpro_vhost=rabbitmqctl add_vhost $&#123;pro_vhost&#125;# 添加一个新用户user_name=pwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`rabbitmqctl add_user $&#123;user_name&#125; mq@$&#123;pwd&#125;;echo &quot;$&#123;user_name&#125;用户密码是：mq@$&#123;pwd&#125;&quot;#该命令使用户 user_name 具有 pro_vhost 中所有资源的配置、写、读权限以便管理其中的资源# rabbitmqctl set_permissions -p vhost User ConfP WriteP ReadPrabbitmqctl set_permissions -p $&#123;pro_vhost&#125; $&#123;user_name&#125; &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;; # 用户绑定 tags# tags决定了用户更高级的权限，## 超级管理员(administrator)：可登陆管理控制台(启用management plugin的情况下)，可查看所有的信息，并且可以对用户，策略(policy)进行操作。## 监控者(monitoring)：可登陆管理控制台(启用management plugin的情况下)，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)## 策略制定者(policymaker)：可登陆管理控制台(启用management plugin的情况下), 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)## 普通管理者(management)：仅可登陆管理控制台(启用management plugin的情况下)，无法看到节点信息，也无法对策略进行管理。## 其他(other)：无法登陆管理控制台，通常就是普通的生产者和消费者。user_tag=rabbitmqctl set_user_tags $&#123;user_name&#125; $&#123;user_tag&#125;;#查看所有用户rabbitmqctl list_users;</code></pre><h3 id="状态">状态</h3><pre><code class="language-bash">rabbitmqctl statusrabbitmqctl list_queues -p &lt;vhost_name&gt;</code></pre><h3 id="队列">队列</h3><pre><code class="language-bash">rabbitmqctl list_queues --vhost &lt;&gt;</code></pre><h2 id="php使用">php使用</h2><pre><code>wget https://github.com/alanxz/rabbitmq-c/releases/download/v0.7.1/rabbitmq-c-0.7.1.tar.gz;tar xf rabbitmq-c-0.7.1.tar.gz;cd rabbitmq-c-0.7.1;./configure --prefix=/usr/local/rabbitmq-c-0.7.1;make &amp;&amp; make install;cd ..;wget https://pecl.php.net/get/amqp-1.9.3.tgz;tar xf amqp-1.9.3.tgz;cd amqp-1.9.3;/usr/local/php/bin/phpize;./configure --with-php-config=/usr/local/php/bin/php-config --with-amqp --with-librabbitmq-dir=/usr/local/rabbitmq-c-0.7.1;make &amp;&amp; make install;php.ini里添加生成的amqp.so的路径信息extension = /usr/local/php/lib/php/extensions/no-debug-non-zts-20160303/amqp.so</code></pre>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
          <category> rabbitmq </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> rabbitmq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞动态agent</title>
      <link href="posts/3b5cd3e1/"/>
      <url>posts/3b5cd3e1/</url>
      
        <content type="html"><![CDATA[<h2 id="流程">流程</h2><p>jenkins-master -&gt; pipeline -&gt; kubernetes 插件 -&gt; kubernetes 集群 -&gt; jenkins-agent -&gt; jenkins-master</p><h2 id="凭证信息">凭证信息</h2><p>通过 kube config 里的加密信息获取凭据所需的证书</p><pre><code class="language-bash">certificate-authority-data=client-certificate-data=client-key-data=echo &quot;$&#123;certificate-authority-data&#125;&quot; | base64 -d &gt; ca.crtecho &quot;$&#123;client-certificate-data&#125;&quot; | base64 -d &gt; client.crtecho &quot;$&#123;client-key-data&#125;&quot; | base64 -d &gt; client.keyopenssl pkcs12 -export -out cert.pfx -inkey client.key -in client.crt -certfile ca.crt</code></pre><p>将 pkcs12 传输到 jenkins 凭据中</p><p><img src="/posts/3b5cd3e1/image-20210116152344875.png" alt="image-20210116152344875"></p><p>💁你需要在凭据【密码】位置中输入你生成 pkcs12 证书时输入的密码.</p><h2 id="安装插件">安装插件</h2><ol><li>安装 kubernetes 插件</li><li>添加一个新的插件配置</li></ol><p><img src="/posts/3b5cd3e1/image-20210116152033813.png" alt="image-20210116152033813"></p><ol start="3"><li>确保插件配置【连接测试】显示成功连接</li></ol><h2 id="配置-pipeline-动态生成-agent">配置 pipeline 动态生成 agent</h2><p>关于 agent 的 pod 模板，默认插件已经提供了一个，只不过你看不到。因此哪怕你什么都不做，你 pipeline stages 也会在默认的 agent pod 里执行。</p><p>我们一般选择给 agent pod 添加一个额外的容器，这个额外的容器里包含了我们执行 stages 所需要的环境，例如叫 jenkinstools。</p><p>需要注意的是：</p><p>多容器 pod 模板有诸多的硬性限制：</p><p>Multiple containers can be defined in a pod. One of them is automatically created with name <code>jnlp</code>, and runs the Jenkins JNLP agent service, with args <code>$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;</code>, and will be the container acting as Jenkins agent.</p><p>Other containers must run a long running process, so the container does not exit. If the default entrypoint or command just runs something and exit then it should be overridden with something like <code>cat</code> with <code>ttyEnabled: true</code>.</p><p><strong>WARNING</strong> If you want to provide your own Docker image for the JNLP agent, you <strong>must</strong> name the container <code>jnlp</code> so it overrides the default one. Failing to do so will result in two agents trying to concurrently connect to the master.</p><ol><li>jnlp agent 容器会自动创建，哪怕你没有定义。</li><li>如果你自定义了 JNLP 服务所在的容器，那么负责连接 master 的 jnlp agent 容器名必须叫 jnlp。</li><li>其它容器，也就是我们用来执行 stages 的容器，必须运行一个持久的程序，以便于容器不会自动退出。例如 cat 命令并附加一个伪终端。</li><li>stages 阶段执行的时候，必须明确的指定额外容器，否则会默认在 jnlp 容器里执行. （这个不是很确定，但是我测试是这样）</li></ol><p>下图是我的 pod 模板配置（部分截图）</p><p><img src="/posts/3b5cd3e1/image-20210126182630608.png" alt="image-20210126182630608"></p><p>在 pipeline 里调用插件配置</p><pre><code>pipeline&#123;    agent&#123;        kubernetes&#123;            label &quot;jenkins-agent&quot; // pod 模板标签            cloud 'kubernetes'  // 插件配置名        &#125;    &#125;    stages &#123;        stage('Start Command On Remote Machine') &#123;            steps &#123;                container('jenkinstools') &#123;  // 明确指定 pod 里的执行容器名                    script&#123;                        mytools.getRemoteIP(serverIP)                        mytools.ansible(ansibleRemoteUser,&quot;$&#123;ansibleShellCommand&#125;&quot;)                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyun☞oss</title>
      <link href="posts/b5d7d6ee/"/>
      <url>posts/b5d7d6ee/</url>
      
        <content type="html"><![CDATA[<h2 id="构建-oss-存储桶">构建 oss 存储桶</h2><ul><li>执行脚本（执行本脚本跑到阿里云的云端cli去执行）</li></ul><pre><code class="language-python">#!/usr/bin/python3.6# -*- coding: utf-8 -*-###  Usage：https://help.aliyun.com/document_detail/32027.html##  Github：https://github.com/aliyun/aliyun-oss-python-sdk##  author: zyhimport oss2, osfrom oss2.models import (LifecycleExpiration, LifecycleRule,                        BucketLifecycle,AbortMultipartUpload,                        TaggingRule, Tagging, StorageTransition,                        NoncurrentVersionStorageTransition,                        NoncurrentVersionExpiration)from oss2.models import Tagging, TaggingRule#from aliyunsdkcore.client import AcsClientfrom aliyunsdkcore.acs_exception.exceptions import ClientExceptionfrom aliyunsdkcore.acs_exception.exceptions import ServerExceptionfrom aliyunsdkram.request.v20150501.CreatePolicyRequest import CreatePolicyRequest####################################region = '我是区域ID'bucketName = '我是桶名'project = '我是标签project的值'akey = skey = ###################################endpoint = 'http://oss-&#123;0&#125;.aliyuncs.com'.format(region)auth = oss2.Auth(akey,skey)bucket = oss2.Bucket(auth, endpoint, bucketName)# create bucketbucket.create_bucket()# add tagrule = TaggingRule()rule.add('project', project)tagging = Tagging(rule)bucket.put_bucket_tagging(tagging)# init dirsbucket.put_object('conf/README','我是存放配置的目录')bucket.put_object('data/README','我是存放数据的目录')bucket.put_object('hive/README','我是存放hive的目录')bucket.put_object('backup/README','我是存放备份的目录')bucket.put_object('logs/7days/README','我是存放保留7天的日志目录')bucket.put_object('logs/15days/README','我是存放保留15天的日志目录')bucket.put_object('logs/30days/README','我是存放保留30天的日志目录')bucket.put_object('logs/60days/README','我是存放保留60天的日志目录')bucket.put_object('logs/90days/README','我是存放保留90天的日志目录')bucket.put_object('logs/180days/README','我是存放永久保留的日志目录')# add lifecyclerule1 = LifecycleRule('rule1', 'logs/7days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=7))rule2 = LifecycleRule('rule2', 'logs/15days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=15))rule3 = LifecycleRule('rule3', 'logs/30days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=30))rule4 = LifecycleRule('rule4', 'logs/60days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=60))rule5 = LifecycleRule('rule5', 'logs/90days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=90))rule6 = LifecycleRule('rule6', 'logs/180days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=180))rule7 = LifecycleRule('rule7', 'logs/longlasting/',                      status=LifecycleRule.ENABLED,                      storage_transitions=[StorageTransition(days=60,storage_class=oss2.BUCKET_STORAGE_CLASS_IA),                          StorageTransition(days=180,storage_class=oss2.BUCKET_STORAGE_CLASS_ARCHIVE)])lifecycle = BucketLifecycle([rule1, rule2, rule3, rule4, rule5, rule6, rule7])bucket.put_bucket_lifecycle(lifecycle)os.system(&quot;sed 's#ossBucketName#&#123;0&#125;#g' local.policy.default &gt; local_&#123;0&#125;.policy&quot;.format(bucketName))os.system(&quot;sed 's#ossBucketName#&#123;0&#125;#g' role.policy.default &gt; role_&#123;0&#125;.policy&quot;.format(bucketName))os.system(&quot;aliyun ram CreatePolicy --PolicyName oss-&#123;0&#125;-local --PolicyDocument \&quot;`cat local_&#123;0&#125;.policy`\&quot;&quot;.format(bucketName))os.system(&quot;aliyun ram CreatePolicy --PolicyName oss-&#123;0&#125;-role --PolicyDocument \&quot;`cat role_&#123;0&#125;.policy`\&quot;&quot;.format(bucketName))</code></pre><ul><li>远程用户策略（执行脚本前需要添加的）</li></ul><pre><code class="language-json">&#123;  &quot;Version&quot;: &quot;1&quot;,  &quot;Statement&quot;: [    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [                &quot;oss:GetObject&quot;,                &quot;oss:PutObject&quot;,                &quot;oss:DeleteObject&quot;,                &quot;oss:ListObject&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName/logs/7days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/15days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/30days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/60days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/90days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/180days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/longlasting/*&quot;,        &quot;acs:oss:*:*:ossBucketName/conf/*&quot;,        &quot;acs:oss:*:*:ossBucketName/data/*&quot;,        &quot;acs:oss:*:*:ossBucketName/backup/*&quot;,        &quot;acs:oss:*:*:ossBucketName/hive/*&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;1.1.1.1&quot;,            &quot;2.2.2.2&quot;          ]        &#125;      &#125;    &#125;,    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;oss:List*&quot;,        &quot;oss:GetBucketLocation&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;1.1.1.1&quot;,            &quot;2.2.2.2&quot;          ]        &#125;      &#125;    &#125;  ]&#125;</code></pre><ul><li>角色用户策略（执行脚本前需要添加的）</li></ul><pre><code class="language-json">&#123;  &quot;Version&quot;: &quot;1&quot;,  &quot;Statement&quot;: [    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [                &quot;oss:GetObject&quot;,                &quot;oss:PutObject&quot;,                &quot;oss:DeleteObject&quot;,                &quot;oss:ListObject&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName/logs/7days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/15days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/30days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/60days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/90days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/180days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/longlasting/*&quot;,        &quot;acs:oss:*:*:ossBucketName/conf/*&quot;,        &quot;acs:oss:*:*:ossBucketName/data/*&quot;,        &quot;acs:oss:*:*:ossBucketName/backup/*&quot;,        &quot;acs:oss:*:*:ossBucketName/hive/*&quot;      ]    &#125;,    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;oss:List*&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName&quot;      ]    &#125;  ]&#125;</code></pre><h2 id="使用">使用</h2><p>以角色授权方式+aliyun cli命令方式走起.</p><p>😔，aliyun cli 的文档和使用一言难尽=。=</p><p><strong>使用前请确保角色已经关联了对应权限以及角色已经绑定到了ECS上</strong></p><pre><code class="language-bash"># /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;</code></pre><blockquote><p>本脚本，会让任何一个会话登陆的时候就拿到角色拥有的权限</p></blockquote><pre><code class="language-bash">##导入角色source /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;BucketName=test#需要注意的是，如果请求端资源与oss不在一个大区，则endpoint地址需要用下述外网地址： 关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！Endpoint=&quot;http://oss-$&#123;Region&#125;.aliyuncs.com&quot;##查询## 默认查询是递归查询，-d 只查询一层aliyun oss ls oss://$&#123;BucketName&#125;/ -d -e $&#123;Endpoint&#125;##基本的上传或下载##上传文件 a.file 到 oss://test/ aliyun oss cp a.file oss://$&#123;BucketName&#125;/ -e $&#123;Endpoint&#125;##基本的递归上传##上传目录 abc 下的文件到 oss://test/ 下，如果有重复内容，则需要加入 --force：关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！aliyun oss cp abc oss://$&#123;BucketName&#125;/ --recursive -e $&#123;Endpoint&#125;##复杂的递归上传##上传目录 abc 下的 .lzo 结尾的文件到 oss://test/ 下.##严禁在源目录里执行 --recursive 参数.  关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！##即禁止执行 aliyun oss cp . oss://$&#123;BucketName&#125;/ --recursive aliyun oss cp abc/ oss://$&#123;BucketName&#125;/ --include='*.lzo' --recursive --force -e $&#123;Endpoint&#125;##同步目录 sync 指令变更##同步目录 abc 下的文件到 oss://$&#123;BucketName&#125;/ 下，如有重复，则忽略aliyun oss cp abc oss://$&#123;BucketName&#125;/ --recursive -u -e $&#123;Endpoint&#125;</code></pre><h2 id="开发向-sdk">开发向 sdk</h2><p>关于php sdk访问对象存储的文档</p><p>php oss 对象 sdk<br><a href="https://packagist.org/packages/aliyuncs/oss-sdk-php?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6">https://packagist.org/packages/aliyuncs/oss-sdk-php?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6</a><br>php ram role sdk<br><a href="https://packagist.org/packages/alibabacloud/credentials?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6">https://packagist.org/packages/alibabacloud/credentials?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6</a></p>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
            <tag> oss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络☞S5700配置攻击朔源</title>
      <link href="posts/5290b79d/"/>
      <url>posts/5290b79d/</url>
      
        <content type="html"><![CDATA[<h2 id="目的">目的</h2><p>判断网络攻击来源</p><h2 id="设置管理员ip为白名单">设置管理员ip为白名单</h2><pre><code class="language-bash">acl 3666rule 1 permit ip source 10.200.15.1 0.0.0.0</code></pre><h2 id="根据IP判断">根据IP判断</h2><pre><code class="language-bash">cpu-defend policy 1auto-defend enableauto-defend attack-packet sample 5auto-defend trace-type source-mac source-ip source-portvlanauto-defend protocol arp icmp dhcp telnetauto-defend threshold 60auto-defend alarm enableauto-defend alarm threshold 60auto-defend whitelist 1 acl 3666</code></pre><h2 id="根据端口判断">根据端口判断</h2><pre><code class="language-bash">cpu-defend policy 1auto-port-defend enableauto-port-defend attack-packet sample 5auto-port-defend protocol arp-request arp-reply dhcp icmpauto-port-defend protocol arp-request threshold 30auto-port-defend protocol arp-reply threshold 30auto-port-defend protocol dhcp threshold 30auto-port-defend protocol icmp threshold 30auto-port-defend alarm enableauto-port-defend whitelist 1 acl 3666</code></pre><h2 id="应用策略">应用策略</h2><pre><code class="language-bash">cpu-defend-policy 1 global</code></pre><h2 id="查看攻击来源ip">查看攻击来源ip</h2><pre><code class="language-bash">display auto-defend attack-source</code></pre><h2 id="查看攻击来源端口">查看攻击来源端口</h2><pre><code class="language-bash">display auto-port-defend attack-source</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 交换机 </tag>
            
            <tag> S5700 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞ubuntu20.04纯命令行网卡配置</title>
      <link href="posts/e4f7ce4c/"/>
      <url>posts/e4f7ce4c/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>netplan 用于屏蔽各种网络管理器，它可以让你用一份配置，就帮你把网络给配置好，而无需你考虑网络控制器如何使用。</p><p>20.04 已经默认安装了 netplan.</p><h2 id="记录当前网卡标识名">记录当前网卡标识名</h2><pre><code class="language-bash">ip addr show # ===1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: enp0s31f6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000    link/ether d8:9e:f3:31:83:c8 brd ff:ff:ff:ff:ff:ff</code></pre><p>例如上述中的 <code>enp0s31f6</code></p><h2 id="编辑配置文件">编辑配置文件</h2><pre><code class="language-bash">cat /etc/netplan/00-installer-config.yaml# ===# This is the network config written by 'subiquity'network:  ethernets:    enp0s31f6:      addresses: [10.200.10.2/24]      dhcp4: no      optional: true      gateway4: 10.200.10.1      nameservers:        addresses: [10.200.10.1]  version: 2  renderer: networkd</code></pre><h2 id="加载配置">加载配置</h2><pre><code class="language-bash">netplan applysystemctl restart networkd-dispatcher.service</code></pre><blockquote><p>之后如果再修改 ip， 再次执行 <code>netplay apply</code> 即可</p></blockquote><h2 id="参考">参考</h2><p><a href="https://netplan.io/examples/#configuration">https://netplan.io/examples/#configuration</a></p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ubuntu </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞mysql慢sql预警</title>
      <link href="posts/ab1247fb/"/>
      <url>posts/ab1247fb/</url>
      
        <content type="html"><![CDATA[<h2 id="用途">用途</h2><p>检测数据库中执行缓慢的sql，并记录预警</p><h2 id="脚本">脚本</h2><pre><code class="language-bash">#!/bin/bash# by zyh# 检查数据库耗时过长的语句# 所需配置文件:# 1. userfile.txt 用于判断程序用户的sql是否超时，超时了杀死  ##用户名      密码    超时时间# 2. database.conf 需要show full processlist权限  ##Username=  ##Password=  ##Mysqlhostname=  ##Database=  ##Port=# bash start.sh database.conf 放到计划任务里# -------------------------------------------------------------------------------#--变量--basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`currtime=`date &quot;+%Y%m%d_%H%M%S&quot;`#Username=#Password=#Mysqlhostname=#Database=#Port=source $basedir/$1datadir=$basedir/data/$Database-$1/$currtime[[ -d $datadir ]] || mkdir -p $datadir#--main--mysql -s -r -u$Username -p$Password -h$Mysqlhostname -P$Port -e 'show full processlist' | grep -v 'Sleep' |grep -v 'system' | egrep '(Query|Execute)' | grep -i $'\tselect' &gt; $datadir/$Database.allawk -F'\t' 'BEGIN&#123;OFS=&quot;\t&quot;&#125;ARGIND==1&#123;warntime[$1]=$3;warnpwd[$1]=$2&#125; \                ARGIND==2&#123; \                        for( nametime in warntime ) &#123; \                                if( $2 == nametime &amp;&amp; $6 &gt; warntime[$2] ) &#123; \                                        print warnpwd[$2],$0;break \                                &#125; \                        &#125; \                &#125;' $basedir/userfile.txt $datadir/$Database.all &gt; $datadir/$Database.warncat $datadir/$Database.warn | while read warningpwd id warningname other;do        echo &quot;# $&#123;warningname&#125;: $id $other&quot;        echo &quot;mysql -s -r -u$warningname -p$warningpwd -h$Mysqlhostname -P$Port -e 'kill '&quot;$id&quot;'&quot;done &gt; $datadir/kill.sql[[ -s $datadir/kill.sql ]] &amp;&amp; python $&#123;basedir&#125;/sendmail.py '收件人邮箱地址' &quot;[SQL-TimeOut]-$Database&quot; &quot;($Mysqlhostname)The attachment content is timeout SQL information&quot; '抄送邮箱地址'  &quot;$datadir/kill.sql&quot; || rm -rf $datadir</code></pre><pre><code class="language-python">#!/usr/bin/python#coding:utf-8# by zyh# python sendmail.py '收件人邮箱地址'  '主题' '邮件内容' '抄送邮箱地址' '附件绝对路径'import smtplibfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartimport sysreload(sys)sys.setdefaultencoding('utf8')mail_host = 'smtp服务器地址'  # 例如 smtp.gmail.com:587mail_user = '发件人邮箱'      # 例如 it@gmail.commail_pass = '发件人密码'def send_mail(*args):    argsNum=len(args[0])    if argsNum == 4:        filename,to_list, subject, content=args[0]    elif argsNum == 5:        filename,to_list, subject, content, cc_list=args[0]    elif argsNum == 6:        filename,to_list, subject, content, cc_list, subfile=args[0]    me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;    #初始化邮件对象 msg    msg = MIMEMultipart()    msg['From'] = me    msg['to'] = to_list    emailList=to_list    # 主题    msg['Subject'] = Header(subject, 'utf-8').encode()    # 内容    msg.attach(MIMEText(content, 'plain', 'utf-8'))    # 抄送    if 'cc_list' in vars():        msg['Cc'] = cc_list        emailstring = to_list+','+cc_list        emailList=emailstring.split(&quot;,&quot;)    # 附件    if 'subfile' in vars():        att1 = MIMEText(open((&quot;%s&quot;%subfile),'rb').read(), 'base64', 'utf8')        att1[&quot;Content-Type&quot;] = 'text/plain'        att1[&quot;Content-Disposition&quot;] = 'attachment; filename='+subfile.split('/')[-1] # 这里的filename可以任意写，写什么名字，邮件中显示什么名字        msg.attach(att1)    # 发送    try:        print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)        print &quot;connect mail server suesscc&quot;        s.starttls()        s.login(mail_user,mail_pass)        print &quot;login mail server suesscc&quot;        s.sendmail(me,emailList,msg.as_string())        s.close()        return True    except Exception,e:        print &quot;%s\t%s&quot;%(to_list,str(e))        return Falseif __name__ == &quot;__main__&quot;:    if len(sys.argv) &lt; 4:        exit()    send_mail(sys.argv)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>web☞服务器承载能力计算</title>
      <link href="posts/7c44c2e8/"/>
      <url>posts/7c44c2e8/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>我们在评估一个系统需要多少个服务器来支撑的时候，一般会涉及到多个点。</p><ul><li>pv</li><li>qps</li><li>并发</li><li>平均响应时间</li></ul><p>如果你已经搜索过相关站点，你会看到一个公式：并发=qps*平均响应时间</p><p>说实话，这个公式很蛋疼，猛地一看让人一脸懵逼。这里我按照我的方式来说一下。</p><p>首先将公式变形， QPS = 工作进程数 / 平均响应时间</p><p>其次，换算两边时间单位都为秒级， QPS = （1秒 / 秒级平均响应时间）* 工作进程数</p><p>再次，将<code>并发</code>理解为系统的<code>工作进程</code>，也就是如果并发是100，那么你可以简单的理解为理想情况下100个工作进程同一时间只能处理100个请求。</p><p>那么，<code>1秒 / 秒级平均响应时间</code>就是指每个工作进程1秒能处理多少个请求。</p><p>所以，最终得到 <code>QPS</code>就是系统每秒可以处理的请求数。</p><p>如果你日PV是 300W，那么一般情况下，每天80%的请求量都集中在每天20%的时间里。因此我们可以计算出高峰时间的秒级请求量是 3000000 * 0.8  / （3600 * 24 * 0.2） = 139</p><p>因此，如果你的系统根据 <code>QPS = 工作进程数 / 平均响应时间</code> 得到的 QPS 大于 139，那么你的系统就可以顶住。</p><p>如果小于，那么你有两个选择：</p><ol><li>需要增加工作进程数，也就是增加服务器数量或者服务器里的工作进程</li><li>优化系统，降低平均响应时间</li></ol><blockquote><p>⚠️增加服务器的工作进程，并不一定会产生正向效应，因为工作进程越多，那么进程就可能频繁切换，反而导致工作进程不干活。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> web </tag>
            
            <tag> 承载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞16kubeadm集群升级</title>
      <link href="posts/7538d100/"/>
      <url>posts/7538d100/</url>
      
        <content type="html"><![CDATA[<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</a></p><h1>升级核心节点</h1><h2 id="列出当前kubeadm版本列表">列出当前kubeadm版本列表</h2><pre><code class="language-bash">yum list --showduplicates kubeadm --disableexcludes=kubernetes</code></pre><p>–disableexcludes=kubernetes 只允许kubernetes库</p><h2 id="所有核心节点：提前下载好升级所需的镜像">所有核心节点：提前下载好升级所需的镜像</h2><p>这里假设从 1.18.6 =》 1.19.3</p><pre><code class="language-bash">kubeadm config images list --kubernetes-version=1.19.3===k8s.gcr.io/kube-apiserver:v1.19.3k8s.gcr.io/kube-controller-manager:v1.19.3k8s.gcr.io/kube-scheduler:v1.19.3k8s.gcr.io/kube-proxy:v1.19.3k8s.gcr.io/pause:3.2k8s.gcr.io/etcd:3.4.3-0k8s.gcr.io/coredns:1.6.7</code></pre><pre><code class="language-bash">#!/bin/bash# kubeadm config images listimages=(kube-apiserver:v1.19.3kube-controller-manager:v1.19.3kube-scheduler:v1.19.3kube-proxy:v1.19.3pause:3.2etcd:3.4.3-0coredns:1.6.7)for imageName in $&#123;images[@]&#125;;do    docker pull registry.aliyuncs.com/google_containers/$&#123;imageName&#125;    docker tag registry.aliyuncs.com/google_containers/$&#123;imageName&#125; k8s.gcr.io/$&#123;imageName&#125;    docker rmi registry.aliyuncs.com/google_containers/$&#123;imageName&#125;done</code></pre><h2 id="主核心节点：安装目标版本的kubeadm">主核心节点：安装目标版本的kubeadm</h2><pre><code class="language-bash">yum install -y kubeadm-1.19.3-0 --disableexcludes=kubernetes</code></pre><p>升级计划是以kubeadm的版本为基准的，如果你kubeadm的版本是1.18，那么之后kubeadm列出的升级信息就是升级到1.18的最新稳定版</p><h2 id="主核心节点：停止调度">主核心节点：停止调度</h2><p>这里以k8s01节点为例</p><pre><code class="language-bash:">kubectl drain k8s01 --ignore-daemonsets</code></pre><p>–ignore-daemonsets 忽略 daemonsets，因为 daemonsets 会在驱逐节点上重建 pod 从而导致驱逐失败。</p><pre><code class="language-bash">kubectl get node===NAME    STATUS                     ROLES    AGE   VERSIONk8s01   Ready,SchedulingDisabled   master   40h   v1.18.6</code></pre><h2 id="主核心节点：列出升级计划">主核心节点：列出升级计划</h2><p>在k8s01列出升级计划</p><pre><code class="language-bash">kubeadm upgrade plan</code></pre><p>会输出三部分内容，第一步是当前集群信息</p><pre><code>[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[preflight] Running pre-flight checks.[upgrade] Running cluster health checks[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.18.6[upgrade/versions] kubeadm version: v1.19.3[upgrade/versions] Latest stable version: v1.19.3[upgrade/versions] Latest stable version: v1.19.3[upgrade/versions] Latest version in the v1.18 series: v1.18.10[upgrade/versions] Latest version in the v1.18 series: v1.18.10</code></pre><p>第二部分是升级信息，升级信息又分为两部分。</p><ul><li>升级到当前版本的最新稳定</li></ul><pre><code>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT   CURRENT       AVAILABLEkubelet     3 x v1.18.6   v1.18.10Upgrade to the latest version in the v1.18 series:COMPONENT                 CURRENT   AVAILABLEkube-apiserver            v1.18.6   v1.18.10kube-controller-manager   v1.18.6   v1.18.10kube-scheduler            v1.18.6   v1.18.10kube-proxy                v1.18.6   v1.18.10CoreDNS                   1.6.7     1.7.0etcd                      3.4.3-0   3.4.3-0You can now apply the upgrade by executing the following command:        kubeadm upgrade apply v1.18.10</code></pre><ul><li>升级到最新版本</li></ul><pre><code>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT   CURRENT       AVAILABLEkubelet     3 x v1.18.6   v1.19.3Upgrade to the latest stable version:COMPONENT                 CURRENT   AVAILABLEkube-apiserver            v1.18.6   v1.19.3kube-controller-manager   v1.18.6   v1.19.3kube-scheduler            v1.18.6   v1.19.3kube-proxy                v1.18.6   v1.19.3CoreDNS                   1.6.7     1.7.0etcd                      3.4.3-0   3.4.13-0You can now apply the upgrade by executing the following command:        kubeadm upgrade apply v1.19.3</code></pre><p>第三部分是手动更新部分</p><pre><code class="language-:">The table below shows the current state of component configs as understood by this version of kubeadm.Configs that have a &quot;yes&quot; mark in the &quot;MANUAL UPGRADE REQUIRED&quot; column require manual config upgrade orresetting to kubeadm defaults before a successful upgrade can be performed. The version to manuallyupgrade to is denoted in the &quot;PREFERRED VERSION&quot; column.API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIREDkubeproxy.config.k8s.io   v1alpha1          v1alpha1            nokubelet.config.k8s.io     v1beta1           v1beta1             no</code></pre><p>⭐️如果<code>MANUAL UPGRADE REQUIRED</code>标记是<code>yes</code>，则你需要手动进行升级。手动升级的前提是需要自行提供配置文件</p><pre><code class="language-bash">kubeadm upgrade apply --config &lt;配置文件&gt;</code></pre><p>另外，kubeadm upgrade 总是会刷新证书。</p><h2 id="主核心节点：升级">主核心节点：升级</h2><pre><code class="language-bash">kubeadm upgrade apply v1.19.3</code></pre><h2 id="主核心节点：升级网络插件">主核心节点：升级网络插件</h2><p>这里我用的是 flannel. 所以我只是简单的重新执行一边</p><pre><code class="language-bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改里面的 &quot;Network&quot;: &quot;10.244.0.0/16&quot;, 变更为你自己的 pod 网段，即kubeadm初始化阶段的 --pod-network-cidrkubectl apply -f  kube-flannel.yml</code></pre><h2 id="主核心节点：解锁使其可以调度">主核心节点：解锁使其可以调度</h2><pre><code class="language-bash">kubectl uncordon k8s01kubectl get node===NAME    STATUS   ROLES    AGE   VERSIONk8s01   Ready    master   46h   v1.18.6k8s02   Ready    master   45h   v1.18.6k8s03   Ready    master   45h   v1.18.6</code></pre><p>确保k8s01处于Ready状态，这里暂时版本还是旧版本，因为你服务没有重启</p><h2 id="次要核心节点：升级">次要核心节点：升级</h2><pre><code class="language-bash">appVersion=1.19.3-0nodeHostname=yum install -y kubeadm-$&#123;appVersion&#125; --disableexcludes=kuberneteskubectl drain $&#123;nodeHostname&#125; --ignore-daemonsetskubeadm upgrade nodekubectl uncordon $&#123;nodeHostname&#125;</code></pre><p>你只需执行上述命令，无需执行其它命令</p><h2 id="所有核心节点：升级-kubelet-和-kubectl">所有核心节点：升级 kubelet 和 kubectl</h2><pre><code class="language-bash">appVersion=1.19.3-0yum install -y kubelet-$&#123;appVersion&#125; kubectl-$&#123;appVersion&#125; --disableexcludes=kubernetes</code></pre><p>重启两个组件</p><pre><code class="language-bash">systemctl daemon-reloadsystemctl restart kubelet</code></pre><h2 id="所有核心节点：开启-kube-scheduler-和-kube-controller-manager-端口">所有核心节点：开启 kube-scheduler 和 kube-controller-manager 端口</h2><p>若发现<code>kubectl get cs</code> 两个服务无法连接，则可以看下配置端口是否为0，如果是则执行下列命令</p><pre><code class="language-bash">sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml</code></pre><p>🌟不一定需要执行，具体以实际环境为准</p><h1>升级工作节点</h1><p>我这里测试环境，核心节点和工作节点是重叠的，所以无需再升级工作节点。</p><pre><code class="language-bash">appVersion=1.19.3-0nodeName=# 升级kubeadmyum install -y kubeadm-$&#123;appVersion&#125; --disableexcludes=kubernetes# 锁定nodekubectl drain $&#123;nodeName&#125; --ignore-daemonsets# 升级nodekubeadm upgrade node# 升级 kubelet 和 kubectlyum install -y kubelet-$&#123;appVersion&#125; kubectl-$&#123;appVersion&#125; --disableexcludes=kubernetes# 重启systemctl daemon-reloadsystemctl restart kubelet# 解锁nodekubectl uncordon $&#123;nodeName&#125;</code></pre><h1>主核心节点升级失败，且自动回滚失败</h1><h2 id="主核心节点：尝试强制升级">主核心节点：尝试强制升级</h2><pre><code class="language-bash">appVersion=1.19.3-0kubeadm upgrade apply --force $&#123;appVersion&#125;</code></pre><h2 id="无论如何都失败，手动恢复">无论如何都失败，手动恢复</h2><p>升级前，kubernetes 会备份 etcd 和 静态pod的内容</p><p>对应关系如下：</p><p><code>/etc/kubernetes/tmp/kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;/etcd</code>= <code>/var/lib/etcd/</code></p><p><code>/etc/kubernetes/tmp/kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code>= <code> /etc/kubernetes/manifests/</code></p><p>将等号左边的路径内容覆盖到等号右边，以便于静态pod重建</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> kubeadm </tag>
            
            <tag> 升级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞15证书管理</title>
      <link href="posts/55da993d/"/>
      <url>posts/55da993d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>k8s默认的CA证书有效期是10年。默认签发的服务证书有效期是1年。</p><p>k8s支持自动轮换证书。</p><p>k8s支持轮换时候的证书有效期。不过最长是10年。（我这边设置为100年，没效果）</p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</a></p><h2 id="检查当前控制面节点里证书">检查当前控制面节点里证书</h2><pre><code class="language-bash">kubeadm alpha certs check-expiration</code></pre><p>你可以通过此命令查看各类证书的有效期</p><h2 id="触发自动轮换证书的行为">触发自动轮换证书的行为</h2><p>当你通过kubeadm upgrade node命令进行升级或者通过control plane进行升级的时候，k8s会自动轮换证书。</p><h2 id="通过命令手动触发证书轮换">通过命令手动触发证书轮换</h2><p><code>kubeadm alpha certs renew all</code>你可以通过此命令进行所有证书轮换</p><p>⭐️请注意，如果你运行了高可用集群，则需要在所有的control-plane节点上运行此命令，也就是说主节点上运行。</p><h2 id="为kubelet配置证书自动轮转">为kubelet配置证书自动轮转</h2><p><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/">https://kubernetes.io/docs/tasks/tls/certificate-rotation/</a></p><p>⭐️1.18和1.19有差异。1.18是已经默认开启的，不过默认的轮转的证书有效期是1年</p><p>使用自动轮转，需要开启两个服务的配置：</p><ul><li><p>开启kubelet启动标记：<code>--rotate-certificates=true</code></p><p>⭐️开启确认命令：<code>kubectl describe cm/kubelet-config-1.18 -n kube-system | grep 'rotateCertificates'</code></p><p>⭐️动态的修改kubelet <a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#reconfiguring-the-kubelet-on-a-running-node-in-your-cluster">https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#reconfiguring-the-kubelet-on-a-running-node-in-your-cluster</a></p><pre><code class="language-bash"># 请先安装 jq 命令. 他是 epel 里的# 开启代理监听kubectl proxy --port=8001 # 导出kubelet配置NODE_NAME=&quot;节点名&quot;; curl -sSL &quot;http://localhost:8001/api/v1/nodes/$&#123;NODE_NAME&#125;/proxy/configz&quot; | jq '.kubeletconfig|.kind=&quot;KubeletConfiguration&quot;|.apiVersion=&quot;kubelet.config.k8s.io/v1beta1&quot;' &gt; kubelet_configz_$&#123;NODE_NAME&#125;# 编辑完配置，重新导入，并确认新配置对象名kubectl -n kube-system create configmap node-config-new --from-file=kubelet=kubelet_configz_$&#123;NODE_NAME&#125; --append-hash -o yaml# 调整每一个node对象，添加新的配置对象kubectl edit node $&#123;NODE_NAME&#125;===configSource:    configMap:        name: CONFIG_MAP_NAME # 将这个名字替换成新的配置对象名        namespace: kube-system        kubeletConfigKey: kubelet# 检查配置生效kubectl get node $&#123;NODE_NAME&#125; -o json | jq '.status.config'</code></pre></li><li><p>调整默认证书有效期</p><p>开启<code>kube-controller-manager</code>启动标记：<code>--cluster-signing-duration</code>，这个标记默认是1年，你可以改成10年 <code>87600h0m0s</code></p><p>修改位置：<code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code></p><p>修改生效：kube-controller-manager pod 将自动重建</p><p>⚠️如果是1.18，则启动标记是<code>--experimental-cluster-signing-duration</code></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> 证书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞14认证授权</title>
      <link href="posts/a5ed67a6/"/>
      <url>posts/a5ed67a6/</url>
      
        <content type="html"><![CDATA[<p>这里是一个认证的一个基本流程：</p><p><img src="/posts/a5ed67a6/image-20201016114248549.png" alt="image-20201016114248549"></p><p>即： kubectl =&gt; 用户认证 =&gt; 用户授权 =&gt; 入口控制 -&gt; 资源对象</p><p>用户认证：检查递交信息包含的证书，用户名</p><p>用户授权：给用户授权权限策略，授权方式有多种：ABAC mode, RBAC Mode, and Webhook mode</p><p>入口控制器：具有特殊功能的过滤器，他们会把请求拦截下来，如果请求违反了过滤器的配置，则请求会被拒绝。</p><p>当你从使用者角度来看的时候，k8s把用户分为普通用户和服务用户。普通用户对外（也就是上图里的Human），服务用户对内（也就是上图里的Pod）。</p><p>请记住，k8s并没有普通用户的<strong>实体对象</strong>。因此，只要你递交的用户拥有k8s集群内部CA所签发的证书，那么这个用户就会被rbac子系统认为是有效的。</p><h2 id="普通用户">普通用户</h2><h3 id="构建普通用户，用于用户认证阶段">构建普通用户，用于用户认证阶段</h3><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user">https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user</a></p><ol><li><p>创建私钥key, 证书请求csr</p><p>创建私钥会让你填写一些属性，这里有两个属性特别重要，分别是指定用户名的属性CN和指定用户组的属性O</p><pre><code>openssl genrsa -out zyh.key 2048</code></pre></li></ol><p>openssl req -new -key zyh.key -out zyh.csr -subj “/CN=zyh/O=it”</p><pre><code>2. 构建k8s的CertificateSigningRequest对象第一步：csr文件进行base64编码```bashRequestStr=`cat zyh.csr | base64 | tr -d &quot;\n&quot;`</code></pre><p>第二步：将编码后的内容写入 request 字段</p><pre><code class="language-yaml">cat &lt;&lt;EOF | kubectl apply -f -apiVersion: certificates.k8s.io/v1beta1kind: CertificateSigningRequestmetadata:  name: zyhspec:  groups:  - system:authenticated  request: $&#123;RequestStr&#125;  signerName: kubernetes.io/kube-apiserver-client  usages:  - client authEOF</code></pre><p>⭐️usages字段必须是<code>client auth</code></p><p>🌟如果你用的是1.19版本，那么apiVersion是<code>certificates.k8s.io/v1</code></p><p>⭐️需要注意的是，证书签发默认只有1年有效期</p><p>🌟确保当前 csr 没有重名申请</p><ol start="3"><li><p>批准证书请求，并获取证书</p><p>批准：</p><pre><code class="language-bash">kubectl get csrkubectl certificate approve zyh</code></pre><p>获取：</p><pre><code class="language-bash">kubectl get csr zyh -o jsonpath='&#123;.status.certificate&#125;' | base64 --decode &gt; zyh.crt</code></pre><p>⭐️检查证书有效期：​<code>openssl x509 -in zyh.crt -noout -dates</code></p><p>至此，一个普通用户就创建完毕了。但是它没有任何权限，还需要进行授权。</p></li></ol><h3 id="创建角色和绑定角色，用于用户授权阶段">创建角色和绑定角色，用于用户授权阶段</h3><p>授权就是给这个用户绑定一个具体的角色。角色代表了一种权限集合。</p><ol><li><p>创建角色对象资源，创建一个developer角色，权限是拥有pod资源的增删改查，且<strong>命名空间是developer</strong></p><pre><code class="language-bash">kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods --namespace=developer</code></pre></li><li><p>创建角色绑定对象资源，且<strong>命名空间是developer</strong></p><pre><code class="language-bash">kubectl create rolebinding developer-binding-zyh --role=developer --user=zyh --namespace=developer</code></pre></li></ol><h3 id="添加普通用户到kubectl的配置文件">添加普通用户到kubectl的配置文件</h3><ol><li><p>添加用户证书到kubectl配置</p><pre><code class="language-bash">kubectl config set-credentials zyh --client-key=zyh.key --client-certificate=zyh.crt --embed-certs=true</code></pre></li><li><p>设置用户上下文，方便进行用户切换</p><pre><code class="language-bash">kubectl config set-context zyh --cluster=kubernetes --user=zyh</code></pre></li><li><p>通过用户上下文进行用户切换</p><pre><code class="language-bash">kubectl config use-context zyh</code></pre></li><li><p>测试权限</p><pre><code class="language-bash">kubectl get pod -n developer===No resources found in developer namespace.kubectl get svc -n developer===Error from server (Forbidden): services is forbidden: User &quot;zyh&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; in the namespace &quot;developer&quot;</code></pre></li></ol><p>通过上述创建过程，你可以发现第一阶段和第二阶段其实是分离的。也就是说，你可以创建多个用户，然后用同一个角色绑定多个用户。</p><p>而用户的有效期，取决于你证书的有效期。</p><h2 id="服务用户">服务用户</h2><p>服务账户默认是自动化创建的。<a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#service-account-automation">https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#service-account-automation</a></p><p>暂时还没发现手动创建的场景。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> authorization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞09存储资源</title>
      <link href="posts/1080c6b6/"/>
      <url>posts/1080c6b6/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>在使用云服务的时候,我们创建磁盘,需要在存储服务中提交一个请求,里面包含了磁盘类型,磁盘大小.之后,我们再把这个申请的磁盘挂载到所需的计算资源上即可.</p><p>而在k8s中. 当使用存储的时候，我们会涉及到以下主要概念</p><p>构建存储的概念：</p><ul><li><p>volume # 这个概念指的是提供的存储服务，例如云商的云盘（aws-ebs，阿里云-云盘），也可以是nfs/cephfs。</p></li><li><p>pv 或者 storageclass # 这个概念你可以认为是云服务商的存储服务的接口（例如web控制台操作界面）。当然，你需要自己提供接口所需要的存储信息，否则，k8s就不知道应该如何使用 volume。</p></li><li><p>pvc # 这个概念你可以认为是你向云服务商存储接口提交的创建请求。通过 pvc，你就可以拿到一块可用的存储盘了。</p></li></ul><p>使用存储的概念：</p><ul><li>podtemplate.spec.volumes  # 这个概念你可以认为是云服务商中你将磁盘绑定到计算资源</li><li>podtemplate.spec.containers.volumeMounts  # 这个概念你可以认为是你登陆到云服务商中的计算资源中,并使用mount命令进行实际挂载.</li></ul><p>你可能会发现，从一个正常逻辑来说，一个volume如果要在计算资源中使用，它是需要格式化一个文件系统的，但是我上面的概念是直接就挂载成了一个目录。这是因为k8s已经在你通过pv资源帮你创建好了文件系统。当然你也可以通过将pv的模式改为块设备，不过这样一来，程序就需要确认是否可以识别块设备了。我想一般的应用程序是用不上这个类型的。<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode</a></p><h2 id="pv">pv</h2><p>pod 与 pv 与 pvc 之间是强依赖关系。在资源对象被调用的过程中，任何一个删除操作都不会立即执行，而是在生命周期结束之后才会执行。</p><p>分为静态和动态</p><p>静态pv: 需要先创建一定数量的pv, 才能通过pvc申请成功.</p><p>动态pv: 需要先构建供应商.主流的云商存储基本都有供应商, 区别在于是内置了,还是需要自行外部创建.然后再通过pvc去申请.</p><p>内置供应商列表: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></p><p>外置供应商部署: <a href="https://github.com/kubernetes-retired/external-storage">https://github.com/kubernetes-retired/external-storage</a></p><h3 id="静态PV">静态PV</h3><p>静态PV, 用户无法自行通过pvc去申请, 因为存储管理员需要先创建好匹配容量的pv才可以.就如同你想构建一个云服务器，但需要先创建好一个云盘。</p><p>一个pv的例子, 这里以 nfs 为例:</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name:  nas-it-local-pv  # 定义 pv 名字, 会被 pvc 引用  namespace: itspec:  claimRef:    name: nas-it-local-pvc    namespace: it  capacity:    storage: 100Gi  # 定义大小  accessModes:  - ReadWriteMany   # 定义访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /mnt/data001/nfs-k8s   # 定义 nfs 共享路径    server: 10.200.16.250        # 定义 nfs 服务器地址</code></pre><ul><li><p>accessModes 决定了pod可以如何使用pv. 它有三种模式:</p><ol><li><p><strong>ReadWriteOnce</strong>(单pod读写)</p></li><li><p><strong>ReadOnlyMany</strong>(多pod只读)</p></li><li><p><strong>ReadWriteMany</strong>(多pod读写)</p><p>这里是官方列举的各种存储类型支持的模式: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a></p></li></ol></li><li><p>persistentVolumeReclaimPolicy 有三种模式:</p><ol><li><p><strong>Retain</strong>(只删pvc)</p></li><li><p><strong>Delete</strong>(同时删除pv和pvc)</p></li><li><p><strong>Recycle</strong>(同时删除pv和pvc,并清空pv存储的资源).</p><p>详情见 <a href="#persistentVolumeReclaimPolicy">persistentVolumeReclaimPolicy</a></p></li></ol></li></ul><h3 id="静态PV对应的PVC">静态PV对应的PVC</h3><pre><code class="language-yaml">kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-it-local-pvc  namespace: itspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi  volumeName: nas-it-local-pv</code></pre><h3 id="可能会用到的卷类型：本地">可能会用到的卷类型：本地</h3><p>本地卷是一种静态卷，因为它强依赖具体的node，所以POD无需进行节点亲和性绑定。</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name: k8s01-pv-localspec:  capacity:    storage: 100Gi  volumeMode: Filesystem  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Delete  storageClassName: local-storage  local:    path: /mnt/disks/ssd1  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:  - 容器          - k8s01</code></pre><h3 id="动态PV">动态PV</h3><p>动态PV, 用户可以自行通过PVC向StroageClass申请资源. 就如同你在构建阿里云的ECS的时候，只需要告知使用多大的磁盘，云服务器创建的时候会自动申请。</p><p>动态PV的构建需要一个新的资源对象StroageClass插件。插件在设计的时候，都会根据volume类型拥有自己的一些特定参数。例如云服务的云盘一般都可以设置请求的磁盘类型。你可以在这里找到各种支持的volume类型以及它们的属性字段：<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></p><p>不过，有些volume类型的供应商已经有对应的内置插件，有些则没有。</p><p>这里以网络存储nfs为例. nfs 属于外置供应商，因此没有内置插件。所以需要先构建<strong>nfs-client</strong>插件。</p><pre><code class="language-yaml">helm install nfs-client stable/nfs-client-provisioner --set storageClass.name=nfs-client --set nfs.server=10.200.16.250 --set nfs.path=/mnt/data001/nfs-k8s --set storageClass.reclaimPolicy=Delete --set storageClass.archiveOnDelete=true --set storageClass.allowVolumeExpansion=true</code></pre><blockquote><p>nfs.server是nfs服务器地址</p><p>nfs.path是nfs共享目录</p><p>parameters.archiveOnDelete 当回收策略在被执行的时候（<a href="#persistentVolumeReclaimPolicy">persistentVolumeReclaimPolicy</a>），pv删除的同时申请的存储资源是否要归档。flase就是不归档直接删除。这里归档的意思就是将申请的存储资源的物理路径前加一个archived前缀，即：archived-${NS_NAME}-${PVC_NAME}-${PV_NAME}。</p><p>storageClass.allowVolumeExpansion 开启卷扩展。这个选项可以让你动态的调整 pvc 请求的对象大小。</p></blockquote><p>关于nfs-client的各种配置参数,可以查看文档https://github.com/helm/charts/tree/master/stable/nfs-client-provisioner#configuration</p><p>nfs-client会自动帮你创建一个 StroageClass。</p><pre><code class="language-yaml">kubectl get StorageClass/nfs-client -o yamlallowVolumeExpansion: trueapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  annotations:    meta.helm.sh/release-name: nfs-client    meta.helm.sh/release-namespace: default  labels:    app: nfs-client-provisioner    app.kubernetes.io/managed-by: Helm    chart: nfs-client-provisioner-1.2.9    heritage: Helm    release: nfs-client  managedFields:  - apiVersion: storage.k8s.io/v1  ......  name: nfs-client  resourceVersion: &quot;9314040&quot;  selfLink: /apis/storage.k8s.io/v1/storageclasses/nfs-client  uid: 3d695640-0cba-4660-9c92-53d870649b2cparameters:  archiveOnDelete: &quot;flase&quot;provisioner: cluster.local/nfs-client-nfs-client-provisionerreclaimPolicy: DeletevolumeBindingMode: Immediate</code></pre><p>你可以通过添加 <code>metadata.annotations.storageclass.kubernetes.io/is-default-class: true</code> 来将此存储类设置为集群默认存储类。这样，当pvc中没有显式的指定存储类的时候，将会用默认存储类。</p><p>在此例子中，你无法添加，不过你可以在创建之前通过 storageClass.defaultClass 选项设置。因为上述例子是通过helm创建的，创建完后存储类配置不可被修改。</p><h3 id="动态PV对应的PVC">动态PV对应的PVC</h3><pre><code class="language-yaml">kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-it-local-pvcspec:  storageClassName: nfs-client  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi</code></pre><p>在这里,我们通过 <code>spec.storageClassName</code>字段来调用名字叫 nfs-client 存储类。</p><h3 id="PV回收策略-persistentVolumeReclaimPolicy">PV回收策略 persistentVolumeReclaimPolicy</h3><p><strong>Retain</strong> 只删除 PVC, PV保留, PV不能被用于其它Pod. 如果想复用存储资源,则需重新构建PV和PVC。假设以aws角度来看，就是EC2删了，web控制台还能看到EBS。</p><ul><li>在静态PV下,PV的名字是固定的,因此删除PVC之后,可以采用原有的PV配置重建PV继而分配到原有的资源.</li><li>在动态PV下,PV的名字是动态生成的, 因此删除PVC之后, 当PVC重新申请资源的时候, PV默认将会分配到一个新的资源路径.</li></ul><p>⭐️对于Retain策略，如果你想精准的将新pvc绑定到某个残留的pv，则可以参照[保留一个静态PV, 并告知pvc强制请求pv](#保留一个静态PV, 并告知pvc强制请求pv)</p><p><strong>Delete</strong> 同时删除PV和PVC. 存储资源是否删除取决于每一个StorageClass中parameters的定义。</p><ul><li>当StorageClass是nfs-client时，开启 <code>parameters.archiveOnDelete=true</code> ，就可以定义不删除而是将其归档。假设以aws角度来看，就是web控制台里的EC2和EBS都没了，但是aws在后台还偷偷的帮你将EBS的里的数据归档保存了。</li><li>当StorageClass是微软/aws/阿里云的云盘的时候，一般默认是删除云盘。从而避免产生大量闲置云盘。</li></ul><p><strong>Recycle</strong> 删除创建的所有对象和存储资源.</p><ul><li>当前只有nfs和hostPath支持.</li></ul><h3 id="存储资源物理层面回收">存储资源物理层面回收</h3><p>k8s官方建议不要使用回收策略 <strong>Recycle</strong>, 因为它无法反悔。k8s建议管理员自己去删除. 例如通过一个pod去删除.pod例子如下:</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: pv-recycler  namespace: defaultspec:  restartPolicy: Never  volumes:  - name: vol    hostPath:      path: /any/path/it/will/be/replaced   # 这里应该填写需要删除的内容根路径. 根路径应该是node本地可以访问的物理资源地址.  containers:  - name: pv-recycler    image: &quot;k8s.gcr.io/busybox&quot;    command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&quot;$(ls -A /scrub)\&quot; || exit 1&quot;]    volumeMounts:    - name: vol      mountPath: /scrub</code></pre><p>上述例子会构建一个 pod，pod会将本地的 /any/path/it/will/be/replaced 路径里的内容都删除掉。</p><h3 id="如何保留一个静态PV-并告知pvc强制请求pv">如何保留一个静态PV, 并告知pvc强制请求pv</h3><p>默认情况下，pvc找静态pv是自动的，例如会自动找大小匹配的pv。这可能有时候不符合我们的需求。例如我们想给一个数据库构建一个固定强依赖的pv。</p><p>此时需要在构建pv的时候，添加 <code>spec.claimRef</code>。例如：</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name: foo-pvspec:  claimRef:    name: foo-pvc    namespace: foo</code></pre><p>这个例子是将 foo-pv 强制保留给 foo-pvc. 也就是说其它 pvc 无法使用 foo-pv。</p><p>之后，我们在 pvc 中添加 <code>spec: volumeName</code> 。例如：</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: foo-pvc  namespace: foospec:  volumeName: foo-pv</code></pre><h3 id="在线扩缩（仅支持动态pv）">在线扩缩（仅支持动态pv）</h3><p>要实现在线调整大小，需要满足下列条件：</p><ol><li><p>feature-gates的<code>ExpandInUsePersistentVolumes</code>被开启。它在1.15版本以后默认开启。</p></li><li><p>StorageClass的<code>allowVolumeExpansion</code>被开启。这意味着必须是动态pv。</p></li><li><p>文件系统是XFS, Ext3, or Ext4。这个需要StorageClass的支持。一般来说volume属于块设备类型的资源才支持，比如aws的ebs。</p><blockquote><p>你可以在这里确认StorageClass是否支持fstype属性。<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters">https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters</a></p></blockquote></li></ol><p>满足上述条件后，你就可以通过 pvc 修改存储大小了。即时 pvc 正在被使用。</p><h3 id="局限性">局限性</h3><p>当一个 pod 同时申请多个 volume 的时候，可能会出现 volume-A 申请成功，volume-B 因物理容量不够申请失败的问题，此时 pod 将卡住。这种情况下，需要手动介入去清理。</p><h3 id="手动介入">手动介入</h3><p>需要我们手动的时候，都是资源申请失败或者不小心删除了PVC之类的。而不管是什么，我们第一目的是数据不丢。</p><p>因此，我们在清理故障对象资源的时候，应该遵循下列步骤：</p><ol><li>将pv的回收策略<code>persistentVolumeReclaimPolicy</code>定义为<code>Retain</code></li><li>删除pvc，这个时候 pv 对象将会保留，但是 pv 状态会变成 Released，这个状态下 pv 无法再被使用</li><li>删除pv对象字段<code>spec.claimRef</code>，从而将pv与pvc解绑，从而使 pv 状态变为 Available</li><li>重建pvc，并将pvc对象字段<code>volumeName</code>设置为pv的名字</li><li>恢复pv的回收策略。</li></ol><h2 id="PVC如何与POD结合使用">PVC如何与POD结合使用</h2><p>这里以上述例子中的 nfs 卷类型为例。</p><pre><code class="language-yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: nas-it-local-dep  namespace: it  labels:    app: nas-it-localspec:  serviceName: nas-it-local-vsftpd  revisionHistoryLimit: 10  replicas: 1  selector:    matchLabels:      app: nas-it-local  template:    metadata:      labels:        app: nas-it-local    spec:      hostNetwork: true      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io/hostname                operator: In                values:  - 容器                - k8s01      containers:      - name: vsftp        image: fauria/vsftpd        ports:        - containerPort: 20        - containerPort: 21        env:        - name: LOG_STDOUT          value: STDOUT        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: nas-it-local-vol          subPath: nas.it.local/data          mountPath: /home/vsftpd        - name: nas-it-local-vc          subPath: vsftpd.conf          mountPath: /etc/vsftpd/vsftpd.conf          readOnly: true        - name: nas-it-local-vc          subPath: virtual_users.txt          mountPath: /etc/vsftpd/virtual_users.txt          readOnly: true        - name: nas-it-local-vc          subPath: admin          mountPath: /etc/vsftpd/virtual/admin          readOnly: true        - name: nas-it-local-vc          subPath: yuangong          mountPath: /etc/vsftpd/virtual/yuangong          readOnly: true      volumes:      - name: nas-it-local-vol        persistentVolumeClaim:          claimName: nas-it-local-pvc      - name: nas-it-local-vc        configMap:          name: nas-it-local-cm</code></pre><ol><li><p>创建 <code>volumes：nas-it-local-vol</code>，并绑定 <code>PVC：nas-it-local-pvc</code></p><pre><code class="language-yaml">      volumes:      - name: nas-it-local-vol        persistentVolumeClaim:          claimName: nas-it-local-pvc</code></pre></li><li><p>创建 <code>volumeMounts</code>挂载 <code>volumes：nas-it-local-vol</code></p><p>这里将 nfs 共享目录中<code>nas.it.local/data</code>里的数据挂载到容器里的 <code>/home/vsftpd</code>路径</p><pre><code class="language-yaml">        volumeMounts:        - name: nas-it-local-vol          subPath: nas.it.local/data          mountPath: /home/vsftpd</code></pre><p>⭐️subPath不存在的时候，nfs 共享目录会自动创建。</p></li></ol><hr><h2 id="持久卷与临时卷">持久卷与临时卷</h2><p>以上方式都是持久卷方式，k8s还有一种临时卷，用来进行非必须的数据存储。例如缓存/会话一类的。临时卷数据将随着POD生命周期的终止而终止。</p><p>k8s支持的临时卷主要是这两种：</p><ul><li><p>emptyDir</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: k8s.gcr.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /cache      name: cache-volume  volumes:  - name: cache-volume    emptyDir: &#123;&#125;</code></pre><p>emptyDir还支持构建一个内存层缓存，也就是 tmpfs。你可以通过<code>emptyDir.medium: &quot;Memory&quot;</code>来开启。</p></li><li><p>generic ephemeral volumes 这一种是更加灵活的emptyDir，可以是网络存储，如果插件支持，则各种持久卷的特性它都有可能支持。当前1.18不支持，从1.19开始支持，1.19也是alpha，默认不开启。</p><p>具体使用看https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装库☞vsftpd</title>
      <link href="posts/894f9595/"/>
      <url>posts/894f9595/</url>
      
        <content type="html"><![CDATA[<h2 id="配置文件">配置文件</h2><pre><code class="language-yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: nas-it-local-dep  namespace: it  labels:    app: nas-it-localspec:  serviceName: nas-it-local-vsftpd  revisionHistoryLimit: 10  replicas: 1  selector:    matchLabels:      app: nas-it-local  template:    metadata:      labels:        app: nas-it-local    spec:      hostNetwork: true      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io/hostname                operator: In                values:                - k8s01  # 这里绑定 pod 所在节点      containers:      - name: vsftp        image: fauria/vsftpd        ports:        - containerPort: 20        - containerPort: 21        env:        - name: LOG_STDOUT          value: STDOUT        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: nas-it-local-vol          subPath: nas.it.local/data          mountPath: /home/vsftpd        - name: nas-it-local-vc          subPath: vsftpd.conf          mountPath: /etc/vsftpd/vsftpd.conf          readOnly: true        - name: nas-it-local-vc          subPath: virtual_users.txt          mountPath: /etc/vsftpd/virtual_users.txt          readOnly: true        - name: nas-it-local-vc          subPath: admin          mountPath: /etc/vsftpd/virtual/admin          readOnly: true        - name: nas-it-local-vc          subPath: yuangong          mountPath: /etc/vsftpd/virtual/yuangong          readOnly: true      volumes:      - name: nas-it-local-vol        persistentVolumeClaim:          claimName: nas-it-local-pvc      - name: nas-it-local-vc        configMap:          name: nas-it-local-cm---kind: ConfigMapapiVersion: v1metadata:  name: nas-it-local-cm  namespace: itdata:  vsftpd.conf: |    # Run in the foreground to keep the container running:    background=NO    # Allow anonymous FTP? (Beware - allowed by default if you comment this out).    anonymous_enable=NO    # Uncomment this to allow local users to log in.    local_enable=YES    ## Enable virtual users    guest_enable=YES    ## Virtual users will use the same permissions as anonymous    virtual_use_local_privs=YES    # Uncomment this to enable any form of FTP write command.    write_enable=YES    ## PAM file name    pam_service_name=vsftpd_virtual    ## Home Directory for virtual users    user_sub_token=$USER    local_root=/home/vsftpd/$USER    user_config_dir=/etc/vsftpd/virtual    # You may specify an explicit list of local users to chroot() to their home    # directory. If chroot_local_user is YES, then this list becomes a list of    # users to NOT chroot().    chroot_local_user=YES    # Workaround chroot check.    # See https://www.benscobie.com/fixing-500-oops-vsftpd-refusing-to-run-with-writable-root-inside-chroot/    # and http://serverfault.com/questions/362619/why-is-the-chroot-local-user-of-vsftpd-insecure    allow_writeable_chroot=YES    ## Hide ids from user    hide_ids=YES    ## Enable logging    xferlog_enable=YES    xferlog_file=/var/log/vsftpd/vsftpd.log    ## Enable active mode    port_enable=YES    connect_from_port_20=YES    ftp_data_port=20    ## Disable seccomp filter sanboxing    seccomp_sandbox=NO    ### Variables set at container runtime    pasv_address=10.200.16.10    pasv_max_port=21110    pasv_min_port=21100    pasv_addr_resolve=NO    pasv_enable=YES    file_open_mode=0666    local_umask=077    xferlog_std_format=NO    reverse_lookup_enable=YES    pasv_promiscuous=NO    port_promiscuous=NO  virtual_users.txt: |    admin    admin    yuangong    yuangong  admin: |    local_root=/home/vsftpd    anon_world_readable_only=NO    write_enable=YES    anon_mkdir_write_enable=YES    anon_upload_enable=YES    anon_other_write_enable=YES  yuangong: |    local_root=/home/vsftpd/yuangong    anon_world_readable_only=NO    write_enable=NO    anon_mkdir_write_enable=NO    anon_upload_enable=NO    anon_other_write_enable=NO---apiVersion: v1kind: PersistentVolumemetadata:  name:  nas-it-local-pv  # 定义 pv 名字, 会被 pvc 引用  namespace: itspec:  claimRef:    name: nas-it-local-pvc    namespace: it  capacity:    storage: 100Gi  # 定义大小  accessModes:  - ReadWriteMany   # 定义访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /mnt/data001/nfs-k8s   # 定义 nfs 共享路径    server: 10.200.16.250        # 定义 nfs 服务器地址---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-it-local-pvc  namespace: itspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi  volumeName: nas-it-local-pv</code></pre><ul><li>这是一个<strong>主动模式</strong>的 ftp. 鉴于 k8s 特殊的网络环境.以及端口的繁琐配置. 所以采用主动模式, 可以让我们更舒服一些.</li><li>pv和pvc方面根据你自己的环境自行调整.</li><li>你需要自行将pod强制绑定到某个物理节点的ip上. 避免因pod重构时漂移到其它ip. 配置文件里绑定的是k8s01</li></ul><h2 id="用户创建">用户创建</h2><p>上述配置会创建admin和yuangong两个用户。</p><p>admin用户拥有所有权。根目录是/home/vsftpd</p><p>yuangong用户只有下载权限。根目录是/home/vsftpd/yuangong</p><h3 id="添加新用户">添加新用户</h3><ul><li><p>添加用户名和密码到 virtual_users.txt 配置</p></li><li><p>添加用户配置到cm对象中</p><pre><code class="language-yaml">  我是用户名: |    local_root=/home/vsftpd/&lt;我是用户名&gt;    anon_world_readable_only=NO    write_enable=YES    anon_mkdir_write_enable=YES    anon_upload_enable=YES    anon_other_write_enable=YES</code></pre></li><li><p>创建用户目录 /home/vsftpd/&lt;我是用户名&gt;</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vsftpd </tag>
            
            <tag> 安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞monit</title>
      <link href="posts/ae41ae33/"/>
      <url>posts/ae41ae33/</url>
      
        <content type="html"><![CDATA[<h2 id="安装">安装</h2><pre><code class="language-bash">yum install monit -y</code></pre><h2 id="主配置">主配置</h2><pre><code class="language-bash"># vim /etc/monitrcset daemon  10 # 调整周期</code></pre><p>⭐️官方配置文档: <a href="https://mmonit.com/monit/documentation/monit.html">https://mmonit.com/monit/documentation/monit.html</a></p><h2 id="程序保活配置">程序保活配置</h2><p>默认存放位置: /etc/monit.d/</p><p>这里以 pidfile 检测方式为例</p><p>nginx的配置:</p><pre><code class="language-bash">check process nginx with pidfile /var/run/nginx.pid  start program = &quot;/usr/sbin/nginx&quot;  stop program = &quot;/usr/bin/killall nginx&quot;if changed pid then alert</code></pre><p>上述nginx配置的意思:</p><ol><li><p>检测方式: pidfile</p></li><li><p>开关程序路径</p></li><li><p>发现pid改变,则触发动作: 预警</p></li></ol><p>tomcat的配置:</p><pre><code class="language-bash">check process tomcat-8080 with pidfile /opt/tomcat/tomcat-8080/bin/tomcat.pid  start program = &quot;/usr/bin/bash /opt/tomcat/tomcat-8080/bin/startup.sh&quot;    as uid &quot;root&quot; and gid &quot;root&quot;  stop program = &quot;/usr/bin/ps aux | /usr/bin/grep '/opt/tomcat/tomcat-8080/bin/bootstrap.jar' | /usr/bin/awk '&#123;print &quot;kill -9 &quot;$2&#125;' | /usr/bin/bash&quot;    as uid &quot;root&quot; and gid &quot;root&quot;if failed port 8080 then alert</code></pre><p>上述tomcat配置的意思:</p><ol><li>检测方式: pidfile</li><li>开关程序路径, 并且执行的时候需要以 root 权限执行</li><li>发现 8080 不通, 则触发动作: 预警</li></ol><p>🌟需要注意的是, 所有的文件路径都需要是绝对路径, 包括命令.</p><h2 id="启动-关闭">启动/关闭</h2><pre><code class="language-bash">systemctl start/stop/reload monit</code></pre><p>🌟当monit启动之后, 它就会加载保活配置, 并进行检测.</p><h2 id="日志">日志</h2><pre><code>/var/log/monit.log</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> monit </category>
          
      </categories>
      
      
        <tags>
            
            <tag> monit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞13高级版负载均衡ingress</title>
      <link href="posts/7627a41d/"/>
      <url>posts/7627a41d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>ingress 可以提供负载均衡/SSL管理/虚拟命名主机/path路由. 它的目的是增加7层负载功能. 你可以通过 Ingress 将请求转发到各个 Service 对象.</p><p>但是ingress本身并不是规则的执行者. 它只是记录规则, 规则执行者由ingress-controller实现.</p><blockquote><p>ingress 说实话，还是不怎么好用，如果仅仅用来进行 path 转发还可以。。。</p></blockquote><h2 id="特点">特点</h2><ul><li>ingress 提供规则, 它需要和后端业务位于同一个ns中.</li><li>ingress controller 实现规则, 它可以单独是一个ns.因为它会自动去找那些注解含有自己的ingress.</li><li>ingress 只针对 http 和 https. 这些之外的端口暴漏你应该使用 Service.Type=NodePort 或者 Service.Type=LoadBalancer</li></ul><h2 id="流量路径">流量路径</h2><p>internet =&gt; Ingress =&gt; Service</p><p><img src="/posts/7627a41d/image-20201023110559218.png" alt="image-20201023110559218"></p><h2 id="Ingress">Ingress</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></p><h3 id="一个例子">一个例子</h3><p>client =&gt; <a href="http://one.foo.com/one">http://one.foo.com/one</a> =&gt; Ingress = &gt; Service(one):8001 =&gt; pod</p><p>client =&gt; http://*.foo.com/other =&gt; Ingress  =&gt; Service(other):8002 =&gt; pod</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: minimal-ingress  annotations:    nginx.ingress.kubernetes.io/rewrite-target: /spec:  rules:  - host: one.foo.com    http:      paths:      - path: /one        pathType: Prefix        backend:          service:            name: one            port:              number: 8001  - host: &quot;*.foo.com&quot;    http:      paths:      - pathType: Prefix        path: &quot;/other&quot;        backend:          service:            name: other            port:              number: 8002</code></pre><p>🌟请注意, <code>networking.k8s.io/v1</code>必须是v1.19+才可以使用, 如果你不是,则应该使用<code>networking.k8s.io/v1beta1</code></p><p>既然Ingress是和nginx一个层次的服务,那必然会有一些相同的概念.</p><p>nginx.servername:  这里是 spec.rules[].host</p><p>nginx.location: 这里是 spec.rules.http.paths[].path</p><p>nginx.upstream: 这里是 spec.rules.http.paths[].backend</p><h3 id="关于-spec-rules-host">关于 spec.rules[].host</h3><p>这里有一些注意事项. host 支持统配匹配. 不过 <code>*</code>不能跨级匹配. 例如,</p><p>*.abc.com 可以匹配 <a href="http://one.abc.com">one.abc.com</a> 但是不能匹配 <a href="http://two.one.abc.com">two.one.abc.com</a>.</p><h3 id="关于-spec-rules-http-paths-path">关于 spec.rules.http.paths[].path</h3><p>这里有一些注意事项.如上例子所述. pathType 定义了 path 的类型. 它有三个类型:</p><ul><li><p>ImplementationSpecific: 已ingress class为基准. 具体如何匹配,需要看选用的ingress class.这是默认的.</p></li><li><p>Exact: 严格匹配. 不能有一点不同.</p></li><li><p>Prefix: 基于&quot;/&quot;分段进行前缀匹配. 例如,</p><ul><li>/aaa/bbb 可以匹配任何 /aaa/bbb/ 开头的, 或者 /aaa/bbb.  因此 /aaa/bbbb 是不能匹配的.</li><li>/aaa 可以匹配任何 /aaa/ 开头的, 或者 /aaa. 因此, /aaaa 是不能匹配的.</li><li>/ 可以匹配任何 / 开头的. 因此, 匹配所有.</li></ul><p>⭐️请注意, Prefix 类型下.你写的匹配字符串首尾都需要加<code>/</code>, 如果你尾部没有加<code>/</code>, 那么 ingress 在匹配的时候, 也会帮你加上<code>/</code>.</p></li></ul><p>当Exact和Prefix混用的时候, 如果一个请求同时匹配了这两个类型下的path, 那么Exact 优先级更高.</p><h3 id="构建TLS">构建TLS</h3><ol><li><h4 id="创建-Secret-对象-导入证书文件">创建 Secret 对象, 导入证书文件</h4><pre><code class="language-yaml">apiVersion: v1kind: Secretmetadata:  name: foo-com-tls  namespace: defaultdata:  tls.crt: base64 encoded cert # 这里需要填写 base64 编码后的 cert 内容. tls.crt 不可更名  tls.key: base64 encoded key  # 这里需要填写 base64 编码后的 key 内容. tls.key 不可更名type: kubernetes.io/tls</code></pre></li><li><h4 id="在Ingress对象中调用-Secret">在Ingress对象中调用 Secret</h4><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: tls-example-ingressspec:  tls:  - hosts:      - https-example.foo.com    secretName: foo-com-tls  rules:  - host: https-example.foo.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: service1            port:              number: 80</code></pre></li></ol><p>通过配置文件, 可以看出来加密到Ingress就停止了(后端Service对象service1的端口是80).这也符合我们在非k8s环境中的流程.</p><p>另外, 不同的Ingress控制器对于TLS这块也有一定差异.具体以控制器本身说明为准.</p><h2 id="Ingress-controller">Ingress controller</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/</a></p><p>Ingress 想要正常运作, 需要Ingress控制器的支持.</p><p>你可以在k8s中部署多个Ingress控制器, 但是你需要通过在 annotations 中用 ingress.class 注解强调你想用哪一款.</p><p>就像下面这样</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata:  annotations:    kubernetes.io/ingress.class: nginx  name: example  namespace: foospec:  rules:    - host: www.example.com      http:        paths:          - backend:              serviceName: exampleService              servicePort: 80            path: /  # This section is only required if TLS is to be enabled for the Ingress  tls:      - hosts:          - www.example.com        secretName: example-tls---apiVersion: v1kind: Secretmetadata:  name: example-tls  namespace: foodata:  tls.crt: &lt;base64 encoded cert&gt;  tls.key: &lt;base64 encoded key&gt;type: kubernetes.io/tls</code></pre><p>这里<code>backend</code>是旧写法，你应该使用新的写法：</p><pre><code class="language-yaml">  backend:    service:      name: exampleService      port:        number: 80</code></pre><p>这里我们选用k8s维护的nginx控制器.它的部署文档是: <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md</a> 或者 <a href="https://kubernetes.github.io/ingress-nginx/deploy/">https://kubernetes.github.io/ingress-nginx/deploy/</a></p><p>我们将通过helm来安装ingress-nginx. (你也可以选择其它安装方式. 在 <a href="https://kubernetes.github.io/ingress-nginx/deploy/">https://kubernetes.github.io/ingress-nginx/deploy/</a> 中可以找到很多不同环境的安装方式. )</p><p>这里是 helm 的安装文档: <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a></p><p>当前它的安装方式应该是:</p><pre><code class="language-bash">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3chmod 700 get_helm.sh./get_helm.sh</code></pre><p>当前ingress-nginx的仓库地址是: <a href="https://hub.helm.sh/charts/ingress-nginx/ingress-nginx">https://hub.helm.sh/charts/ingress-nginx/ingress-nginx</a> 你可以在这里直接找到安装命令</p><pre><code class="language-bash">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginxhelm repo add stable https://kubernetes-charts.storage.googleapis.com/helm repo updatekubectl create namespace ingress-nginx## 安装helm install ingress-nginx ingress-nginx/ingress-nginx -n ingress-nginx## 升级helm upgrade ingress-nginx ingress-nginx/ingress-nginx -n ingress-nginx --install## 自定义配置安装helm show values ingress-nginx/ingress-nginx &gt; ingress-nginx.yamlhelm install ingress-nginx ingress-nginx/ingress-nginx -f ingress-nginx.yaml -n ingress-nginx</code></pre><pre><code class="language-bash">kubectl get all -n ingress-nginx===NAME                                            READY   STATUS    RESTARTS   AGEpod/ingress-nginx-controller-5886685d54-hf6l6   1/1     Running   0          68mNAME                                         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGEservice/ingress-nginx-controller             LoadBalancer   10.96.210.139   &lt;pending&gt;     80:32489/TCP,443:30936/TCP   68mservice/ingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;        443/TCP                      68mNAME                                       READY   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/ingress-nginx-controller   1/1     1            1           68mNAME                                                  DESIRED   CURRENT   READY   AGEreplicaset.apps/ingress-nginx-controller-5886685d54   1         1         1       68m</code></pre><p>我们可以看到 ingress-nginx 控制器创建了一个 pod, 这个 pod 其实就是一个 nginx. 它通过接收 ingress 定义的规则, 来动态的变更nginx配置,从而正确的将流量转发给后端应用的service对象.</p><pre><code class="language-bash">kubectl describe pod/ingress-nginx exec -it pod/ingress-nginx-controller-5886685d54-hf6l6===    Requests:      cpu:      100m      memory:   90Mi</code></pre><p>当前配置默认没有做limit限制. 所以需要关注ingress-nginx创建的pod所在物理节点的性能是否可以满足. 建议强制绑定到多个node节点, 这批节点专门跑用来进行ingress控制器.</p><pre><code class="language-bash">kubectl --namespace ingress-nginx exec -it pod/ingress-nginx-controller-5886685d54-hf6l6 -- /bin/bash==cat /etc/nginx/nginx.conf</code></pre><p>这里我们可以看到ingress-nginx会动态的加载ingress定义的路由规则.</p><p>最后, 我们可能最需要关注的是 <code>service/ingress-nginx-controller</code> 对象. 因为它的类型决定了, 我们如何解析将要暴漏的服务域名.</p><p>我们可以看到通过helm默认安装的 ingress crotroller 的 Service 对象是LoadBalancer类型. 这种类型我们一般是用于云端环境的. 而我这里测试环境是裸机,因此你可以看到这里它一直无法获取到<code>EXTERNAL-IP</code>.</p><p>如何解决这个问题.k8s官方文档这里提供了多种裸机安装下的解决方案. <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/</a></p><p>我们这里通过 <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb</a> 方案来解决 LB 类型的 <code>EXTERNAL-IP</code>获取问题. 其它方案并不是采用的 LB 类型, 因此这里不再叙说.</p><p>⚠️metallb 方案在当前并不是一个成熟的方案,当然它应该是可用的😄 所以用于本地测试环境是可以的. 线上环境建议直接用云服务的LB即可.</p><h2 id="metallb">metallb</h2><p><a href="https://metallb.universe.tf/installation/">https://metallb.universe.tf/installation/</a></p><p>metallb 模拟了云环境中的负载均衡器功能.</p><p>🌟If you’re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode.</p><p>如果你是ipvs 且 k8s 版本在1.14.2以上, 那么你需要启动严格arp模式.</p><pre><code class="language-bash">kubectl edit configmap -n kube-system kube-proxy===ipvs:  strictARP: true</code></pre><p>开始安装</p><pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yamlkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml# 下面这个命令仅在你初始安装的时候需要运行kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=&quot;$(openssl rand -base64 128)&quot;</code></pre><p>metallb将会在一个新的<code>ns: metallb-system</code>下创建.</p><p>开始配置地址池</p><p>metallb通过一个地址池(cm对象)来给LB分发ip. 我们将为 metallb 下发一个内网地址池. 请记住, 这些地址必须是没有被其它资源占用的.</p><pre><code class="language-yaml"># metallb.cm.yamlapiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: default      protocol: layer2      addresses:      - 10.200.16.11-10.200.16.29</code></pre><pre><code class="language-bash">kubectl apply -f metallb.cm.yaml</code></pre><p>下面,让我们再看看 ingress crotroller 的 svc 对象信息.</p><pre><code class="language-bash">kubectl get svc -n ingress-nginx===NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                      AGEingress-nginx-controller             LoadBalancer   10.96.210.139   10.200.16.11   80:32489/TCP,443:30936/TCP   88mingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;         443/TCP                      88m</code></pre><p>终于<code>svc/ingress-nginx-controller</code>拿到了一个<code>EXTERNAL-IP</code>, 现在你就可以将域名解析到<code>EXTERNAL-IP</code>, 并通过这个域名来访问你的应用了.(而且它必须是能通过80访问的.)😄</p><p>同时, 让我们来看看 ingress 对象信息.</p><p>⭐️我暴漏的应用在ns: it中.</p><pre><code class="language-bash">kubectl get ingress -n it===NAME                   CLASS    HOSTS          ADDRESS        PORTS   AGE test-it-local-ingress   &lt;none&gt;   test.it.local   10.200.16.11   80      106m</code></pre><p>可以看到. ingress 的信息也更新了, 并拿到了 metallb 的下发的ip.</p><p>你可以在任意一个节点上通过 ipvs 规则来看到转发情况.</p><pre><code class="language-bash">ipvsadm -L -n===TCP  10.200.16.11:80 rr  -&gt; 10.97.2.57:80                Masq    1      0          0TCP  10.200.16.11:443 rr  -&gt; 10.97.2.57:443               Masq    1      0          0</code></pre><p>这里 10.97.2.57 是 ingress 控制器的 pod ip.</p><h2 id="ingress-nginx-暴漏非80和非443端口">ingress-nginx 暴漏非80和非443端口</h2><p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/">https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/</a></p><p>默认你会发现ingress无法暴漏80和443之外的端口.这是因为默认ingress-nginx并没有开启4层转发.</p><h3 id="开启ingress-nginx中pod的4层转发">开启ingress-nginx中pod的4层转发</h3><p>检查 deployment: ingress-nginx 中 pod 模板是否开启了下列参数</p><pre><code class="language-bash">deploymentName=`kubectl get all -n ingress-nginx | grep deployment | awk '&#123;print $1&#125;'`kubectl edit $&#123;deploymentName&#125; -n ingress-nginx===- args:    - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services    - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</code></pre><h3 id="创建暴漏端口的配置文件">创建暴漏端口的配置文件</h3><pre><code class="language-yaml">apiVersion: v1kind: ConfigMapmetadata:  name: tcp-services  namespace: ingress-nginxdata:  8080: &quot;it/test-it-local-svc:8080&quot;</code></pre><p>这里8080: &quot;it/test-it-local-svc:8080&quot;<code>第一个8080指的是ingress-nginx的pod对象里需要监听的端口, </code>it/test-it-local-svc:8080` 指的是ns:it下的svc对象test-it-local-svc的8080端口</p><h3 id="添加暴漏端口到ingress-nginx中svc对象">添加暴漏端口到ingress-nginx中svc对象</h3><pre><code class="language-bash">kubectl get all -n ingress-nginx | grep service | awk '&#123;print $1&#125;' kubectl edit service/ingress-nginx-controller -n ingress-nginx===  - name: test-8080    port: 8080    protocol: TCP    targetPort: 8080</code></pre><p>三步过后, 不出意外, 你将可以通过ingress-nginx的pod对象中的nginx.conf文件看到tcp的转发配置.</p><pre><code class="language-bash">        # TCP services        server &#123;                preread_by_lua_block &#123;                        ngx.var.proxy_upstream_name=&quot;tcp-it-test-it-local-svc-8080&quot;;                &#125;                listen                  8080;                proxy_timeout           600s;                proxy_pass              upstream_balancer;        &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> ingress </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞12有状态服务StatefulSet</title>
      <link href="posts/4bf29cf0/"/>
      <url>posts/4bf29cf0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>当我们构建一个有状态的应用的时候, 例如 mysql / ftp / 包含session的管理界面等. 我们可能会有下列预期:</p><ul><li>稳定的、唯一的网络标识符。</li><li>稳定的、持久的存储。</li><li>有序的、优雅的部署和缩放。</li><li>有序的、自动的滚动更新。</li></ul><p>StatefulSet 就是干这个的.</p><h2 id="一个例子">一个例子</h2><pre><code class="language-yaml:">apiVersion: v1kind: Servicemetadata:  name: nginx  labels:    app: nginxspec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata:  name: webspec:  serviceName: &quot;nginx&quot;  replicas: 2  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: k8s.gcr.io/nginx-slim:0.8        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates:  - metadata:      name: www    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      resources:        requests:          storage: 1Gi</code></pre><h2 id="特点">特点</h2><ul><li>pod 拥有唯一的顺序索引和稳定的网络身份(不管怎么删, 名字都不变, ip 会变). pod 被部署的时候其名字是按照<code>&lt;statefulset name&gt;-&lt;ordinal index&gt;</code>创建的(web-0, web-1). 这个名字就是他们的hostname.(这是默认行为)</li><li>pod 不管怎么删, pod与pvc之间的关系都不会混乱.</li><li>pod 之间可以通过固定的域名互相访问.</li><li>启动 pod 的时候, 按照索引, 从 0 开始. 一个一个启动. 不会同时启动多个. (这是默认行为)</li><li>删除 pod 的时候, 按照索引倒序删除, 从最后一个开始, 一个一个删除. 不会同时删除多个. (这是默认行为)</li><li>删除 pod 的时候, pvc 和 pv 并不会删除.</li><li>Service 必须是 headless .</li></ul><blockquote><p>你可以通过设置statefulset管理策略, 来改变上述默认行为</p><p><a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy">https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy</a></p></blockquote><h2 id="扩展-缩放">扩展/缩放</h2><p>扩展: <code>kubectl scale sts web --replicas=5</code></p><p>缩放: <code>kubectl patch sts web -p '&#123;&quot;spec&quot;:&#123;&quot;replicas&quot;:3&#125;&#125;'</code></p><p>当然你也可以通过修改配置文件里 replicas 的值来进行变更.</p><h2 id="更新">更新</h2><p>有状态应用的更新和无状态应用的更新有些差异.</p><p>有状态应用的更新策略(spec.updateStrategy)是 RollingUpdate 和 OnDelete. 其中 RollingUpdate 是默认策略, 这个是自动滚动更新. 而OnDelete的意思是只有你手动删除了pod才会更新.</p><p>更新是针对pod模板的. 例如:</p><pre><code class="language-bash">kubectl patch statefulset web --type='json' -p='[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;gcr.io/google_containers/nginx-slim:0.8&quot;&#125;]'</code></pre><blockquote><p>这里更新了nginx的镜像.</p></blockquote><p>更新遵循以下原则:</p><ul><li><p>所有的 pod 需要是就绪状态.</p></li><li><p>更新采用索引倒序进行(即从索引最后一个号码开始更新), 并且是一个一个的更新.</p></li><li><p>当更新失败的时候, 已经收到更新的pod将保持更新后的版本, 没有开始更新的pod将恢复到更新前的版本.</p></li></ul><p>🌟需要注意的是, 如果你的更新文件(二进制文件故障或者配置文件)有问题从而导致更新失败, 你可能需要强制回滚. 以下是说明信息:</p><p>即你恢复了更新前的模板,却发现statefulset依然不正常, 则你需要<strong>手动删除</strong>所有由错误模板产出的pod.</p><p>在这之后, statefulset将会采用更新前的模板重建pod.</p><p>(<a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#forced-rollback">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#forced-rollback</a>)</p><h2 id="阶段更新-staging-an-update">阶段更新(staging an update)</h2><p>有时候我们可能并不想一次更新所有, 此时可以进行阶段更新.</p><p>阶段更新的意思就是通过在索引上设置一个点. 当pod的索引大于等于这个点的时候, 才会更新. (默认点是索引0)</p><pre><code class="language-bash">kubectl patch statefulset web -p '&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:3&#125;&#125;&#125;&#125;'</code></pre><p>这个意思是设置索引3为阶段分割点.</p><p>如果你想回到默认更新, 则只需要调整分割点, 重新执行上述命令即可.</p><h2 id="删除">删除</h2><p>删除分为联级删除和非联级删除.</p><p>联级删除就是 statefulset 和 pod 都删除. 这是默认行为.</p><p>非联级删除, 只会删除 statefulset. 你可以通过删除的时候附加<code>--cascade=false</code>开启它.</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> StatefulSet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞12守护进程</title>
      <link href="posts/63d8d3d4/"/>
      <url>posts/63d8d3d4/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>DaemonSet 确保指定的node上都存在一份 pod. 常用来构建一些批量代理性质的app. 例如监控节点app, 或者日志节点app</p><h2 id="特点">特点</h2><ul><li>当一个节点添加到集群, 或从集群中删除, 则 pod 会被添加/删除</li><li>当DaemonSet被删除, 则 pod 都会被删除.</li><li>pod模板中的 RestartPolicy 必须是默认值, 也就是 Always.</li><li>创建后.spec.selector 不可修改.</li><li>默认调度在所有符合条件的node上.</li></ul><h2 id="一个例子">一个例子</h2><pre><code class="language-yaml">apiVersion: apps/v1kind: DaemonSetmetadata:  name: fluentd-elasticsearch  namespace: kube-system  labels:    k8s-app: fluentd-loggingspec:  selector:    matchLabels:      name: fluentd-elasticsearch  template:    metadata:      labels:        name: fluentd-elasticsearch    spec:      tolerations:      # this toleration is to have the daemonset runnable on master nodes      # remove it if your masters can't run pods      - key: node-role.kubernetes.io/master        effect: NoSchedule      containers:      - name: fluentd-elasticsearch        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2        resources:          limits:            memory: 200Mi          requests:            cpu: 100m            memory: 200Mi        volumeMounts:        - name: varlog          mountPath: /var/log        - name: varlibdockercontainers          mountPath: /var/lib/docker/containers          readOnly: true      terminationGracePeriodSeconds: 30      volumes:      - name: varlog        hostPath:          path: /var/log      - name: varlibdockercontainers        hostPath:          path: /var/lib/docker/containers</code></pre><p>这是一个日志采集pod. 它将k8s集群节点的 /var/log 和 /var/lib/docker/containers 挂载到 pod 中. 这样, elasticsearch 就可以获取到k8s集群所有节点的信息了.</p><h2 id="pod运行在部分node上">pod运行在部分node上</h2><p>你有两种方式处理pod的部分调度.</p><h3 id="通过节点标签-nodeSelector-来选择">通过节点标签(nodeSelector)来选择.</h3><p><code>.spec.template.spec.nodeSelector</code>可以让你仅在匹配的node上创建pod.</p><p>例如: 仅在拥有ssd磁盘的节点上构建</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  nodeSelector:    disktype: ssd</code></pre><h3 id="通过节点亲和性-nodeAffinity-来选择">通过节点亲和性(nodeAffinity)来选择</h3><p><code>.spec.affinity.nodeAffinity</code> , 它有以下属性: (每一个属性由两部分意思组成)</p><p>requiredDuringScheduling<strong>IgnoredDuringExecution</strong><br>表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。</p><p>requiredDuringScheduling<strong>RequiredDuringExecution</strong><br>表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中RequiredDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点。</p><p>preferredDuringScheduling<strong>IgnoredDuringExecution</strong><br>表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。</p><p>preferredDuringScheduling<strong>RequiredDuringExecution</strong><br>表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: with-node-affinityspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: kubernetes.io/e2e-az-name            operator: In            values:            - e2e-az1            - e2e-az2      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: another-node-label-key            operator: In            values:            - another-node-label-value  containers:  - name: with-node-affinity    image: gcr.io/google_containers/pause:2.0</code></pre><p>这个配置的意思是pod<strong>必须被调度</strong><a href="http://xn--kubernetes-m11qu01s34wc.io/e2e-az-name=e2e-az1%E6%88%96%E8%80%85kubernetes.io/e2e-az-name=e2e-az2">到标签kubernetes.io/e2e-az-name=e2e-az1或者kubernetes.io/e2e-az-name=e2e-az2</a> 的节点。与此同时, 还将在上述条件的基础上, <strong>优先调度</strong>到额外拥有标签为another-node-label-key=another-node-label-value的节点上。</p><h2 id="污点和容忍度Taint-and-toleration">污点和容忍度Taint-and-toleration</h2><p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</a></p><p>k8s通过污点来剔除某些故障节点, 或构建一些专属节点.</p><p>k8s通过容忍度来确保某些pod可以忽略污点.</p><p>在正常情况下. 没有任何容忍度的pod不会被调度到存在污点的node上. (当出现节点故障时候, 默认会附加一个容忍度, 容忍度失效300秒)</p><p>但DaemonSet却需要忽略这些污点, 以便于pod长存. (例如你构建的监控pod)</p><p>以下是DaemonSet自动附加的容忍度, 它将忽略这些污点. (截至到v1.19)</p><pre><code class="language-bash">node.kubernetes.io/not-readynode.kubernetes.io/unreachablenode.kubernetes.io/disk-pressurenode.kubernetes.io/memory-pressurenode.kubernetes.io/unschedulablenode.kubernetes.io/network-unavailable</code></pre><p>上述情况基本保证了DaemonSet的pod不会因各种意外情况导致pod被驱逐.</p><h2 id="访问DaemonSet-pod">访问DaemonSet pod</h2><p>你可以通过构建一个 Headless Service 并通过 endpoint  来访问各个 pod.</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> DaemonSet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞11-2namespace资源约束</title>
      <link href="posts/ed4c29d5/"/>
      <url>posts/ed4c29d5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://kubernetes.io/docs/concepts/policy/limit-range/">https://kubernetes.io/docs/concepts/policy/limit-range/</a></p><p>namespace 资源约束(LimitRange)防止pod的配置超出预期.从而让我们可以更方便的管理资源.</p><h2 id="特点">特点</h2><ul><li>提供 pod 的默认资源约束.</li><li>限制pod的资源约束.</li><li>LimitRange配置策略的构建和变更仅影响之后的pod</li></ul><h2 id="流程">流程</h2><h3 id="构建LimitRange">构建LimitRange</h3><pre><code class="language-yaml">apiVersion: v1kind: LimitRangemetadata:  name: cpu-mem-storage-min-max-defaultspec:  limits:  - type: Container    max:      cpu: &quot;1000m&quot;      memory: &quot;1G&quot;    min:      cpu: &quot;50m&quot;      memory: &quot;50M&quot;    default:      cpu: &quot;250m&quot;      memory: &quot;250M&quot;    defaultRequest:      cpu: &quot;50m&quot;      memory: &quot;50M&quot;  - type: PersistentVolumeClaim    max:      storage: 30G    min:      storage: 8G</code></pre><pre><code class="language-bash">kubectl apply -f limitrange-default.yaml --namespace=default</code></pre><p>上述例子为 default 命名空间加了一个资源策略.效果如下:</p><ul><li><p>当pod没有任何限制的时候, pod 规则如下遵循 default 和 defaultRequest 的属性</p><ul><li><p>limit: cpu=250m mem=250M</p></li><li><p>request: cpu=50m mem=50M</p></li><li><p>8G &lt; pvc &lt; 30G</p><p>会全部走limitrange的默认值</p></li></ul></li><li><p>当pod设置了limit: cpu=1000m mem=1G的时候, pod规则如下</p><ul><li><p>limit: cpu=1000m mem=1G</p></li><li><p>request: cpu=1000m mem=1G</p></li><li><p>8G &lt; pvc &lt; 30G</p><p>这里比较奇葩, 官方就是这么设定的. 虽然你没设置request, 但是request 不会走默认值, 会继承 limit 的自定义值</p></li></ul></li><li><p>当pod只设置了request: cpu=100m mem=150M的时候, pod规则如下</p><ul><li><p>limit: cpu=250m mem=250M</p></li><li><p>request: cpu=100m mem=150M</p></li><li><p>8G &lt; pvc &lt; 30G</p><p>这里符合大家的逻辑…没设置就是走默认.</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> namespace </tag>
            
            <tag> 资源约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞11-1pod资源约束</title>
      <link href="posts/64fa9c84/"/>
      <url>posts/64fa9c84/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a></p><p>资源约束在我们平时使用的时候主要是cpu和内存层面, 以及本地临时存储(emptyDir)</p><p>k8s资源约束分为两种: request(软约束) 和 limits(硬约束)</p><p>request(软约束)确保了pod中的容器至少可以使用这么多资源. 不过当节点如果没有其它容器,则此容器可以突破request限制.</p><p>limits(硬约束) 则使硬性的限制了容器资源上限. 如果容器请求的内存大于了limits,则会收到oom错误. 如果容器请求的cpu大于了limits, 则不会产出错误, 因为cpu对于程序来说,它不是一个硬性指标.如果cpu资源不够,只会导致程序卡顿到死…</p><p>最后,k8s会严格按照pod定义的资源限制进行调度,即时某个节点上有大量空闲资源,但只要空闲资源不能满足pod的资源定义,就不能调度到这个节点上.</p><h2 id="写法">写法</h2><ul><li><code>spec.containers[].resources.limits.cpu </code>cpu限制</li><li><code>spec.containers[].resources.limits.memory</code> 内存限制</li><li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code>   不常用, 可以忽略</li><li><code>spec.containers[].resources.limits.ephemeral-storage</code> empty临时存储限制</li><li><code>spec.containers[].resources.requests.cpu</code></li><li><code>spec.containers[].resources.requests.memory</code></li><li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt; </code> 不常用, 可以忽略</li><li><code>spec.containers[].resources.requests.ephemeral-storage</code></li></ul><h2 id="单位">单位</h2><p>k8s将一个超线程称为一个vcpu. 1vcpu=1000m. 我们在定义资源限制时, 应该始终用 m 作为单位.假设你限制0.5个vcpu,则填写500m.</p><p>k8s的内存和临时存储单位和平时我们所用的没什么区别. 你只需要记住 K/M/G/T/P/E 这些即可. 例如, 100M就是100兆</p><p>一个例子:</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: frontendspec:  containers:  - name: app    image: images.my-company.example/app:v4    resources:      requests:        memory: &quot;64M&quot;        cpu: &quot;250m&quot;        ephemeral-storage: &quot;2Gi&quot;      limits:        memory: &quot;128M&quot;        cpu: &quot;500m&quot;        ephemeral-storage: &quot;4Gi&quot;  - name: log-aggregator    image: images.my-company.example/log-aggregator:v6    resources:      requests:        memory: &quot;64M&quot;        cpu: &quot;250m&quot;        ephemeral-storage: &quot;2Gi&quot;      limits:        memory: &quot;128M&quot;        cpu: &quot;500m&quot;        ephemeral-storage: &quot;4Gi&quot;</code></pre><h2 id="资源限制如何运作">资源限制如何运作</h2><p>k8s会通过kubelet将pod定义的资源限制传递给容器.</p><p>如果你容器使用的是docker.</p><p>cpu软限制将对标docker的–cpu-shares. 而cpu硬限制会告诉容器每100ms可以使用的CPU时间总量是  limits.cpu * 100.</p><blockquote><p>关于docker的–cpu-shares, 可以参考https://docs.docker.com/engine/reference/run/#cpu-share-constraint</p><p>总的来说, --cpu-shares 会让容器按照所设定的分值比例去使用cpu.不过, 在多核心节点上, 这个规则又不是很适用. 按照官方的说法, 当多核心cpu的时候,它的规则是:</p><p>On a multi-core system, the shares of CPU time are distributed over all CPU cores. Even if a container is limited to less than 100% of CPU time, it can use 100% of each individual CPU core.</p><p>For example, consider a system with more than three cores. If you start one container <code>&#123;C0&#125;</code> with <code>-c=512</code> running one process, and another container <code>&#123;C1&#125;</code> with <code>-c=1024</code> running two processes, this can result in the following division of CPU shares:</p><pre><code>PID    containerCPUCPU share100    &#123;C0&#125;0100% of CPU0101    &#123;C1&#125;1100% of CPU1102    &#123;C1&#125;2100% of CPU2</code></pre><p>这里三个容器,都是单核心程序</p></blockquote><p>内存的限制没有什么特别需要注意的.</p><h2 id="容器中的可见资源">容器中的可见资源</h2><p>默认情况下,不管你如何设置资源限制,容器里的可见资源都等于节点资源.也就是说你在容器里查看/proc/meminfo显示的资源并不是你设置的.这个问题会带来很多麻烦.</p><p>因为很多程序的参数都是根据可见资源来设定的.例如nginx的auto, 或者jvm里的内存参数.</p><blockquote><p>从 Java 8u191开始, jvm 已经默认实现了容器感知( -XX:+UseContainerSupport). 因此无需安装下面的 lxcfs 方案.</p><p>并且, 在容器中建议只设置如下内存参数:</p><p>-XX:MaxRAMPercentage  最大堆内存=物理内存百分比, 建议为容器限制的50%-75% . 毕竟还需要预留其它内存.</p></blockquote><p>而社区常见的作法是用<code>lxcfs</code> 来提升资源的可见性.<code>lxcfs</code> 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器, 它也可以支持Docker容器.</p><p>LXCFS通过用户态文件系统, 在容器中提供下列 <code>procfs</code> 的文件.</p><pre><code class="language-bash">/proc/cpuinfo/proc/diskstats/proc/meminfo/proc/stat/proc/swaps/proc/uptime</code></pre><p>与我们资源限制有关的, 主要是 cpuinfo 和 meminfo.</p><p>目前社区的做法如下:</p><ol><li><p>所有节点安装 fuse-libs 包.</p><pre><code class="language-bash">yum install -y fuse-libs</code></pre></li><li><p>安装部署lxcfs</p><pre><code class="language-bash">git clone https://github.com/denverdino/lxcfs-admission-webhook.gitcd lxcfs-admission-webhookvim deployment/lxcfs-daemonset.yaml=== 修正配置里的 apiVersion=== 当前git里的代码是陈旧的...代码里的版本在 1.18 已经被废弃). 具体归属于什么版本, 请参考k8s官方api文档 === https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#daemonset-v1-appsapiVersion:apps/v1  kubectl apply -f deployment/lxcfs-daemonset.yamlkubectl get pod#等待 lxcfs pod 状态成 running#如果你发现 pod 始终出错，那么执行下列命令：kubectl delete -f deployment/lxcfs-daemonset.yamlrm -rf /var/lib/lxcfskubectl apply -f deployment/lxcfs-daemonset.yamldeployment/install.shkubectl get secrets,pods,svc,mutatingwebhookconfigurations=== 查看各个对象状态</code></pre></li><li><p>给相关namespace注入lxcfs，例如default</p><pre><code class="language-bash">kubectl label namespace default lxcfs-admission-webhook=enabled</code></pre></li><li><p>重启添加了资源限制的pod, 此时 /proc/cpuinfo 和 /proc/meminfo 将会正确显示.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> pod </tag>
            
            <tag> 资源约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞09应用配置与密码与信息提供</title>
      <link href="posts/4b0e4a5/"/>
      <url>posts/4b0e4a5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>kubernetes 可以以环境变量或者以文件的方式提供信息给容器。这些信息存放在 secret 和 configmap 对象里。</p><p>所以步骤一般是这样的。</p><ol><li>创建secret 和 configmap 对象</li></ol><h2 id="以文件方式提供">以文件方式提供</h2><p>kubernetes 提供了一种卷类型 projected volume。</p><p>写法：</p><pre><code class="language-yaml:">        volumeMounts:        - name: all-in-one          mountPath: /etc/allinfo          readonly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: user      - secret:          name: pass      - configmap:          name: content      - downwardAPI:          items:            - path: &quot;labels&quot;              fieldRef:                fieldPath: metadata.labels            - path: &quot;annotations&quot;              fieldRef:                fieldPath: metadata.annotations</code></pre><blockquote><p>user, pass, content, labels, annotations 将会以文件形式存在于容器挂载点 /etc/allinfo 内。</p></blockquote><p>创建 secret: user 和 secret: pass</p><pre><code class="language-bash">echo -n &quot;admin&quot; &gt; ./username.txtkubectl create secret generic user --from-file=./username.txtecho -n &quot;1f2d1e2e67df&quot; &gt; ./username.txtkubectl create secret generic pass --from-file=./password.txt</code></pre><p>创建 configmap: content</p><pre><code class="language-bash">echo -n &quot;userinfo&quot; &gt; ./content.txtkubectl create configmap content --from-file=./content.txt</code></pre><p>downwardAPI 内容直接写入</p><h2 id="配置ConfigMap">配置ConfigMap</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/configmap/">https://kubernetes.io/docs/concepts/configuration/configmap/</a></p><p>ConfigMap 常用在两种情况中. 第一种是提供环境变量.第二种是提供配置文件</p><p>一个简单配置例子如下</p><pre><code class="language-yaml">kind: ConfigMapapiVersion: v1metadata:  name: cm-demo  namespace: defaultdata:  data.1: hello  data.2: world  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>上述例子中, 包含的内容有</p><p>环境变量方式:</p><pre><code class="language-yaml">  data.1: hello  data.2: world</code></pre><p>配置文件方式:</p><pre><code class="language-yaml">  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>nginx.config 是文件名, 管道符 | 下面是文件内容.</p><p>需要注意的是, 内容依然要遵循 yaml 的缩进规则</p><p>使用方式如下:</p><pre><code class="language-yaml">    spec:      containers:      - name: nginx        image: nginx        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $&#123;DATA.1&#125; $&#123;DATA.2&#125;&quot;]        ports:        - containerPort: 80        env:        - name: DATA1          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.1        - name: DATA2          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.2        volumeMounts:        - name: cm-demo-vc          subPath: nginx.conf          mountPath: /etc/nginx/nginx.conf      volumes:      - name: cm-demo-vc        configMap:          name: cm-demo</code></pre><p>配置文件数据卷方式:</p><ul><li>通过在 spec.volumes 中定义一个关联声明(cm.demo-vc). 它一端被 spec.containers.volumeMounts 调用, 另一端关联 configMap. 然后通过 spec.containers.volumeMounts 绑定 configMap 后, 就可以使用 subPath 调用配置文件, 并使用 mountPath 挂载到容器里的具体路径上.</li><li>这种方式下,configMap更新后,pod内挂载的也会同时更新.</li></ul><p>环境变量方式:</p><ul><li>通过在 spec.containers.env 中定义. 例子中, DATA1 是环境变量的键, DATA1的值通过valueFrom定义.最终,你可以在容器中使用环境变量DATA1和DATA2</li></ul><h2 id="密码Secret">密码Secret</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a></p><p>secret对象里的值需要写入加密后的.</p><p>secret对象也可以被当作文件挂载或环境变量注入</p><p>一个简单例子:</p><p>我们定义一个用户密码对,分别时username和password</p><p>secret对象要求值必须进行base64编码加密(当type为Opaque的时候).</p><pre><code class="language-bash">[root@k8s00 test-yaml]# echo -n &quot;admin&quot; | base64YWRtaW4=[root@k8s00 test-yaml]# echo -n &quot;admin321&quot; | base64YWRtaW4zMjE=</code></pre><pre><code class="language-yaml"># 创建 secret 对象 mysecretapiVersion: v1kind: Secretmetadata:  name: mysecret  namespace: defaulttype: Opaquedata:  username: YWRtaW4=  password: MWYyZDFlMmU2N2Rm---apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    command:    - sleep    - &quot;3600&quot;    env:      - name: SECRET_USERNAME        valueFrom:          secretKeyRef:            name: mysecret            key: username      - name: SECRET_PASSWORD        valueFrom:          secretKeyRef:            name: mysecret            key: password    volumeMounts:    - name: foo      mountPath: &quot;/etc/foo&quot;      readOnly: true  volumes:  - name: foo    secret:      secretName: mysecret</code></pre><p>在通过上述配置创建好资源后,我们进入pod,进行测试.</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl exec -it mypod -- /bin/sh===可以看到两个加密信息生成了两个软连接,并指向了隐藏文件/ # ls -l /etc/foototal 0lrwxrwxrwx    1 root     root            15 Sep 17 07:18 password -&gt; ..data/passwordlrwxrwxrwx    1 root     root            15 Sep 17 07:18 username -&gt; ..data/username===环境变量/ # echo $&#123;SECRET_USERNAME&#125;admin===文件方式/ # cat /etc/foo/usernameadmin/ # </code></pre><h2 id="限制">限制</h2><ul><li>机密资源限制在同一命名空间</li><li>数据大小不能超过1MB</li><li>当pod引用不存在的机密时,pod无法启动</li></ul><h2 id="其它">其它</h2><ol><li><p>不可变的机密(k8s v1.19以上)</p><pre><code class="language-yaml">secret.immutable: true</code></pre><p>这种方式用来构建永久不可变的密码,且构建后无法更改,且构建后使用它的pod也无法卸载掉它.</p><p>因此,一旦构建后想更改,则它和它关联的pod都要删除重建.</p><p>优点:</p><ul><li>避免错误的更新</li><li>显著提高集群性能</li></ul></li><li><p>命令方式创建</p><p>kubectl create configmap/secret <name> xxx</name></p><p>这里xxx可以用两种方式:</p><ul><li>–from-literal=<key>=<value> 指定kv</value></key></li><li>–from-file=&lt;文件/目录&gt; 当为目录的时候,会递归将目录里的文件都写入对象中</li></ul></li><li><p>隐藏的密码信息文件</p><p>你可以将 secret.data.<key> 写成 secret.data.&lt;.key&gt; 来隐藏它.</key></p></li></ol><h2 id="信息提供Downward-API">信息提供Downward API</h2><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></p><p>一个比较合适的解决方案是采用Downward API传递信息到容器. 然后程序根据信息来设置.</p><p>一个简单的例子: 将资源限制数据挂载到容器/etc/podinfo</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: kubernetes-downwardapi-volume-example-2spec:  containers:    - name: client-container      image: k8s.gcr.io/busybox:1.24      command: [&quot;sh&quot;, &quot;-c&quot;]      args:      - while true; do          echo -en '\n';          if [[ -e /etc/podinfo/cpu_limit ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_limit; fi;          if [[ -e /etc/podinfo/cpu_request ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_request; fi;          if [[ -e /etc/podinfo/mem_limit ]]; then            echo -en '\n'; cat /etc/podinfo/mem_limit; fi;          if [[ -e /etc/podinfo/mem_request ]]; then            echo -en '\n'; cat /etc/podinfo/mem_request; fi;          sleep 5;        done;      resources:        requests:          memory: &quot;32Mi&quot;          cpu: &quot;125m&quot;        limits:          memory: &quot;64Mi&quot;          cpu: &quot;250m&quot;      volumeMounts:        - name: podinfo          mountPath: /etc/podinfo  volumes:    - name: podinfo      downwardAPI:        items:          - path: &quot;cpu_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.cpu              divisor: 1m          - path: &quot;cpu_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.cpu              divisor: 1m          - path: &quot;mem_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.memory              divisor: 1Mi          - path: &quot;mem_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.memory              divisor: 1Mi</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> ConfigMap </tag>
            
            <tag> Secret </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞10应用配置与密码与信息提供</title>
      <link href="posts/4b0e4a5/"/>
      <url>posts/4b0e4a5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>kubernetes 可以以环境变量或者以文件的方式提供信息给容器。这些信息存放在 secret 和 configmap 对象里。</p><p>所以步骤一般是这样的。</p><ol><li>创建secret 和 configmap 对象</li></ol><h2 id="以文件方式提供">以文件方式提供</h2><p>kubernetes 提供了一种卷类型 projected volume。</p><p>写法：</p><pre><code class="language-yaml:">        volumeMounts:        - name: all-in-one          mountPath: /etc/allinfo          readonly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: user      - secret:          name: pass      - configmap:          name: content      - downwardAPI:          items:            - path: &quot;labels&quot;              fieldRef:                fieldPath: metadata.labels            - path: &quot;annotations&quot;              fieldRef:                fieldPath: metadata.annotations</code></pre><blockquote><p>user, pass, content, labels, annotations 将会以文件形式存在于容器挂载点 /etc/allinfo 内。</p></blockquote><p>创建 secret: user 和 secret: pass</p><pre><code class="language-bash">echo -n &quot;admin&quot; &gt; ./username.txtkubectl create secret generic user --from-file=./username.txtecho -n &quot;1f2d1e2e67df&quot; &gt; ./username.txtkubectl create secret generic pass --from-file=./password.txt</code></pre><p>创建 configmap: content</p><pre><code class="language-bash">echo -n &quot;userinfo&quot; &gt; ./content.txtkubectl create configmap content --from-file=./content.txt</code></pre><p>downwardAPI 内容直接写入</p><h2 id="配置ConfigMap">配置ConfigMap</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/configmap/">https://kubernetes.io/docs/concepts/configuration/configmap/</a></p><p>ConfigMap 常用在两种情况中. 第一种是提供环境变量.第二种是提供配置文件</p><p>一个简单配置例子如下</p><pre><code class="language-yaml">kind: ConfigMapapiVersion: v1metadata:  name: cm-demo  namespace: defaultdata:  data.1: hello  data.2: world  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>上述例子中, 包含的内容有</p><p>环境变量方式:</p><pre><code class="language-yaml">  data.1: hello  data.2: world</code></pre><p>配置文件方式:</p><pre><code class="language-yaml">  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>nginx.config 是文件名, 管道符 | 下面是文件内容.</p><p>需要注意的是, 内容依然要遵循 yaml 的缩进规则</p><p>使用方式如下:</p><pre><code class="language-yaml">    spec:      containers:      - name: nginx        image: nginx        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $&#123;DATA.1&#125; $&#123;DATA.2&#125;&quot;]        ports:        - containerPort: 80        env:        - name: DATA1          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.1        - name: DATA2          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.2        volumeMounts:        - name: cm-demo-vc          subPath: nginx.conf          mountPath: /etc/nginx/nginx.conf      volumes:      - name: cm-demo-vc        configMap:          name: cm-demo</code></pre><p>配置文件数据卷方式:</p><ul><li>通过在 spec.volumes 中定义一个关联声明(cm.demo-vc). 它一端被 spec.containers.volumeMounts 调用, 另一端关联 configMap. 然后通过 spec.containers.volumeMounts 绑定 configMap 后, 就可以使用 subPath 调用配置文件, 并使用 mountPath 挂载到容器里的具体路径上.</li><li>这种方式下,configMap更新后,pod内挂载的也会同时更新.</li></ul><p>环境变量方式:</p><ul><li>通过在 spec.containers.env 中定义. 例子中, DATA1 是环境变量的键, DATA1的值通过valueFrom定义.最终,你可以在容器中使用环境变量DATA1和DATA2</li></ul><h2 id="密码Secret">密码Secret</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a></p><p>secret对象里的值需要写入加密后的.</p><p>secret对象也可以被当作文件挂载或环境变量注入</p><p>一个简单例子:</p><p>我们定义一个用户密码对,分别时username和password</p><p>secret对象要求值必须进行base64编码加密(当type为Opaque的时候).</p><pre><code class="language-bash">[root@k8s00 test-yaml]# echo -n &quot;admin&quot; | base64YWRtaW4=[root@k8s00 test-yaml]# echo -n &quot;admin321&quot; | base64YWRtaW4zMjE=</code></pre><pre><code class="language-yaml"># 创建 secret 对象 mysecretapiVersion: v1kind: Secretmetadata:  name: mysecret  namespace: defaulttype: Opaquedata:  username: YWRtaW4=  password: MWYyZDFlMmU2N2Rm---apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    command:    - sleep    - &quot;3600&quot;    env:      - name: SECRET_USERNAME        valueFrom:          secretKeyRef:            name: mysecret            key: username      - name: SECRET_PASSWORD        valueFrom:          secretKeyRef:            name: mysecret            key: password    volumeMounts:    - name: foo      mountPath: &quot;/etc/foo&quot;      readOnly: true  volumes:  - name: foo    secret:      secretName: mysecret</code></pre><p>在通过上述配置创建好资源后,我们进入pod,进行测试.</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl exec -it mypod -- /bin/sh===可以看到两个加密信息生成了两个软连接,并指向了隐藏文件/ # ls -l /etc/foototal 0lrwxrwxrwx    1 root     root            15 Sep 17 07:18 password -&gt; ..data/passwordlrwxrwxrwx    1 root     root            15 Sep 17 07:18 username -&gt; ..data/username===环境变量/ # echo $&#123;SECRET_USERNAME&#125;admin===文件方式/ # cat /etc/foo/usernameadmin/ # </code></pre><h2 id="限制">限制</h2><ul><li>机密资源限制在同一命名空间</li><li>数据大小不能超过1MB</li><li>当pod引用不存在的机密时,pod无法启动</li></ul><h2 id="其它">其它</h2><ol><li><p>不可变的机密(k8s v1.19以上)</p><pre><code class="language-yaml">secret.immutable: true</code></pre><p>这种方式用来构建永久不可变的密码,且构建后无法更改,且构建后使用它的pod也无法卸载掉它.</p><p>因此,一旦构建后想更改,则它和它关联的pod都要删除重建.</p><p>优点:</p><ul><li>避免错误的更新</li><li>显著提高集群性能</li></ul></li><li><p>命令方式创建</p><p>kubectl create configmap/secret <name> xxx</name></p><p>这里xxx可以用两种方式:</p><ul><li>–from-literal=<key>=<value> 指定kv</value></key></li><li>–from-file=&lt;文件/目录&gt; 当为目录的时候,会递归将目录里的文件都写入对象中</li></ul></li><li><p>隐藏的密码信息文件</p><p>你可以将 secret.data.<key> 写成 secret.data.&lt;.key&gt; 来隐藏它.</key></p></li></ol><h2 id="信息提供Downward-API">信息提供Downward API</h2><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></p><p>一个比较合适的解决方案是采用Downward API传递信息到容器. 然后程序根据信息来设置.</p><p>一个简单的例子: 将资源限制数据挂载到容器/etc/podinfo</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: kubernetes-downwardapi-volume-example-2spec:  containers:    - name: client-container      image: k8s.gcr.io/busybox:1.24      command: [&quot;sh&quot;, &quot;-c&quot;]      args:      - while true; do          echo -en '\n';          if [[ -e /etc/podinfo/cpu_limit ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_limit; fi;          if [[ -e /etc/podinfo/cpu_request ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_request; fi;          if [[ -e /etc/podinfo/mem_limit ]]; then            echo -en '\n'; cat /etc/podinfo/mem_limit; fi;          if [[ -e /etc/podinfo/mem_request ]]; then            echo -en '\n'; cat /etc/podinfo/mem_request; fi;          sleep 5;        done;      resources:        requests:          memory: &quot;32Mi&quot;          cpu: &quot;125m&quot;        limits:          memory: &quot;64Mi&quot;          cpu: &quot;250m&quot;      volumeMounts:        - name: podinfo          mountPath: /etc/podinfo  volumes:    - name: podinfo      downwardAPI:        items:          - path: &quot;cpu_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.cpu              divisor: 1m          - path: &quot;cpu_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.cpu              divisor: 1m          - path: &quot;mem_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.memory              divisor: 1Mi          - path: &quot;mem_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.memory              divisor: 1Mi</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> ConfigMap </tag>
            
            <tag> Secret </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞08负载均衡service</title>
      <link href="posts/d3b80b5f/"/>
      <url>posts/d3b80b5f/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>在实际业务中,因业务压力问题,经常会有多个后端服务副本,它们共同承担请求.我们使用云服务的时候,可以购买阿里云的slb或者aws的elb/alb等负载均衡器向这些后端副本分发流量. 并通过这些负载均衡向公网暴漏内网这些后端程序.</p><p>k8s设计了一个service对象来实现这一目的.</p><h2 id="k8sIP">k8sIP</h2><p>在提Service对象之前,需要先知道k8s中存在的三种IP.即 NodeIP, ClusterIP, PodIP</p><p>NodeIP 就是物理节点ip, 这个没得说, 玩家自己定义</p><p>ClusterIP 是k8s的一个虚拟ip, 本身没有任何实体, 也就是VIP</p><p>PodIP 是容器共享的一个网络命名空间对应的ip, 一个pod里的容器共用.</p><h2 id="Service">Service</h2><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/">https://kubernetes.io/zh/docs/concepts/services-networking/service/</a></p><p>Service对象通过servicespec.type来设定类型, 共计4个类型: ClusterIP, NodePort, LoadBalancer, ExternalName. 这四个类型可以分为两类</p><ul><li>创建Service对象供集群内部访问</li></ul><p>ClusterIP(默认类型): 反向代理集群内的pod. 供集群内其它服务访问. 流量过程是: 集群内部其它服务=&gt;ClusterIP:端口=&gt;Pod</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  name: myservicespec:  selector:    app: myapp  ports:  - protocol: TCP    port: 80    targetPort: 8080    name: myapp-http</code></pre><blockquote><p>上述例子中, 集群内部访问myservice.default的时候,此时会访问到app=myapp的pod</p><p>这里port是Service暴露端口, targetPort是pod暴露端口. 默认情况下, targetPort将等于port</p><p>spec.type没有定义是因为ClusterIP是Service对象的默认类型</p></blockquote><p>ExternalName: 构建一个CNAME解析(service对象-CNAME-其它域名).  我能想到的主要是让集群内部服务可以访问到集群外部服务.</p><p>例如:</p><pre><code class="language-yaml">kind: ServiceapiVersion: v1metadata:  name: my-service  namespace: prodspec:  type: ExternalName  externalName: my.database.example.com</code></pre><blockquote><p>集群内部访问my-service.prod的时候, <a href="http://xn--k8sdnsmy-vq8m536awioci6an95by58d3far6b.database.example.com">将通过k8s的dns服务返回my.database.example.com</a></p></blockquote><ul><li>供集群外部访问</li></ul><p>NodePort: 反向代理集群内的pod(使用NAT在每一个集群node上的相同端口上公开Service, 是ClusterIP类型的超集). 供集群外服务访问. 流量过程是: 集群外=&gt;任意节点ip:端口=&gt;ClusterIP:端口=&gt;Pod</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  name: myservicespec:  selector:    app: myapp  type: NodePort  ports:  - protocol: TCP    nodePort: 31000    port: 80    targetPort: 80    name: myapp-http</code></pre><blockquote><p>集群外部此时可以访问任意物理节点ip:31000, 此时可以访问到集群内部app=myapp的pod.</p><p>这里nodePort是物理节点暴露端口, port是Service暴露端口, targetPort是pod暴露端口.</p><p>nodePort可以不定义, 则此时会自动从30000-32767随机分配.</p></blockquote><p>LoadBalancer: 对接云商的负载均衡服务, 给Service分配一个固定IP. 方便云服务商的LB服务绑定. (是NodePort类型的超集). 流量过程是: 集群外=&gt;云服务LB=&gt;任意物理节点ip:端口=&gt;ClusterIP:端口=&gt;Pod</p><p>这种类型建议直接参考官方文档: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer</a></p><h2 id="Headless-Service">Headless Service</h2><p>这是一种特殊的对象, 从配置上来看,它和ClusterIP类型的区别就是<code>service.spec.clusterIP: None</code>.</p><p>它具有以下特点:</p><ul><li>结合StatefulSet对象使用, 从而创建一连串名称有序的有状态应用副本.</li><li>pod拥有整个生命周期内唯一且不变的域名. 因此 pod 之间可以通过域名访问.它的格式应该是这样的:<code>&lt;podname&gt;-&lt;index&gt;.&lt;svcname&gt;.&lt;nsname&gt;.svc.cluster.local</code></li></ul><p>一个关于zk集群构建的k8s官方例子: <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/">https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/</a></p><h2 id="粘性会话">粘性会话</h2><p>有些时候,我们需要会话黏性,你可以通过service.spec.sessionAffinity=ClientIP来设置.并同时可以通过service.spec.sessionAffinityConfig.clientIP.timeoutSeconds来设置会话保持时间,它默认是3小时.</p><h2 id="多端口">多端口</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services">https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services</a></p><p>有些时候,某个服务可能同时存在多个端口,例如web服务,会同时有80和443.这种情况下你需要针对两个端口分别设定一个端口名称.即 <a href="http://service.spec.ports.name">service.spec.ports.name</a></p><h2 id="环境变量">环境变量</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables">https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables</a></p><p>当service创建的时候,kubelet会生成一批环境变量.你可以在其它pod中使用这些环境变量访问service.不过前提是service对象必须提前创建.</p><p>不过,我觉得一般来说,有dns方式,都会通过dns去访问.而且dns也没有先创建service这一个限制.</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞07无状态服务deployment-pod扩缩</title>
      <link href="posts/f6e328b5/"/>
      <url>posts/f6e328b5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>k8s的pod缩放功能,和aws的auto scaling功能是一回事.虽然可能没有aws的auto scaling功能强大.</p><p>k8s的pod缩放功能称之为HPA(Horizontal Pod Autoscaler),它可以基于设定的cpu阈值,来自动调整deployment中的pod数量.</p><h2 id="metrics-server">metrics-server</h2><p><a href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a></p><p>metrics 服务器可以通过资源度量值 API 对外提供度量数据，Horizontal Pod Autoscaler 正是根据此 API 来获取度量数据.如果没有此服务,HPA将无法工作.</p><pre><code class="language-bash">wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml</code></pre><p>默认 metrics-server 的 deployment 无法直接使用, 我们需要添加几个参数,  来禁止 ca 认证和开通 dns</p><p>在 deployment.spec.template.spec.containers 中新加入下列配置:</p><pre><code class="language-yaml">        command:          - /metrics-server          - --kubelet-insecure-tls          - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname</code></pre><pre><code class="language-bash">kubectl apply -f components.yaml</code></pre><h2 id="hpa">hpa</h2><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</a></p><p>创建一个用于测试的web服务, hpa-test.yaml</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: php-apachespec:  selector:    matchLabels:      run: php-apache  replicas: 1  template:    metadata:      labels:        run: php-apache    spec:      containers:      - name: php-apache        image: k8s.gcr.io/hpa-example        ports:        - containerPort: 80        resources:          limits:            cpu: 500m          requests:            cpu: 200m---apiVersion: v1kind: Servicemetadata:  name: php-apache  labels:    run: php-apachespec:  ports:  - port: 80  selector:    run: php-apache</code></pre><p>给web服务开启hpa</p><p>命令方式:</p><pre><code class="language-bash">kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=5</code></pre><p>声明方式:</p><pre><code class="language-yaml">apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: php-apachespec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: php-apache  minReplicas: 1  maxReplicas: 5  targetCPUUtilizationPercentage: 50</code></pre><blockquote><p>这里的意思是, cpu 阈值50%, 最小pod数1, 最大pod数5. hpa会将所有pod的平均cpu利用率维持在50%, 并且pod数量在1~5的范围内波动</p></blockquote><p>查看当前cpu使用率</p><pre><code class="language-bash">kubectl get hpa===NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          57m</code></pre><p>添加测试用的客户端, 访问web服务, 增加web服务的cpu使用率</p><pre><code class="language-bash">kubectl run -it --rm load-generator --image=busybox /bin/shwhile true; do wget -q -O- http://php-apache; done=== 会输出大量OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!</code></pre><p>在经过一段时间等待后(不会超过1分钟, 默认监控抓取数据间隔时间是1分钟),我们可以看到 pod 数量发生变化</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          60m[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          60m[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   250%/50%   1         5         1          61m[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   250%/50%   1         5         1          61m[root@k8s00 test-yaml]# kubectl get deployment/php-apacheNAME         READY   UP-TO-DATE   AVAILABLE   AGEphp-apache   5/5     5            5           68m[root@k8s00 test-yaml]# </code></pre><p>现在,关闭客户端请求,等待1分钟以上.再次看hpa, 这时候cpu利用率已经下降</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   11%/50%   1         5         5          64m</code></pre><p>这时候,再看pod数量,它应该已经开始缩减,但是这个缩减并不是在cpu使用率下降之后就立即执行,而是内部有一个算法.它避免因立即执行从而导致资源出现反复波动.</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl get deployment php-apacheNAME         READY   UP-TO-DATE   AVAILABLE   AGEphp-apache   5/5     5            5           70m[root@k8s00 test-yaml]# kubectl describe deployment/php-apacheName:                   php-apacheNamespace:              defaultCreationTimestamp:      Wed, 16 Sep 2020 10:22:32 +0800Labels:                 &lt;none&gt;Annotations:            deployment.kubernetes.io/revision: 1Selector:               run=php-apacheReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailableStrategyType:           RollingUpdateMinReadySeconds:        0RollingUpdateStrategy:  25% max unavailable, 25% max surgePod Template:  Labels:  run=php-apache  Containers:   php-apache:    Image:      k8s.gcr.io/hpa-example    Port:       80/TCP    Host Port:  0/TCP    Limits:      cpu:  500m    Requests:      cpu:        200m    Environment:  &lt;none&gt;    Mounts:       &lt;none&gt;  Volumes:        &lt;none&gt;Conditions:  Type           Status  Reason  ----           ------  ------  Progressing    True    NewReplicaSetAvailable  Available      True    MinimumReplicasAvailableOldReplicaSets:  &lt;none&gt;NewReplicaSet:   php-apache-5c4f475bf5 (1/1 replicas created)Events:  Type    Reason             Age    From                   Message  ----    ------             ----   ----                   -------  Normal  ScalingReplicaSet  14m    deployment-controller  Scaled up replica set php-apache-5c4f475bf5 to 4  Normal  ScalingReplicaSet  14m    deployment-controller  Scaled up replica set php-apache-5c4f475bf5 to 5  Normal  ScalingReplicaSet  6m50s  deployment-controller  Scaled down replica set php-apache-5c4f475bf5 to 4  Normal  ScalingReplicaSet  4m49s  deployment-controller  Scaled down replica set php-apache-5c4f475bf5 to 1</code></pre><h2 id="其它">其它</h2><p>当前hpa的api版本是autoscaling/v1,它只能抓取cpu指标.如果你想抓取内存指标(当前也只支持内存)或自定义指标,那么需要将版本变更为autoscaling/v2beta2</p><p>autoscaling/v2beta2版本的配置有了一些变化,且可能随时会有新的变化.你可以在</p><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics</a></p><p>找到它的例子.</p><pre><code class="language-yaml">apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata:  name: php-apachespec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: php-apache  minReplicas: 1  maxReplicas: 10  metrics:  - type: Resource    resource:      name: cpu      target:        type: Utilization        averageUtilization: 50status:  observedGeneration: 1  lastScaleTime: &lt;some-time&gt;  currentReplicas: 1  desiredReplicas: 1  currentMetrics:  - type: Resource    resource:      name: cpu      current:        averageUtilization: 0        averageValue: 0</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞06无状态服务deployment-pod更新</title>
      <link href="posts/18e4fa73/"/>
      <url>posts/18e4fa73/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>对于更新,我们一般会采用滚动更新,即:</p><ul><li>先扩展一部分新版本</li><li>删除一部分老版本</li><li>重复上述行为,直至所有老版本被替换.</li></ul><p>对于回滚,我们想回滚到任何一个之前的版本.</p><p>而deployment对象可以帮我们实现这些.</p><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></p><h2 id="更新">更新</h2><p>一个更新nginx版本的例子. 首先,我们创建一个nginx的deployment</p><pre><code class="language-bash">kubectl apply -f nginx-dep.yaml===apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-dep  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        ports:        - containerPort: 80</code></pre><blockquote><p>这里dep的配置中spec.template.spec.container.image=nginx, 我没有设置具体版本</p></blockquote><p>此时,查看版本历史, 这里涉及到 rollout 指令, 回滚的时候也会用到.</p><pre><code class="language-bash">kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         &lt;none&gt;</code></pre><p>此时我们发现没有关于这个版本1的相关描述, 我们手动进行添加</p><pre><code class="language-bash">kubectl annotate deployment/nginx-dep kubernetes.io/change-cause=&quot;first&quot;kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         first</code></pre><blockquote><p>你也可以在执行写入命令的时候, 追加 --record 来将执行的命令写入到当前change-cause</p></blockquote><p>现在, 将所有deployment中所有nginx的image版本从默认变更为了1.19.2</p><pre><code class="language-bash">kubectl set image deployment/nginx-dep nginx=nginx:1.19.2 &amp;&amp; kubectl annotate deployment/nginx-dep kubernetes.io/change-cause=&quot;nginx image to 1.19.2&quot;kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         first2         nginx image to 1.19.2</code></pre><p>我们再生成一个版本1.18</p><pre><code class="language-bash">kubectl set image deployment/nginx-dep nginx=nginx:1.18 &amp;&amp; kubectl annotate deployment/nginx-dep kubernetes.io/change-cause=&quot;nginx image to 1.18&quot;kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         first2         nginx image to 1.19.23         nginx image to 1.18</code></pre><p>这样,我们总共有三个版本, 你可以通过追加版本号来看具体的版本内容</p><pre><code class="language-bash">kubectl rollout history deployment/nginx-dep --revision=&lt;num&gt;</code></pre><h2 id="回滚">回滚</h2><p>我们选择将nginx版本回滚到first第一个版本.</p><pre><code class="language-bash">kubectl rollout undo deployment/nginx-dep --to-revision=1kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE2         nginx image to 1.19.23         nginx image to 1.184         first</code></pre><h2 id="策略">策略</h2><p>deployment 的 .spec 中可以添加一些策略.</p><pre><code class="language-bash">spec:    minReadySeconds: 3  # pod 就绪时间, 在此时间之前, deploy 认为 pod 还没有准备好. 默认0  revisionHistoryLimit: 10 # 最大版本保留次数. 默认10   strategy:    type: rollingUpdate # 定义变更策略. 除了 rollingUpdate, 还可以是Recreate.Recreate指的是先删除所有pod,再创建.    rollingUpdate:      maxUnavailable: 30% # 变更期间最多存在多少个不可用的pod      maxSurge: 30% # 变更期间最多存在多少个超出已定义副本的pod数量</code></pre><blockquote><p>maxUnavailable 和 maxSurge 设置相等即可. 即开多少个新的, 就关多少个老的</p></blockquote><h2 id="小结">小结</h2><pre><code class="language-bash">kubectl rollout history # 查看对象历史kubectl rollout undo # 对象版本回退kubectl rollout status # 展示执行状态kubectl rollout pause # 暂停此次版本操作行为kubectl rollout resume # 恢复此次版本操作行为kubectl set image # 修改镜像配置kubectl annotate # 添加一个注释</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞常用命令</title>
      <link href="posts/e6ae9474/"/>
      <url>posts/e6ae9474/</url>
      
        <content type="html"><![CDATA[<h2 id="查询库表占用空间">查询库表占用空间</h2><pre><code class="language-mysql">SELECT    table_schema AS '数据库',    table_name AS '表名',    table_rows AS '记录数',    TRUNCATE (data_length / 1024 / 1024 / 1024, 3) AS '数据容量(GB)',    TRUNCATE (index_length / 1024 / 1024 / 1024 , 3) AS '索引容量(GB)',TRUNCATE (DATA_FREE / 1024 / 1024 / 1024, 3) AS '已占用空间但未使用(GB)'FROM    information_schema. TABLESWHERE    table_schema in ('new_data','megable_main','megable_active')ORDER BY    data_length DESC;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞05pod探测器</title>
      <link href="posts/9aa66899/"/>
      <url>posts/9aa66899/</url>
      
        <content type="html"><![CDATA[<h2 id="探测器">探测器</h2><blockquote><p><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes">https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes</a></p></blockquote><p>探测器有三种：存活探测器，就绪探测器，启动探测器</p><p>kubelet 使用存活探测器来知道什么时候要重启容器。 例如，存活探测器可以捕捉到死锁程序（进程在，但程序已无法进一步执行）。 这样的情况下重启容器有助于让应用程序在有问题的情况下更可用。</p><p>kubelet 使用就绪探测器可以知道容器什么时候准备好了并可以开始接受请求流量， 当一个 Pod 内的所有容器都准备好了，才能把这个 Pod 看作就绪了。 这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端。 在 Pod 还没有准备好的时候，会从 Service 的负载均衡器中被剔除的。</p><p>kubelet 使用启动探测器可以知道应用程序容器什么时候启动了。 如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查， 确保这些存活、就绪探测器不会影响应用程序的启动。 这可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。</p><h2 id="存活探测器-livenessProbe">存活探测器 livenessProbe</h2><p>目的在于确认容器是否健康，并在不健康的时候重启容器。它将在容器整个生命周期中运行。</p><p>探测器有三种探测方式，命令方式，http方式，tcp方式。具体可以上述官方文档</p><p>存活探测器包含两个时间段，一个是初始宽容期阶段 initialDelaySeconds，一个是初始阶段之后的探测间隔时间 periodSeconds</p><p>http方式例子如下：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  labels:    test: liveness  name: liveness-httpspec:  containers:  - name: liveness    image: k8s.gcr.io/liveness    args:    - /server    livenessProbe:      httpGet:        path: /healthz        port: 8080        httpHeaders:        - name: Custom-Header          value: Awesome      initialDelaySeconds: 3      periodSeconds: 3</code></pre><blockquote><p>探测器会认为 200&lt;=x&lt;400 的状态码为正常。</p></blockquote><h2 id="就绪探测器-readinessProbe">就绪探测器 readinessProbe</h2><p>目的在于仅确认容器是否健康，但是并不删除容器。它将在容器整个生命周期中运行。</p><p>例如：</p><pre><code class="language-yaml:">apiVersion: v1kind: Podmetadata:  name: goproxy  labels:    app: goproxyspec:  containers:  - name: goproxy    image: k8s.gcr.io/goproxy:0.1    ports:    - containerPort: 8080    readinessProbe:      tcpSocket:        port: 8080      initialDelaySeconds: 5      periodSeconds: 10    livenessProbe:      tcpSocket:        port: 8080      initialDelaySeconds: 15      periodSeconds: 20</code></pre><p>这里可以看到就绪探针的时间定义是要比存活探针提前的。即容器启动5秒后就进行就绪探测，容器启动15秒后进行存活探测。</p><h2 id="启动探测器-startupProbe">启动探测器 startupProbe</h2><p>目的在于设置一个容器启动宽容期。当启动探测器确认容器没问题后，后续的健康状态将交给存活探测器。</p><p>例如：</p><pre><code class="language-yaml">ports:- name: liveness-port  containerPort: 8080  hostPort: 8080livenessProbe:  httpGet:    path: /healthz    port: liveness-port  failureThreshold: 1  periodSeconds: 10startupProbe:  httpGet:    path: /healthz    port: liveness-port  failureThreshold: 30  periodSeconds: 10</code></pre><h2 id="最佳实践">最佳实践</h2><ol><li>存活和就绪探针的健康状态接口应该是两个互相独立的接口</li><li>存活探针的健康状态接口应该是直接返回状态结果，不应该有任何逻辑</li><li>就绪探针的健康状态接口应该提供就绪所需要的逻辑。但是不应该添加重新构建就绪的逻辑。例如：处理http请求的程序需要一个数据库连接的就绪状态，那么就应该在就绪探针的健康状态接口里添加数据库连接的检查逻辑。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞预警</title>
      <link href="posts/4ebbd3a8/"/>
      <url>posts/4ebbd3a8/</url>
      
        <content type="html"><![CDATA[<p>定义预警分为几个步骤</p><ol><li>定义预警媒介</li><li>定义预警用户</li><li>定义预警动作</li></ol><p>触发器达到阈值，调用预警动作，预警动作调用预警用户，预警用户调用预警媒介</p><h2 id="预警媒介">预警媒介</h2><p>配置-报警媒介类型，这里有两个要素</p><h3 id="报警媒介类型">报警媒介类型</h3><p>类型有很多种，比如邮件，自定义脚本，这里我们选自定义脚本，来定义一个企业微信预警</p><p><img src="/posts/4ebbd3a8/image-20200902144015196.png" alt="image-20200902144015196"></p><p>脚本需要接收三个参数：</p><p>{ALERT.SENDTO}：收件人</p><p>{ALERT.SUBJECT}：主题</p><p>{ALERT.MESSAGE}：内容</p><p>我们一般只用{ALERT.SENDTO}和{ALERT.MESSAGE}即可</p><h3 id="信息模板">信息模板</h3><p>定义信息模板，一般我们定义告警模板/恢复模板/自动注册模板</p><p>告警模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144408513.png" alt="image-20200902144408513"></p><pre><code>【[骷髅]】=》&#123;TRIGGER.SEVERITY&#125; 事件 : &#123;EVENT.ID&#125;【问题时间】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125; 【问题名字】: &#123;EVENT.NAME&#125;【问题主机】: &#123;HOST.NAME&#125;【问题级别】: &#123;EVENT.SEVERITY&#125;【阈值信息】: &#123;TRIGGER.NAME&#125;【阈值项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;【Operational data】: &#123;EVENT.OPDATA&#125;&#123;TRIGGER.URL&#125;</code></pre><p>恢复模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144504865.png" alt="image-20200902144504865"></p><pre><code>【[OK]】=》&#123;TRIGGER.SEVERITY&#125; 事件 : &#123;EVENT.ID&#125;【问题时间】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125; 【问题恢复时间】: &#123;EVENT.RECOVERY.DATE&#125;  &#123;EVENT.RECOVERY.TIME&#125;【问题名字】: &#123;EVENT.NAME&#125;【问题主机】: &#123;HOST.NAME&#125;【问题级别】: &#123;EVENT.SEVERITY&#125;【阈值信息】: &#123;TRIGGER.NAME&#125;【阈值项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;【Operational data】: &#123;EVENT.OPDATA&#125;&#123;TRIGGER.URL&#125;</code></pre><p>自动注册模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144535568.png" alt="image-20200902144535568"></p><pre><code>自动注册: 主机名: &#123;HOST.HOST&#125;主机ip: &#123;HOST.IP&#125;代理端口: &#123;HOST.PORT&#125;</code></pre><h3 id="放置发送信息的脚本">放置发送信息的脚本</h3><p>具体位置以zb server的安装和配置为基准.</p><pre><code class="language-python">#!/usr/bin/env python3&quot;&quot;&quot;@author: zyh@contact: aaa103439@hotmail.com@software: vscode@file: sendchat.py@time: 2020/02/05@use: pip3 install requests configparser&quot;&quot;&quot;import sys, os, requests, pathlib, json, configparserimport loggingimport logging.handlersfrom datetime import datetimeclass PySendchat():    def __init__(self, corpid, agentid, secret, touser, content):        self.corpid=corpid        self.agentid=agentid        self.secret=secret        self.touser=touser        self.content=content        LOG_PATHDIR=os.path.dirname(os.path.abspath(__file__))        LOG_FILENAME = '&#123;0&#125;/sendchat.log'.format(LOG_PATHDIR)        self.my_logger = logging.getLogger('SendChat')        self.my_logger.setLevel(logging.INFO)        # Add the log message handler to the logger        handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=102400000, backupCount=5)        # create formatter and add it to the handlers        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')        handler.setFormatter(formatter)        self.my_logger.addHandler(handler)    def gettoken(self):        self.my_logger.info('-----------------------------------------------------------------------------------------')        pwd=os.path.dirname(os.path.abspath(__file__))        tokenfile='&#123;0&#125;/wechat.&#123;1&#125;'.format(pwd,self.agentid)        if pathlib.Path(tokenfile).exists():            tokenfilectime=os.path.getctime(tokenfile)            currenttime=datetime.now().timestamp()            dtime=currenttime-tokenfilectime            self.my_logger.info('&#123;0&#125; lived &#123;1&#125;s.'.format(tokenfile, dtime))            if dtime &gt;= 7200:                try:                    os.remove(tokenfile)                    self.my_logger.info('Token file &#123;0&#125;: delete success'.format(tokenfile))                except Exception as e:                    self.my_logger.error('Token file:&#123;0&#125; delete error.Reason:&#123;1&#125;'.format(tokenfile,e))                    exit        # check token file        try:            tokensize = os.path.getsize(tokenfile)        except Exception as e:            self.my_logger.info('Token file is not exist.Reason:&#123;0&#125;'.format(e))            tokensize = 0        # get token from token file        if tokensize != 0:            with open(tokenfile, 'rb') as fd:                token = fd.read() # get token success                self.my_logger.info('Get token from token file.')            jsonObject = json.loads(token.decode(encoding='utf8'))            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token        # get token from weixin api        else:            try:                self.my_logger.info('New Token Create.')                f = requests.get('https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&#123;0&#125;&amp;corpsecret=&#123;1&#125;'.format(self.corpid, self.secret))                token = f.content                self.my_logger.info('Get token from weixin api.')                jsonObject = json.loads(token.decode(encoding='utf8'))                errcode=int(jsonObject.get(&quot;errcode&quot;))                if errcode != 0:                    errmsg=jsonObject.get(&quot;errmsg&quot;)                    self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(errmsg))                    exit()            except Exception as e:                self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(e))                exit()            try:                self.my_logger.info('Write token to &#123;0&#125;.'.format(tokenfile))                with open(tokenfile, 'wb') as fd:                    fd.write(token)            except Exception as e:                self.my_logger.error('Write &#123;0&#125; error!Reason:&#123;1&#125;'.format(tokenfile,e))                exit()            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token    def sendmsg(self):        accessToken = self.gettoken()        self.my_logger.info('Token:&#123;0&#125;'.format(accessToken))        sendMapDirectroy = &#123;&#125;        sendMapDirectroy[&quot;agentid&quot;] = self.agentid        sendMapDirectroy[&quot;touser&quot;] = self.touser        sendMapDirectroy[&quot;msgtype&quot;] = &quot;text&quot;        sendMapDirectroy[&quot;safe&quot;] = &quot;0&quot;        contentDirectory = &#123;&#125;        sendMapDirectroy[&quot;text&quot;] = contentDirectory        contentDirectory[&quot;content&quot;] = self.content        bodyStr = json.dumps(sendMapDirectroy, ensure_ascii=False).encode(encoding=&quot;utf-8&quot;)        try:            f = requests.post(url=&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s&quot; % accessToken,                          data=bodyStr, timeout=5)            self.my_logger.info(f.content)        except Exception as e:            self.my_logger.error('Send chat network error!Reason:&#123;0&#125;'.format(e))if __name__ == '__main__':    appname = sys.argv[1]    content = sys.argv[3]    # read conf.ini    conf = configparser.ConfigParser()    conf_path = os.path.dirname(os.path.abspath(__file__))    conf_ini = &quot;&#123;0&#125;/conf.ini&quot;.format(conf_path)    if pathlib.Path(conf_ini).exists():        conf.read(conf_ini)        corpid = conf.get(&quot;wechat&quot;, &quot;corpid&quot;)        appinfo = conf.get(&quot;app&quot;, appname)        agentid = appinfo.split(':')[0]        secret = appinfo.split(':')[1]        groupname = conf.get(&quot;group&quot;, appname)        touser = groupname.split(':')[0]        chatobj = PySendchat(corpid, agentid, secret, touser, content)    else:        print('conf.ini error')        exit()    try:        chatobj.sendmsg()    except:        chatobj.my_logger.error(&quot;Send chat failure!&quot;)</code></pre><pre><code class="language-ini"># conf.ini[wechat]corpid = [app]it = &lt;app_agent_id&gt;:&lt;app_secret&gt;[group]it = usera|userb</code></pre><p>eg:</p><p>python3 <a href="http://wechat.py">wechat.py</a> it ‘’ &lt;预警内容&gt;</p><p>因 zabbix 调用脚本并非 root 用户，所以给脚本附加 x 权限，避免权限问题。</p><pre><code class="language-bash">chmod a+x wechat.py# 因脚本会生成其它文件# 所以还需要给脚本所在的目录添加其它用户的写入权限chmod 757 alertscripts</code></pre><h2 id="预警用户">预警用户</h2><p>管理-用户-添加用户，这里有三要素</p><h3 id="用户">用户</h3><p>你无需登陆这个用户，所以密码可以尽量的复杂</p><p><img src="/posts/4ebbd3a8/image-20200902150131147.png" alt="image-20200902150131147"></p><h3 id="报警媒介">报警媒介</h3><p><img src="/posts/4ebbd3a8/image-20200902150145022.png" alt="image-20200902150145022"></p><blockquote><p>我们定义这个用户被调用的时候，将会发送信息给it组，至于it组包含哪些人员，则由脚本定义。</p></blockquote><h3 id="权限">权限</h3><p><img src="/posts/4ebbd3a8/image-20200902150153011.png" alt="image-20200902150153011"></p><h2 id="预警动作">预警动作</h2><p>配置-动作-左上角（触发器动作Trigger actions），这里我们需要定义两个要素</p><ol><li>什么条件下执行动作</li><li>动作的实际行为</li></ol><h3 id="条件">条件</h3><p>需要注意的是计算方式</p><blockquote><p>问题没有被制止的意思：没有人为的关闭预警</p></blockquote><p><img src="/posts/4ebbd3a8/image-20200902150806009.png" alt="image-20200902150806009"></p><h3 id="操作">操作</h3><p><img src="/posts/4ebbd3a8/image-20200902151037597.png" alt="image-20200902151037597"></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
            <tag> wechat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞容器搭建</title>
      <link href="posts/25f36846/"/>
      <url>posts/25f36846/</url>
      
        <content type="html"><![CDATA[<h2 id="创建网络">创建网络</h2><pre><code class="language-bash">docker network create -d bridge zbnet</code></pre><h2 id="mysql">mysql</h2><pre><code class="language-bash">mkdir -p /export/docker-data-mysql/&#123;config,logs,data&#125;docker run --name mysql57 \-p 3306:3306 \--network zbnet \--restart=always \--mount 'type=bind,src=/export/docker-data-mysql/config,dst=/etc/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/logs,dst=/var/log/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/data,dst=/var/lib/mysql' \-e MYSQL_ROOT_PASSWORD=123456 \-d mysql:5.7</code></pre><h2 id="zabbix-server">zabbix-server</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-server-mysql">https://hub.docker.com/r/zabbix/zabbix-server-mysql</a></p></blockquote><p>安装 centos + server + mysql 版本</p><p>默认，环境变量 <code>MYSQL_USER</code> and <code>MYSQL_PASSWORD</code> are <code>zabbix</code>, <code>zabbix</code>.</p><pre><code>mkdir -p /export/docker-data-zbserver/alertscriptsdocker volume create zabbixServerEtcdocker run --name=zabbix_server \-p 10051:10051 \--network zbnet \--restart=always \--mount 'type=volume,src=zabbixServerEtc,dst=/etc/zabbix' \--mount 'type=bind,src=/export/docker-data-zbserver/alertscripts,dst=/usr/lib/zabbix/alertscripts' \-e DB_SERVER_HOST=&quot;mysql57&quot; \-e MYSQL_DATABASE=&quot;zabbixserver&quot; \-e MYSQL_USER=&quot;zabbix&quot; \-e MYSQL_PASSWORD=&quot;zabbix&quot; \-e MYSQL_ROOT_PASSWORD=&quot;123456&quot; \-d zabbix/zabbix-server-mysql:centos-latest</code></pre><ol><li><p>默认安装完之后，容器内缺少一些开发环境，这会导致我们的告警脚本执行失败，比如我是用 python3 写的微信告警，而环境里没有。因此需要额外加装。</p><pre><code class="language-bash">docker logs -f zabbix_server # 可以看到如下错误203:20200831:030724.713 Failed to execute command &quot;/usr/lib/zabbix/alertscripts/wechat.py 'it' '自动注册: xxx-use-001-10-240-128-100' '主机名: xxx-use-001-10-240-128-100主机ip: xxx代理端口: 10050'&quot;: env: 'python3': No such file or directory</code></pre><pre><code class="language-bash"># 安装 python3 和模块docker exec -it -u root zabbix_server yum install python3docker exec -it -u root zabbix_server pip3 install requests configparser</code></pre></li><li><p>alertscripts 目录需要有 other 可执行权限</p><pre><code class="language-bash">chmod o+w /export/docker-data-zbserver/alertscripts</code></pre><p>因为 zabbix_server 在执行告警脚本的时候，用的是普通用户</p></li></ol><h2 id="zabbix-server-web">zabbix-server-web</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-web-nginx-mysql">https://hub.docker.com/r/zabbix/zabbix-web-nginx-mysql</a></p></blockquote><pre><code class="language-bash"># 安装字体yum install google-noto-sans-simplified-chinese-fonts.noarch -ydocker run --name zabbix_web \-p 80:8080 \-p 443:8443 \--restart=always \--network zbnet \--mount 'type=bind,src=/usr/share/fonts/google-noto/NotoSansSC-Regular.otf,dst=/usr/share/zabbix/assets/fonts/DejaVuSans.ttf' \-e DB_SERVER_HOST=&quot;mysql57&quot; \-e MYSQL_DATABASE=&quot;zabbixserver&quot; \-e MYSQL_USER=&quot;zabbix&quot; \-e MYSQL_PASSWORD=&quot;zabbix&quot; \-e MYSQL_ROOT_PASSWORD=&quot;123456&quot; \-e ZBX_SERVER_HOST=&quot;zabbix_server&quot; \-e PHP_TZ=&quot;Asia/Shanghai&quot; \-d zabbix/zabbix-web-nginx-mysql:latest</code></pre><blockquote><p><a href="http://ip">http://ip</a> 即可</p><p>初始账户密码是 Admin/zabbix</p></blockquote><p>到这里，你就可以访问zabbix了，不过这时候你会发现zabbix提示agent不可达</p><p><img src="/posts/25f36846/image-20200901172913999.png" alt="image-20200901172913999"></p><p>其原因是我们还没创建agent</p><p>需要注意的是，因为默认zabbix server的主机配置项监听的是 127.0.0.1:10050, 并且主机名配置的是</p><p><code>zabbix server</code>. 这和我们本文档有一些冲突，所以需要修改一些配置。</p><ol start="2"><li>修改主机配置中的 hostname 为 agent 容器中的环境变量 ZBX_HOSTNAME。这里我们为 zabbix_server</li><li>修改主机配置中的监听地址为 zabbix_server，且监听方式为 dns</li></ol><p>最终修改完如下：</p><p><img src="/posts/25f36846/image-20200901175159400.png" alt="image-20200901175159400"></p><h2 id="zabbix-agent">zabbix-agent</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-agent">https://hub.docker.com/r/zabbix/zabbix-agent</a></p></blockquote><pre><code class="language-bash">docker volume create zbAgentEtcdocker run --name zabbix_agent \--network=container:zabbix_server \--mount 'type=volume,src=zbAgentEtc,dst=/etc/zabbix/zabbix_agentd.d' \-e ZBX_DEBUGLEVEL=&quot;3&quot; \-e ZBX_HOSTNAME=&quot;zabbix_server&quot; \-e ZBX_SERVER_HOST=&quot;zabbix_server&quot; \-d zabbix/zabbix-agent:latest</code></pre><p>这里 zabbix_agent 采用网络模式为容器模式，并加入到 zabbix_server 容器中，以便于 zabbix_server 可以找到 zabbix_agent</p><p>其它配置变量详见上面官方文档</p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞离线迁移脚本</title>
      <link href="posts/c83ca575/"/>
      <url>posts/c83ca575/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">databases=&quot;&quot;mysqltonew()&#123;    olddatabasehost=    olddatabaseport=    olddatabase=$1    oldUserName=    oldpassword=    newdatabasehost=    newolddatabaseport=    newdatabase=$&#123;olddatabase&#125;    newUserName=    newpassword=    # utf8 / utf8mb4    read -p &quot;character [utf8/utf8mb4/latin1]:&quot; Character    # 导出老库    mysqldump -u$&#123;oldUserName&#125; -p$&#123;oldpassword&#125; -h$&#123;olddatabasehost&#125; -P$&#123;olddatabaseport&#125; --default-character-set=$&#123;Character&#125; --single-transaction $&#123;olddatabase&#125; &gt; $&#123;olddatabase&#125;.sql    # 创建新库    mysql -u$&#123;newUserName&#125; -p$&#123;newpassword&#125; -h$&#123;newdatabasehost&#125; -P$&#123;newolddatabaseport&#125; -e &quot;CREATE DATABASE $&#123;newdatabase&#125; DEFAULT CHARSET $&#123;Character&#125; COLLATE $&#123;Character&#125;_general_ci;&quot;    # 导入新库    mysql -u$&#123;newUserName&#125; -p$&#123;newpassword&#125; -h$&#123;newdatabasehost&#125; -P$&#123;newolddatabaseport&#125; --default-character-set=$&#123;Character&#125; $&#123;newdatabase&#125; &lt; $&#123;olddatabase&#125;.sql&#125;for i in $&#123;databases&#125;;do        echo &quot;start $i&quot;        mysqltonew $idone</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
            <tag> mysqldump </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞04pod钩子函数</title>
      <link href="posts/3ec72f7e/"/>
      <url>posts/3ec72f7e/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>我们在实际工作中，可能会遇到容器程序的准备工作，以及容器结束之前的处理工作。比如，容器开始之前下载一些包，或者容器结束之前上传日志等。</p><p>k8s 给我们提供了两个函数用于处理这些工作，分别是 postStart和 preStop</p><h2 id="postStart">postStart</h2><ol><li>它将在容器创建后立即运行，但不保证在 ENTRYPOINT 之前运行</li><li>它是同步状态，如果它卡住，pod 无法达到 running 状态</li><li>它位于容器 lifecycle 中</li></ol><p>示例：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: poststart-testspec:  containers:  - name: poststart-test    image: nginx    lifecycle:      postStart:        exec:          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo This is postStart-test &gt; /usr/share/nginx/html/index.html&quot;]</code></pre><pre><code class="language-bash">[root@k8s00 ~]# kubectl apply -f poststart-test.yamlpod/poststart-test created[root@k8s00 ~]# kubectl get pod -o wideNAME             READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATESpoststart-test   0/1     ContainerCreating   0          7s    &lt;none&gt;   k8s02   &lt;none&gt;           &lt;none&gt;[root@k8s00 ~]# kubectl get pod -o wideNAME             READY   STATUS    RESTARTS   AGE   IP          NODE    NOMINATED NODE   READINESS GATESpoststart-test   1/1     Running   0          10s   10.97.2.6   k8s02   &lt;none&gt;           &lt;none&gt;[root@k8s00 ~]# curl 10.97.2.6This is postStart-test</code></pre><h2 id="preStop">preStop</h2><ol><li>它将在容器结束之前运行</li><li>它是同步的，如果它卡住，pod 将停留在 running 状态，无法达到 failed 状态</li><li>它位于容器 lifecycle 中</li></ol><p>示例：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: prestop-testspec:  containers:  - name: prestop-test    image: nginx    volumeMounts:    - name: prestop-test-tmp      mountPath: /usr/share    lifecycle:      preStop:        exec:          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo This is preStop &gt; /usr/share/message&quot;]  volumes:  - name: prestop-test-tmp    hostPath:      path: /tmp</code></pre><pre><code class="language-bash">[root@k8s00 ~]# kubectl delete pod prestop-test# 这里 prestop-test 被调度到 k8s02[root@k8s02 ~]# cat /tmp/messageThis is preStop</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞03pod</title>
      <link href="posts/94518966/"/>
      <url>posts/94518966/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>pod 可以说是 k8s 的基础单元. 我觉得可以类比云环境的ecs/ec2这一类的基本计算单元.而 pod 上运行的容器, 可以类比为ecs/ec2上的app程序.</p><p>你总能在k8s的各类资源中找到云环境对应的资源影子. 如果你用过GCP,你会更有这种感觉.</p><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/">https://kubernetes.io/docs/concepts/workloads/pods/</a></p><h2 id="Pod-与-控制器">Pod 与 控制器</h2><p>pod 一般不单独使用, 因为单独使用意味着没有高可用.k8s建议 pod 要始终和控制器一起使用. 你可以认为控制器就如同云环境中的资源目标组.这个概念的实际对象一般是于负载均衡服务的缩放组概念. 例如aws的auto scaling.</p><p>k8s将app控制器分为三种:</p><ul><li>Deployment 无状态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li><li>StatefulSet 有状态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a></li><li>DaemonSet 守护态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a></li></ul><p>还有CronJob这类计划任务类型的.</p><p>这里我们不说控制器如何用. 但是你可以自行了解.</p><p>控制器既然类同云服务的缩放组概念. 那么它必然需要依托于镜像模板和缩放规则.</p><p>镜像模板在k8s中叫 pod 模板(pod template).</p><h2 id="Pod-模板">Pod 模板</h2><p>特点:</p><ul><li>模板修改会导致重建新的pod, 并终止现有pod.</li></ul><p>例子:</p><pre><code class="language-yaml">apiVersion: batch/v1beta1kind: CronJobmetadata:  name: hellospec:  schedule: &quot;*/1 * * * *&quot;  jobTemplate:    spec:      template:   ## 从这里开始, 以下内容就是 Pod 模板.        spec:          containers:          - name: hello            image: busybox            args:            - /bin/sh            - -c            - date; echo Hello from the Kubernetes cluster          restartPolicy: OnFailure</code></pre><h2 id="Pod-存储-网络">Pod 存储/网络</h2><h3 id="pod-存储">pod 存储</h3><p>这是一个大问题. 如果你想真正的使用k8s的pod资源, 那么需要先看这一部分的内容.</p><p>简单来说, 存储资源主要分网络和本地两大类.</p><p>本地这一类一般用于临时或者特殊环境. 就如同云服务中的存储类节点里的那种本地盘, 它不可靠. 因为pod本身默认是不强制绑定某个节点的,因此如果你pod异常了,那么它有可能重建的时候漂移到其它节点.</p><p>网络这一类,可以对接的有云服务厂家的存储资源,也可以对接自建的nfs这一类网络存储.</p><p>细致的说明, 参考官方文档https://kubernetes.io/docs/concepts/storage/</p><h3 id="pod-网络">pod 网络</h3><p>鉴于前面提到的pod类同于ecs/ec2. 因此. pod中的容器都是共享网络的.所以这些容器都有相同的ip, 端口范围, 主机名.</p><p>k8s的网络基于各种插件.每一种插件的实现详情见官网. <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model">https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model</a></p><p>如果你是本地搭建, 那么常用的插件是Flannel. 如果你是在云服务上搭建,那么建议使用云服务已有的k8s服务.</p><p>如果你必须在云服务上自己搭建,那么aws/azure/gcp都有对应的网络插件.它可以让你在k8s中结合使用云服务的网络组件.</p><h2 id="静态pod">静态pod</h2><p>特点：</p><ol><li>永远运行在固定节点</li><li>由所在节点的kubelet管理，但只负责保活，即pod崩溃重生</li><li>kubelet会让apiserver创建一个镜像pod，便于可以通过kubectl查询到静态pod</li></ol><p>配置：</p><ol><li>存放在 /etc/kubernetes/manifests 当采用kubeadm安装的时候，一般位于此目录。具体需要去看kubelet配置。</li><li>配置本身可以按照标准pod方式来创建</li></ol><p>检测：</p><ol><li>kubelet会定期检测配置目录加载配置创建/重建pod</li></ol><blockquote><p>当你通过kubeadm创建的时候，那么k8s的几个重要组件均会以静态pod的方式在master节点上创建，你可以在/etc/kubernetes/manifests/这里找到他们的配置</p></blockquote><pre><code class="language-bash">[root@k8s00 ~]# ll /etc/kubernetes/manifests/total 16-rw------- 1 root root 1848 Aug 25 16:28 etcd.yaml-rw------- 1 root root 2709 Aug 25 16:28 kube-apiserver.yaml-rw------- 1 root root 2564 Aug 25 16:33 kube-controller-manager.yaml-rw------- 1 root root 1120 Aug 25 16:33 kube-scheduler.yaml</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞02v1.18.6安装</title>
      <link href="posts/9602dad/"/>
      <url>posts/9602dad/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>本文编写的时候，官方已经到了1.18.8，但是鉴于国内环境，非官方源没找到1.18.8，所以最后用的1.18.6<br>另外，文中出现的宿主机ip，请自行修改为自己的环境ip</p><h2 id="机器">机器</h2><p>机器系统都是 centos7,</p><p>三个节点，均为主节点，同时也是工作节点，并利用 keepalived + haproxy 进行高可用</p><table><thead><tr><th>hostname</th><th>ip</th><th>type</th></tr></thead><tbody><tr><td>k8sapi</td><td>10.200.16.100</td><td>vip</td></tr><tr><td>k8s01</td><td>10.200.16.101</td><td>master</td></tr><tr><td>k8s02</td><td>10.200.16.102</td><td>master backup</td></tr><tr><td>k8s03</td><td>10.200.16.103</td><td>master backup</td></tr></tbody></table><p>请务必确保内网可以通过表格里的 <code>hostname</code> 解析到对应的 <code>ip</code></p><p>请务必将系统的 hostname 改为上述表里的 hostname</p><h2 id="以下命令在所有节点上运行">以下命令在所有节点上运行</h2><ol><li><p>时间同步</p><pre><code class="language-bash">yum install chrony -ysed -i  '1a server cn.pool.ntp.org prefer iburst' /etc/chrony.confsystemctl restart chronydsystemctl enable chronydchronyc activity</code></pre></li><li><p>系统配置</p><pre><code class="language-bash"># 加载模块modprobe overlaymodprobe br_netfilter# 添加配置cat &gt; /etc/sysctl.d/99-kubernetes-cri.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-iptables  = 1net.ipv4.ip_forward                 = 1net.bridge.bridge-nf-call-ip6tables = 1EOF# 配置生效sysctl --system# 清空防火墙systemctl stop firewalld.service iptables.servicesystemctl disable firewalld.servicesystemctl disable iptables.service;iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X# 关闭selinuxsetenforce 0sed -i 's@^\(SELINUX=\).*@\1disabled@' /etc/sysconfig/selinuxsed -i 's@^\(SELINUX=\).*@\1disabled@' /etc/selinux/config# 关闭 swap，kubelet 1.18 要求.如果你fstab也有，请一并注释swapoff -a# 安装 ipvsyum install ipvsadm -yipvsadm --clearcat&gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt; 'EOF'ipvs_mods_dir=&quot;/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs&quot;for i in $(ls $ipvs_mods_dir | grep -o &quot;^[^.]*&quot;); do    /sbin/modinfo -F filename $i  &amp;&gt; /dev/null    if [ $? -eq 0 ]; then        /sbin/modprobe $i    fidoneEOFchmod +x /etc/sysconfig/modules/ipvs.modules/etc/sysconfig/modules/ipvs.modules</code></pre></li><li><p>安装 docker</p><blockquote><p><a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker">https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker</a></p></blockquote><pre><code class="language-bash">yum install -y yum-utils \  device-mapper-persistent-data \  lvm2yum-config-manager \    --add-repo \    https://download.docker.com/linux/centos/docker-ce.repoyum list docker-ce --showduplicates | sort -r# 安装兼容k8s的docker版本yum install -y \  containerd.io-1.2.13 \  docker-ce-19.03.11 \  docker-ce-cli-19.03.11#sed -i '/ExecStart=/a ExecStartPort=/usr/sbin/iptables -P FORWARD ACCEPT' /usr/lib/systemd/system/docker.service;mkdir -p /etc/docker;cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;data-root&quot;: &quot;/export/docker-data-root&quot;,  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;100m&quot;  &#125;,  &quot;storage-driver&quot;: &quot;overlay2&quot;,  &quot;storage-opts&quot;: [    &quot;overlay2.override_kernel_check=true&quot;  ]&#125;EOFmkdir -p /etc/systemd/system/docker.service.dsystemctl daemon-reloadsystemctl restart dockersystemctl enable docker</code></pre></li><li><p>安装 kubelet kubeadm kubectl</p><blockquote><p>阿里巴巴镜像点：</p><p><a href="https://developer.aliyun.com/mirror/kubernetes">https://developer.aliyun.com/mirror/kubernetes</a></p></blockquote></li></ol><blockquote><p>google 官方安装文档 <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p></blockquote><pre><code class="language-bash">cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearchenabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOF# 确定 kube* 版本yum list kube* --showduplicates --disableexcludes=kubernetes# 选择要安装的版本Version=1.18.6# 安装 kube 管理组件和 kubeletyum install -y kubelet-$&#123;Version&#125; kubeadm-$&#123;Version&#125; kubectl-$&#123;Version&#125; --disableexcludes=kubernetessystemctl enable --now kubelet</code></pre><blockquote><p>kubelet 1.18 的时候，当你使用 docker 的时候，kubeadm 会通过 kubelet 自动设置 cgroup 和 docker 一致。</p></blockquote><h2 id="master节点">master节点</h2><h3 id="安装高可用环境（k8s01，k8s02，k8s03）">安装高可用环境（k8s01，k8s02，k8s03）</h3><ul><li>安装haproxy和keepalived</li></ul><pre><code class="language-bash">yum install haproxy keepalived -y</code></pre><ul><li>haproxy配置</li></ul><pre><code class="language-bash">cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.defaultvim /etc/haproxy/haproxy.cfg===global    log /dev/log local0    log /dev/log local1 notice    daemondefaults    mode                    http    log                     global    option                  httplog    option                  dontlognull    option http-server-close    option forwardfor       except 127.0.0.0/8    option                  redispatch    retries                 1    timeout http-request    10s    timeout queue           20s    timeout connect         5s    timeout client          20s    timeout server          20s    timeout http-keep-alive 10s    timeout check           10sfrontend apiserver    bind *:8443    mode tcp    option tcplog    default_backend apiserverbackend apiserver    option httpchk GET /healthz    http-check expect status 200    mode tcp    option ssl-hello-chk    balance     roundrobin        server k8sapivip 10.200.16.101:6443 check        server k8sapivip 10.200.16.102:6443 check        server k8sapivip 10.200.16.103:6443 check</code></pre><ul><li>keepalived配置A （这个配置在三个master节点不一样）</li></ul><pre><code class="language-bash">cp /etc/keepalived/keepalived.conf  /etc/keepalived/keepalived.conf.defaultvim /etc/keepalived/keepalived.conf===global_defs &#123;    router_id LVS_DEVEL&#125;vrrp_script check_apiserver &#123;  script &quot;/etc/keepalived/check_apiserver.sh&quot;  interval 3  weight -2  fall 10  rise 2&#125;vrrp_instance VI_1 &#123;    state MASTER  # 改我改我：一个MASTER，两个BACKUP    interface ens192  # 物理网卡名    virtual_router_id 51    priority 101  # 改我改我：MASTER 101，两个BACKUP 100    authentication &#123;        auth_type PASS        auth_pass 42    &#125;    virtual_ipaddress &#123;        10.200.16.100    &#125;    track_script &#123;        check_apiserver    &#125;&#125;</code></pre><ul><li>keepalived配置B：检测脚本</li></ul><pre><code class="language-bash">vim /etc/keepalived/check_apiserver.sh===#!/bin/sherrorExit() &#123;    echo &quot;*** $*&quot; 1&gt;&amp;2    exit 1&#125;APISERVER_VIP=10.200.16.100APISERVER_DEST_PORT=6443curl --silent --max-time 2 --insecure https://localhost:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://localhost:$&#123;APISERVER_DEST_PORT&#125;/&quot;if ip addr | grep -q $&#123;APISERVER_VIP&#125;; then    curl --silent --max-time 2 --insecure https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/&quot;fichmod a+x /etc/keepalived/check_apiserver.sh</code></pre><ul><li>启动keepalived和haproxy</li></ul><pre><code class="language-bash">systemctl start keepalived haproxysystemctl enable keepalived haproxy</code></pre><h3 id="拉取容器镜像（k8s01，k8s02，k8s03）">拉取容器镜像（k8s01，k8s02，k8s03）</h3><blockquote><p>鉴于网络问题，所以国内一般无法直接运行初始化命令，因此最好先自行安装好包</p></blockquote><pre><code class="language-bash"># 查看对应版本 k8s 所需包kubeadm config images list --kubernetes-version=$&#123;Version&#125;# 运行下列脚本安装所需的包# ---------------------我是脚本开头--------------------#!/bin/bash# kubeadm config images listimages=(kube-apiserver:v$&#123;Version&#125;kube-controller-manager:v$&#123;Version&#125;kube-scheduler:v$&#123;Version&#125;kube-proxy:v$&#123;Version&#125;pause:3.2etcd:3.4.3-0coredns:1.6.7)for imageName in $&#123;images[@]&#125;;do    docker pull registry.aliyuncs.com/google_containers/$&#123;imageName&#125;    docker tag registry.aliyuncs.com/google_containers/$&#123;imageName&#125; k8s.gcr.io/$&#123;imageName&#125;    docker rmi registry.aliyuncs.com/google_containers/$&#123;imageName&#125;done# ---------------------我是脚本结尾--------------------# 如果某些包无法下载，则需自行去 dockerhub 上找，并在下载完毕后，添加 k8s 自己的 tags，例如 coredns# docker pull coredns/coredns:1.6.7 # docker tag coredns/coredns:1.6.7 k8s.gcr.io/coredns:1.6.7# docker rmi coredns/coredns:1.6.7</code></pre><h3 id="初始化安装（k8s01）">初始化安装（k8s01）</h3><p>(info: <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/%EF%BC%89">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/）</a></p><p>采取配置文件安装模式</p><pre><code class="language-bash">kubeadm config print init-defaults &gt; kubeadm.yaml.defaultcat &gt; kubeadm.yaml &lt;&lt; EOF# kubeadm.yaml 将默认的配置进行修改apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.8272827282sksksu   # 改我，改成你自己随意的字符串  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 10.200.16.101  # 改我，改成配置所在节点的物理ip  bindPort: 6443nodeRegistration:  criSocket: /var/run/dockershim.sock  name: k8s01 # 改我，理论上它会自动获取配置所在节点的 hostname  taints:  - effect: NoSchedule    key: node-role.kubernetes.io/master---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;controlPlaneEndpoint: &quot;k8sapi:8443&quot; # 新加我dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: v1.19.3 # 改我，改成你所需安装的版本networking:  dnsDomain: cluster.local  serviceSubnet: 10.96.0.0/16 # 改我，定义 svc 的网段  podSubnet: 10.97.0.0/16 # 改我，定义 pod 的网段scheduler: &#123;&#125;--- # 新加我apiVersion: kubeproxy.config.k8s.io/v1alpha1 # 新加我kind: KubeProxyConfiguration # 新加我mode: ipvs  # 新加我EOF# 进行初始化kubeadm init --config kubeadm.yaml# 记录初始化生成的命令，特别是 kubeadm join</code></pre><p>配置通过kubectl访问集群</p><pre><code class="language-bash">mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# kubectl 命令补全source &lt;(kubectl completion bash)# You should now deploy a pod network to the cluster.# Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:# https://kubernetes.io/docs/concepts/cluster-administration/addons/</code></pre><pre><code class="language-bash"># 添加用户上下文到配置里，方便切换用户kubectl config set-context admin --cluster=kubernetes --user=kubernetes-admin</code></pre><p>调整 kube-controller-manager 和 scheduler 的配置.</p><pre><code class="language-bash"># 检查状态kubectl get cs# 你可能会发现,出现服务连接拒绝问题controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refusedscheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused# 原因是这两个服务配置默认端口是0。至于为啥就不晓得了# 你需要注释掉两个配置里的端口（- --port=0），恢复为默认端口sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml# 重启 kubeletsystemctl restart kubelet# 再次检查kubectl get cs===NAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</code></pre><p>配置网络插件，网络插件常用的有两种，</p><p>第一种是 flannel，涉及到更多的 iptables 规则</p><p>第二种是 calico，涉及到更多的路由规则</p><ul><li>flannel (本文档选用的插件)</li></ul><blockquote><p><a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a> 代码托管地址</p></blockquote><pre><code class="language-bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改里面的 &quot;Network&quot;: &quot;10.244.0.0/16&quot;, 变更为你自己的 pod 网段，即kubeadm初始化阶段的 --pod-network-cidrkubectl apply -f  kube-flannel.yml</code></pre><ul><li>calico</li></ul><blockquote><p><a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart">https://docs.projectcalico.org/getting-started/kubernetes/quickstart</a> 安装文档</p></blockquote><pre><code class="language-bash">kubectl apply -f https://docs.projectcalico.org/manifests/tigera-operator.yamlwget https://docs.projectcalico.org/manifests/custom-resources.yaml# custom-resources.yaml 修改里面的网络为 pod 网段kubectl apply -f custom-resources.yaml</code></pre><blockquote><p>注意，当你使用了 calico 后， 会生成一些 cni 的配置，这些配置会导致你返回 flannel 的时候出现问题。例如无法创建 cni</p><p>你可以使用 find / -name ‘*calico*’ 找到所有信息，然后都删除</p></blockquote><p>默认kubeadm安装完后，禁止调度pod到master上。你可以通过下面的命令，关闭所有master节点的禁止调度</p><pre><code class="language-bash"> kubectl taint nodes --all node-role.kubernetes.io/master-</code></pre><p>配置web控制台</p><blockquote><p><a href="https://github.com/kubernetes/dashboard">https://github.com/kubernetes/dashboard</a> 代码托管地址</p></blockquote><pre><code class="language-bash">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml &amp;&amp; mv recommended.yaml kubernetes-dashboard.yaml# 这里我们要修改一些东西# 默认控制台的 svc 配置类型不是 NodePort，这会导致我们无法直接通过宿主机ip访问，这里我们为了方便，将 dashboard 的 svc 加入NodePort类型#---spec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort#---# 还需要修改一下部署的位置，如果你集群中已经加入了多个节点，则会导致 pod 分发到其它节点上。这里我们强制分发到 master 上. 找到 kind: Deployment 配置，并修改两个 pod 的分发位置为 nodeName: &lt;master 节点主机名&gt;#---    spec:      nodeName: k8s00      containers:        - name: kubernetes-dashboard          image: kubernetesui/dashboard:v2.0.3              spec:      nodeName: k8s00      containers:        - name: dashboard-metrics-scraper          image: kubernetesui/metrics-scraper:v1.0.4          #---# 然后创建kubectl create -f kubernetes-dashboard.yaml# 添加访问 token# 官方文档： https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md# 通过下面内容创建 kube-db-auth.yamlcat &gt; kube-db-auth.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata:  name: admin-user  namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: admin-userroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: admin-user  namespace: kubernetes-dashboardEOFkubectl apply -f kube-db-auth.yaml# 通过下面命令拿到 tokenkubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '&#123;print $1&#125;')  # 通过 https://&lt;masterip&gt;:30001 访问</code></pre><h3 id="获取集群加入命令（k8s01）">获取集群加入命令（k8s01）</h3><pre><code class="language-bash"># 命令格式kubeadm join xxx:8443 \--token xxx \--discovery-token-ca-cert-hash xxx \--control-plane --certificate-key xxx</code></pre><p>找回命令</p><pre><code class="language-bash"># 找回 tokenkubeadm token list# 找回 discovery-token-ca-cert-hashopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'</code></pre><p>命令重建</p><pre><code class="language-bash"># 创建新的 token 并输出 join 命令kubeadm token create --print-join-command===kubeadm join k8sapi:8443 --token h77qhb.cjj8ig4t2v15dncm     --discovery-token-ca-cert-hash sha256:b7138bb76d19d3d7baf832e46559c400f4fe544d8f0ee81a87f# 创建新的 --certificate-keykubeadm init phase upload-certs --upload-certs===fd996c7c0c2047c9c10a377f25a332bf4b5b00ca</code></pre><h3 id="k8s02和k8s03添加到集群">k8s02和k8s03添加到集群</h3><pre><code class="language-bash">kubeadm join k8sapi:8443 \--token h77qhb.cjj8ig4t2v15dncm \--discovery-token-ca-cert-hash sha256:b7138bb76d19d3d7baf832e46559c400f4fe544d8f0ee81a87f \--control-plane --certificate-key fd996c7c0c2047c9c10a377f25a332bf4b5b00ca# 根据提示，执行相关命令，一般都是下面的命令mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config</code></pre><p>如果etcd中曾经有k8s02和k8s03的节点信息，则你需要先从etcd中删除，否则加入的时候，会卡在检测etcd处，并最终报错.</p><p>删除etcd信息方式：</p><pre><code class="language-bash"># 输出 etcd 节点 idETCD=`docker ps|grep etcd|grep -v POD|awk '&#123;print $1&#125;'`docker exec \  -it $&#123;ETCD&#125; \  etcdctl \  --endpoints https://127.0.0.1:2379 \  --cacert /etc/kubernetes/pki/etcd/ca.crt \  --cert /etc/kubernetes/pki/etcd/peer.crt \  --key /etc/kubernetes/pki/etcd/peer.key \  member list===2f401672c9a538f1, started, k8s01, https://10.200.16.101:2380, https://10.200.16.101:2379, falsed6d9ca2a70f6638e, started, k8s02, https://10.200.16.102:2380, https://10.200.16.102:2379, falseee0e9340a5cfb4d7, started, k8s03, https://10.200.16.103:2380, https://10.200.16.103:2379, false# 假设这里我要删除 k8s03ETCD=`docker ps|grep etcd|grep -v POD|awk '&#123;print $1&#125;'`docker exec \  -it $&#123;ETCD&#125; \  etcdctl \  --endpoints https://127.0.0.1:2379 \  --cacert /etc/kubernetes/pki/etcd/ca.crt \  --cert /etc/kubernetes/pki/etcd/peer.crt \  --key /etc/kubernetes/pki/etcd/peer.key \  member remove ee0e9340a5cfb4d7</code></pre><h2 id="worker节点">worker节点</h2><blockquote><p>加入命令，位于 k8s01初始化命令尾部，worker加入的时候，不需要添加 --control-plane --certificate-key</p></blockquote><pre><code class="language-bash">kubeadm join k8sapi:8443 \--token h77qhb.cjj8ig4t2v15dncm \--discovery-token-ca-cert-hash sha256:b7138bb76d19d3d7baf832e46559c400f4fe544d8f0ee81a87f</code></pre><h2 id="其他内容">其他内容</h2><h3 id="容器中显示正确的可见资源">容器中显示正确的可见资源</h3><p>默认情况下,不管你如何设置资源限制,容器里的可见资源都等于节点资源.也就是说你在容器里查看/proc/meminfo显示的资源并不是你设置的.这个问题会带来很多麻烦.</p><p>因为很多程序的参数都是根据可见资源来设定的.例如nginx的auto, 或者jvm里的内存参数.</p><blockquote><p>从 Java 8u191开始, jvm 已经默认实现了容器感知( -XX:+UseContainerSupport). 因此无需安装下面的 lxcfs 方案.</p><p>并且, 在容器中建议只设置如下内存参数:</p><p>-XX:MaxRAMPercentage  最大堆内存=物理内存百分比, 建议为容器限制的50%-75% . 毕竟还需要预留其它内存.</p></blockquote><p>而社区常见的作法是用<code>lxcfs</code> 来提升资源的可见性.<code>lxcfs</code> 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器, 它也可以支持Docker容器.</p><p>LXCFS通过用户态文件系统, 在容器中提供下列 <code>procfs</code> 的文件.</p><pre><code class="language-bash">/proc/cpuinfo/proc/diskstats/proc/meminfo/proc/stat/proc/swaps/proc/uptime</code></pre><p>与我们资源限制有关的, 主要是 cpuinfo 和 meminfo.</p><p>目前社区的做法如下:</p><ol><li><p>所有节点安装 fuse-libs 包.</p><pre><code class="language-bash">yum install -y fuse-libs</code></pre></li><li><p>集群部署lxcfs deployment</p><pre><code class="language-bash">git clone https://github.com/denverdino/lxcfs-admission-webhook.gitcd lxcfs-admission-webhookvim deployment/lxcfs-daemonset.yaml=== 修正配置里的 apiVersion=== 当前git里的代码是陈旧的...代码里的版本在 1.18 已经被废弃). 具体归属于什么版本, 请参考k8s官方api文档 === https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#daemonset-v1-appsapiVersion:apps/v1  kubectl apply -f deployment/lxcfs-daemonset.yamlkubectl get pod#等待 lxcfs pod 状态成 running#如果你发现 pod 始终出错，那么执行下列命令：kubectl delete -f deployment/lxcfs-daemonset.yamlrm -rf /var/lib/lxcfskubectl apply -f deployment/lxcfs-daemonset.yamldeployment/install.shkubectl get secrets,pods,svc,mutatingwebhookconfigurations</code></pre></li><li><p>给相关namespace注入lxcfs，例如default</p><pre><code class="language-bash">kubectl label namespace default lxcfs-admission-webhook=enabled</code></pre></li><li><p>重启添加了资源限制的pod, 此时 /proc/cpuinfo 和 /proc/meminfo 将会正确显示.</p></li></ol><h3 id="集群指标监控服务">集群指标监控服务</h3><p>添加 metrics-server  <a href="https://github.com/kubernetes-sigs/metrics-server#configuration">https://github.com/kubernetes-sigs/metrics-server#configuration</a></p><pre><code class="language-bash">helm show values stable/metrics-server &gt; metrics-server.yaml# 修改配置添加额外的启动参数(arg)args:  - --kubelet-preferred-address-types=InternalIP  - --kubelet-insecure-tlshelm install metrics-server stable/metrics-server -f metrics-server.yamlkubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;</code></pre><blockquote><p>kubectl top 指令需要指标才能输出</p></blockquote><hr><h2 id="kubectl-命令文档">kubectl 命令文档</h2><blockquote><p><a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands</a></p></blockquote><pre><code class="language-bash"># 查询集群网络的appkubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools</code></pre><h2 id="删除与清理">删除与清理</h2><pre><code class="language-bash"># 从集群里删除某个节点# master execkubectl drain &lt;NODE_ID&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;NODE_ID&gt;# worker execkubeadm resetiptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -Xipvsadm --clear;ip link set cni0 down &amp;&amp; ip link delete cni0;ip link set flannel.1 down &amp;&amp; ip link delete flannel.1;ip link set kube-ipvs0 down &amp;&amp; ip link delete kube-ipvs0;ip link set dummy0 down &amp;&amp; ip link delete dummy0;rm -rf /var/lib/cni/rm -rf $HOME/.kube;</code></pre><h2 id="节点一致性测试">节点一致性测试</h2><blockquote><p><a href="https://kubernetes.io/docs/setup/node-conformance/">https://kubernetes.io/docs/setup/node-conformance/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞记录点</title>
      <link href="posts/e1096625/"/>
      <url>posts/e1096625/</url>
      
        <content type="html"><![CDATA[<ol><li><p>pv和pvc的绑定关系是终生的. 也就是说你删除了当前pvc, pv依然不能被其它pvc所绑定.</p><p>删除pvc后, pv的状态将从bound转为Released.</p><p>如果你想让pv从新绑定到原来的pvc上, 则需要两个步骤:</p><ol><li>先创建原来的pvc.</li><li>删除当前pv并重新创建, 或者修改当前pv的配置, 删除spec.claimRef信息.</li></ol></li><li><p>pod无法正常启动，报错：“mounting \“/var/lib/lxcfs/proc/loadavg\”<br>to rootfs \&quot;/export/docker-data-root/overlay2/710d09a6715d88a01b417ba1a669dab69b67c3d57e576c1ae6d79aa03e1b294a/merged”</p><p>根据信息可知问题点应该是 lxcfs 问题。</p><p>查看故障pod所在节点的 lxcfs pod 日志，得到信息</p><p>“fuse: mountpoint is not empty<br>fuse: if you are sure this is safe, use the ‘nonempty’ mount option”</p><p>因此删除节点 lxcfs 目录，并重建 lxcfs pod</p><pre><code class="language-bash">rm -rf  /var/lib/lxcfs/ &amp;&amp; kubectl delete pod/lxcfs-hthgq</code></pre></li><li><p>变更pv回收策略</p><pre><code class="language-bash">kubectl patch pv &lt;your-pv-name&gt; -p '&#123;&quot;spec&quot;:&#123;&quot;persistentVolumeReclaimPolicy&quot;:&quot;Retain&quot;&#125;&#125;'</code></pre></li><li><p>kubernetes qos</p><p>kubernetes 有三种策略，分别是Guaranteed和Burstable和BestEffort</p><p>Guaranteed：任何容器的cpu请求值和限制值必须一样。内存同样如此</p><p>Burstable：最少有一个容器的cpu请求值和限制值必须一样。内存同样如此</p><p>BestEffort： 没有任何限制</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞搭建</title>
      <link href="posts/65820315/"/>
      <url>posts/65820315/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>prometheus 监控的构建，相比 zabbix 来说，还是要麻烦一些。</p><p>当然，如果你完全熟悉之后，配置文件化也更容易被其他系统所修改。</p><h1>监控端组件</h1><p>搞之前，先创建一个网络</p><p>docker network create promsnet</p><h2 id="grafana">grafana</h2><blockquote><p>数据展示，部署在 proms 端</p></blockquote><pre><code class="language-bash">mkdir -p /export/docker-data-grafana/&#123;data,conf&#125;chmod 777 /export/docker-data-grafana/datadocker run --name grafana \--network promsnet \--mount 'type=bind,src=/export/docker-data-grafana/data,dst=/var/lib/grafana' \--mount 'type=bind,src=/export/docker-data-grafana/conf,dst=/usr/share/grafana/conf' \-p 3000:3000 -d grafana/grafana:7.3.5</code></pre><blockquote><p>默认账户/初始密码都是admin</p><p>默认配置库是sqlite 3</p></blockquote><h3 id="配置">配置</h3><blockquote><p>/export/docker-data-grafana/conf/default.ini</p></blockquote><pre><code class="language-ini">[server]domain = 域名http_port = 端口[database]type = sqlite3host = 127.0.0.1:3306name = grafanauser = rootpassword =[smtp]enabled = truehost = smtp.feishu.cn:465user = password = skip_verify = truefrom_address = from_name = GrafanaAdmin</code></pre><h2 id="alertmanager-告警管理">alertmanager 告警管理</h2><p>告警管理器，接收 proms 发来的告警，并加以处理后发出</p><pre><code class="language-bash">docker volume create alertmanagerdocker run --name alertmanager -d \  --network promsnet \  -p 9093:9093 \  --mount 'type=volume,src=alertmanager,dst=/etc/alertmanager' \prom/alertmanager</code></pre><h3 id="配置-2">配置</h3><p>alertmanager.yml</p><pre><code class="language-bash">global:  resolve_timeout: 5mroute:  group_by: ['alertname']  group_wait: 10s  group_interval: 10s  repeat_interval: 1h  receiver: 'wechat'receivers:- name: 'wechat'  wechat_configs:  - corp_id: ''    to_user: ''    agent_id: ''    api_secret: ''    send_resolved: trueinhibit_rules:  - source_match:      severity: 'critical'    target_match:      severity: 'warning'    equal: ['alertname', 'dev', 'instance']</code></pre><p>上面是微信的，你也可以webhook方式，来走其它方式，例如飞书/钉钉</p><pre><code class="language-bash">global:  resolve_timeout: 5mroute:  group_by: ['instance']  group_wait: 10s  group_interval: 10s  repeat_interval: 1h  receiver: 'web.hook.prometheusalert'receivers:- name: 'web.hook.prometheusalert'  webhook_configs:  - url: ''   # 这里填写 webhook 调用inhibit_rules:  - source_match:      severity: 'critical'    target_match:      severity: 'warning'    equal: ['alertname', 'dev', 'instance']</code></pre><p>💁飞书/钉钉，可以用部署 <a href="https://gitee.com/feiyu563/PrometheusAlert">PrometheusAlert</a></p><h3 id="告警分组、收敛、静默">告警分组、收敛、静默</h3><p>分组的意思，就是将某一个组内同一时期的告警合并发送，例如根据实例来分组。</p><p>收敛的意思，就是告警A规则和告警B规则同时触发，但是告警A规则出现的时候，必然会触发告警B，此时只发告警A，例如MYSQL机器挂了，那么只需要发机器挂掉的告警，MYSQL的告警就没必要发送了。</p><p>静默的意思，就是用户已知这个时间点会触发告警规则，但是无需触发，例如高压力定时任务引起磁盘IO告警，虽然会触发平均指标告警，但是在用户认定的安全范围内，因而无需触发。</p><ul><li>分组</li></ul><p>根据 label 进行分组，同组的预警尽量一次性发出</p><pre><code class="language-yaml">route:  group_by: ['alertname'] # 告警分组  group_wait: 10s # 分组等待时间，也就是说第一个告警等待同组内其它告警来临的时间  group_interval: 5m # 分组发送不同告警规则的静默周期，以及发送失败的静默周期  repeat_interval: 1h # 分组发送相同告警规则的静默周期，如果时间结束，告警状态未变，将再次发送.</code></pre><ul><li>收敛</li></ul><p>举例说明：当主机挂了，此时只需要发送主机挂掉的预警，无需再发送因主机挂掉而产出的其它预警。</p><pre><code class="language-yaml">inhibit_rules:  - source_match:   # 匹配最底层的那个告警规则，比如服务器挂了      severity: 'critical'       target_match:   # 匹配当 source_match 触发的时候，无需告警的规则。      severity: 'warn'    equal: ['instance']   # 根据标签确定哪些匹配到 target_match 的需要忽略。例如，equal: instance 那么当 source_match 和 target_match 的 instance 值一样的时候，target_match 被忽略。</code></pre><blockquote><p>标签的 key ，可以是你在 proms.rule 中自定义，也可以是被监控端组件采集数据后发给 proms 的。</p></blockquote><h2 id="Prometheus-主程">Prometheus 主程</h2><pre><code class="language-bash">mkdir /export/docker-data-proms/&#123;data,conf&#125; -pchmod 777 /export/docker-data-proms/data</code></pre><h3 id="配置-3">配置</h3><h4 id="主配置">主配置</h4><p>/export/docker-data-proms/promethesu.yml</p><pre><code class="language-:"># my global configglobal:  scrape_interval:     15s # 设置采集间隔为每15秒一次。默认值为每1分钟一次。  evaluation_interval: 15s # 每15秒评估一次规则。默认为每1分钟一次。  # scrape_timeout is set to the global default (10s).# 告警管理器配置alerting:  alertmanagers:  - static_configs:    - targets:      - alertmanager:9093# 一次加载规则，并根据全局的'evaluation_interval'定期评估规则。rule_files:  - &quot;/etc/prometheus/conf/rules/*.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.#  - job_name: 'prometheus'#    static_configs:#    - targets: ['proms:9090']#  - job_name: &quot;grafana&quot;#    static_configs:#    - targets: ['grafana:3000']  - job_name: &quot;dockersInfo&quot;    static_configs:    - targets: ['192.168.1.1:10052','192.168.1.2:10052']  - job_name: &quot;node_exporter&quot;    static_configs:    - targets: ['192.168.1.1:9100','192.168.1.2:9100']</code></pre><h4 id="定义需要告警的指标规则">定义需要告警的指标规则</h4><p>将采集的数据指标通过 expr 进行运算，并通过 record 进行命名</p><h5 id="主机监控指标规则">主机监控指标规则</h5><p>/export/docker-data-proms/conf/rules/node-exporter-record-rules.yml</p><pre><code class="language-bash"># node-exporter-record-rules.yml# 标签 job 关联主配置定义的任务 node_exporter，获取任务传递的数据，从而抽取信息定义 expr# 给 expr 表达式设置一个别名 record, 别名可以被其它 rules 调用groups:  - name: node_exporter-record    rules:    - expr: up&#123;job=~&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:up      labels:        desc: &quot;节点是否在线, 在线1,不在线0&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;    - expr: time() - node_boot_time_seconds&#123;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:node_uptime      labels:        desc: &quot;节点的运行时间&quot;        unit: &quot;s&quot;        job: &quot;node_exporter&quot;###############################################################################################                              cpu                                                           #    - expr: (1 - avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:total:percent      labels:        desc: &quot;节点的cpu总消耗百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:idle:percent      labels:        desc: &quot;节点的cpu idle百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;iowait&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:iowait:percent      labels:        desc: &quot;节点的cpu iowait百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;system&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:system:percent      labels:        desc: &quot;节点的cpu system百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;user&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:user:percent      labels:        desc: &quot;节点的cpu user百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=~&quot;softirq|nice|irq|steal&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:other:percent      labels:        desc: &quot;节点的cpu 其他的百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;###############################################################################################                                    memory                                                  #    - expr: node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:total      labels:        desc: &quot;节点的内存总量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_memory_MemFree_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free      labels:        desc: &quot;节点的剩余内存量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125; - node_memory_MemFree_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used      labels:        desc: &quot;节点的已使用内存量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125; - node_memory_MemAvailable_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:actualused      labels:        desc: &quot;节点用户实际使用的内存量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: (1-(node_memory_MemAvailable_bytes&#123;job=&quot;node_exporter&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used:percent      labels:        desc: &quot;节点的内存使用百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: ((node_memory_MemAvailable_bytes&#123;job=&quot;node_exporter&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free:percent      labels:        desc: &quot;节点的内存剩余百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;###############################################################################################                                   load                                                     #    - expr: sum by (instance) (node_load1&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load1      labels:        desc: &quot;系统1分钟负载&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;    - expr: sum by (instance) (node_load5&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load5      labels:        desc: &quot;系统5分钟负载&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;    - expr: sum by (instance) (node_load15&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load15      labels:        desc: &quot;系统15分钟负载&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;###############################################################################################                                 disk                                                       #    - expr: node_filesystem_size_bytes&#123;job=&quot;node_exporter&quot; ,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:total      labels:        desc: &quot;节点的磁盘总量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_filesystem_avail_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:free      labels:        desc: &quot;节点的磁盘剩余空间&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_filesystem_size_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125; - node_filesystem_avail_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:used      labels:        desc: &quot;节点的磁盘使用的空间&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr:  (1 - node_filesystem_avail_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_size_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:used:percent      labels:        desc: &quot;节点的磁盘的使用百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: irate(node_disk_reads_completed_total&#123;job=&quot;node_exporter&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:count:rate      labels:        desc: &quot;节点的磁盘读取速率&quot;        unit: &quot;次/秒&quot;        job: &quot;node_exporter&quot;    - expr: irate(node_disk_writes_completed_total&#123;job=&quot;node_exporter&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:count:rate      labels:        desc: &quot;节点的磁盘写入速率&quot;        unit: &quot;次/秒&quot;        job: &quot;node_exporter&quot;    - expr: (irate(node_disk_written_bytes_total&#123;job=&quot;node_exporter&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:mb:rate      labels:        desc: &quot;节点的设备读取MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;node_exporter&quot;    - expr: (irate(node_disk_read_bytes_total&#123;job=&quot;node_exporter&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:mb:rate      labels:        desc: &quot;节点的设备写入MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;node_exporter&quot;###############################################################################################                                filesystem                                                  #    - expr:   (1 -node_filesystem_files_free&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_files&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filesystem:used:percent      labels:        desc: &quot;节点的inode的剩余可用的百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;##############################################################################################                                filefd                                                     #    - expr: node_filefd_allocated&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:count      labels:        desc: &quot;节点的文件描述符打开个数&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: node_filefd_allocated&#123;job=&quot;node_exporter&quot;&#125;/node_filefd_maximum&#123;job=&quot;node_exporter&quot;&#125; * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:percent      labels:        desc: &quot;节点的文件描述符打开百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;##############################################################################################                                network                                                    #    - expr: avg by (environment,instance,device) (irate(node_network_receive_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:bit:rate      labels:        desc: &quot;节点网卡eth0每秒接收的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:bit:rate      labels:        desc: &quot;节点网卡eth0每秒发送的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:packet:rate      labels:        desc: &quot;节点网卡每秒接收的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:packet:rate      labels:        desc: &quot;节点网卡发送的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:error:rate      labels:        desc: &quot;节点设备驱动器检测到的接收错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:error:rate      labels:        desc: &quot;节点设备驱动器检测到的发送错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: node_tcp_connection_states&#123;job=&quot;node_exporter&quot;, state=&quot;established&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:established:count      labels:        desc: &quot;节点当前established的个数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;    - expr: node_tcp_connection_states&#123;job=&quot;node_exporter&quot;, state=&quot;time_wait&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:timewait:count      labels:        desc: &quot;节点timewait的连接数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;    - expr: sum by (environment,instance) (node_tcp_connection_states&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:total:count      labels:        desc: &quot;节点tcp连接总数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;##############################################################################################                                process                                                    #    - expr: node_processes_state&#123;state=&quot;Z&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:process:zoom:total:count      labels:        desc: &quot;节点当前状态为zoom的个数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;##############################################################################################                                other                                                    #    - expr: abs(node_timex_offset_seconds&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:time:offset      labels:        desc: &quot;节点的时间偏差&quot;        unit: &quot;s&quot;        job: &quot;node_exporter&quot;##############################################################################################    - expr: count by (instance) ( count by (instance,cpu) (node_cpu_seconds_total&#123; mode='system'&#125;) ) * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:count</code></pre><h5 id="容器监控指标规则">容器监控指标规则</h5><p>/export/docker-data-proms/conf/rules/dockersInfo-record-rules.yml</p><pre><code class="language-yaml">groups:  - name: node_exporter-record    rules:    - expr: up&#123;job=~&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:up      labels:        desc: &quot;节点是否在线, 在线1,不在线0&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;    - expr: time() - node_boot_time_seconds&#123;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:node_uptime      labels:        desc: &quot;节点的运行时间&quot;        unit: &quot;s&quot;        job: &quot;node_exporter&quot;###############################################################################################                              cpu                                                           #    - expr: (1 - avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:total:percent      labels:        desc: &quot;节点的cpu总消耗百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:idle:percent      labels:        desc: &quot;节点的cpu idle百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;iowait&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:iowait:percent      labels:        desc: &quot;节点的cpu iowait百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;system&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:system:percent      labels:        desc: &quot;节点的cpu system百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=&quot;user&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:user:percent      labels:        desc: &quot;节点的cpu user百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;node_exporter&quot;,mode=~&quot;softirq|nice|irq|steal&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:other:percent      labels:        desc: &quot;节点的cpu 其他的百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;###############################################################################################                                    memory                                                  #    - expr: node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:total      labels:        desc: &quot;节点的内存总量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_memory_MemFree_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free      labels:        desc: &quot;节点的剩余内存量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125; - node_memory_MemFree_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used      labels:        desc: &quot;节点的已使用内存量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125; - node_memory_MemAvailable_bytes&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:actualused      labels:        desc: &quot;节点用户实际使用的内存量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: (1-(node_memory_MemAvailable_bytes&#123;job=&quot;node_exporter&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used:percent      labels:        desc: &quot;节点的内存使用百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: ((node_memory_MemAvailable_bytes&#123;job=&quot;node_exporter&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;node_exporter&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free:percent      labels:        desc: &quot;节点的内存剩余百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;###############################################################################################                                   load                                                     #    - expr: sum by (instance) (node_load1&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load1      labels:        desc: &quot;系统1分钟负载&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;    - expr: sum by (instance) (node_load5&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load5      labels:        desc: &quot;系统5分钟负载&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;    - expr: sum by (instance) (node_load15&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load15      labels:        desc: &quot;系统15分钟负载&quot;        unit: &quot; &quot;        job: &quot;node_exporter&quot;###############################################################################################                                 disk                                                       #    - expr: node_filesystem_size_bytes&#123;job=&quot;node_exporter&quot; ,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:total      labels:        desc: &quot;节点的磁盘总量&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_filesystem_avail_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:free      labels:        desc: &quot;节点的磁盘剩余空间&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr: node_filesystem_size_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125; - node_filesystem_avail_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:used      labels:        desc: &quot;节点的磁盘使用的空间&quot;        unit: byte        job: &quot;node_exporter&quot;    - expr:  (1 - node_filesystem_avail_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_size_bytes&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:used:percent      labels:        desc: &quot;节点的磁盘的使用百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: irate(node_disk_reads_completed_total&#123;job=&quot;node_exporter&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:count:rate      labels:        desc: &quot;节点的磁盘读取速率&quot;        unit: &quot;次/秒&quot;        job: &quot;node_exporter&quot;    - expr: irate(node_disk_writes_completed_total&#123;job=&quot;node_exporter&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:count:rate      labels:        desc: &quot;节点的磁盘写入速率&quot;        unit: &quot;次/秒&quot;        job: &quot;node_exporter&quot;    - expr: (irate(node_disk_written_bytes_total&#123;job=&quot;node_exporter&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:mb:rate      labels:        desc: &quot;节点的设备读取MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;node_exporter&quot;    - expr: (irate(node_disk_read_bytes_total&#123;job=&quot;node_exporter&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:mb:rate      labels:        desc: &quot;节点的设备写入MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;node_exporter&quot;###############################################################################################                                filesystem                                                  #    - expr:   (1 -node_filesystem_files_free&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_files&#123;job=&quot;node_exporter&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filesystem:used:percent      labels:        desc: &quot;节点的inode的剩余可用的百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;##############################################################################################                                filefd                                                     #    - expr: node_filefd_allocated&#123;job=&quot;node_exporter&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:count      labels:        desc: &quot;节点的文件描述符打开个数&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;    - expr: node_filefd_allocated&#123;job=&quot;node_exporter&quot;&#125;/node_filefd_maximum&#123;job=&quot;node_exporter&quot;&#125; * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:percent      labels:        desc: &quot;节点的文件描述符打开百分比&quot;        unit: &quot;%&quot;        job: &quot;node_exporter&quot;##############################################################################################                                network                                                    #    - expr: avg by (environment,instance,device) (irate(node_network_receive_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:bit:rate      labels:        desc: &quot;节点网卡eth0每秒接收的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:bit:rate      labels:        desc: &quot;节点网卡eth0每秒发送的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:packet:rate      labels:        desc: &quot;节点网卡每秒接收的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:packet:rate      labels:        desc: &quot;节点网卡发送的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:error:rate      labels:        desc: &quot;节点设备驱动器检测到的接收错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:error:rate      labels:        desc: &quot;节点设备驱动器检测到的发送错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;node_exporter&quot;    - expr: node_tcp_connection_states&#123;job=&quot;node_exporter&quot;, state=&quot;established&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:established:count      labels:        desc: &quot;节点当前established的个数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;    - expr: node_tcp_connection_states&#123;job=&quot;node_exporter&quot;, state=&quot;time_wait&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:timewait:count      labels:        desc: &quot;节点timewait的连接数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;    - expr: sum by (environment,instance) (node_tcp_connection_states&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:total:count      labels:        desc: &quot;节点tcp连接总数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;##############################################################################################                                process                                                    #    - expr: node_processes_state&#123;state=&quot;Z&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:process:zoom:total:count      labels:        desc: &quot;节点当前状态为zoom的个数&quot;        unit: &quot;个&quot;        job: &quot;node_exporter&quot;##############################################################################################                                other                                                    #    - expr: abs(node_timex_offset_seconds&#123;job=&quot;node_exporter&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:time:offset      labels:        desc: &quot;节点的时间偏差&quot;        unit: &quot;s&quot;        job: &quot;node_exporter&quot;##############################################################################################    - expr: count by (instance) ( count by (instance,cpu) (node_cpu_seconds_total&#123; mode='system'&#125;) ) * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:count</code></pre><h4 id="定义监控指标阈值规则">定义监控指标阈值规则</h4><h5 id="主机监控指标阈值">主机监控指标阈值</h5><p>/export/docker-data-proms/conf/rules/node-exporter-alert-rules.yml</p><pre><code class="language-bash"># node-exporter-alert-rules.yml# 定义告警规则# 通过前一个 rules 文件拿到定义的 record 别名来编写 expr 判断式# 这里定义的告警规则，在触发的时候，都会传递到 alertmanager，最后从传递的信息中抽取所需数据发送给目标人。groups:  - name: node-alert    rules:    - alert: node-down      expr: node_exporter:up == 0      for: 1m      labels:        severity: critical      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 宕机了&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-cpu-high      expr:  node_exporter:cpu:total:percent &gt; 80      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-cpu-iowait-high      expr:  node_exporter:cpu:iowait:percent &gt;= 12      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu iowait 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-load-load1-high      expr:  (node_exporter:load:load1) &gt; (node_exporter:cpu:count) * 1.2      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; load1 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-memory-high      expr:  node_exporter:memory:used:percent &gt; 85      for: 3m      labels:        severity: info      annotations:        summary: &quot;内存使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-high      expr:  node_exporter:disk:used:percent &gt; 80      for: 3m      labels:        severity: info      annotations:        summary: &quot;&#123;&#123; $labels.device &#125;&#125;:&#123;&#123; $labels.mountpoint &#125;&#125; 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-read:count-high      expr:  node_exporter:disk:read:count:rate &gt; 3000      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; iops read 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-write-count-high      expr:  node_exporter:disk:write:count:rate &gt; 3000      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; iops write 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-read-mb-high      expr:  node_exporter:disk:read:mb:rate &gt; 60      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 读取字节数 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-write-mb-high      expr:  node_exporter:disk:write:mb:rate &gt; 60      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 写入字节数 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-filefd-allocated-percent-high      expr:  node_exporter:filefd_allocated:percent &gt; 80      for: 10m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 打开文件描述符 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netin-error-rate-high      expr:  node_exporter:network:netin:error:rate &gt; 4      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包进入的错误速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netin-packet-rate-high      expr:  node_exporter:network:netin:packet:rate &gt; 35000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包进入速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netout-packet-rate-high      expr:  node_exporter:network:netout:packet:rate &gt; 35000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包流出速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-tcp-total-count-high      expr:  node_exporter:network:tcp:total:count &gt; 40000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; tcp连接数量 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-process-zoom-total-count-high      expr:  node_exporter:process:zoom:total:count &gt; 10      for: 10m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 僵死进程数量 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-time-offset-high      expr:  node_exporter:time:offset &gt; 0.03      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; &#123;&#123; $labels.desc &#125;&#125;  &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;</code></pre><p>关于规则里的表达式语句的写法涉及到 PromQL，可以看一下文档</p><p><a href="https://fuckcloudnative.io/prometheus/3-prometheus/basics.html">https://fuckcloudnative.io/prometheus/3-prometheus/basics.html</a></p><pre><code class="language-bash"># 暂时无用的规则groups:  - name: ap-southeast-1服务器告警    rules:    - alert: 服务器宕机告警      expr: up == 0      for: 3m      labels:        region: ap-southeast-1      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;宕机！&quot;        description: &quot;服务器&#123;&#123;$labels.instance&#125;&#125;已宕机！&quot;    - alert: cpu使用率过高告警      expr: (100 - (avg(irate(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) by(instance)* 100))* on(instance) group_left(nodename) (node_uname_info) &gt; 30      for: 5m      labels:        region: ap-southeast-1      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）CPU使用率过高！&quot;        description: '服务器&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）CPU使用率超过85%(目前使用:&#123;&#123;printf "%.2f" $value&#125;&#125;%)'    - alert: 系统负载过高      expr: (node_load1/count without (cpu, mode) (node_cpu_seconds_total&#123;mode=&quot;system&quot;&#125;))* on(instance) group_left(nodename) (node_uname_info)&gt;1.1      for: 3m      labels:        region: ap-southeast-1      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）系统负载过高！&quot;        description: '&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）当前负载超标率 &#123;&#123;printf "%.2f" $value&#125;&#125;'    - alert: 内存不足告警      expr: (100 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100)* on(instance) group_left(nodename) (node_uname_info) &gt; 80      for: 3m      labels:        region: ap-southeast-1      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）内存使用率过高！&quot;        description: '服务器&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）内存使用率超过80%(目前使用:&#123;&#123;printf "%.2f" $value&#125;&#125;%)'    - alert: 硬盘空间不足告警      expr: (100-(node_filesystem_free_bytes&#123;fstype=~&quot;ext4|xfs&quot;&#125;/node_filesystem_size_bytes &#123;fstype=~&quot;ext4|xfs&quot;&#125;*100) )* on(instance) group_left(nodename) (node_uname_info)&gt; 80      for: 3m      labels:        region: ap-southeast-1      annotations:        summary: &quot;&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）硬盘使用率过高！&quot;        description: '服务器&#123;&#123;$labels.instance&#125;&#125;（&#123;&#123;$labels.nodename&#125;&#125;）硬盘使用率超过80%(目前使用:&#123;&#123;printf "%.2f" $value&#125;&#125;%)'</code></pre><h5 id="容器监控指标阈值">容器监控指标阈值</h5><p>/export/docker-data-proms/conf/rules/dockersInfo-alert-rules.yml</p><pre><code class="language-yaml">groups:  - name: container-alert    rules:    - alert: container-restart-times-high      expr: dockersInfo:container:restart &gt; 5      for: 1m      labels:        severity: warn      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; 15分钟内重启次数超过5次&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: container-cpu-usage-high      expr: dockersInfo:container:cpu:total:percent &gt; 90      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; cpu 使用率持续超过90%.&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: container-cpu-iowait-high      expr: dockersInfo:cpu:iowait:percent &gt; 10      for: 1m      labels:        severity: warn      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; cpu iowait 持续超过10%&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;#    - alert: container-mem-usage-high#      expr: dockersInfo:memory:used:percent &gt; 80#      for: 1m#      labels:#        severity: warn#      annotations:#        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; 内存使用率超过80%&quot;#        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;#        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;#        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;#        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;#        type: &quot;aliyun_meta_ecs_info&quot;</code></pre><h3 id="部署">部署</h3><h4 id="docker-方式">docker 方式</h4><pre><code class="language-bash">docker run -d \--network promsnet \-p 9090:9090 \--name proms \--mount 'type=bind,src=/export/docker-data-proms/prometheus.yml,dst=/etc/prometheus/prometheus.yml'  \--mount 'type=bind,src=/export/docker-data-proms/data,dst=/prometheus' \--mount 'type=bind,src=/export/docker-data-proms/conf,dst=/etc/prometheus/conf' \prom/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus --storage.tsdb.retention=30d</code></pre><h4 id="二进制方式">二进制方式</h4><pre><code class="language-bash">wget https://github.com/prometheus/prometheus/releases/download/v2.20.0/prometheus-2.20.0.linux-amd64.tar.gzmkdir prometheustar xf prometheus-2.20.0.linux-amd64.tar.gz --strip-components 1 -C prometheusrm -rf prometheus-2.20.0.linux-amd64.tar.gzcd prometheus &amp;&amp; baseDir=`pwd`cp prometheus.yml&#123;,.bak&#125; #---cat &gt; /usr/lib/systemd/system/prometheus.service &lt;&lt;EOF[Unit]Description=prometheus server daemon[Service]Restart=on-failureExecStart=$&#123;baseDir&#125;/prometheus --config.file=$&#123;baseDir&#125;/prometheus.yml[Install]WantedBy=multi-user.targetEOF#---systemctl daemon-reloadsystemctl start prometheus</code></pre><h3 id="查看">查看</h3><p>被监控端状态：<a href="http://xxx:9090/targets">http://xxx:9090/targets</a></p><p>通过上述地址，你可以看到配置中 job_name 定义的被监控任务的端点状态</p><p><img src="/posts/65820315/image-20201218112329166.png" alt="image-20201218112329166"></p><h2 id="pushgateway-初期用不上的组件">pushgateway (初期用不上的组件)</h2><blockquote><p>push 模式下的网关</p><p>当前没有现成的推送模式的 node-exporter</p></blockquote><pre><code class="language-bash"># dockerdocker run -d --name=pushgateway -p 9091:9091 prom/pushgateway</code></pre><pre><code class="language-bash"># 一个推送模式的采集脚本示例# cat tcpestab.sh #!/bin/bash# 添加脚本到计划任务中，定时采集# pushgateway ippushgatewayIp=#获取主机名，常传输到Prometheus标签以主机名instance_name=`hostname -f | cut -d'.' -f1`#判断主机名不能是localhost不然发送过的数据不知道是那个主机的 if [ $instance_name == &quot;localhost&quot; ];thenecho &quot;Hostname must not localhost&quot;exit 1fi#自定义key，在Prometheus即可使用key查询label=&quot;count_estab_connections&quot; #获取TCP estab 连接数count_estab_connections=`netstat -an | grep -i 'established' | wc -l`#将数据发送到pushgateway固定格式echo &quot;$label $count_estab_connections&quot;  | curl --data-binary @- http://$pushgatewayIp:9091/metrics/job/pushgateway/instance/$instance_name</code></pre><h1>被监控端组件</h1><h2 id="node-exporter-物理节点监控组件">node-exporter 物理节点监控组件</h2><blockquote><p>宿主数据采集端，部署在被监控主机的9100端口</p></blockquote><pre><code class="language-bash">cd /usr/local/wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gzmkdir node_exportertar xf node_exporter-1.0.1.linux-amd64.tar.gz --strip-components 1 -C node_exportermv node_exporter-1.0.1.linux-amd64.tar.gz srccd node_exporter &amp;&amp; baseDir=`pwd`#---cat &gt; /usr/lib/systemd/system/node_exporter.service &lt;&lt;EOF[Unit]Description=node_exporter server daemon[Service]Restart=on-failureExecStart=$&#123;baseDir&#125;/node_exporter[Install]WantedBy=multi-user.targetEOF#---systemctl daemon-reloadsystemctl start node_exportersystemctl status node_exportercurl http://localhost:9100/metrics # 查看获取的监控数据systemctl enable node_exporter</code></pre><h2 id="cadvisor-容器监控组件">cadvisor 容器监控组件</h2><blockquote><p>容器数据采集端，部署在被监控容器所在宿主的10052端口</p></blockquote><pre><code class="language-bash">dockerRoot=`docker info | awk -F':'  '/Docker Root Dir/&#123;print $2&#125;'|sed 's@^ *@@g'`echo $dockerRootdocker run \  --volume=/:/rootfs:ro \  --volume=/var/run:/var/run:rw \  --volume=/sys:/sys:ro \  --volume=$&#123;dockerRoot&#125;/:/var/lib/docker:ro \  --volume=/dev/disk/:/dev/disk:ro \  --publish=10052:8080 \  --privileged=true \  --detach=true \  --name=cadvisor \  google/cadvisor:latest</code></pre><h1>使用</h1><h2 id="添加-grafana-数据源-proms">添加 grafana 数据源 : proms</h2><p><img src="/posts/65820315/image-20201218135400147.png" alt="image-20201218135400147"></p><h2 id="添加-grafana-监控模板">添加 grafana 监控模板</h2><p><img src="/posts/65820315/image-20201218135502105.png" alt="image-20201218135502105"></p><p>node模板：<a href="https://grafana.com/grafana/dashboards/8919">https://grafana.com/grafana/dashboards/8919</a></p><p>docker模板：<a href="https://grafana.com/grafana/dashboards/10566">https://grafana.com/grafana/dashboards/10566</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞server安装注意事项</title>
      <link href="posts/ed865d2f/"/>
      <url>posts/ed865d2f/</url>
      
        <content type="html"><![CDATA[<ol><li><p>数据库编码一定要安装官方来命令来设置，例如zabbix5编码是 utf8和utf8_bin</p></li><li><p>最后web控制台安装界面，要确保zabbix-server可以访问设置的 hostname 的 10051 端口。如果hostname写的域名，那么要确保zabbix-server可以通过 hosts本地解析或者外网访问10051端口</p></li><li><p>zabbix-server 的 agent 要使用拉取模式，因为官方给的模板无法直接修正为推流模式</p></li><li><p>报警媒介设置</p><p>脚本设置</p><pre><code class="language-bash">脚本参数&#123;ALERT.SENDTO&#125;&#123;ALERT.SUBJECT&#125;&#123;ALERT.MESSAGE&#125;</code></pre><p>问题模板</p><pre><code class="language-bash">故障: &#123;TRIGGER.NAME&#125;【[骷髅]】=》&#123;TRIGGER.SEVERITY&#125; 事件ID : &#123;EVENT.ID&#125;【UTC+0】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;【告警等级】: &#123;TRIGGER.SEVERITY&#125;【主机信息1】: &#123;HOST.NAME&#125;【主机信息2】: &#123;HOSTNAME1&#125;【告警信息】: &#123;TRIGGER.NAME&#125;【告警项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;</code></pre><p>恢复模板</p><pre><code class="language-bash">恢复: &#123;TRIGGER.NAME&#125;【[OK]】=》&#123;TRIGGER.SEVERITY&#125; 事件ID : &#123;EVENT.ID&#125;【UTC+0】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;【告警等级】: &#123;TRIGGER.SEVERITY&#125;【主机信息1】: &#123;HOST.NAME&#125;【主机信息2】: &#123;HOSTNAME1&#125;【告警信息】: &#123;TRIGGER.NAME&#125;【告警项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;</code></pre></li><li><p>预警用户添加超管权限</p></li><li><p>中文字体</p><pre><code class="language-bash">yum install google-noto-sans-simplified-chinese-fonts.noarch -yrm -rf /etc/alternatives/zabbix-web-fontln -s /usr/share/fonts/google-noto/NotoSansSC-Regular.otf /etc/alternatives/zabbix-web-font</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞从容器中访问宿主机docker命令</title>
      <link href="posts/3e0f2183/"/>
      <url>posts/3e0f2183/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>我的jenkins是运行在docker中，但是jenkins官方的镜像里却没有docker命令。</p><p>以至于无法在流水线中打包docker镜像。</p><h2 id="方法">方法</h2><p>首先，需要将docker命令、docker.sock文件以及相关依赖文件映射到容器内。</p><p>其次，以root用户访问容器，在容器中添加docker组，并且组id需要和宿主机中的docker组id一致。</p><p>最后，以root用户访问容器，并将jenkins用户加入到容器中的docker组中。</p><p>最最后，最关键的来了， 一定要重启一下 jenkins 容器。。。</p><h2 id="相关命令">相关命令</h2><pre><code class="language-bash"># 额外的映射文件（宿主机文件和容器内的映射路径，以实际情况为准）-v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7# 以 root 用户访问 jenkins 容器（宿主机的组ID以实际情况为准）docker exec -it -u root jenkins /bin/bashgroupadd -g xxx dockerusermod -aG docker jenkins# 重启 jenkins 容器docker stop jenkins &amp;&amp; docker start jenkins</code></pre><h2 id="注意">注意</h2><p>在容器内执行的 groupadd 和 usermod 命令，需要在每次变更容器镜像后，重新执行，因为命令的相关结果都是容器内数据，清理后不会保留。</p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞ssh-MFA自动登陆</title>
      <link href="posts/5db0cd07/"/>
      <url>posts/5db0cd07/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>ssh 关联MFA后,安全度增加了很多,但是每次手动输入MFA动态口令比较麻烦.所以记录一下自动交互输入MFA口令</p><h2 id="命令安装">命令安装</h2><pre><code class="language-bash">sudo apt install oathtool gnupg2 expect</code></pre><blockquote><p>oathtool 是我们用来生成MFA口令的工具.</p><p>expect 用来编写交互程序</p></blockquote><h2 id="登陆交互脚本">登陆交互脚本</h2><pre><code class="language-bash">#!/usr/bin/expectset timeout 5set MFAToken &quot;我是MFA的TOKEN&quot;spawn 我是ssh命令expect &quot;MFA auth&quot;send &quot;[exec oathtool -b --totp $MFAToken]\r&quot;;interact</code></pre><blockquote><p>将脚本中的TOKEN和命令替换为自己的.</p></blockquote><h2 id="注意">注意</h2><p>如果你发现你登陆不了, 每次都验证错误, 那么你应该检查下你机器的时间是否正常.</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ssh </tag>
            
            <tag> mfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jdk☞安装和配置</title>
      <link href="posts/ac6b3b36/"/>
      <url>posts/ac6b3b36/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>oracle jdk 现在是商业产品了, 所以线上最好还是用 openjdk.</p><h2 id="安装">安装</h2><p><a href="https://adoptopenjdk.net/installation.html#linux-pkg">https://adoptopenjdk.net/installation.html#linux-pkg</a></p><h2 id="配置">配置</h2><pre><code class="language-bash">export JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$PATHexport MANPATH=$JAVA_HOME/manexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/jre/lib/rt.jar</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jdk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞fcgi中alias的使用</title>
      <link href="posts/58a1c0c7/"/>
      <url>posts/58a1c0c7/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>众所周知, nginx 在配置静态资源的时候, root 和 alias 分别是两种指定资源路径的方式.</p><p>如果：</p><p>​url 是 <code>http://xxx.com/god/a.jpg</code>，且 location 匹配的是 <code>/god/</code>.</p><p>则：</p><p>​当用 root 定位资源路径时, root 配置的值=域名<code>xxx.com</code> 部分所对应的路径, 此时 a.jpg 的物理路径就是  $root/god/a.jpg</p><p>​当用 alias 定位资源路径时, alias 配置的值=url<code>xxx.com/god/</code>部分所对应的路径, 此时 a.jpg 的物理路径就是 ${alias}a.jpg</p><p>即</p><pre><code class="language-bash">location ^~ /god/ &#123;    root /export/webapps/xxx.com;&#125;# 或者location ^~ /god/ &#123;    alias /export/webapps/xxx.com/god/;&#125;</code></pre><p>当我采用上述规则, 使用alias配置fcgi的时候,现实给了我暴击…妥妥的404了.</p><h2 id="在fcgi环境下-alias-的配置">在fcgi环境下, alias 的配置</h2><p>废话不多说, 直接上结果.</p><pre><code class="language-bash">location ^~ /god/ &#123;     index  index.php index.html index.htm;     root /export/webapps/xxx.com/;     location ~* &quot;\.php$&quot; &#123;         try_files      $uri =404;         fastcgi_pass   127.0.0.1:9000;         fastcgi_index  index.php;         include        fastcgi.conf;         fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;     &#125; &#125; location ^~ /god/ &#123;    index  index.php index.html index.htm;    alias /export/webapps/xxx.com/god/;    location ~* &quot;\.php$&quot; &#123;        try_files      $uri =404;        fastcgi_pass   127.0.0.1:9000;        include        fastcgi.conf;        fastcgi_param  SCRIPT_FILENAME  $request_filename;    &#125;&#125;</code></pre><blockquote><p>官方的例子:</p><p><a href="https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/#use-request-filename-for-script-filename">https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/#use-request-filename-for-script-filename</a></p></blockquote><p>关键点：</p><ol><li>当你用 alias 而不是 root 的时候, 你需要将$document_root$fastcgi_script_name替换成$request_filename</li><li>当你不在使用$fastcgi_script_name的时候，你需要显性的添加index，而不是fastcgi_index ,因为$request_filename并不能关联fastcgi_index。</li></ol><p>原因在于$document_root的值，来自于root或者alias，$fastcgi_script_name总是拿url_path</p><p>假设你访问 /god/api.php,  而这个文件的物理路径是/export/webapps/xxx.com/god/api.php。</p><p>此时，</p><p>当你用 alias 的时候</p><p>$document_root = alias = /export/webapps/xxx.com/god/</p><p>$fastcgi_script_name = /god/api.php</p><p>$document_root$fastcgi_script_name=/export/webapps/xxx.com/god//god/api.php</p><p>而$request_filename当前文件的请求路径，由root或者alias+uri相对路径</p>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> fcgi </tag>
            
            <tag> php </tag>
            
            <tag> alias </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞角色</title>
      <link href="posts/720afd15/"/>
      <url>posts/720afd15/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>角色的作用就是规范化，将一个playbook各部分分门别类的放置在规定好的目录中。就如同linux系统一样，/etc/就是放配置的，/bin 就是放程序的，/tmp 就是放临时文件的 …</p><p>ansible 会基于官方规定好的目录结构, 去自动加载目录中的文件. 当一个需求很复杂的时候, 我们就可以基于角色对需求进行分组.</p><p>最后, 如果你不按照这个规定来走, 那么ansible角色模块就找不到相关东西.</p><h2 id="角色结构">角色结构</h2><p>这是一个官方项目的例子</p><blockquote><p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html#">https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html#</a>  官方文档</p></blockquote><pre><code class="language-bash">site.ymlwebservers.ymlfooservers.ymlroles/    common/        tasks/        handlers/        files/        templates/        vars/        defaults/        meta/    webservers/        tasks/        defaults/        meta/</code></pre><p>例子中，common 和 webservers 就是两个角色<br>关于角色的目录，必须包含下面列举出来的目录之一，目录可以为空。但是如果你使用到了某个目录，那么部分目录需要包含main.yml文件。</p><ul><li>tasks 角色用到的主要任务列表</li><li>handlers 角色或者角色外用到的任务后续处理</li><li>defaults 角色默认变量</li><li>vars 角色其余变量，优先级大于 defaults 中定义的变量</li><li>files 角色部署中牵扯到的文件</li><li>templates 角色的jinja2模板</li><li>meta 角色的元数据信息，例如角色的属性(作者, 说明, 一些特殊功能等)</li></ul><p>最后，目录中main.yml用来存储主配置信息。在这里，你可以包含其它的子任务，在子任务中详细描述。<br>例如，在 tasks/main.yml 中，通过 import_tasks，包含其它子任务，就像下面这样</p><blockquote><p>main 包含 redhat 和 debian。当系统是 redhat 时，安装 httpd 包，当系统是 debian 时，安装 apache2 包</p></blockquote><pre><code class="language-yaml"># roles/xxx/tasks/main.yml- name: added in 2.4, previously you used 'include'  import_tasks: redhat.yml  when: ansible_facts['os_family']|lower == 'redhat'- import_tasks: debian.yml  when: ansible_facts['os_family']|lower == 'debian'# roles/xxx/tasks/redhat.yml- yum:    name: &quot;httpd&quot;    state: present# roles/xxx/tasks/debian.yml- apt:    name: &quot;apache2&quot;    state: present</code></pre><h2 id="使用角色">使用角色</h2><pre><code class="language-yaml">---# file: site.yml- include: webservers.yml- include: fooservers.yml</code></pre><pre><code class="language-yaml">---# file: webservers.yml- hosts: webservers  roles:    - common    - webservers</code></pre><p>只运行 webservers 角色，可以通过 site.yml 添加 limit 限制执行，也可以直接调用</p><pre><code class="language-bash">ansible-playbook site.yml --limit webserversansible-playbook webservers.yml</code></pre><h2 id="角色优先级">角色优先级</h2><p>角色中针对 tasks, handlers, vars 有一个优先级概念. 优先级大的会覆盖掉优先级小的配置.</p><p>优先级由大到小如下:</p><p><code>cli 层面参数 &gt; role-xxx-dir &gt; playbook-xxx &gt; role-defaults-dir</code></p><p>这里 cli 层面参数指的是 ansible-playbook -e vara=‘a’ test.play 这种外部传递变量的行为</p><p>这里 role-xxx-dir 指的是 role 特定目录里的配置, 例如 tasks, handlers, vars</p><p>这里 playbook-xxx 指的是直接写入 ploybook 中的部分</p><p>这里 role-defaults-dir 指的是角色目录中的 defaults 默认变量目录</p><h2 id="角色复制">角色复制</h2><p>如果你想让一个角色多次执行，就如同下面这样</p><pre><code class="language-yaml">---- hosts: webservers  roles:    - moo    - moo</code></pre><p>有下列两种方式:</p><ol><li>moo，拥有不同的参数</li><li>将<code>allow_duplicates: true</code>写入 moo/meta/main.yml</li></ol><h2 id="角色标签">角色标签</h2><p>我们一定要针对角色中的tasks添加标签(tags). 原因在于, 有些时候, 当我们只想修改部署很久的一堆机器的某个服务时, 我们只需要在执行角色的 playbook 的时候,追加 --tags 即可帮助我们执行项目中某一个任务，而不必执行所有任务.</p><p>比如, 我们有个部署项目, 其中有一个初始化角色( common ). common 中有众多的初始化任务, 其中一个任务是 ntp 服务的 安装/配置/启动/重启 .</p><p>ntp 服务配置如下:</p><pre><code class="language-yaml">---# file: roles/common/tasks/main.yml- name: be sure ntp is installed  yum: pkg=ntp state=installed  tags: ntp- name: be sure ntp is configured  template: src=ntp.conf.j2 dest=/etc/ntp.conf  notify:    - restart ntpd  tags: ntp- name: be sure ntpd is running and enabled  service: name=ntpd state=running enabled=yes  tags: ntp  ---# file: roles/common/handlers/main.yml- name: restart ntpd  service: name=ntpd state=restarted</code></pre><p>现在, 我们只想修改生产环境的 ntp 服务的配置, 那么只需要将 ntp.conf.j2 模板配置好之后, 重新执行这个playbooks即可. 就想下面这样:</p><pre><code class="language-bash">ansible-playbook -i production site.yml --tags ntp</code></pre><h2 id="写好的playbook">写好的playbook</h2><p><a href="https://galaxy.ansible.com/">https://galaxy.ansible.com/</a></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞模板</title>
      <link href="posts/724fda19/"/>
      <url>posts/724fda19/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>当你用ansible进行多机器的配置调整，且调整的东西都一模一样，此时你不会拒绝模板的诱惑。</p><p>ansible的模板是jinja2，所以jinja2的特性，在这里都可以用。</p><blockquote><p>模板中，不要出现任何你觉得模板会忽略的东西，包括但不限于空格</p></blockquote><h2 id="模块-template">模块 template</h2><p>参数：</p><ul><li>src 模板文件路径</li><li>dest 目的文件路径</li></ul><p>牵扯到目的路径，必然有权限参数</p><ul><li>owner 目的属主</li><li>group 目的属组</li><li>mode 目的权限</li></ul><p>覆盖与备份</p><ul><li>force 覆盖，yes / no</li><li>backup 备份， yes / no ， 若为 yes ，则目的重名文件会先改名</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - template:        src: ~/test.j2        dest: ~/test.info</code></pre><h2 id="模板分隔符">模板分隔符</h2><pre><code class="language-bash">&#123;&#123; &#125;&#125; 一般用来填充变量，可以是过滤器，也可以填充表达式，从而返回相应的值，例如 &#123;&#123; 1==1 &#125;&#125; 返回 True&#123;% %&#125; 一般用来填充控制语句&#123;# #&#125; 模板注释语句，并非渲染后会出现#  ... ## 这一种 ansible 貌似不支持，所以可以忽略</code></pre><h3 id="分隔符1">分隔符1</h3><pre><code class="language-jinja2"># &#123;&#123; &#125;&#125;&#123;# 普通变量 #&#125;&#123;&#123; foo.bar &#125;&#125;&#123;&#123; foo['bar'] &#125;&#125;&#123;# 以过滤器 lookup 为例 #&#125;&#123;&#123; lookup('file', '~/test.file') &#125;&#125;&#123;&#123; lookup('env', 'PATH' )&#125;&#125;</code></pre><p>最终目的文件，会输出<code>~/test.file</code> 内容和 <code>$PATH</code> 内容</p><blockquote><p>字符串拼接需要使用<code>~</code>，例如 <code>&quot;name:&quot;~name</code></p></blockquote><h3 id="分隔符2">分隔符2</h3><pre><code class="language-bash"># &#123;% %&#125;# 官网所有的控制列表https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures</code></pre><h4 id="条件控制语句-if">条件控制语句 if</h4><pre><code class="language-jinja2">&#123;% if 条件1 %&#125;  pass&#123;% elif 条件2 %&#125;  pass&#123;% else %&#125;  pass &#123;% endif %&#125;</code></pre><h4 id="循环语句-for">循环语句 for</h4><pre><code class="language-jinja2">&#123;% for i in 可迭代对象 %&#125;  &#123;&#123; i &#125;&#125;&#123;% endfor %&#125;</code></pre><blockquote><p>默认循环后，每一个循环单体独占一行，如果需要删除独占，则需要给第二个%}和第三个控制符{%加减号，最终变为-%}和{%-。</p></blockquote><p>关于字典类型，可以使用 iteritems() 函数，从而方便的获取到字典的 k 和 v。例如</p><pre><code class="language-jinja2">&#123;% for k,v in &#123;'name':'zhangsan', 'gender':'male'&#125;.iteritems() %&#125;  &#123;&#123; k &#125;&#125;:&#123;&#123; v &#125;&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后name:zhangsangender:male</code></pre><h4 id="条件和循环组合语句">条件和循环组合语句</h4><pre><code class="language-jinja2">&#123;% for i in 可迭代对象 if 条件 %&#125;  满足条件语句&#123;% else %&#125;  不满足条件语句&#123;% endfor %&#125;</code></pre><p>例如</p><pre><code class="language-jinja2">&#123;% for i in [1,2,3,4] if i>2 %&#125;&#123;&#123; i &#125;&#125;'s index is &#123;&#123; loop.index &#125;&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后3's index is 14's index is 2</code></pre><blockquote><p>loop.index 是循环体索引，这里可能会有个疑问。</p><p>正常情况下，3和4的索引应该就是3和4，之所以是1和2，原因在于当条件控制和循环控制位于同一行的时候，先行运算的是 <code>[1,2,3,4] if i&gt;2</code>，之后才开始走<code>for</code>循环。</p><p>如果你想输出原始循环体，则需要将条件控制语句另起一行，放在<code>for</code>循环内部</p></blockquote><pre><code class="language-jinja2">&#123;% for i in [1,2,3,4] %&#125;&#123;% if i>2 %&#125;&#123;&#123; i &#125;&#125;'s index is &#123;&#123; loop.index &#125;&#125;&#123;% endif %&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后3's index is 34's index is 4</code></pre><blockquote><p>上述的 loop.index 只是jinja2的一种使用方式，其它方式具体可见官网文档</p><p><a href="https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures">https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures</a></p></blockquote><h4 id="宏-macro">宏 macro</h4><p>宏就是类似于函数的一个东西。</p><pre><code class="language-jinja2">&#123;# 编写宏 #&#125;&#123;% macro func() %&#125;函数体&#123;% endmacro %&#125;&#123;# 调用宏 #&#125;&#123;&#123; func() &#125;&#125;</code></pre><p>例如：</p><pre><code class="language-jinja2">&#123;% macro func(a,b,c=3,d=4) %&#125;&#123;# 宏编写的时候，宏参数，要遵循默认参数在后&#123;&#123; a &#125;&#125;&#123;&#123; b &#125;&#125;&#123;&#123; c &#125;&#125;&#123;&#123; d &#125;&#125;&#123;% endmacro %&#125;&#123;&#123; func(1,2,5) &#125;&#125;</code></pre><pre><code class="language-bash"># 渲染后1254</code></pre><blockquote><p>当给出参数超出了宏所定义的参数时，根据情况，宏会将多余的参数存在变量中，即：</p><p>超出的为非关键字参数，则存放在一个叫<code>varargs</code>的元组中</p><p>超出的为关键字参数，则存放在一个叫<code>kwargs</code>的字典中</p></blockquote><h4 id="call-方法">call 方法</h4><p>如同当前函数的装饰器，可以扩展当前宏的功能</p><pre><code class="language-jinja2">&#123;# 编写宏 func，并调用 caller #&#125;&#123;% macro func(a) %&#125;我有一个&#123;&#123; a &#125;&#125;。&#123;&#123; caller(a) &#125;&#125;&#123;% endmacro %&#125;&#123;# 编写宏 func_ext #&#125;&#123;% macro func_ext(a,b) %&#125;但&#123;&#123; b &#125;&#125;比&#123;&#123; a &#125;&#125;好吃。&#123;% endmacro %&#125;&#123;# 通过 call 关联 func，加载 func_ext #&#125;&#123;% call(a) func('汉堡') %&#125;&#123;&#123; func_ext(a,'三明治') &#125;&#125;&#123;% endcall %&#125;</code></pre><blockquote><p>caller是call的对象，因此caller也是可以给call传参</p></blockquote><pre><code class="language-bash"># 渲染后我有一个汉堡。但三明治比汉堡好吃</code></pre><h2 id="扩展">扩展</h2><blockquote><p>扩展官方文档，可见 <a href="https://jinja.palletsprojects.com/en/master/extensions/">https://jinja.palletsprojects.com/en/master/extensions/</a></p></blockquote><p>这里我只简单的说一下如何启动 <code>for</code> 循环中的 <code>break</code> 和 <code>continue</code>。</p><p><code>ansible</code> 中添加 <code>jinja2</code> 扩展，需要修改主配置文件 <code>/etc/ansible/ansible.cfg</code>，找到 <code>jinja2_extensions </code>，在后面追加扩展配置即可，每一个扩展用逗号<code>,</code>分割。</p><p><code>break</code>和<code>continue</code> 的扩展名叫：<code>jinja2.ext.loopcontrols</code></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-include</title>
      <link href="posts/6ce01f58/"/>
      <url>posts/6ce01f58/</url>
      
        <content type="html"><![CDATA[<h2 id="引入额外任务">引入额外任务</h2><pre><code class="language-yaml">tasks:  - include: add.yml</code></pre><h2 id="绑定-kv-对，从而改变额外任务里的变量">绑定 kv 对，从而改变额外任务里的变量</h2><pre><code class="language-yaml">tasks:  - include: add.yml    var1=hello    var2=world</code></pre><h2 id="绑定-tags-标记">绑定 tags 标记</h2><blockquote><p>可以通过tags执行相应的额外任务</p></blockquote><pre><code class="language-yaml:">tasks:  - include: add1.yml    tags: add1  - include: add2.yml    tags: add2</code></pre><pre><code class="language-bash">ansible-playbook test.play --tags add1 # 仅执行 add1.yml 任务</code></pre><h2 id="绑定-loop-循环">绑定 loop 循环</h2><pre><code class="language-yaml">tasks:  - include: add.yml    loop:      - [1,2,3]# add.yml- debug:  msg: &quot;loop-item: &#123;&#123; item &#125;&#125; in add.yml &quot;</code></pre><h2 id="绑定-when-条件">绑定 when 条件</h2><pre><code class="language-yaml">tasks:  - include: add.yml    when: 1 &lt; 2</code></pre><hr><p>ansible 在当前版本2.9中，推荐使用 import_tasks 和 include_tasks 来替换 include，include 未来有可能不在支持。（为啥总感觉 ansible 各种变呢）</p><p>import_tasks 静态任务导入，静态任务简单来说，就是不能从任务外传递变量到任务中。</p><p>include_tasks 动态任务导入。支持循环传递变量</p><p>import_tasks 绑定 when 的时候，会将 when 的条件一对一的应用到任务文件中列出的所有任务</p><p>include_tasks 绑定 when 的时候，会将 when 的条件仅应用到任务文件。即只要条件为真，任务文件里的所有任务都会执行。</p><p>关于新版写法，绑定 tags 的方式，和旧版差异比较大，例如</p><h4 id="include-tasks">include_tasks</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: test      include_tasks:         file: add.yml        apply:          tags:            - add      tags:        - always        # add.yml- debug:    msg: &quot;&#123;&#123; item &#125;&#125; is ok&quot;  loop: [1,2,3]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> include </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo☞本地搜索</title>
      <link href="posts/f5c218d/"/>
      <url>posts/f5c218d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>安装完 hexo-generator-search 后，发现搜索结果始终是所有文章.</p><p>如果你用的也是 <a href="https://github.com/Molunerfinn/hexo-theme-melody">Melody</a> 主题，那么可以参考如下信息，来确认。</p><h2 id="效果图">效果图</h2><p><img src="/posts/f5c218d/image-20200518121421437.png" alt="image-20200518121421437"></p><h2 id="软件包">软件包</h2><blockquote><p>截至：2020.05.18，软件包如下</p></blockquote><pre><code class="language-bash">  &quot;dependencies&quot;: &#123;    &quot;gitalk&quot;: &quot;^1.6.2&quot;,    &quot;hexo&quot;: &quot;^4.0.0&quot;,    &quot;hexo-asset-image&quot;: &quot;^1.0.0&quot;,    &quot;hexo-deployer-git&quot;: &quot;^2.1.0&quot;,    &quot;hexo-generator-archive&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-category&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-index&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-search&quot;: &quot;^2.4.0&quot;,    &quot;hexo-generator-tag&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-ejs&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-marked&quot;: &quot;^2.0.0&quot;,    &quot;hexo-renderer-pug&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-stylus&quot;: &quot;^1.1.0&quot;,    &quot;hexo-server&quot;: &quot;^1.0.0&quot;,    &quot;react&quot;: &quot;^15.3.1&quot;,    &quot;react-dom&quot;: &quot;^15.3.1&quot;  &#125;</code></pre><h2 id="config-yml">_config.yml</h2><blockquote><p>追加内容如下</p></blockquote><pre><code class="language-yaml">search:  path: search.xml  field: post  content: true</code></pre><h2 id="主题配置">主题配置</h2><blockquote><p>修改内容</p></blockquote><pre><code class="language-yaml">local_search:  enable: true</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞安装</title>
      <link href="posts/6209085/"/>
      <url>posts/6209085/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>此脚本用于安装 nginx;tengine;openresty. 安装版本为：</p><ul><li>nginx: 1.14</li><li>openresty: 1.15.8.3</li><li>tengine: 2.1.2 # 这是一个很古老的版本…</li></ul><h4 id="目录结构">目录结构</h4><p>因为是编译安装，所以产出目录均在 /usr/local/&lt;nginx/openresty/tengine&gt;，除了 logs 做了软链<code> /usr/local/xxx/logs -&gt; /export/logs/nginx</code></p><p><code>/usr/local/xxx/conf 目录结构</code></p><p><img src="/posts/6209085/image-20200515120037239.png" alt="image-20200515120037239"></p><pre><code class="language-bash"># 下面两个主配置文件会告诉你，相应的上下文配置，应该以什么结尾！！！include /usr/local/$&#123;NginxVer&#125;/nginx/conf/server/*.server;include /usr/local/$&#123;NginxVer&#125;/nginx/conf/upstream/*.upstream;</code></pre><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bashbasedir=/usr/local/srccd $basedirrunuser=`whoami`[[ $runuser == 'root' ]] || &#123;     echo &quot;ERROR:执行用户不是$runuser&quot; &amp;&amp; exit &#125;[[ -d /export/logs/nginx ]] || &#123;     echo &quot;/export/logs/nginx/目录不存在&quot; &amp;&amp; exit &#125;CpuNum=`cat /proc/cpuinfo | grep processor | wc -l`read -p &quot;输入安装的Nginx版本:(nginx;tengine;openresty):&quot; NginxVerread -p &quot;输入开发日常操作用户:&quot; KaifaUserread -p &quot;输入nginx worker用户:&quot; NginxWorkerUseruseradd -s /sbin/nologin $&#123;NginxWorkerUser&#125;usermod -a -G $&#123;KaifaUser&#125; $&#123;NginxWorkerUser&#125;cd /usr/local/srcrm -rf $&#123;NginxVer&#125; &amp;&amp; mkdir $&#123;NginxVer&#125;cat&gt;&gt;$basedir/test.com.server&lt;&lt;EOFserver &#123;    listen 80;    server_name test.com;    root /export/$&#123;NginxWorkerUser&#125;/test.com;    #charset koi8-r;    access_log logs/nginx-test.com.access.log main;    error_log logs/nginx-test.com.error.log;    # 关闭日志    location = /favicon.ico &#123;        log_not_found off;        access_log off;    &#125;    # 关闭日志    location = /robots.txt &#123;        auth_basic off;        allow all;        log_not_found off;        access_log off;    &#125;    # 拒绝探测网站根下的隐藏文件 Deny all attempts to access hidden files such as .htaccess, .htpasswd, .DS_Store (Mac).    location ~ /\. &#123;        deny all;        access_log off;        log_not_found off;    &#125;    location / &#123;        #######这个是一个thinkphp框架的伪静态规则，请忽略        if (!-e \$request_filename) &#123;           rewrite ^(.*)\$ /index.php?s=\$1 last;           break;        &#125;        #######        index index.php;    &#125;    #开启浏览器静态文件缓存    location ~ .*\.(html|htm|gif|jpg|jpeg|bmp|png|ico|txt|js|css)\$ &#123;         expires 3h;     &#125;     # 若php-fpm,请保留这里修改    location ~ \.php &#123;        fastcgi_pass 127.0.0.1：9000;        fastcgi_index index.php;        include fastcgi.conf;        fastcgi_connect_timeout 10s;        fastcgi_send_timeout 10s;        fastcgi_read_timeout 10s;        fastcgi_buffers 8 256k;                                   fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;    # 若 http，请保留这里修改    location / &#123;        proxy_pass http://127.0.0.1:8080;        proxy_connect_timeout 300ms;        proxy_send_timeout 300ms;        proxy_read_timeout 300ms;        proxy_max_temp_file_size 1024m;        proxy_set_header   Host         $host;        proxy_set_header   X-Real-IP    $remote_addr;        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;        proxy_buffers 256 4k;        proxy_intercept_errors on;    &#125;&#125;EOFcat&gt;&gt;nginx_status.server&lt;&lt;EOFserver &#123;    listen 80;    server_name 127.0.0.1;   # charset koi8-r;    access_log off;    location /server_status &#123;        stub_status on;        access_log off;        allow 127.0.0.1;        deny all;    &#125;&#125;EOF###################if [[ $NginxVer == 'nginx' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc    wget http://$&#123;NginxVer&#125;.org/download/$&#123;NginxVer&#125;-1.14.0.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-http_v2_module || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125; &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;        deny all;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/conf/upstream/*.upstream;&#125;EOFelif [[ $NginxVer == 'openresty' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc    wget https://openresty.org/download/openresty-1.15.8.3.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-http_v2_module || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125;/nginx &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/nginx/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;            return 444;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/nginx/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/nginx/conf/upstream/*.upstream;&#125;EOFelif [[ $NginxVer == 'tengine' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc jemalloc-devel    wget http://tengine.taobao.org/download/tengine-2.1.2.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-jemalloc || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125; &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;            deny all;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/conf/upstream/*.upstream;&#125;EOFfi</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo☞图片显示404错误的解决办法</title>
      <link href="posts/662e9d4d/"/>
      <url>posts/662e9d4d/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>写作工具：typora</p><p>部署端：github</p><p>部署包：</p><pre><code># 你可以通过hexo的根目录下的package.json来确认版本&quot;hexo&quot;: &quot;^4.0.0&quot;,&quot;hexo-asset-image&quot;: &quot;^1.0.0&quot;</code></pre><p>目前网上大多数的博文描述的场景，均不是当前场景（截至到2020.05.14），所以博文里虽然展示都正常，但是按照博文的操作却会有路径问题，具体表现是图片前多了一级路径（路径应该是1级域名，<a href="http://xn--bvs393b.com">比如.com</a>，.io等，根据你的域名来定）</p><p>那么，请按照下面我的步骤来操作，如果还有问题，那就不是上述我所说的情况了。</p><h4 id="流程">流程</h4><ol><li><p>修改 hexo-asset-image，修改内容如图所示</p><p><img src="/posts/662e9d4d/image-20200901195338475.png" alt="image-20200901195338475">修改typora的图片存放路径，修改内容如图所示</p><p><img src="/posts/662e9d4d/image-20200514185631903.png" alt="image-20200514185631903"></p><blockquote><p>这不是必须的，但是我想没人会拒绝方便的操作。 typora 在进行如上操作后，就可以在你往文章里粘贴图片的时候，自动生成以文件名前缀命名的目录（效果就如同你开启了hexo的post_asset_folder: true参数），并将图片存放在此目录中。</p></blockquote></li><li><p>开启 hexo 的 _config.yml 中 post_asset_folder: true 参数配置</p></li></ol><h4 id="结论">结论</h4><p>经过上述操作，我想你已经可以在本地 md 文件和线上同时看到图片了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞日志切割脚本</title>
      <link href="posts/445cf088/"/>
      <url>posts/445cf088/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>本脚本用于将 nginx 日志进行时间周期切割，并 lzo 压缩，最终上传到 s3。<br>脚本分为三个函数，切割函数，压缩上传函数，删除函数，需要执行哪个，就填写相对应变量。<br>详情可以看脚本注释。</p><blockquote><p>请务必执行前，确认安装了 lzop 和 jq 命令 ，且机器是 aws EC2</p></blockquote><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bash# by zyh# time: 2019-12-13# warning: 使用之前 yum install -y lzop jq# crontab (执行时间周期需要和切割时间周期一致) 重要!!!!!!!# */10 * * * * root bash /export/shell/nginxlog2s3/start.sh &gt; /export/shell/nginxlog2s3/start.log 2&gt;&amp;1# 标识日志名前缀localtag=`curl -sq http://169.254.169.254/latest/dynamic/instance-identity/document/ | jq -r .&quot;accountId&quot;,.&quot;availabilityZone&quot;,.&quot;privateIp&quot; | sed 'N;N;s@\n@_@g'`# ----------------------------------人为变量填写开始区域----------------------------------# 切割时间周期，定位切割后日期的初始写入时间（仅适用于连续切割，且不适用于第一次切割）todaytime=$(date -d &quot;-10 mins&quot; +%Y%m%d)todayhour=$(date -d &quot;-10 mins&quot; +%H)todaytimestr=$(date  -d &quot;-10 mins&quot; +%s)# 企业微信机器人wx_api=''# nginx 日志目录 logs 所在路径, 备份日志目录是 logs/logsbak# 例如日志目录是 /usr/local/nginx/logs，则填写 /usr/loca/nginx, 则切割后本地备份路径是 /usr/local/nginx/logs/logsbaknginx_base=# 日志位于S3的根路径，例如 s3://xxx/logs/xxxdays/nginxS3Base=&quot;&quot;# MvLogList=&quot;a.log b.log c.log&quot;  需要切割的日志，这是必须的MvLogList=&quot;&quot;# LzopS3LogList=&quot;a.log b.log c.log&quot; 需要压缩并上传S3的日志，如果你需要执行此步骤# S3目录格式：$&#123;S3Base&#125;/$&#123;日志名&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/ LzopS3LogList=&quot;&quot;# DeleteLocalLog=&quot;a.log b.log c.log&quot; 需要本地设置保留时间的日志，如果你需要执行此步DeleteLocalLog=&quot;&quot;# 本地保存时间deletetime=$(date -d &quot;72 hours ago&quot; +%s)# ----------------------------------人为变量填写结束区域----------------------------------# 日志原始路径nginx_logs=&quot;$&#123;nginx_base&#125;/logs&quot;# 日志位于本地的切割后备份路径backup_logs=&quot;$&#123;nginx_logs&#125;/logsbak&quot;[[ -d $&#123;backup_logs&#125; ]] || mkdir -p $&#123;backup_logs&#125;# nginx pid 文件路径nginx_pid=&quot;$&#123;nginx_logs&#125;/nginx.pid&quot;[[ -f $&#123;nginx_pid&#125; ]] || &#123;  echo &quot;$&#123;nginx_pid&#125; is not exist!!!!&quot; &amp;&amp; exit&#125;mvlog()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  [[ -d $&#123;backup_logs&#125;/$&#123;NginxLogName&#125; ]] || mkdir -p $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  mv $&#123;nginx_logs&#125;/$&#123;NginxLogName&#125; $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125; &amp;&amp; echo &quot;MV: $&#123;nginx_logs&#125;/$&#123;NginxLogName&#125; to $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;&quot;&#125;lzops3log()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  S3Path=$2  cd $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  lzop $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125; &amp;&amp; aws s3 cp $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo $&#123;S3Path&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/ --quiet &amp;&amp; rm -rf $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo &amp;&amp; echo &quot;UPLOAD: $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo to $&#123;S3Path&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/&quot; || curl &quot;$wx_api&quot; -H 'Content-Type: application/json' -d '&#123;&quot;msgtype&quot;: &quot;markdown&quot;,&quot;markdown&quot;: &#123;&quot;content&quot;: &quot;# `'&quot;$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo&quot;'` 日志上传失败!!!!!!&quot;&#125;&#125;'&#125;deletelocallog()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  cd $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  for logname in `ls`;do    if [[ $&#123;deletetime&#125; -ge $&#123;logname##*_&#125; ]];then      rm -rf $&#123;logname&#125; &amp;&amp; echo &quot;DELETE: $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;deletetime&#125;&quot;    fi  done&#125;#MVfor i in $&#123;MvLogList&#125;;do  mvlog $&#123;i&#125;done#nginx log reloadkill -USR1 `cat $&#123;nginx_pid&#125;`#lzop and to s3for i in $&#123;LzopS3LogList&#125;;do  lzops3log $&#123;i&#125; $&#123;S3Base&#125;/$&#123;i&#125;done#Deletefor i in $&#123;DeleteLocalLog&#125;;do  deletelocallog $&#123;i&#125;done</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> log </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞安装</title>
      <link href="posts/65ab632a/"/>
      <url>posts/65ab632a/</url>
      
        <content type="html"><![CDATA[<h4 id="docker-安装">docker 安装</h4><pre><code class="language-bash">redisName=redisdocker volume create $&#123;redisName&#125;touch /export/docker-data-root/volumes/$&#123;redisName&#125;/_data/redis.confredis.conf 请参考官方默认配置文档，默认配置文档地址： https://redis.io/topics/configdocker run --name  $&#123;redisName&#125; -v  $&#123;redisName&#125;:/data -p 6379:6379 --restart always -d redis redis-server /data/redis.conf </code></pre><h4 id="编译安装">编译安装</h4><pre><code class="language-shell">#!/bin/bash# by zyh# 2018-06-21# DownUrl: redis源码包# RedisBaseDir： redis安装路径# RedisPort: redis端口# RedisMaxMem：redis内存限制# ZabbixBase: zabbix 根路径# 安装完毕后，会输出# 1. redis信息# 2. zabbix需要额外手动添加的命令， 并在zabbix_server_web里，给机器关联上 &lt;Template Redis Auto Discovert Active mode&gt; 模板# 3. monit需要额外手动添加的配置BaseDir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`DownUrl=http://download.redis.io/releases/redis-stable.tar.gzRedisMaxMem=1gRedisPort=6379RedisBaseDir=/export/redis_$&#123;RedisPort&#125;ZabbixBase=/etc/zabbixif [[ $&#123;ZabbixBase&#125; == '/usr/local/zabbix' ]];then    ZabbixShell=$&#123;ZabbixBase&#125;/shell    ZabbixEtc=$&#123;ZabbixBase&#125;/etc/zabbix_agentd.conf.d/redis.confelse    ZabbixShell=/etc/zabbix/shell    ZabbixEtc=/etc/zabbix/zabbix_agentd.d/redis.conffiRedisBaseName=$&#123;RedisBaseDir##*/&#125;export TOP_PID=$$trap 'exit 1' TERMexit_script()&#123;    kill -s TERM $TOP_PID&#125;yum install gcc-c++ -y[[ -d $&#123;RedisBaseDir&#125; ]] &amp;&amp; echo &quot;$&#123;RedisBaseDir&#125;已存在&quot; &amp;&amp; exit_scriptss -tnalp | grep redis | awk '&#123;print $4&#125;' | awk -F':' '&#123;print $2&#125;' | while read line;do    [[ $line -eq $&#123;RedisPort&#125; ]] &amp;&amp; echo &quot;$&#123;RedisPort&#125;已被占用&quot; &amp;&amp; exit_scriptdonecd $&#123;BaseDir&#125; &amp;&amp; mkdir rediswget $&#123;DownUrl&#125; -O redis.tar.gztar xf redis.tar.gz --strip-components 1 -C rediscd redismake PREFIX=$&#123;RedisBaseDir&#125; installmkdir $&#123;RedisBaseDir&#125;/&#123;etc,data,logs&#125;cat&gt;$&#123;RedisBaseDir&#125;/etc/redis.conf &lt;&lt;EOFbind 0.0.0.0protected-mode yesport $&#123;RedisPort&#125;tcp-backlog 511timeout 0tcp-keepalive 300daemonize yessupervised nopidfile $&#123;RedisBaseDir&#125;/redis.pidloglevel warninglogfile &quot;$&#123;RedisBaseDir&#125;/logs/redis.log&quot;databases 16always-show-logo yessave 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename redis_$&#123;RedisPort&#125;.rdbdir $&#123;RedisBaseDir&#125;/data/slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay noslave-priority 100rename-command FLUSHDB GOD_FLUSHDBrename-command FLUSHALL GOD_FLUSHALLrename-command CONFIG GOD_CONFIGrename-command KEYS GOD_KEYSmaxmemory $&#123;RedisMaxMem&#125;maxmemory-policy allkeys-lrulazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush noappendonly noappendfilename &quot;appendonly.aof&quot;appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yesaof-use-rdb-preamble nolua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events &quot;&quot;hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-size -2list-compress-depth 0set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yesEOFcat&gt;$&#123;RedisBaseDir&#125;/redis.sh &lt;&lt; EOF#!/bin/bash# Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.BASEDIR=$&#123;RedisBaseDir&#125;REDISPORT=$&#123;RedisPort&#125;EXEC=\$BASEDIR/bin/redis-serverCLIEXEC=\$BASEDIR/bin/redis-cliPIDFILE=\$BASEDIR/redis.pidCONF=&quot;\$BASEDIR/etc/redis.conf&quot;case &quot;\$1&quot; in    start)        [[ -f \$PIDFILE ]] &amp;&amp; kill -0 \`cat \$PIDFILE\` 2&gt;&gt;\$BASEDIR/crash.log &amp;&amp; echo &quot;\$PIDFILE exists, process is already running or crashed&quot; || &#123;                echo &quot;Starting Redis server...&quot;                \$EXEC \$CONF        &#125;        ;;    stop)        if [ ! -f \$PIDFILE ]        then                echo &quot;\$PIDFILE does not exist, process is not running&quot;        else                PID=\$(cat \$PIDFILE)                echo &quot;Stopping ...&quot;                \$CLIEXEC -p \$REDISPORT shutdown                while [ -x /proc/\$&#123;PID&#125; ]                do                    echo &quot;Waiting for Redis to shutdown ...&quot;                    sleep 1                done                echo &quot;Redis stopped&quot;        fi        ;;    *)        echo &quot;Please use start or stop as first argument&quot;        ;;esacEOFchmod u+x $&#123;RedisBaseDir&#125;/redis.sh#修改内核参数grep -q net.core.somaxconn /etc/sysctl.conf || echo &quot;net.core.somaxconn = 511&quot; &gt;&gt; /etc/sysctl.confgrep -q vm.overcommit_memory /etc/sysctl.conf || &#123;    echo &quot;vm.overcommit_memory = 1&quot; &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -p&#125;grep -q '/sys/kernel/mm/transparent_hugepage/enabled' /etc/rc.local || &#123;    echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled    echo 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' &gt;&gt; /etc/rc.local&#125;#修改zabbix监控cat&gt;$&#123;ZabbixEtc&#125;&lt;&lt;EOF##redis monitorUserParameter=redis_info[*],$&#123;ZabbixShell&#125;/redis_info.sh \$1 \$2 \$3 \$4UserParameter=ip_port_discovery[*],$&#123;ZabbixShell&#125;/ip_port_discovery.sh \$1EOFmkdir $&#123;ZabbixShell&#125;cat&gt;$&#123;ZabbixShell&#125;/redis_info.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashREDISPATH=&quot;/export/redis/bin/redis-cli&quot;HOST=$1PORT=$2REDIS_INFO=&quot;$REDISPATH -h $HOST -p $PORT info&quot;[[ $# -ne 3 ]] &amp;&amp; [[ $# -ne 4 ]] &amp;&amp; &#123; exit&#125;if [[ $# -eq 3 ]];thencase $3 inexist) result=`$REDISPATH -h $HOST -p $PORT ping 2&gt;/dev/null |grep -c PONG` echo $result;;cluster)        result=`$REDIS_INFO|/bin/grep cluster|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;uptime_in_seconds)        result=`$REDIS_INFO|/bin/grep uptime_in_seconds|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;connected_clients)        result=`$REDIS_INFO|/bin/grep connected_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_longest_output_list)        result=`$REDIS_INFO|/bin/grep client_longest_output_list|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_biggest_input_buf)        result=`$REDIS_INFO|/bin/grep client_biggest_input_buf|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;blocked_clients)        result=`$REDIS_INFO|/bin/grep blocked_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#内存maxmemory)        result=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory)        result=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_rss)        result=`$REDIS_INFO|/bin/grep -w used_memory_rss|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk -F'k' '&#123;print $1&#125;'`        echo $result;;used_memory_pct) A=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` B=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` result=`echo $A $B | awk '&#123;printf&quot;%0.2f&quot;,$1/$2&#125;'`        echo $result;;used_memory_peak)        result=`$REDIS_INFO|/bin/grep used_memory_peak|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_lua)        result=`$REDIS_INFO|/bin/grep used_memory_lua|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;mem_fragmentation_ratio)        result=`$REDIS_INFO|/bin/grep mem_fragmentation_ratio|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#rdbrdb_changes_since_last_save)        result=`$REDIS_INFO|/bin/grep rdb_changes_since_last_save|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_bgsave_in_progress)        result=`$REDIS_INFO|/bin/grep rdb_bgsave_in_progress|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_save_time)        result=`$REDIS_INFO|/bin/grep rdb_last_save_time|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_bgsave_status)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_last_bgsave_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;rdb_current_bgsave_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_current_bgsave_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#rdbinfoaof_enabled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_enabled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_scheduled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_scheduled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_last_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_current_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_last_bgrewrite_status)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_bgrewrite_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;#aofinfoaof_current_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_base_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_base_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_rewrite)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_rewrite&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_bio_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_bio_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_delayed_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_delayed_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#statstotal_connections_received)        result=`$REDIS_INFO|/bin/grep -w &quot;total_connections_received&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;total_commands_processed)        result=`$REDIS_INFO|/bin/grep -w &quot;total_commands_processed&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;instantaneous_ops_per_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;instantaneous_ops_per_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;rejected_connections)        result=`$REDIS_INFO|/bin/grep -w &quot;rejected_connections&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;expired_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;expired_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;evicted_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;evicted_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_hits)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_hits&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_misses)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_misses&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_patterns)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_patterns&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;latest_fork_usec)        result=`$REDIS_INFO|/bin/grep -w &quot;latest_fork_usec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;connected_slaves)        result=`$REDIS_INFO|/bin/grep -w &quot;connected_slaves&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;master_link_status)        result=`$REDIS_INFO|/bin/grep -w &quot;master_link_status&quot;|awk -F':' '&#123;print $2&#125;'|/bin/grep -c up`        echo $result;;master_last_io_seconds_ago)        result=`$REDIS_INFO|/bin/grep -w &quot;master_last_io_seconds_ago&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;master_sync_in_progress)        result=`$REDIS_INFO|/bin/grep -w &quot;master_sync_in_progress&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;slave_priority)        result=`$REDIS_INFO|/bin/grep -w &quot;slave_priority&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;#cpuused_cpu_sys)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_sys_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;*) echo &quot;argu error&quot;;;esac#db0:key   elif [[ $# -eq 4 ]];thencase $4 inkeys)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;keys&quot; | awk -F'=|,' '&#123;print $2&#125;'`        echo $result;;expires)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;expires&quot; | awk -F'=|,' '&#123;print $4&#125;'`        echo $result;;avg_ttl)        result=`$REDIS_INFO|/bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;avg_ttl&quot; | awk -F'=|,' '&#123;print $6&#125;'`        echo $result;;*)     echo &quot;argu error&quot; ;;esacfiEOFcat&gt;$&#123;ZabbixShell&#125;/ip_port_discovery.sh&lt;&lt;&quot;EOF&quot;#!/bin/bash#$1是要发现的进程部分词汇portarray=(`sudo ss -tnlp|egrep -i &quot;$1&quot;|awk &#123;'print $4'&#125;|awk -F':' '&#123;if ($NF~/^[0-9]*$/) print $NF&#125;'|sort|uniq`)iparray=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`length=$&#123;#portarray[@]&#125;printf &quot;&#123;\n&quot;printf '\t'&quot;\&quot;data\&quot;:[&quot;for ((i=0;i&lt;$length;i++))  do     printf '\n\t\t&#123;'     printf &quot;\&quot;&#123;#IP&#125;\&quot;:\&quot;$&#123;iparray&#125;\&quot;,\&quot;&#123;#TCP_PORT&#125;\&quot;:\&quot;$&#123;portarray[$i]&#125;\&quot;&#125;&quot;     if [ $i -lt $[$length-1] ];then                printf ','     fi  doneprintf &quot;\n\t]\n&quot;printf &quot;&#125;\n&quot;EOFchmod a+x $&#123;ZabbixShell&#125;/redis_info.shchmod a+x $&#123;ZabbixShell&#125;/ip_port_discovery.shsed -i '2s#/export/redis#'&quot;$&#123;RedisBaseDir&#125;&quot;'#' $&#123;ZabbixShell&#125;/redis_info.shecho '--------------------------------------------------我是 redis 信息-----------------------------------------------------------'echo 'redis相关信息如下：'echo &quot;根路径：$&#123;RedisBaseDir&#125;启动脚本：$&#123;RedisBaseDir&#125;/redis.sh&quot;echo '--------------------------------------------------我是 zabbix 监控信息----------------------------------------------------------'echo '请先安装 zabbix.'echo 'zabbix监控因使用了ss命令，故而需要开启sudo相关信息'echo &quot;#zabbix用户可以以sudo执行sszabbix ALL = NOPASSWD: /usr/sbin/ss#zabbix用户使用sudo无需ttyDefaults:zabbix    !requiretty&quot;echo '---------------------------------------------------我是 monit 监控信息----------------------------------------------------------'echo '请先安装 monit.'echo 'monit配置文件如下:'echo &quot;check process $&#123;RedisBaseName&#125; with pidfile $&#123;RedisBaseDir&#125;/redis.pid  start program = \&quot;$&#123;RedisBaseDir&#125;/redis.sh start\&quot;  stop program = \&quot;$&#123;RedisBaseDir&#125;/redis.sh stop\&quot;if changed pid then alert&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞redis</title>
      <link href="posts/6c093771/"/>
      <url>posts/6c093771/</url>
      
        <content type="html"><![CDATA[<h2 id="zabbix-模板">zabbix 模板</h2><ol><li>自动发现规则</li></ol><p><img src="/posts/6c093771/image-20200520152740815.png" alt="image-20200520152740815"></p><ol start="2"><li><p>过滤器</p><p><img src="/posts/6c093771/image-20200520152922570.png" alt="image-20200520152922570"></p></li><li><p>监控项原型</p><table><thead><tr><th>名称</th><th>键值</th><th>间隔</th><th>历史记录</th><th>趋势</th><th>类型</th></tr></thead><tbody><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; connected_clients[客户端连接数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,connected_clients]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; maxmemory[redis配置的内存上限]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,maxmemory]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; mem_fragmentation_ratio[内存碎片率]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; redis_instantaneous_ops_per_sec[每秒执行的命令个数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,instantaneous_ops_per_sec]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; redis 存活状态</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,exist]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; rejected_connections[被拒绝的客户端连接数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,rejected_connections]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_sys[redis-master sys-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_sys]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_sys_children[redis-children sys-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_sys_children]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_user[redis-master user-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_user]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_user_children[redis-children user-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_user_children]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory[redis层面已使用内存-不含碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_pct[操作系统层面已使用内存百分比]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_peak[操作系统层面已使用内存历史峰值-含内存碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_peak]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_rss[操作系统层面已使用内存-含内存碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_rss]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr></tbody></table></li><li><p>触发器原型</p><table><thead><tr><th>严重性</th><th>名称</th><th>表达式</th></tr></thead><tbody><tr><td>一般严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  内存碎片化超过50%, 剩余可用内存低于30%</code></td><td><code>&#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio].last()&#125;&gt;1.5  and &#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  可用内存低, 存在使用交换分区</code></td><td><code>&#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio].last()&#125;&lt;1  and &#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>一般严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  操作系统层面内存占用百分比过高</code></td><td><code>&#123;Template  Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>灾难</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis 端口无法访问</code></td><td><code>&#123;Template  Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,exist].last()&#125;&lt;&gt;1</code></td></tr></tbody></table></li><li><p>图形原型，就不详细写了，这里只列出我自己的分类</p><table><thead><tr><th>名称</th><th>宽</th><th>高</th><th>图形类别</th></tr></thead><tbody><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis 连接数监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis 其他监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis qps 监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis mem 监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis cpu 监控</code></td><td>900</td><td>200</td><td>正常</td></tr></tbody></table></li></ol><h2 id="脚本">脚本</h2><p>脚本会生成自动发现进程脚本和redis检测脚本</p><p>zabbix的redis配置路径：ZabbixEtc</p><p>zabbix的脚本路径：ZabbixShell</p><p>redis的cli命令路径：RedisCli</p><p>自动发现脚本：ip_port_discovery.sh</p><ul><li>bash ip_port_discovery.sh 进程名或者端口</li></ul><p>redis检测脚本：redis_info.sh</p><ul><li>bash redis_info.sh ip port item</li></ul><pre><code class="language-redis">ZabbixEtc=/etc/zabbix/zabbix_agentd.dZabbixShell=/etc/zabbix/shellRedisCli='docker exec -t redis redis-cli'[[ -d $&#123;ZabbixShell&#125; ]] || mkdir -p $&#123;ZabbixShell&#125;[[ -z $&#123;ZabbixEtc&#125; ]] &amp;&amp; [[ -z $&#123;ZabbixShell&#125; ]] || [[ -z $&#123;RedisCli&#125; ]] &amp;&amp; exitcat&gt;$&#123;ZabbixEtc&#125;/redis.conf&lt;&lt;EOF##redis monitorUserParameter=redis_info[*],$&#123;ZabbixShell&#125;/redis_info.sh \$1 \$2 \$3 \$4UserParameter=ip_port_discovery[*],$&#123;ZabbixShell&#125;/ip_port_discovery.sh \$1EOFcat&gt;$&#123;ZabbixShell&#125;/redis_info.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashRedisCli=HOST=$1PORT=$2REDIS_INFO=&quot;$RedisCli -h $HOST -p $PORT info&quot;[[ $# -ne 3 ]] &amp;&amp; [[ $# -ne 4 ]] &amp;&amp; &#123; exit&#125;if [[ $# -eq 3 ]];thencase $3 inexist) result=`$RedisCli -h $HOST -p $PORT ping 2&gt;/dev/null |grep -c PONG` echo $result;;cluster)        result=`$REDIS_INFO|/bin/grep cluster|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;uptime_in_seconds)        result=`$REDIS_INFO|/bin/grep uptime_in_seconds|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;connected_clients)        result=`$REDIS_INFO|/bin/grep connected_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_longest_output_list)        result=`$REDIS_INFO|/bin/grep client_longest_output_list|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_biggest_input_buf)        result=`$REDIS_INFO|/bin/grep client_biggest_input_buf|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;blocked_clients)        result=`$REDIS_INFO|/bin/grep blocked_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#内存maxmemory)        result=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory)        result=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_rss)        result=`$REDIS_INFO|/bin/grep -w used_memory_rss|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk -F'k' '&#123;print $1&#125;'`        echo $result;;used_memory_pct) A=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` B=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` result=`echo $A $B | awk '&#123;printf&quot;%0.2f&quot;,$1/$2&#125;'`        echo $result;;used_memory_peak)        result=`$REDIS_INFO|/bin/grep used_memory_peak|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_lua)        result=`$REDIS_INFO|/bin/grep used_memory_lua|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;mem_fragmentation_ratio)        result=`$REDIS_INFO|/bin/grep mem_fragmentation_ratio|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#rdbrdb_changes_since_last_save)        result=`$REDIS_INFO|/bin/grep rdb_changes_since_last_save|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_bgsave_in_progress)        result=`$REDIS_INFO|/bin/grep rdb_bgsave_in_progress|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_save_time)        result=`$REDIS_INFO|/bin/grep rdb_last_save_time|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_bgsave_status)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_last_bgsave_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;rdb_current_bgsave_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_current_bgsave_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#rdbinfoaof_enabled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_enabled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_scheduled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_scheduled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_last_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_current_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_last_bgrewrite_status)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_bgrewrite_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;#aofinfoaof_current_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_base_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_base_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_rewrite)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_rewrite&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_bio_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_bio_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_delayed_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_delayed_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#statstotal_connections_received)        result=`$REDIS_INFO|/bin/grep -w &quot;total_connections_received&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;total_commands_processed)        result=`$REDIS_INFO|/bin/grep -w &quot;total_commands_processed&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;instantaneous_ops_per_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;instantaneous_ops_per_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;rejected_connections)        result=`$REDIS_INFO|/bin/grep -w &quot;rejected_connections&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;expired_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;expired_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;evicted_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;evicted_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_hits)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_hits&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_misses)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_misses&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_patterns)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_patterns&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;latest_fork_usec)        result=`$REDIS_INFO|/bin/grep -w &quot;latest_fork_usec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;connected_slaves)        result=`$REDIS_INFO|/bin/grep -w &quot;connected_slaves&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;master_link_status)        result=`$REDIS_INFO|/bin/grep -w &quot;master_link_status&quot;|awk -F':' '&#123;print $2&#125;'|/bin/grep -c up`        echo $result;;master_last_io_seconds_ago)        result=`$REDIS_INFO|/bin/grep -w &quot;master_last_io_seconds_ago&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;master_sync_in_progress)        result=`$REDIS_INFO|/bin/grep -w &quot;master_sync_in_progress&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;slave_priority)        result=`$REDIS_INFO|/bin/grep -w &quot;slave_priority&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;#cpuused_cpu_sys)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_sys_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;*) echo &quot;argu error&quot;;;esac#db0:key   elif [[ $# -eq 4 ]];thencase $4 inkeys)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;keys&quot; | awk -F'=|,' '&#123;print $2&#125;'`        echo $result;;expires)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;expires&quot; | awk -F'=|,' '&#123;print $4&#125;'`        echo $result;;avg_ttl)        result=`$REDIS_INFO|/bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;avg_ttl&quot; | awk -F'=|,' '&#123;print $6&#125;'`        echo $result;;*)     echo &quot;argu error&quot; ;;esacfiEOFcat&gt;$&#123;ZabbixShell&#125;/ip_port_discovery.sh&lt;&lt;&quot;EOF&quot;#!/bin/bash#$1是要发现的进程部分词汇portarray=(`sudo ss -tnlp|egrep -i &quot;$1&quot;|awk &#123;'print $4'&#125;|awk -F':' '&#123;if ($NF~/^[0-9]*$/) print $NF&#125;'|sort|uniq`)iparray=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`length=$&#123;#portarray[@]&#125;printf &quot;&#123;\n&quot;printf '\t'&quot;\&quot;data\&quot;:[&quot;for ((i=0;i&lt;$length;i++))  do     printf '\n\t\t&#123;'     printf &quot;\&quot;&#123;#IP&#125;\&quot;:\&quot;$&#123;iparray&#125;\&quot;,\&quot;&#123;#TCP_PORT&#125;\&quot;:\&quot;$&#123;portarray[$i]&#125;\&quot;&#125;&quot;     if [ $i -lt $[$length-1] ];then                printf ','     fi  doneprintf &quot;\n\t]\n&quot;printf &quot;&#125;\n&quot;EOFchmod a+x $&#123;ZabbixShell&#125;/redis_info.shchmod a+x $&#123;ZabbixShell&#125;/ip_port_discovery.shsed -i 's#RedisCli=#RedisCli=\&quot;'&quot;$&#123;RedisCli&#125;&quot;'\&quot;#' $&#123;ZabbixShell&#125;/redis_info.shecho '------------------------------------我是 zabbix 监控信息----------------------------------'echo '编辑 visudo，添加如下信息'echo &quot;#zabbix用户可以以sudo执行sszabbix ALL = NOPASSWD: /usr/sbin/ss#zabbix用户使用sudo无需ttyDefaults:zabbix    !requiretty&quot;echo '若redis运行在docker中，执行如下命令'echo 'usermod -a -G docker zabbix'</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞web监控</title>
      <link href="posts/3d20f83e/"/>
      <url>posts/3d20f83e/</url>
      
        <content type="html"><![CDATA[<h2 id="1-构建-zabbix-agentd-端配置">1. 构建 zabbix_agentd 端配置</h2><pre><code class="language-bash"># 目录结构[root@ip-10-230-10-105 zabbix]# pwd/etc/zabbix[root@ip-10-230-10-105 zabbix]# tree &#123;etc,shell&#125;etc├── zabbix_agentd.conf├── zabbix_agentd.conf.bak└── zabbix_agentd.conf.d    └── http_status.conf # 我是 zabbix_agentd 数据项配置shell└── web    ├── http_status.py # 我是自动发现脚本 + 数据采集脚本    └── WEB.txt  # 我是自动发现的数据源2 directories, 5 files</code></pre><pre><code class="language-bash"># zabbix_agentd 配置 http_status.confUserParameter=web.site.code[*],/usr/bin/python /etc/zabbix/shell/web/http_status.py web_site_code $1UserParameter=web.site.discovery,/usr/bin/python /etc/zabbix/shell/web/http_status.py web_site_discovery</code></pre><pre><code class="language-bash"># 自动发现规则配置文件 WEB.txt# 一行一个监控地址# get 原样写入# post 模仿get多加一个?https://abc.com??&lt;post_kv&gt;https://abc.com?&lt;get_kv&gt;</code></pre><pre><code class="language-python">#!/usr/bin/env python#encoding=utf-8# python2.7# 自动发现脚本 + 数据采集脚本 http_status.py# 请将我添加 o+x 权限import urllib2, sys, json, ConfigParser, os a1 = sys.argv[1]def web_site_code(args):    response = None    WEB_TXT = &quot;%s/WEB.txt&quot; % (os.path.dirname(os.path.realpath(__file__)))    for line in open(WEB_TXT):        if args in line:            if &quot;??&quot; in line:                line = line.strip('\n').split(&quot;??&quot;)            elif &quot;?&quot; in line:                line = line.strip('\n').split(&quot;?&quot;)            else:                line = line.strip('\n')            try:                try:                    response = urllib2.urlopen(line[0], data=line[1], timeout=5)                    print response.code                except IndexError:                    response = urllib2.urlopen(line[0], timeout=5)                    print response.code                finally:                    response = urllib2.urlopen(line, timeout=5)                    print response.code            except urllib2.URLError,e:                if hasattr(e, 'code'):                    print e.code                elif hasattr(e, 'reason'):                    print 53            finally:                if response:                    response.close()                exit()def web_site_discovery():    Dict = &#123;&quot;data&quot;:[]&#125;    WEB_TXT = &quot;%s/WEB.txt&quot; % (os.path.dirname(os.path.realpath(__file__)))    for line in open(WEB_TXT):        if &quot;??&quot; in line:            line = line.strip('\n').split(&quot;??&quot;)            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line[0]&#125;        elif &quot;?&quot; in line:            line = line.strip('\n').split(&quot;?&quot;)            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line[0]&#125;        else:            line = line.strip('\n')            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line&#125;        Dict[&quot;data&quot;].append(line)    print json.dumps(Dict, indent=2)if a1 == 'web_site_code':    url = sys.argv[2]    web_site_code(url)elif a1 == 'web_site_discovery':    web_site_discovery()</code></pre><blockquote><p>{ #SITENAME} 自动发现脚本输出的重要变量 servername 地址，将会用于 web 控制台配置</p></blockquote><h2 id="2-构建-web-控制台配置">2. 构建 web 控制台配置</h2><ul><li><p>构建一个主机项，监控 agentd</p></li><li><p>在自动发现规则里，构建监控项原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20200616182457232.png" alt="image-20200616182457232"></p></li><li><p>在自动发现规则里，构建触发器原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20200616182509904.png" alt="image-20200616182509904"></p><blockquote><p>100秒以内，收集到三次不是200的数据，就报警</p></blockquote></li><li><p>在自动发现规则里，构建图形原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20200616182538573.png" alt="image-20200616182538573"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix ☞ nginx</title>
      <link href="posts/b35b5965/"/>
      <url>posts/b35b5965/</url>
      
        <content type="html"><![CDATA[<blockquote><p>确保相关目录地址如下：</p><p>/etc/zabbix/shell</p><p>/etc/zabbix/zabbix_agentd.d</p><p>/usr/local/nginx/conf/server</p></blockquote><ul><li>1，nginx增加配置 server_status.server</li></ul><pre><code class="language-bash:">cat &gt; /usr/local/nginx/conf/server/server_status.server &lt;&lt; 'EOF'server &#123;    listen       80;    server_name  127.0.0.1;    #charset koi8-r;    access_log  off;    location /server_status &#123;        stub_status on;        access_log off;        allow 127.0.0.1;        deny all;    &#125;&#125;EOF/usr/local/nginx/sbin/nginx -t &amp;&amp; /usr/local/nginx/sbin/nginx -s reload</code></pre><hr><ul><li>2，<a href="http://xn--nginx-e86hk14jmnl025b.sh">添加脚本nginx.sh</a>  (确保a+x权限)</li></ul><pre><code class="language-bash:">mkdir /etc/zabbix/shell -p;cat &gt; /etc/zabbix/shell/nginx.sh &lt;&lt; 'EOF'#!/bin/bash  function active &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Active' | awk '&#123;print $NF&#125;'&#125;function reading &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Reading' | awk '&#123;print $2&#125;'&#125;function writing &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt;/dev/null| grep 'Writing' | awk '&#123;print $4&#125;'&#125;function waiting &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Waiting' | awk '&#123;print $6&#125;'&#125;function accepts &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $1&#125;'&#125;function handled &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $2&#125;'&#125;function requests &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $3&#125;'&#125;function qps &#123;        NGINX_STATUS_URL=&quot;http://127.0.0.1/server_status&quot;        #若是tnginx，则最后应输出d[length(d)-1]        requestold=`/usr/bin/curl -s $&#123;NGINX_STATUS_URL&#125; | /usr/bin/awk '/server accepts handled requests/&#123;getline a;split(a,d);print d[length(d)]&#125;'`        TimeWait=1        sleep $TimeWait        requestnew=`/usr/bin/curl -s $&#123;NGINX_STATUS_URL&#125; | /usr/bin/awk '/server accepts handled requests/&#123;getline a;split(a,d);print d[length(d)]&#125;'`        if [ $requestnew -gt 0 ];then                QPS=`echo &quot;( $requestnew - $requestold ) / $TimeWait&quot; | /usr/bin/bc`        fi        echo $QPS&#125;# Run the requested function  $1EOFchmod a+x /etc/zabbix/shell/nginx.sh</code></pre><hr><ul><li>3，配置zabbix客户端zabbix_agentd.conf</li></ul><pre><code class="language-bash:">cat &gt; /etc/zabbix/zabbix_agentd.d/nginx.conf &lt;&lt; 'EOF'#monitor nginx  UserParameter=nginx.accepts,/etc/zabbix/shell/nginx.sh acceptsUserParameter=nginx.handled,/etc/zabbix/shell/nginx.sh handledUserParameter=nginx.requests,/etc/zabbix/shell/nginx.sh requestsUserParameter=nginx.connections.active,/etc/zabbix/shell/nginx.sh activeUserParameter=nginx.connections.reading,/etc/zabbix/shell/nginx.sh readingUserParameter=nginx.connections.writing,/etc/zabbix/shell/nginx.sh writingUserParameter=nginx.connections.waiting,/etc/zabbix/shell/nginx.sh waitingUserParameter=nginx.connections.qps,/etc/zabbix/shell/nginx.sh qpsEOF</code></pre><hr><ul><li>4，在服务的对应主机上添加模板</li></ul>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞agent和proxy（yum）</title>
      <link href="posts/1587f6e8/"/>
      <url>posts/1587f6e8/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://repo.zabbix.com/zabbix/">https://repo.zabbix.com/zabbix/</a></p></blockquote><h2 id="导入-zabbix-源">导入 zabbix 源</h2><pre><code class="language-bash：">rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm</code></pre><h2 id="agent-安装">agent 安装</h2><pre><code class="language-shell">yum install zabbix-agent -ysystemctl enable zabbix-agent</code></pre><h2 id="agent主动模式配置文件">agent主动模式配置文件</h2><pre><code># 主机名前缀name_prefix=# 自动发现用的元数据host_meta_data=''# server或者proxy地址server_proxy_addr=local_ip=`curl -sq http://169.254.169.254/latest/meta-data/local-ipv4`mv /etc/zabbix/zabbix_agentd.conf /etc/zabbix/zabbix_agentd.conf.bak;cat &gt; /etc/zabbix/zabbix_agentd.conf &lt;&lt; EOFHostname=$&#123;name_prefix&#125;_$&#123;local_ip&#125;StartAgents=0ServerActive=$&#123;server_proxy_addr&#125;PidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.log#DebugLevel=4Include=/etc/zabbix/zabbix_agentd.d/*.conf#被监控端到服务器获取监控项的周期RefreshActiveChecks=60#被监控端存储监控信息的空间大小BufferSize=1000MaxLinesPerSecond=200#超时时间Timeout=10#自动发现用的元信息HostMetadata=$&#123;host_meta_data&#125;EOFvi  /etc/zabbix/zabbix_agentd.confsystemctl start zabbix-agent</code></pre><h2 id="proxy-安装命令">proxy 安装命令</h2><pre><code class="language-bash:">yum install zabbix-proxy-mysql -y# 解压数据库文件, 并自行导入zcat /usr/share/doc/zabbix-proxy-mysql-*/schema.sql.gz &gt; schema.sql</code></pre><h2 id="proxy-配置">proxy 配置</h2><pre><code class="language-bash">mv /etc/zabbix/zabbix_proxy.conf /etc/zabbix/zabbix_proxy.conf.bak;cat &gt; /etc/zabbix/zabbix_proxy.conf  &lt;&lt; 'EOF'Server=ServerPort=10051Hostname=LogFile=/var/log/zabbix/zabbix_proxy.logPidFile=/var/run/zabbix/zabbix_proxy.pidDBHost=DBPort=DBName=DBUser=DBPassword=Timeout=4LogSlowQueries=3000ConfigFrequency=60DataSenderFrequency=60StartDiscoverers=5CacheSize=128MStartDBSyncers=20HistoryCacheSize=256MHistoryIndexCacheSize=32MEOFvi /etc/zabbix/zabbix_proxy.conf</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 微信预警</title>
      <link href="posts/f9b08c30/"/>
      <url>posts/f9b08c30/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># conf.ini[wechat]corpid = [app]it = &lt;app_agent_id&gt;:&lt;app_secret&gt;[group]it = usera|userb</code></pre><pre><code class="language-python">#!/usr/bin/env python3&quot;&quot;&quot;@author: zyh@contact: aaa103439@hotmail.com@software: vscode@file: sendchat.py@time: 2020/02/05&quot;&quot;&quot;import sys, os, requests, pathlib, json, configparserimport loggingimport logging.handlersfrom datetime import datetimeclass PySendchat():    def __init__(self, corpid, agentid, secret, touser, content):        self.corpid=corpid        self.agentid=agentid        self.secret=secret        self.touser=touser        self.content=content        LOG_PATHDIR=os.path.dirname(os.path.abspath(__file__))        LOG_FILENAME = '&#123;0&#125;/sendchat.log'.format(LOG_PATHDIR)        self.my_logger = logging.getLogger('SendChat')        self.my_logger.setLevel(logging.INFO)        # Add the log message handler to the logger        handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=102400000, backupCount=5)        # create formatter and add it to the handlers        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')        handler.setFormatter(formatter)        self.my_logger.addHandler(handler)    def gettoken(self):        self.my_logger.info('-----------------------------------------------------------------------------------------')        pwd=os.path.dirname(os.path.abspath(__file__))        tokenfile='&#123;0&#125;/wechat.&#123;1&#125;'.format(pwd,self.agentid)        if pathlib.Path(tokenfile).exists():            tokenfilectime=os.path.getctime(tokenfile)            currenttime=datetime.now().timestamp()            dtime=currenttime-tokenfilectime            self.my_logger.info('&#123;0&#125; lived &#123;1&#125;s.'.format(tokenfile, dtime))            if dtime &gt;= 7200:                try:                    os.remove(tokenfile)                    self.my_logger.info('Token file &#123;0&#125;: delete success'.format(tokenfile))                except Exception as e:                    self.my_logger.error('Token file:&#123;0&#125; delete error.Reason:&#123;1&#125;'.format(tokenfile,e))                    exit        # check token file        try:            tokensize = os.path.getsize(tokenfile)        except Exception as e:            self.my_logger.info('Token file is not exist.Reason:&#123;0&#125;'.format(e))            tokensize = 0        # get token from token file        if tokensize != 0:            with open(tokenfile, 'rb') as fd:                token = fd.read() # get token success                self.my_logger.info('Get token from token file.')            jsonObject = json.loads(token.decode(encoding='utf8'))            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token        # get token from weixin api        else:            try:                self.my_logger.info('New Token Create.')                f = requests.get('https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&#123;0&#125;&amp;corpsecret=&#123;1&#125;'.format(self.corpid, self.secret))                token = f.content                self.my_logger.info('Get token from weixin api.')                jsonObject = json.loads(token.decode(encoding='utf8'))                errcode=int(jsonObject.get(&quot;errcode&quot;))                if errcode != 0:                    errmsg=jsonObject.get(&quot;errmsg&quot;)                    self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(errmsg))                    exit()            except Exception as e:                self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(e))                exit()            try:                self.my_logger.info('Write token to &#123;0&#125;.'.format(tokenfile))                with open(tokenfile, 'wb') as fd:                    fd.write(token)            except Exception as e:                self.my_logger.error('Write &#123;0&#125; error!Reason:&#123;1&#125;'.format(tokenfile,e))                exit()            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token    def sendmsg(self):        accessToken = self.gettoken()        self.my_logger.info('Token:&#123;0&#125;'.format(accessToken))        sendMapDirectroy = &#123;&#125;        sendMapDirectroy[&quot;agentid&quot;] = self.agentid        sendMapDirectroy[&quot;touser&quot;] = self.touser        sendMapDirectroy[&quot;msgtype&quot;] = &quot;text&quot;        sendMapDirectroy[&quot;safe&quot;] = &quot;0&quot;        contentDirectory = &#123;&#125;        sendMapDirectroy[&quot;text&quot;] = contentDirectory        contentDirectory[&quot;content&quot;] = self.content        bodyStr = json.dumps(sendMapDirectroy, ensure_ascii=False).encode(encoding=&quot;utf-8&quot;)        try:            f = requests.post(url=&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s&quot; % accessToken,                          data=bodyStr, timeout=5)            self.my_logger.info(f.content)        except Exception as e:            self.my_logger.error('Send chat network error!Reason:&#123;0&#125;'.format(e))if __name__ == '__main__':    appname = sys.argv[1]    content = sys.argv[3]    # read conf.ini    conf = configparser.ConfigParser()    conf_path = os.path.dirname(os.path.abspath(__file__))    conf_ini = &quot;&#123;0&#125;/conf.ini&quot;.format(conf_path)    if pathlib.Path(conf_ini).exists():        conf.read(conf_ini)        corpid = conf.get(&quot;wechat&quot;, &quot;corpid&quot;)        appinfo = conf.get(&quot;app&quot;, appname)        agentid = appinfo.split(':')[0]        secret = appinfo.split(':')[1]        groupname = conf.get(&quot;group&quot;, appname)        touser = groupname.split(':')[0]        chatobj = PySendchat(corpid, agentid, secret, touser, content)    else:        print('conf.ini error')        exit()    try:        chatobj.sendmsg()    except:        chatobj.my_logger.error(&quot;Send chat failure!&quot;)</code></pre><p>eg:</p><p>python <a href="http://sendchat.py">sendchat.py</a> it ‘’ &lt;预警内容&gt;</p><blockquote><p>创建好app，并关联用户到app</p><p>执行上述命令，会将预警内容通过&lt;app_agent_id&gt; 应用发送给用户usera和userb</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 企业微信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 远程磁盘检测</title>
      <link href="posts/1fac36d/"/>
      <url>posts/1fac36d/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">#!/bin/bash # 文件名：disklog.sh # 用途：监视远程系统的磁盘使用情况 BaseDir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $BaseDirlogfile=&quot;disk.log&quot; if [[ -n $1 ]];then     logfile=$1 fishellexecuser=`whoami`if [[ $shellexecuser == root ]];then    rm -rf /root/.ssh/known_hostselse    rm -rf /home/$shellexecuser/.ssh/known_hostsfiprintf &quot;%-8s %-14s %-9s %-8s %-6s %-6s %-6s %s\r\n&quot; &quot;Date&quot; &quot;IP ADDRESS&quot; &quot;Device&quot; &quot;Capacity&quot; &quot;Used&quot; &quot;Free&quot; &quot;Percent&quot; &quot;Status&quot; &gt; $logfile ##################### 手动填写区# 提供远程主机IP地址列表 1.1.1.1 2.2.2.2 3.3.3.3IP_LIST=# 监控阈值(百分比) 只填写数字 1 到 100DiskPct=# 执行用户UserName=''# 执行用户所需私钥, 此文件需要与脚本同级目录PemName=''# 企业微信bot机器人地址wx_api=''##################### 手动填写区for ip in $IP_LIST;do     ssh -i $PemName -o StrictHostKeyChecking=no $&#123;UserName&#125;@$ip 'df -H' | grep ^/dev/ &gt; /tmp/$$.df     while read line;do         cur_date=`date  &quot;+%F_%R&quot;`        printf &quot;%-8s %-14s &quot; $cur_date $ip         echo $line | awk '&#123; printf(&quot;%-9s %-8s %-6s %-6s %-8s&quot;, $1,$2,$3,$4,$5); &#125;'         pusg=$(echo $line | egrep -o &quot;[0-9]+%&quot;)         pusg=$&#123;pusg/\%/&#125;;         if [ $pusg -lt $DiskPct ];then             echo OK        else             echo ALERT         fi     done &lt; /tmp/$$.df     rm -rf /tmp/$$.dfdone &gt;&gt; $&#123;logfile&#125;sed -n '1p' $&#123;logfile&#125; &gt; alert.logawk '$NF == &quot;ALERT&quot;&#123;print $0&#125;' $&#123;logfile&#125; &gt;&gt; alert.log#sed -i '1i &quot;磁盘阈值：'&quot;$DiskPct&quot;'&quot;' alert.logcontent=`cat alert.log`grep -q 'ALERT' alert.log &amp;&amp; &#123;curl &quot;$wx_api&quot;  -H 'Content-Type: application/json'  \-d '   &#123;        &quot;msgtype&quot;: &quot;text&quot;,        &quot;text&quot;: &#123;            &quot;content&quot;: &quot;'&quot;$content&quot;'&quot;        &#125;   &#125;'&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 磁盘 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 邮件预警</title>
      <link href="posts/f8f9b4d0/"/>
      <url>posts/f8f9b4d0/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># $1 收件人# $2 主题# $3 内容smtpServer=  # smtp 服务器地址，例如 smtp.gmail.com:xxxsendUserEmail='it@abc.com'sendUserPassword=  # 一般发件人邮箱密码都是专用密码，并非web密码/usr/local/bin/sendEmail -f $&#123;sendUserEmail&#125; -t $1 -u &quot;$2&quot; -m &quot;$3&quot; -s $&#123;smtpServer&#125; -xu $&#123;sendUserEmail&#125; -xp $&#123;sendUserPassword&#125; -o message-charset=utf-8</code></pre><pre><code class="language-python">#!/usr/bin/python#coding:utf-8# by zyh# python sendmail.py '收件人邮箱地址'  '主题' '邮件内容' '抄送邮箱地址' '附件绝对路径'import smtplibfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartimport sysreload(sys)sys.setdefaultencoding('utf8')mail_host = 'smtp服务器地址'  # 例如 smtp.gmail.com:587mail_user = '发件人邮箱'      # 例如 it@gmail.commail_pass = '发件人密码'def send_mail(*args):    argsNum=len(args[0])    if argsNum == 4:        filename,to_list, subject, content=args[0]    elif argsNum == 5:        filename,to_list, subject, content, cc_list=args[0]    elif argsNum == 6:        filename,to_list, subject, content, cc_list, subfile=args[0]    me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;    #初始化邮件对象 msg    msg = MIMEMultipart()    msg['From'] = me    msg['to'] = to_list    emailList=to_list    # 主题    msg['Subject'] = Header(subject, 'utf-8').encode()    # 内容    msg.attach(MIMEText(content, 'plain', 'utf-8'))    # 抄送    if 'cc_list' in vars():        msg['Cc'] = cc_list        emailstring = to_list+','+cc_list        emailList=emailstring.split(&quot;,&quot;)    # 附件    if 'subfile' in vars():        att1 = MIMEText(open((&quot;%s&quot;%subfile),'rb').read(), 'base64', 'utf8')        att1[&quot;Content-Type&quot;] = 'text/plain'        att1[&quot;Content-Disposition&quot;] = 'attachment; filename='+subfile.split('/')[-1] # 这里的filename可以任意写，写什么名字，邮件中显示什么名字        msg.attach(att1)    # 发送    try:        print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)        print &quot;connect mail server suesscc&quot;        s.starttls()        s.login(mail_user,mail_pass)        print &quot;login mail server suesscc&quot;        s.sendmail(me,emailList,msg.as_string())        s.close()        return True    except Exception,e:        print &quot;%s\t%s&quot;%(to_list,str(e))        return Falseif __name__ == &quot;__main__&quot;:    if len(sys.argv) &lt; 4:        exit()    send_mail(sys.argv)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 邮件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞企业微信机器人</title>
      <link href="posts/8c33f887/"/>
      <url>posts/8c33f887/</url>
      
        <content type="html"><![CDATA[<h4 id="shell">shell</h4><pre><code class="language-bash">wechatUrl=wechatData=curl $&#123;wechatUrl&#125; -H 'Content-Type: application/json' -d '&#123;&quot;msgtype&quot;: &quot;markdown&quot;,&quot;markdown&quot;: &#123;&quot;content&quot;: &quot;`'&quot;$wechatData&quot;'`&quot;&#125;&#125;'</code></pre><h4 id="python">python</h4><pre><code class="language-python">import json, requestswechatData=&#123;&quot;msgtype&quot;: &quot;text&quot;,&quot;text&quot;: &#123;&quot;content&quot;: &quot;&quot;&#125;&#125;wechatData['text']['content']='广州今日天气：29度，大部分多云，降雨概率：60%'wechatData['text']['mentioned_list']=[&quot;zyh&quot;]  # all 代表群组所有人wechatData=json.dumps(wechatData)wechatUrl=requests.post(url=wechatUrl, headers=&#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;, data=wechatData, timeout=5)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 企业微信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞logrotate安装</title>
      <link href="posts/421d605e/"/>
      <url>posts/421d605e/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>logrotate 可以帮助我们进行日志切割，搭配 cron 服务，就可以自动的进行轮转</p><h4 id="logrotate-版本更新">logrotate 版本更新</h4><blockquote><p>确保 logrotate 支持小时级别的管理，替换/usr/sbin/logrotate,并附加x权限，我这里有一个二进制版本<a href="D:%5Czyh.cool%5Csource_posts%5C%E6%97%A5%E5%BF%97%5C%E5%85%B6%E5%AE%83%5C%E6%97%A5%E5%BF%97%E2%98%9Elogrotate%E5%AE%89%E8%A3%85%5Clogrotate">logrotate</a></p><p>或者也可以直接去 github 上拉取https://github.com/logrotate/logrotate</p></blockquote><h4 id="添加-logrotate-配置">添加 logrotate 配置</h4><pre><code class="language-bash"># 添加所需切割的日志配置cat &gt; /etc/logrotate.d/nginx &lt;&lt; 'EOF'/usr/local/nginx/logs/access.log &#123;  # 定义日志位置 hourly    # 按照小时切割 rotate 2  # 最多保留两份切割日志 missingok nocompress sharedscripts postrotate  /bin/kill -USR1 `cat /usr/local/nginx/logs/nginx.pid 2&gt;/dev/null` 2&gt;/dev/null || true endscript&#125;EOF</code></pre><h4 id="添加-crontab-配置">添加 crontab 配置</h4><pre><code class="language-bash"># 添加logrotate执行脚本cp /etc/cron.daily/logrotate /etc/cron.hourly/</code></pre><h4 id="重载-crond-服务">重载 crond 服务</h4><pre><code class="language-bash">systemctl reload crond</code></pre>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> logrotate </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-过滤器</title>
      <link href="posts/f43dca37/"/>
      <url>posts/f43dca37/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>不管是过滤器，lookup，query，with_xxx，很多都是获取我们想要的信息。</p><h4 id="过滤器">过滤器</h4><blockquote><p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html">https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html</a></p></blockquote><blockquote><p>处理变量值，从而获取想要的信息.</p><p>过滤器本身是 jinja2 或者 ansible 官方定义的</p></blockquote><h4 id="简例">简例</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    vara: abcde    varb: [1,2,3,A,b,C,d]    varc: 123    vard: [ 1,2,3,[4,5,6],4,5 ]  tasks:    - name: show upper      debug:        msg: &quot;&#123;&#123; vara | upper&#125;&#125;&quot;</code></pre><blockquote><p>简例中的 upper 即是过滤器，它可以将 vara 中的所有字母元素大写，最终输出 ABCDE</p></blockquote><h4 id="常用的过滤器">常用的过滤器</h4><pre><code class="language-yaml">#将字符串开头和结尾的空格去除msg: &quot;&#123;&#123; vara | trim &#125;&#125;&quot;#返回字符串或列表长度,length与count等效,可以写为countmsg: &quot;&#123;&#123; varb | length &#125;&#125;&quot;# 绝对值msg: &quot;&#123;&#123; varc | abs &#125;&#125;&quot;# 排序(降序排序)msg: &quot;&#123;&#123; varb | sort(reverse=true) &#125;&#125;&quot;# 将列表中第一层嵌套列表元素展开并入列表中,并取出新列表中的最大元素msg: &quot;&#123;&#123; vard | flatten(levels=1) | max &#125;&#125;&quot;# 随机返回一个元素msg: &quot;&#123;&#123; varb | random &#125;&#125;&quot;# 去重msg: &quot;&#123;&#123; vard | unique &#125;&#125;&quot;# 并集msg: &quot;&#123;&#123; varb | union(vard) &#125;&#125;&quot;# 交集msg: &quot;&#123;&#123; varb | intersect(vard) &#125;&#125;&quot;# 补集，取出存在于 varb，但不存在于 vard 中的元素msg: &quot;&#123;&#123; varb | difference(vard) &#125;&#125;&quot;# 去除两个列表交集后的元素msg: &quot;&#123;&#123; varb | symmetric_difference(vard) &#125;&#125;&quot;# 变量未定义，返回默认值 newmsg: &quot;&#123;&#123; vare | default('new')&#125;&#125;&quot;# 变量未定义或者定义但为空，返回默认值 newmsg: &quot;&#123;&#123; vare | default('new', boolean=true)&#125;&#125;&quot;# 变量未定义时，忽略某个参数file: xxxx  mode=&#123;&#123; vare | default(omit)&#125;&#125;&quot;  # 若 vare 不存在，则忽略mode参数</code></pre><h4 id="json-query">json_query</h4><blockquote><p>获取特定数据</p></blockquote><pre><code>1. 查询字符串可用变量代替，增加可读性 loop: &quot;&#123;&#123; domain_definition | json_query(server_name_cluster1_query) &#125;&#125;&quot; vars:    server_name_cluster1_query: &quot;domain.server[?cluster=='cluster1'].port&quot;</code></pre><ol start="2"><li>查询条件</li></ol><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    users:    - name: zhangsan      gender: male    - name: lisi      gender: female  tasks:    - name: test 1      debug:        msg: &quot;&#123;&#123; users | json_query('[?name==`zhangsan`].gender') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;male&quot;    ]&#125;</code></pre><h4 id="map">map</h4><blockquote><p>映射</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    users:    - name: zhangsan      gender: male    - name: lisi      gender: female  tasks:    - name: test 1      debug:        msg: &quot;&#123;&#123; users|map(attribute='name') | list &#125;&#125;&quot;    - name: test 2      debug:        msg: &quot;&#123;&#123; users | json_query('[*].name') &#125;&#125;&quot;</code></pre><pre><code class="language-bash"># test 2 是采用 json_query 方式，test1和test2结果一样ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;zhangsan&quot;,         &quot;lisi&quot;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-文本文件操作</title>
      <link href="posts/4235ddf4/"/>
      <url>posts/4235ddf4/</url>
      
        <content type="html"><![CDATA[<h4 id="var-定义">var 定义</h4><blockquote><p>通过变量，修改playbook</p><p>可直接写入 playbook， 也可以写入文件，然后 playbook 通过 vars_files 引用</p><p>关键词:</p><ul><li>vars</li><li>vars_files # 一次性加载文件内部数据，不支持文件动态修改或添加新变量</li></ul></blockquote><h4 id="简例">简例</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  vars_files: ~/vars.yml  vars:    father: Zhang San  tasks:    - name: test vars      shell: echo &quot;&#123;&#123; father &#125;&#125; - &#123;&#123; children.son_name &#125;&#125; success&quot; &gt;&gt; ~/son.log</code></pre><pre><code class="language-yaml">children:  son_name: Zhang Xiaosan</code></pre><pre><code class="language-bash"># son.log 内容Zhang San - Zhang Xiaosan success</code></pre><h4 id="var-注册">var 注册</h4><blockquote><p>当我们想将某个任务的结果写入一个变量的时候，我们可以用register来进行注册</p><p>关键词:</p><ul><li>register</li><li>debug<ul><li>var 输出变量值</li><li>msg 输出字符串</li></ul></li></ul></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  tasks:    - name: test register      shell: echo &quot;register success&quot; &gt; ~/register.log      register: resultInfo    - name: show register result      debug:        msg: &quot;Oh my god&quot;        var: resultInfo</code></pre><h4 id="var-交互">var 交互</h4><blockquote><p>提供一个用户输入信息的机会，和 shell 里面的 read -p 一致。</p><p>关键词：</p><ul><li>vars_prompt</li></ul></blockquote><pre><code class="language-bash">---- hosts: localhost  remote_user: zyh  vars_prompt:    - name: &quot;fatherName&quot;      prompt: &quot;What's your father name&quot;      default: ZhangSan      private: no  tasks:    - name: show father name      shell: echo &quot;&#123;&#123; fatherName &#125;&#125;&quot; &gt; ~/prompt.log</code></pre><blockquote><p>private yes=隐藏输入内容 no=显示输入内容</p><p>default 默认值</p><p>encrypt “sha512_crypt” 将变量值加密，一般用于传递密码，比如传递给 user 模块的 password 参数</p></blockquote><h4 id="var-命令行传入">var 命令行传入</h4><blockquote><p>一般用于临时强制覆盖playbook中定义好的变量</p><p>关键词：</p><ul><li>-e 或者 --extra-vars<ul><li>参数后面，可以跟随多个变量kv对，每一个kv对用空格隔开</li><li>参数后面，@filePath 可以传入变量文件，文件中的变量均可以被引用</li></ul></li></ul></blockquote><pre><code class="language-bash">ansible-play test.play -e &quot;fatherName=Laowang&quot;</code></pre><h4 id="var-作用域">var 作用域</h4><table><thead><tr><th>创建方式</th><th>调用位置</th><th>作用域</th></tr></thead><tbody><tr><td>vars</td><td>play和tasks</td><td>当前play或者当前tasks，无法跨主机</td></tr><tr><td>set_fact</td><td>tasks</td><td>跨play，默认无法跨主机</td></tr><tr><td>register</td><td>tasks</td><td>跨play，默认无法跨主机</td></tr></tbody></table><p>若要使得 set_fact 和 register 跨主机使用，则需要引入内置变量 <code>hostvars</code> 例如 hostvars.&lt;主机名&gt;.&lt;变量名&gt;</p><blockquote><p>其它内置变量：</p><ul><li><p>ansible_version # 版本</p></li><li><p>hostvars # 存储play中的所有主机变量</p></li><li><p>play_hosts # 存储play中的所有主机名</p></li><li><p>inventory_hostname  # 存储当前主机名</p></li><li><p>inventory_hostname_short  # 存储当前主机名简称（其实就是获取主机名第一级，例如001.localhost，那么获取的就是001）</p></li><li><p>groups # 存储所有分组信息，包括all和ungrouped</p></li><li><p>group_names # 存储当前play中主机的所属组名</p></li><li><p>inventory_dir # 存储主机清单文件所在路径</p></li></ul></blockquote><h4 id="var-动态获取新变量">var 动态获取新变量</h4><blockquote><p>关键词：include_vars</p><p>用于任务重载变量文件，从而获取任务期间变量文件修改的数据</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars_files: ~/test.yaml  tasks:    - name: get varb - max      debug:        msg: &quot;&#123;&#123; varb |  max &#125;&#125;&quot;    - name: lineinfile      lineinfile:        regexp: ^varb        line: &quot;varb: [1,2,3,4]&quot;        path: ~/test.yaml    - include_vars: ~/test.yaml    - name: get varb - max again      debug:        msg: &quot;&#123;&#123; varb |  max &#125;&#125;&quot;</code></pre><pre><code class="language-bash">#### test.yaml 变量文件初始内容:vara: 123                                                                 varb: [1,2,3]  #### 最终结果：第一次 get varb 任务输出 3, 第二次 get varb 任务输出 4</code></pre><blockquote><p>include_vars 模块常用参数：</p><ul><li><p>file 读取某个变量文件</p></li><li><p>dir 读取某个目录的所有变量文件</p></li><li><p>depth 递归层深，仅在 dir 启用的时候有意义</p></li><li><p>files_matching 正则匹配文件名，仅在 dir 启用的时候有意义</p></li><li><p>ignore_files 忽略某个列表，列表中的元素可以为正则表达式</p></li><li><p>name: 变量 x  将读取的文件内容集中复制给变量 x，例如上例中变量 x 为 {vara: 123, varb: [1,2,3,4]}</p></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞输出个性化开机状态</title>
      <link href="posts/3988a5f2/"/>
      <url>posts/3988a5f2/</url>
      
        <content type="html"><![CDATA[<h4 id="前沿">前沿</h4><p>觉得默认的登陆不够给力，无法忽悠机器，用wower的话来说，就是先祖忽悠着你</p><h4 id="效果图在此">效果图在此</h4><p><img src="/posts/3988a5f2/image-20200515110044304.png" alt="image-20200515110044304"></p><h4 id="脚本在此">脚本在此</h4><blockquote><p>将脚本放置到 /etc/profile.d/status.sh</p></blockquote><pre><code class="language-bash">#!/bin/bash# Author: zyh# 需先安装 toilet 和 cowsay 命令# yum install epel-release -y# yum install https://rpmfind.net/linux/openmandriva/4.1/repository/x86_64/unsupported/release/toilet-0.2-3-omv4000.x86_64.rpm cowsay -yuser=$USERhome=$HOME## blue to echofunction blue()&#123;    echo -e &quot;\033[34m[Info] $1\033[0m&quot;    &#125;## green to echofunction green()&#123;    echo -e &quot;\033[32m[Success] $1\033[0m&quot;    &#125;## Errorfunction red()&#123;    echo -e &quot;\033[31m\033[01m[Error] $1\033[0m&quot;    &#125;# warningfunction yellow()&#123;    echo -e &quot;\033[33m\033[01m[Warn] $1\033[0m&quot;    &#125;## Error to warning with blinkfunction bred()&#123;    echo -e &quot;\033[31m\033[01m\033[05m[Error] $1\033[0m&quot;    &#125;# Error to warning with blinkfunction byellow()&#123;    echo -e &quot;\033[33m\033[01m\033[05m[Warn] $1\033[0m&quot;    &#125;publicip=`curl -s http://169.254.169.254/latest/meta-data/public-ipv4`localip=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`echo -e &quot;$publicip $localip&quot; | cowsay -f tux | toilet -f term  --gay# * Check if we're somewhere in /homeif [ ! -d $&#123;home&#125; ];then    return 0fi# * Calculate last loginlastlog=`lastlog -u $&#123;user&#125; | grep $&#123;user&#125; | awk '&#123;for(i=3;i&lt;=NF;++i) printf(&quot;%s &quot;,$i)&#125;'`# * Print Outputecho &quot; ::::::::::::::::::::::::::::::::::-STATUS-::::::::::::::::::::::::::::::::::&quot;#  * Check RAM Usagesfree_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemAvailable/&#123;free=$2&#125;END&#123;print free/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;' /proc/meminfo)app_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemFree/&#123;free=$2&#125;/Buffers/&#123;buffers=$2&#125;/^Cached/&#123;cached=$2&#125;END&#123;print (total-free-buffers-cached)/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;'  /proc/meminfo)all_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemFree/&#123;free=$2&#125;END&#123;print (total-free)/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;'  /proc/meminfo)blue &quot; Free Memory : $&#123;free_mem_usages&#125;&quot;blue &quot; Application Memory Usages : $&#123;app_mem_usages&#125;&quot;blue &quot; System Memory Usages : $&#123;all_mem_usages&#125;&quot;# * Check Disk Usagesdiskusages=$(df -PH | awk '&#123;printf &quot;%-40s%-15s%-15s%-15s%-15s%-15s\n&quot;, $1,$2,$3,$4,$5,$6&#125;')blue &quot; Disk Usages :&quot;echo &quot;$&#123;diskusages&#125;&quot; | toilet -f term --metal -w 200# * Check Load Averageloadaverage=$(top -n 1 -b | grep &quot;load average:&quot; | awk '&#123;print $(NF-2) $(NF-1) $NF&#125;')blue &quot; Load Average: $loadaverage&quot;</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>问题记录</title>
      <link href="posts/1ddb9d4e/"/>
      <url>posts/1ddb9d4e/</url>
      
        <content type="html"><![CDATA[<h3 id="Q-包管理器安装软件出现">Q:包管理器安装软件出现</h3><pre><code class="language-bash">insserv:  loop involving service xxx at depth 2</code></pre><h4 id="A-删除-xxx-服务，如果-xxx-服务已删除，清理-xxx-服务的启动脚本-etc-init-d-xxx">A:删除 xxx 服务，如果 xxx 服务已删除，清理 xxx 服务的启动脚本 /etc/init.d/xxx</h4><hr><h3 id="Q-crontab-如何修改时区">Q:crontab 如何修改时区</h3><h4 id="A-在crontab文件最上方添加命令，例如芝加哥时区">A:在crontab文件最上方添加命令，例如芝加哥时区</h4><pre><code class="language-bash">TZ='America/Chicago'                                          CRON_TZ='America/Chicago'  `</code></pre><hr><h3 id="Q-su-user-c-“command”-命令出错">Q:su - user -c “command” 命令出错</h3><h4 id="A-需要用户开启登陆权限，即-etc-passwd-中不能使-sbin-nologin">A:需要用户开启登陆权限，即 /etc/passwd 中不能使 /sbin/nologin</h4><hr><h3 id="Q-zabbix-自动发现异常，表面看不出问题">Q:zabbix 自动发现异常，表面看不出问题</h3><h4 id="A-zabbix-agent-端开启-debug-模式，配置加入参数-DebugLevel-4">A:zabbix-agent 端开启 debug 模式，配置加入参数 DebugLevel=4</h4><hr><h3 id="Q-adminer-无效的CSRF令牌-Invalid-CSRF-token">Q:adminer: 无效的CSRF令牌(Invalid CSRF token)</h3><h4 id="A-nginx-worker-user-用户无法访问-php-session-目录">A:nginx worker user 用户无法访问 php session 目录.</h4><pre><code class="language-bash">chgrp $&#123;nginxWorkerUser&#125; $&#123;phpSessionDir&#125;# 如果是yum或者apt安装,那么php的session一般是 /var/lib/php/session</code></pre><hr><h3 id="Q-python3-No-module-named-‘PIL’">Q:python3 : No module named ‘PIL’</h3><h4 id="A-pip3-install-pillow">A:pip3 install pillow</h4><hr><h3 id="Q-python-workon-命令找不到">Q:python: workon 命令找不到</h3><h4 id="A">A:</h4><pre><code class="language-bash:">pip install virtualenvwrapperecho 'export PATH=~/.local/bin:$PATH' &gt;&gt; ~/.bashrc # 根据所用shell来决定文件路径</code></pre><hr><h3 id="Q-jira配置163的smtp连接超时-但是服务器终端telnet正常">Q:jira配置163的smtp连接超时,但是服务器终端telnet正常</h3><h4 id="A-2">A:</h4><p><img src="/posts/1ddb9d4e/image-20200917111208620.png" alt="image-20200917111208620"></p><h3 id="Q-tomcat-添加-pid-文件">Q: tomcat 添加 pid 文件</h3><h4 id="A-catalina-sh-文件的-PRGDIR-dirname-PRG-下面添加新行-CATALINA-PID-PRGDIR-tomcat-pid-此时将在-bin-目录下创建-tomcat-pid">A:  <a href="http://catalina.sh">catalina.sh</a> 文件的 <code>PRGDIR=dirname &quot;$PRG&quot;</code>下面添加新行 <code>CATALINA_PID=$PRGDIR/tomcat.pid</code> 此时将在 bin/ 目录下创建 tomcat.pid</h4>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 问题记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-tags</title>
      <link href="posts/960fc783/"/>
      <url>posts/960fc783/</url>
      
        <content type="html"><![CDATA[<h4 id="tags的定义">tags的定义</h4><blockquote><p>tags 可以让你在执行playbook的时候，有选择地执行某些任务，因此 tags 是 tasks 下的关键词</p></blockquote><pre><code class="language-bash">ansible-playbook test.play &lt;--tags-args&gt;</code></pre><h4 id="tags的参数">tags的参数</h4><blockquote><ul><li>–tags=tag_name  执行具有 tag_name 任务</li><li>–skip-tags=tag_name 忽略具有 tag_name 任务</li><li>–list-tags 输出所有</li></ul></blockquote><blockquote><p>tag_name 内置值 ：</p><ul><li>tagged 有tag的task，表示执行具有标记的任务</li><li>untagged 没有tag的task，表示执行不具有标记的任务</li><li>all 所有task，表示执行所有任务</li></ul></blockquote><h4 id="tags的内置标记">tags的内置标记</h4><blockquote><ul><li>always 总是执行某个 task</li><li>never 永远不执行某个 task</li></ul></blockquote><h4 id="tags-的位置">tags 的位置</h4><blockquote><p>位于play或者tasks都可以，本身具有继承属性，也就是tasks里的tags会继承play的tags</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  tags: father  tasks:    - name: test tag son      tags: son,children      shell: echo &quot;son is here!&quot; &gt; ~/son.log    - name: test tag daughter      tags: daughter,children      shell: echo &quot;daughter is here!&quot; &gt; ~/daughter.log</code></pre><pre><code class="language-bash">ansible-playbook test.play --tags=son # 只会生成 son.logansible-playbook test.play --tags=father  # 因继承机制，会生成 son.log 和 daughter.logansible-playbook test.play --tags=children # 因都含有，同样会生成 son.log 和 daughter.log</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-错误捕获</title>
      <link href="posts/c4126d5a/"/>
      <url>posts/c4126d5a/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>类似于 python 中的 try…except…finally，ansible 可以用 block…rescue…always</p><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - block:        - shell: mkdir /file      rescue:        - debug:            msg: &quot;No operation permission&quot;      always:        - debug:            msg: &quot;Task End!&quot;</code></pre><blockquote><p>创建 file 目录失败，则输出&quot;No operation permission&quot;, 最终总是输出“Task End!”</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-条件判断</title>
      <link href="posts/29683589/"/>
      <url>posts/29683589/</url>
      
        <content type="html"><![CDATA[<h4 id="关键词">关键词</h4><ul><li>when</li></ul><h4 id="运算符">运算符</h4><ul><li>==  !=  &gt;  &lt;  &gt;=  &lt;=</li><li>and or not</li><li>( ) 组合，例如 ( a and b ) or c</li></ul><blockquote><p>ansible 某个 task 报错，会导致任务终止，而ignore_errors: true 可以忽略某个任务的条件不满足</p></blockquote><h4 id="is-语句-或者-is-not-语句">is 语句 或者 is not 语句</h4><pre><code class="language-yaml">tasks:  - name: show is xxx    debug:      msg: &quot;xxx is ok&quot;    when: var is xxx</code></pre><blockquote><p>判断文件</p><ul><li>xxx 是 exists ，表示若 var 存在，条件为真</li><li>xxx 是 file, 表示若 var 是文件，条件为真</li><li>xxx 是 directory， 表示若 var 是目录，条件为真</li><li>xxx 是 link，表示若 var 是软连接，条件为真</li><li>xxx 是 mount，表示若 var 是挂载点，条件为真</li></ul></blockquote><blockquote><p>判断变量</p><ul><li>若 xxx 是 defined, 表示若 var 已定义，条件为真</li><li>若 xxx 是 undefined， 表示若 var 未定义，条件为真</li><li>若 xxx 是 none， 表示若 var 是空，条件为真</li></ul></blockquote><blockquote><p>判断任务状态</p><ul><li>若 xxx 是 success， 若 var 为某任务返回结果，则任务状态成功，条件为真</li><li>若 xxx 是 failure， 若 var 为某任务返回结果，则任务状态失败，条件为真</li><li>若 xxx 是 change，若 var 为某任务返回结果，则任务状态改变，条件为真</li><li>若 xxx 是 skip， 若 var 为某任务返回结果，则任务被忽略，条件为真</li></ul></blockquote><blockquote><p>判断字符串</p><ul><li><p>若 xxx 是 string，若 var 是字符串，条件为真</p></li><li><p>若 xxx 是 lower，若 var 是纯小写，条件为真</p></li><li><p>若 xxx 是 upper，若 var 是纯大写，条件为真</p></li></ul></blockquote><blockquote><p>判断数字</p><ul><li><p>若 xxx 是 number， 若 var 是数字，条件为真。 var: “123” ,这里 var 是字符串，不是数字</p></li><li><p>若 xxx 是 even，若 var 是偶数，条件为真</p></li><li><p>若 xxx 是 odd， 若 var 是奇数，条件为真</p></li><li><p>若 xxx 是 divisibleby(num), 若 var 可以被 num 整除，条件为真</p></li></ul></blockquote><blockquote><p>判断集合</p><ul><li>若 xxx 是 subset(list)，若 var 是 list 的子集，条件为真</li><li>若 xxx 是 superset(list), 若 var 是 list 的父集，条件为真</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-循环</title>
      <link href="posts/14bc184e/"/>
      <url>posts/14bc184e/</url>
      
        <content type="html"><![CDATA[<h4 id="常见的循环">常见的循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show debug info      debug:        msg: &quot;&#123;&#123; item &#125;&#125;&quot;      with_items:        - [ a,b ]        - [ A,B, [ D,E,F ]]</code></pre><blockquote><ul><li><p>with_list 输出最表层元素，在简例中，会输出 [ a,b ] 和 [ A,B, [ D,E,F ] ]</p></li><li><p>with_item 递归输出所有层元素</p></li><li><p>with_together 合并两个列表，元素按照对应下标结合，如果某一方列表元素缺失，则用null代替</p></li><li><p>with_indexed_items 最表层所有列表合并为一个新列表并循环。item由{ list.index: list.value } 构成。在简例中，新列表是[ a,b,A,B, [ D,E,F ]]</p><pre><code class="language-yaml"></code></pre></li></ul><p>msg: “Index:&#123;&#123; item.0 &#125;&#125;, Value:&#123;&#123; item.1 &#125;&#125;”</p><pre><code>```bashok: [localhost] =&gt; (item=[0, u'a']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:0, Vaule:a&quot;&#125;ok: [localhost] =&gt; (item=[1, u'b']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:1, Vaule:b&quot;&#125;ok: [localhost] =&gt; (item=[2, u'A']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:2, Vaule:A&quot;&#125;ok: [localhost] =&gt; (item=[3, u'B']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:3, Vaule:B&quot;&#125;ok: [localhost] =&gt; (item=[4, [u'D', u'E', u'F']]) =&gt; &#123;    &quot;msg&quot;: &quot;Index:4, Vaule:[u'D', u'E', u'F']&quot;&#125;</code></pre><ul><li>with_random_choice 随机输出一个最表层列表元素，简例中输出 [a,b] 或者 [ A,B, [ D,E,F ]]</li></ul></blockquote><h4 id="dict-字典循环">dict 字典循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show debug info      debug:        msg: &quot;Name:&#123;&#123; item.key &#125;&#125;, gender:&#123;&#123; item.value &#125;&#125;&quot;      with_dict:        Zhangsan: male        Lisi: female</code></pre><blockquote><p>输出所有字典</p></blockquote><h4 id="sequence-序列循环">sequence 序列循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show sequence info      debug:        msg: &quot;&#123;&#123; item &#125;&#125;&quot;      with_sequence:        start=1        end=5        stride=2        format=&quot;I'm %0.4f&quot;</code></pre><blockquote><ul><li>with_sequence 获取奇偶数，start和end是起止点，stride 是步长（步长可以为负值），format是格式化</li></ul></blockquote><h4 id="nested-嵌套循环">nested 嵌套循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show nested info      debug:        msg: &quot;mkdir /mnt/&#123;&#123; item.0 &#125;&#125;/&#123;&#123; item.1 &#125;&#125;&quot;      with_nested:        - [ a,b ]        - [ A,B,C ]</code></pre><blockquote><p>两个列表做笛卡尔积, 例如构建环境目录</p></blockquote><h4 id="subelements-子元素循环">subelements 子元素循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                    age: 18                    content:                            - eating                            - sleeping                            - play ogre                  - name: Maris                    gender: female                    age: 20                    content:                            - eating                            - sleeping                            - shopping  tasks:          - name: test vars 2            debug:                    msg: &quot;&#123;&#123; item.0.name &#125;&#125; - &#123;&#123; item.0.gender &#125;&#125; - &#123;&#123; item.1 &#125;&#125;&quot;            with_subelements:              - &quot;&#123;&#123; users &#125;&#125;&quot;              - content</code></pre><blockquote><p>分解 , 选中列表内某一个列表元素 content, 与作为一个临时整体的剩余元素构建笛卡尔积，形成 item</p></blockquote><h4 id="file-文件循环">file 文件循环</h4><pre><code class="language-yaml">with_file:  /mnt/a.ini  /mnt/b.ini</code></pre><blockquote><p>始终循环获取ansible主机里文件的内容。（与目标主机无关）</p></blockquote><h4 id="fileglob-寻找通配符匹配的文件">fileglob 寻找通配符匹配的文件</h4><pre><code class="language-yaml">with_fileglob:  - /home/zyh/test/dirA/*  - /home/zyh/test/dirB/[0-9].ini</code></pre><blockquote><p>始终循环获取ansible主机指定目录中匹配的文件名和路径。（与目标主机无关）</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-系统相关</title>
      <link href="posts/84eb900/"/>
      <url>posts/84eb900/</url>
      
        <content type="html"><![CDATA[<h4 id="cron">cron</h4><blockquote><p>crontab 计划任务</p><p>参数介绍：</p><p>name 计划任务注释，多次操作同名任务，只会修改，而不会新加</p><p>时间参数：</p><ul><li><p>minute hour day month weekday</p></li><li><p>special_time : @reboot @yearly @monthly @weekly @daily @hourly (每xxx执行)</p></li></ul><p>user 添加到指定用户计划任务中</p><p>job 计划任务执行命令</p><p>state 当值为absent时，指删除任务. 只需指定 name.</p><p>disabled 注释任务，若任务信息和之前不一致，会同时修改任务</p><p>backup 先备份再操作, 备份文件位于 /tmp/crontabxxxx</p></blockquote><pre><code class="language-bash">ansible localhost -m cron -a &quot;name='test cron module' user=zyh special_time=hourly job='ls /home/zyh &gt; /home/zyh/cron.log 2&gt;&amp;1'&quot;ansible localhost -m cron -a &quot;name='test cron module' state=absent&quot;</code></pre><h4 id="service">service</h4><blockquote><p>调用远程系统自身的服务管理模块，例如 centos6 的 service ，或者 centos7 的 systemctl</p><p>参数介绍:</p><p>name 服务名</p><p>state 执行动作 started, stopped, restarted, reloaded</p><p>enabled 开机自启动</p></blockquote><h4 id="user">user</h4><blockquote><p>用户管理</p><p>常用参数介绍：</p><p>name 用户名</p><p>group 用户组 groups 用户附加组</p><ul><li>append 额外附加用户附加组</li></ul><p>shell 指定默认shell，比如/usr/sbin/nologin</p><p>state 值为 absent 表示删除用户，值为 present 表示用户必须存在</p><ul><li>remove 删除用户时，同时删除用户家目录</li></ul><p>password 用户密码。（需要传递加密密码，不能是明文密码）</p><pre><code class="language-python">import crypt:passwd=print(crypt.crypt(passwd))</code></pre><p>generate_ssh_key 相当于远程执行 ssh-keygen 命令（不加任何参数，一路回车）。若已经存在~/.ssh/{id_rsa, id_rsa.pub}, 则不执行</p><ul><li>ssh_key_file 自定义私钥名和私钥存放路径, 公钥也会在自定义路径下生成</li></ul></blockquote><h4 id="group">group</h4><blockquote><p>管理用户组</p><p>参数介绍：</p><p>name 组名</p><p>state 组状态, 值为 absent 指删除(组本身并非用户主要组)</p></blockquote><h4 id="setup">setup</h4><blockquote><p>获取机器信息</p><p>参数介绍：</p><p>gather_subset 获取某个子集（all, min, hardware, network, virtual, ohai, facter）</p><p>filter 获取某个集合的某个key</p><p>fact_path 自定义信息存放目录</p></blockquote><pre><code class="language-ini"># setup 默认会搜索目标主机/etc/ansible/facts.d 下的自定义信息,例如 family.ini[family]father=Zhangsanson=Zhangxiaosan</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-handle</title>
      <link href="posts/b0929188/"/>
      <url>posts/b0929188/</url>
      
        <content type="html"><![CDATA[<h4 id="功能">功能</h4><p>用一个短路判断来说，就是两者是串联关系，handlers 用来处理任务后续</p><p>tasks &amp;&amp; handlers</p><p>tasks &amp;&amp; handlers - listen （handlers 组）</p><h4 id="handlers-的-playbook-样本">handlers 的 playbook 样本</h4><pre><code class="language-yaml">---- hosts: localhost  become: true  remote_user: root  tasks:  - name: check nginx    shell: /usr/sbin/nginx -t    notify:      log        - meta: flush_handlers     handlers:  - name: log    shell: echo &quot;nginx check success&quot; &gt; ~/playbook.log</code></pre><blockquote><p>多个tasks的时候，tasks后面的 <code>meta: flush_handlers</code> 可以让tasks执行完，立马执行关联的handlers。否则handlers会在所有tasks执行完后，才开始执行</p></blockquote><h4 id="handlers-listen-的-playbook-样本">handlers-listen 的 playbook 样本</h4><pre><code class="language-yaml">---- hosts: localhost  become: true  remote_user: root  tasks:  - name: check nginx    shell: /usr/sbin/nginx -t    notify:      log group    handlers:  - name: log1    listen: log group    shell: echo &quot;nginx check success 1&quot; &gt; ~/playbook.log  - name: log2    listen: log group    shell: echo &quot;nginx check success 2&quot; &gt;&gt; ~/playbook.log</code></pre><blockquote><p>handlers 通过 listen 绑定在一起， tasks 关联 liasten 绑定 handlers 组，最终 playbook.log 将会写入两行信息</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> handle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞备份</title>
      <link href="posts/2c2d50a0/"/>
      <url>posts/2c2d50a0/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>本文主要记录 redis 的两种数据磁盘固化方式。<br>涉及到相关参数，简单的命令操作等。</p><h4 id="rdb">rdb</h4><blockquote><p>通过fork一个子进程来存储某一时刻redis数据(bgsave方式)。rdb持久化是默认方式。</p><p>特点：</p><ul><li><p>小幅度丢失数据(取决于save或者bgsave命令的执行周期)，这里我们不说save，应该不会用到这个</p></li><li><p>恢复速度快</p></li></ul></blockquote><pre><code class="language-bash"># 压缩rdb文件rdbcompression yes# rdb 文件名称dbfilename redis-6379.rdb# rdb文件保存目录dir /redis/data/</code></pre><ul><li>数据自动写入策略 (满足下列规则，就执行bgsave)</li></ul><pre><code class="language-bash"># 900s内至少达到一条写命令save 900 1# 300s内至少达至10条写命令save 300 10# 60s内至少达到10000条写命令save 60 10000</code></pre><ul><li>数据人工写入策略 (每10分钟计划任务调用一次bgsave)</li></ul><pre><code class="language-bash">*/10 * * * * root /export/redis/bin/redis-cli -h 127.0.0.1 bgsave &gt;&gt; /export/redis/bgsave.log 2&gt;&amp;1  </code></pre><blockquote><p>bgsave 因需要fork子进程，所以需要额外预留空闲的物理内存，在overcommit_memory=1开启的情况下，预留内存大小 &gt; 周期变化数据大小</p></blockquote><h4 id="aof">aof</h4><blockquote><p>记录的是redis每一次的写入操作记录</p><p>特点：</p><ul><li>恢复速度慢</li><li>丢失数据小</li></ul></blockquote><pre><code class="language-bash"># 开启aof机制appendonly yes# aof文件名appendfilename &quot;appendonly.aof&quot;# 写入策略,always表示每个写操作都保存到aof文件中,也可以是everysec或no。 everysec 每秒一次appendfsync everysec# 默认不重写aof文件,意思就是每次 appendfsync 就压缩整合aof文件，避免aof过大，不推荐开启，影响性能no-appendfsync-on-rewrite no# 保存目录dir /redis/data/</code></pre><ul><li>人工计划任务重写</li></ul><pre><code class="language-bash">*/10 * * * * root /export/redis/bin/redis-cli -h 127.0.0.1 bgrewriteaof &gt;&gt; /export/redis/bgrewriteaof.log 2&gt;&amp;1</code></pre><ul><li>aof 因服务器挂掉损坏可以修复</li></ul><pre><code class="language-bash">redis-check-aof -fix file.aof</code></pre><h4 id="结论">结论</h4><p>rdb 或者 aof 根据业务二选一即可，没必要都开启，但是不管是哪一种，都可以在人工计划任务之后，复刻一份备份文件到云端对象存储中。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 备份 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-软件包管理</title>
      <link href="posts/3d9b9ec1/"/>
      <url>posts/3d9b9ec1/</url>
      
        <content type="html"><![CDATA[<h4 id="apt-repository">apt_repository</h4><blockquote><p>ubuntu 下：</p><p>repo 指定库地址，例如 nginx 地址 ppa:nginx/stable</p><p>state 值为 absent 时为删除</p></blockquote><pre><code class="language-bash">ansible localhost -m apt_repository -a &quot;repo=ppa:nginx/stable&quot; -b --ask-become-pass</code></pre><pre><code class="language-bash">localhost | CHANGED =&gt; &#123;    &quot;changed&quot;: true,     &quot;repo&quot;: &quot;ppa:nginx/stable&quot;,     &quot;state&quot;: &quot;present&quot;&#125;#( 04/24/20@ 3:10PM )( zyh@zyh ):~   cat  /etc/apt/sources.list.d/ppa_nginx_stable_bionic.listdeb http://ppa.launchpad.net/nginx/stable/ubuntu bionic main</code></pre><h4 id="apt">apt</h4><blockquote><p>常用参数：</p><p>name 包名</p><p>state 包状态 （absent-删除，latest-最新包,  present-默认安装） latest 相当于升级包</p><p>upgrade 升级 （yes，dist，full，no-默认）</p><p><a href="https://docs.ansible.com/ansible/latest/modules/apt_module.html#apt-module">https://docs.ansible.com/ansible/latest/modules/apt_module.html#apt-module</a></p></blockquote><pre><code class="language-bash">ansible localhost -m apt -a &quot;name=nginx state=present&quot; -b --ask-become-pass</code></pre><pre><code class="language-bash">localhost | CHANGED =&gt; &#123;    &quot;cache_update_time&quot;: 1587713114,     &quot;cache_updated&quot;: false,     &quot;changed&quot;: true,     &quot;stderr&quot;: &quot;&quot;,     &quot;stderr_lines&quot;: [],     &quot;stdout&quot;: &quot;Reading package lists...\nBuilding dependency tree.......</code></pre><h4 id="yum-repository">yum_repository</h4><blockquote><p>name 仓库名</p><p>baseurl 仓库地址</p><p>enabled （yes-默认，no)</p><p>gpgcheck (yes, no)</p><p>gpgcakey 指定 gpg ca 公钥</p><p>state (present-默认，absent-删除)</p></blockquote><pre><code class="language-bash">ansible localhost -m yum_repository -a &quot;name=epel baseurl=https://download.fedoraproject.org/pub/epel/$releasever/$basearch/&quot;</code></pre><h4 id="yum">yum</h4><blockquote><p>name 包名</p><p>state (absent-删除，present-安装-默认值，latest-更新)</p><p>disable_gpg_check 关闭gpg检查（用于源gpg检查没有的情况）</p><p>enablerepo 安装包的时候，先临时启用某个源</p><p>disablerepo 安装包的时候，先临时禁用某个源</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-命令调用</title>
      <link href="posts/1e274151/"/>
      <url>posts/1e274151/</url>
      
        <content type="html"><![CDATA[<h4 id="shell">shell</h4><blockquote><p>远程执行一个命令</p><p>部分参数解析：</p><p>chdir 远程工作目录</p><p>executable 远程执行shell，需要绝对路径</p></blockquote><pre><code class="language-bash">ansible localhost -m shell -a &quot;chdir=/ ls&quot;</code></pre><pre><code class="language-bash">localhost | CHANGED | rc=0 &gt;&gt;binbootdevetchome...</code></pre><h4 id="script">script</h4><blockquote><p>远程执行一个ansible主机环境的脚本</p><p>部分参数解析：</p><p>chdir 远程工作目录</p></blockquote><pre><code class="language-bash">cat test.sh#!/bin/bashfor i in `ls /export`;do        echo $idone#-------------ansible -i hosts test -m script -a &quot;/home/zyh/test.sh&quot;</code></pre><pre><code class="language-bash">10.200.10.212 | CHANGED =&gt; &#123;    &quot;changed&quot;: true,     &quot;rc&quot;: 0,     &quot;stderr&quot;: &quot;Shared connection to 10.200.10.212 closed.\r\n&quot;,     &quot;stderr_lines&quot;: [        &quot;Shared connection to 10.200.10.212 closed.&quot;    ],     &quot;stdout&quot;: &quot;jdk1.8.0_191\r\njdk8\r\nsen\r\n&quot;,     &quot;stdout_lines&quot;: [        &quot;jdk1.8.0_191&quot;,         &quot;jdk8&quot;,         &quot;sen&quot;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-文本文件操作</title>
      <link href="posts/4235ddf4/"/>
      <url>posts/4235ddf4/</url>
      
        <content type="html"><![CDATA[<h2 id="文本操作">文本操作</h2><h4 id="file">file</h4><blockquote><p>path 文件对象地址<br>state 文件类型或者动作状态 （touch: 针对文件, directory：针对目录, link：针对软连接, hard：针对硬链接)</p><p>src 软硬链接的源文件</p><p>owner 属主</p><p>group 属组</p><p>mode 数字权限</p><p>recurse 递归操作</p></blockquote><h4 id="blockinfile">blockinfile</h4><blockquote><p>在指定位置，插入文本块，并在文本块开头和结尾添加标记. 标记用来确认文本块的位置，一些参数会通过标记位置来修改文本块。</p><p>注释格式:</p><p># BEGIN xxx</p><p># END xxx</p><p>参数简介：</p><p>path 文件对象地址</p><p>block 需要添加的文本块</p><p>marker 自定义标记 xxx 部分，如果存在相同标记，则优先处理相同标记的文本块。</p><p>state 状态为absent时，删除标记包括的文本块</p><p>insertafter 正则或者EOF，插入匹配的指定行之后</p><p>insertbefore 正则或者BOF，插入匹配的指定行之前</p><p>backup 先备份，再操作，备份文件后缀是时间戳</p><p>create 文件不存在，则创建</p></blockquote><pre><code class="language-bash"># 这条命令中，如果marker标记已经存在，则insertafter将无效-m blockinfile -a 'path= block=&quot; &quot; marker=&quot;#&#123;mark&#125; xxx&quot; insertafter=&quot;正则&quot;'</code></pre><h4 id="lineinfile">lineinfile</h4><blockquote><p>根据指定的内容，进行替换或删除</p><p>参数简介：</p><p>path 文件对象地址。</p><p>line 指定行内容（在没有正则的情况下，需要全匹配）。</p><p>regexp 通过正则匹配行，并将此行替换成 line 指定的内容，regexp有额外扩展参数，例如 backref。</p><ul><li><p>line  若line匹配到某行，则不修改，若无匹配，则添加line至末尾。</p></li><li><p>regexp + line  若有regexp匹配到某行，替换匹配行为line；如果没有匹配，则将line追加到行尾。此时，regexp不支持分组。</p></li><li><p>regexp + backrefs （true）+ line  若有regexp匹配到某行，替换匹配行为line；如果没有匹配，则保持源文件不变；此时，regexp支持分组。</p></li></ul><p>state 状态为absent时，删除匹配行</p><p>insertafter 正则或者EOF，插入匹配的指定行之后</p><p>insertbefore 正则或者BOF，插入匹配的指定行之前</p><p>backup 先备份，再操作</p><p>create 文件不存在，则创建</p></blockquote><pre><code class="language-bash">-m lineinfile -a 'path= line=&quot; &quot; insertafter=&quot;正则&quot;'</code></pre><h4 id="replace">replace</h4><blockquote><p>替换文件对象中符合匹配的字符串</p><p>path 文件对象地址</p><p>regexp 正则匹配</p><p>replace 替换后的字符串</p><p>backup 先备份，再操作</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-aws</title>
      <link href="posts/15aa7d2e/"/>
      <url>posts/15aa7d2e/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://github.com/ansible/ansible/blob/stable-2.9/contrib/inventory/">https://github.com/ansible/ansible/blob/stable-2.9/contrib/inventory/</a></p><p><a href="https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html">https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html</a></p><p><a href="https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html#inventory-script-example-aws-ec2">https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html#inventory-script-example-aws-ec2</a> （深坑，脚本404，找到了脚本，各种错误，请扔一边）</p></blockquote><h3 id="前言">前言</h3><p>通过 ansible 获取大区下 ec2 资源信息</p><h3 id="授权">授权</h3><pre><code class="language-shell">export AWS_ACCESS_KEY_ID='AK123'export AWS_SECRET_ACCESS_KEY='abc123'export EC2_INI_PATH=ec2.ini</code></pre><h3 id="库存-inventory">库存(inventory)</h3><pre><code class="language-ini">[local]localhost</code></pre><h3 id="Playbook">Playbook</h3><pre><code class="language-yaml">---  - name: test ec2    hosts: local    gather_facts: no   # 我们要这信息干什么？我们是有目标的    connection: local # 木有定义资源    tasks:      - name: get ec2 info        ec2_instance_info:          region: cn-north-1        register: data_output      - name: show ec2 info        debug:          msg: &quot;&#123;&#123; data_output|json_query('instances[*].network_interfaces[*].private_ip_address') &#125;&#125;&quot;</code></pre><h3 id="执行">执行</h3><pre><code class="language-shell">ansible-playbook -i hosts ec2.yml</code></pre><h3 id="输出">输出</h3><pre><code class="language-shell">TASK [show ec2 info] ******************************************************************************************************************************************************ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        [            &quot;10.100.10.250&quot;        ],         [            &quot;10.100.10.252&quot;        ],         [            &quot;10.100.10.210&quot;        ],         [            &quot;10.100.10.251&quot;        ]    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>windows 包管理工具 scoop</title>
      <link href="posts/22b82d75/"/>
      <url>posts/22b82d75/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-powershell">Set-ExecutionPolicy RemoteSigned -scope CurrentUseriwr -useb get.scoop.sh | iexscoop install aria2scoop config aria2-max-connection-per-server 16scoop config aria2-split 16scoop config aria2-min-split-size 1Mscoop bucket add extrasscoop install Terminus</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> windows </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows </tag>
            
            <tag> scoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞安装和基本配置</title>
      <link href="posts/d9cdb70/"/>
      <url>posts/d9cdb70/</url>
      
        <content type="html"><![CDATA[<ul><li><p>jenkins</p><blockquote><p><a href="https://github.com/jenkinsci/docker/blob/master/README.md">https://github.com/jenkinsci/docker/blob/master/README.md</a></p></blockquote><pre><code class="language-bash">jenkinsDomain=gitlabDomain=gitlabWanIP=jenkinsDns=docker volume create jenkins_homedocker run --name jenkins --hostname $&#123;jenkinsDomain&#125; --add-host $&#123;gitlabDomain&#125;:$&#123;gitlabWanIP&#125; --restart always -d -v jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7 -p 8080:8080 -p 50000:50000 --dns $&#123;jenkinsDns&#125; jenkins/jenkins:lts</code></pre><pre><code class="language-bash">docker exec -it -u root jenkins /bin/bashcurl https://bootstrap.pypa.io/get-pip.py -o get-pip.pypython get-pip.pypip install awsclipip install ansiblegroupadd -g &lt;docker-group-id&gt; dockerusermod -aG docker jenkinsdocker stop jenkins &amp;&amp; docker start jenkins</code></pre></li><li><p>用户和角色管理</p><ol><li><p>安装插件 Role-based Authorization Strategy</p></li><li><p>启用插件 Configure Global Security 中启用 Role-Based Strategy 策略</p><p><img src="/posts/d9cdb70/image-20200515180703925.png" alt="image-20200515180703925"></p></li><li><p>配置全局角色和项目角色 Manage and Assign Roles - Manage Roles</p><p>全局角色<strong>Global roles</strong> 设置两个： admin 和 read</p><p><img src="/posts/d9cdb70/image-20200515181449965.png" alt="image-20200515181449965"></p><p>项目角色<strong>Project roles</strong>：每一个项目设置一个</p><p>Pattern: <code>.*\.&lt;项目名&gt; </code></p><p>权限: 看图</p><p><img src="/posts/d9cdb70/image-20200515181359533.png" alt="image-20200515181359533"></p></li><li><p>创建项目用户</p></li><li><p>分配角色 Manage and Assign Roles - Assign Roles</p><p>给管理员分配 admin，给项目用户分配 read 和 cp (cp是我设置的项目角色)</p><p><img src="/posts/d9cdb70/image-20200515181740598.png" alt="image-20200515181740598"></p></li></ol></li><li><p>安装配置文件插件 Config File Provider</p></li><li><p>自动安装 git ，jdk，maven （这种方式只有在进行了一次构建后，才会安装）</p></li><li><p>maven 私服配置文件</p><ol><li><p>通过 Config File Provider 添加一个项目maven配置</p></li><li><pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;  &lt;servers&gt;    &lt;server&gt;     &lt;id&gt;&lt;Repository Policy名&gt;&lt;/id&gt;     &lt;username&gt;&lt;/username&gt;     &lt;password&gt;&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;     &lt;id&gt;&lt;Repository Policy名&gt;&lt;/id&gt;      &lt;username&gt;&lt;/username&gt;      &lt;password&gt;&lt;/password&gt;    &lt;/server&gt;  &lt;/servers&gt;  &lt;profiles&gt;  &lt;profile&gt;&lt;id&gt;&lt; 仓库名 &gt;&lt;/id&gt;&lt;repositories&gt;  &lt;repository&gt;&lt;id&gt;&lt;maven 仓库组或仓库ID&gt;&lt;/id&gt;&lt;url&gt;&lt;maven 私服具体仓库组或仓库地址&gt;&lt;/url&gt;&lt;releases&gt;  &lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt;&lt;snapshots&gt;  &lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt;  &lt;/repository&gt;&lt;/repositories&gt;  &lt;/profile&gt;  &lt;/profiles&gt;  &lt;activeProfiles&gt; &lt;activeProfile&gt;项目名&lt;/activeProfile&gt;  &lt;/activeProfiles&gt;&lt;/settings&gt;</code></pre></li><li><p>然后构建项目的时候，构建环境-Provide Configuration files-Files，并且Build-高级-Settings file-Provided settings.xml</p></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞安装-包管理方式</title>
      <link href="posts/b73a61bf/"/>
      <url>posts/b73a61bf/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>介绍如何通过yum或者apt-get安装php和php-fpm<br>适合php7.2</p><h4 id="centos">centos</h4><blockquote><p>安装源 <a href="https://webtatic.com/">https://webtatic.com/</a></p></blockquote><pre><code class="language-bash">yum install epel-releaserpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpmyum install gcc-c++ geoip-devel -yyum install php72w-cli php72w-devel mod_php72w php72w-fpm php72w-opcache php72w-gd php72w-bcmath php72w-xml -y# php72w-lzo php72w-yaf 没有直接的包mkdir /export/logs/php -pcd /etc/php-fpm.d/ &amp;&amp; mv www.conf www.conf.bakwebName=wwwcat &gt; www.conf &lt;&lt; EOF[$&#123;webName&#125;]user = webappsgroup = webappslisten = 0.0.0.0:9000pm = staticpm.max_children = 20pm.max_requests = 1024pm.status_path = /php-fpm_statusrequest_slowlog_timeout = 2sslowlog = /export/logs/php/php-slow.logphp_admin_value[error_log] = /export/logs/php/www-error.logphp_admin_flag[log_errors] = onphp_value[session.save_handler] = filesphp_value[session.save_path]    = /var/lib/php/sessionphp_value[soap.wsdl_cache_dir]  = /var/lib/php/wsdlcacheEOF# 安装源中没有的模块, 假设模块是rediscd /usr/local/srcphpModule=yafwget https://pecl.php.net/get/$&#123;phpModule&#125; &amp;&amp; mkdir $&#123;phpModule&#125;-src &amp;&amp; tar xf $&#123;phpModule&#125; --strip-components 1 -C $&#123;phpModule&#125;-srccd $&#123;phpModule&#125;-src &amp;&amp; phpize &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make installcat &gt; /etc/php.d/$&#123;phpModule&#125;.ini &lt;&lt; EOF; Enable zip extension moduleextension=$&#123;phpModule&#125;.soEOF</code></pre><h4 id="ubuntu">ubuntu</h4><pre><code class="language-bash">add-apt-repository ppa:ondrej/phpapt-get install php7.2 php7.2-dev php7.2-fpm php7.2-mysql php7.2-curl php7.2-json php7.2-mbstring php7.2-xml  php7.2-intl php7.2-yac php7.2-yaf php7.2-redis php7.2-lzo php7.2-geoip php7.2-pecl php7.2-pear php7.2-dev php7.2-gd php7.2-zip php7.2-xml php7.2-bcmath</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞基础信息</title>
      <link href="posts/d3e413b7/"/>
      <url>posts/d3e413b7/</url>
      
        <content type="html"><![CDATA[<h3 id="安装">安装</h3><pre><code class="language-shell">apt-get install python3-pippip3 install ansible --user -i https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><blockquote><p>pip 安装方式，不会生成默认配置<br><a href="https://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg">https://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg</a></p></blockquote><h3 id="关闭-known-hosts-检查">关闭 known_hosts 检查</h3><pre><code class="language-ini"># /etc/ansible/ansible.cfg or ~/.ansible.cfg[defaults]host_key_checking = False</code></pre><h3 id="库存和变量">库存和变量</h3><pre><code class="language-ini"># /etc/ansible/hosts 默认位置，但可自定义，并通过 -i 来调用############################################################################# 单主机mail.example.com# http_port 主机变量[webservers]www[01:50].example.comhttp_port=80[dbservers]db-[a:f].example.comansible_connection=ssh        ansible_ssh_user=mysql# 组变量 ==&gt; 组名:vers[dbservers:vars]mysql_port=3306# 嵌套组 ==&gt; 父组:children[webproject:children]webserversdbservers</code></pre><blockquote><p>中括号表示分组，可以用组名代替组资源 ;</p></blockquote><h3 id="结构化变量">结构化变量</h3><blockquote><p>采用 yaml 配置，格式：</p><pre><code class="language-yaml">---  变量:值</code></pre></blockquote><pre><code class="language-shell">/etc/ansible/group_vars/&lt;组名&gt; # &lt;组名&gt;文件或路径下的文件，均会被认为是&lt;组名&gt;变量/etc/ansible/host_vars/&lt;主机名&gt; # &lt;主机名&gt;文件或路径下的文件，均会被认为是&lt;组名&gt;变量</code></pre><h4 id="常用的变量">常用的变量</h4><pre><code class="language-shell">ansible_ssh_host      将要连接的远程主机名.与你想要设定的主机的别名不同的话,可通过此变量设置.ansible_ssh_port      ssh端口号.如果不是默认的端口号,通过此变量设置.ansible_ssh_user      默认的 ssh 用户名ansible_ssh_pass      ssh 密码(这种方式并不安全,我们强烈建议使用 --ask-pass 或 SSH 密钥)ansible_sudo_pass      sudo 密码(这种方式并不安全,我们强烈建议使用 -b --ask-become-pass)ansible_sudo_exe (new in version 1.8)      sudo 命令路径(适用于1.8及以上版本)ansible_connection      与主机的连接类型.比如:local, ssh 或者 paramiko. Ansible 1.2 以前默认使用 paramiko.1.2 以后默认使用 'smart','smart' 方式会根据是否支持 ControlPersist, 来判断'ssh' 方式是否可行.ansible_ssh_private_key_file      ssh 使用的私钥文件.适用于有多个密钥,而你不想使用 SSH 代理的情况.ansible_shell_type      目标系统的shell类型.默认情况下,命令的执行使用 'sh' 语法,可设置为 'csh' 或 'fish'.ansible_python_interpreter      目标主机的 python 路径.适用于的情况: 系统中有多个 Python, 或者命令路径不是&quot;/usr/bin/python&quot;,比如  \*BSD, 或者 /usr/bin/python      不是 2.X 版本的 Python.我们不使用 &quot;/usr/bin/env&quot; 机制,因为这要求远程用户的路径设置正确,且要求 &quot;python&quot; 可执行程序名不可为 python以外的名字(实际有可能名为python26).      与 ansible_python_interpreter 的工作方式相同,可设定如 ruby 或 perl 的路径....</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工具库</title>
      <link href="posts/3827a60a/"/>
      <url>posts/3827a60a/</url>
      
        <content type="html"><![CDATA[<p><a href="https://encycolorpedia.cn/323e4e">十六进制颜色代码表，图表，调色板，绘图&amp;油漆</a></p><p><a href="http://gosspublic.alicdn.com/ram-policy-editor/index.html">阿里云 ram 策略生成器</a></p><p><a href="https://develop.aliyun.com/tools/sdk?#/python">阿里云SDK频道</a></p><p><a href="https://app.xuty.tk/static/app/index.html">表情锅</a></p><p><a href="https://whoer.net/zh">测试国外出口ip</a></p><p><a href="http://www.ip-api.com/">测试国外出口ip</a></p><p><a href="https://www.wondercv.com/">超级简历WonderCV - HR推荐简历模板,智能简历制作工具,专业中英文简历模板免费下载</a></p><p><a href="https://help.aliyun.com/knowledge_detail/50270.html?spm=a2c4g.11186623.6.621.483534bfFo31Sm">各地区管局备案规则</a></p><p><a href="https://www.ipplus360.com/">更精准的全球IP地址定位平台_IP问问 -埃文科技(ipplus360.com)</a></p><p><a href="http://xn--eqrt2g.xn--vuq861b/">工信部-域名.信息</a></p><p><a href="http://explainshell.com/">解析命令 explainshell.com - match command-line arguments to their help text</a></p><p><a href="https://haveibeenpwned.com/Passwords">密码泄露检测</a></p><p><a href="https://tuna.moe/">清华大学 TUNA 协会 - Home</a></p><p><a href="http://zh.thetimenow.com/time-zone-converter.php">时区转换生成</a></p><p><a href="https://help.aliyun.com/document_detail/116378.html?spm=a2c4g.11186623.2.17.6f36578flUOrcy#concept-188715">使用redis-shake迁移RDB文件内的数据</a></p><p><a href="https://www.17ce.com/">网站测速|网站速度测试|网速测试|电信|联通|网通|全国|监控|CDN|PING|DNS 17CE.COM</a></p><p><a href="https://devhints.io/">语言/工具语法常用摘要</a></p><p><a href="https://ipchaxun.com/">域名反查ip</a></p><p><a href="https://www.whatsmydns.net/">域名解析检查</a></p><p><a href="https://intodns.com/">域名状态报告</a></p><p><a href="https://github.com/ireaderlab/alex">alex:web压力测试工具</a></p><p><a href="http://www.kammerl.de/ascii/AsciiSignature.php">Ascii Text / Signature Generator motd动态开机提醒</a></p><p><a href="https://www.json2yaml.com/convert-yaml-to-json">Convert YAML to JSON</a></p><p><a href="https://csr.chinassl.net/generator-csr.html">CSR文件生成工具-中国数字证书CHINASSL</a></p><p><a href="https://apps.evozi.com/apk-downloader/">gp apk 下载</a></p><p><a href="http://tool.520101.com/wangluo/ipjisuan/">ip地址在线计算器</a></p><p><a href="https://ipv6-test.com/validate.php">IPv6 站点测试</a></p><p><a href="http://grokconstructor.appspot.com/do/match#result">logstash grok 测试</a></p><p><a href="https://github.com/DoubleLabyrinth/MobaXterm-keygen">MobaXterm-keygen 密钥</a></p><p><a href="http://msdn.itellyou.cn/">MSDN, 我告诉你</a></p><p><a href="https://api.aliyun.com/#/cli">OpenAPI Explorer</a></p><p><a href="https://www.pyman.com.cn/">pyman网址导航</a></p><p><a href="http://rpm.pbone.net/">RPM Search</a></p><p><a href="https://www.cnblogs.com/zhaoruiqing/articles/12870209.html">Typora的Emoji指令</a></p><p><a href="https://paste.ubuntu.com/">Ubuntu Pastebin</a></p><p><a href="http://www.92csz.com/study/UnixToolbox-zh_CN.html">Unix Toolbox - 中文版</a></p><p><a href="https://www.dotcom-tools.com/">Website Performance Test Tools</a></p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞时间</title>
      <link href="posts/cd32aa48/"/>
      <url>posts/cd32aa48/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>本文中描述的并不能涵盖所有时间设置命令，只是记录了经常用到的一些。</p><h2 id="时间同步">时间同步</h2><pre><code>sudo yum erase ntp*sudo yum -y install chronysudo service chronyd startsed -i '1i server 10.200.16.101 prefer iburst' /etc/chrony.conf</code></pre><blockquote><p>10.200.16.101 替换成ntp服务器</p></blockquote><h2 id="时区修改">时区修改</h2><p>如果你用的是7以上的centos或者ubuntu</p><p>那么可以用 timedatectl 命令来设置时区</p><p>timedatectl --list-timezones</p><p>timedatectl --set-timezone Asia/Shanghai</p><hr><h2 id="没有timedatectl命令的话">没有timedatectl命令的话</h2><h4 id="修改会话时区">修改会话时区</h4><pre><code class="language-bash">echo &quot;TZ='UTC+0'; export TZ&quot; &gt;&gt; ~/.bash_profile</code></pre><blockquote><p>需要注意以下几点：</p><ol><li>UTC8 表示西8区</li><li>tzselect 可以帮你查看时区有哪些</li><li>UTC 方式，无法识别冬令时和夏令时，所以建议用地区名称，例如 asia/shanghai</li></ol></blockquote><h4 id="修改crontab时区">修改crontab时区</h4><p>在 crontab 用户配置最上面加入，例如添加芝加哥时区</p><pre><code class="language-bash">TZ='America/Chicago'CRON_TZ='America/Chicago'</code></pre><blockquote><p>关于时区设置方面，不建议修改配置，因为不够灵活</p></blockquote><h4 id="修正时间-写入硬件时钟">修正时间, 写入硬件时钟</h4><pre><code class="language-bash">yum install ntp -yntpdate cn.ntp.org.cnhwclock -wecho '0 12 * * * /usr/sbin/ntpdate cn.ntp.org.cn &gt; /dev/null 2&gt;&amp;1' &gt;&gt; /etc/crontab</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 时区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞ELK简单部署-容器方式</title>
      <link href="posts/3475106c/"/>
      <url>posts/3475106c/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><ul><li>各组件总下载页: <a href="https://www.elastic.co/cn/downloads/">https://www.elastic.co/cn/downloads/</a></li><li>容器下载页: <a href="https://www.docker.elastic.co">https://www.docker.elastic.co</a></li></ul><ol><li>Elasticsearch 搜索分析 <a href="https://www.elastic.co/cn/downloads/elasticsearch">https://www.elastic.co/cn/downloads/elasticsearch</a></li><li>Logstash 转换输出 <a href="https://www.elastic.co/cn/downloads/logstash">https://www.elastic.co/cn/downloads/logstash</a></li><li>Filebeat 收集 <a href="https://www.elastic.co/cn/downloads/beats/filebeat">https://www.elastic.co/cn/downloads/beats/filebeat</a></li><li>Kibana 展示 <a href="https://www.elastic.co/cn/downloads/kibana">https://www.elastic.co/cn/downloads/kibana</a></li></ol><h4 id="数据过程">数据过程:</h4><p>Filebeat ☞ Logstash ☞ Elasticsearch (master node) + data node ☞ Kibana</p><h4 id="安装步骤">安装步骤</h4><pre><code class="language-bash"># dockeryum install docker -y 或者 yum install docker-ce -yyum install python3-pip -ypip3 install docker-compose 或者 pip install docker-composesystemctl enable docker# 修改 docker 默认数据目录vi /etc/docker/daemon.json&#123;&quot;data-root&quot;: &quot;/export/docker-data-root&quot;&#125;systemctl start docker</code></pre><pre><code class="language-yml"># elasticsearch 具体安装命令和版本请以下载页中对应的docker安装方式页里命令为基准sysctl -a | grep  vm.max_map_count  # 查看是否过小, 如果过小执行下一条echo 'vm.max_map_count=262144' &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -pmkdir -p /export/docker-compose-data/es;touch /export/docker-compose-data/docker-compose.yml;touch /export/docker-compose-data/es/es01.yml;touch /export/docker-compose-data/es/es02.yml;mkdir -p /export/docker-compose-data/kibana;touch /export/docker-compose-data/kibana/kibana.yml;mkdir -p /export/docker-compose-data/logstashtouch /export/docker-compose-data/logstash/logstash.yml;</code></pre><pre><code class="language-bash"># es01 master 和 data# es02 datacat &gt; /export/docker-compose-data/es/es01.yml &lt;&lt; EOFnode.master: truenode.data: truehttp.port: 9200network.host: 0.0.0.0http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; xpack.security.enabled: falseEOFcat &gt; /export/docker-compose-data/es/es02.yml &lt;&lt; EOFnode.master: falsenode.data: truehttp.port: 9200network.host: 0.0.0.0http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; xpack.security.enabled: falseEOFcat &gt; /export/docker-compose-data/kibana/kibana.yml &lt;&lt; EOFxpack.monitoring.ui.container.elasticsearch.enabled: truei18n.locale: zh-CNEOFcat &gt; /export/docker-compose-data/logstash/logstash.yml &lt;&lt; EOFnode.name: logstashhttp.host: 0.0.0.0http.port: 9600log.level: infoconfig.reload.automatic: trueconfig.reload.interval: 10sconfig.support_escapes: falseEOF</code></pre><h4 id="编写-logstash-管道文件">编写 logstash 管道文件</h4><pre><code class="language-yml"># 示例 /export/docker-compose-data/logstash/pipeline/es-curator.confinput &#123;file &#123;path =&gt; &quot;/mnt/info.log&quot;type =&gt; &quot;es-curator&quot;start_position =&gt; &quot;beginning&quot;&#125;&#125;output &#123;if [type] == &quot;es-curator&quot; &#123;elasticsearch &#123;hosts=&gt; [&quot;es01:9200&quot;]index=&gt; &quot;es-curator-%&#123;+YYYY-MM-dd&#125;&quot;&#125;&#125;&#125;</code></pre><h4 id="启动服务docker-compose配置文件">启动服务docker-compose配置文件</h4><blockquote><p><a href="https://docs.docker.com/compose/compose-file/compose-file-v2/">https://docs.docker.com/compose/compose-file/compose-file-v2/</a></p><p>本配置文件参考 2.3 版本, 请勿使用 3.x 版本, 因为它的资源层 deploy, docker-compose命令不支持</p></blockquote><pre><code class="language-yml">cat &gt; /export/docker-compose-data/docker-compose.yml &lt;&lt; EOFversion: '2.3'services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1    container_name: es01    environment:      - node.name=es01      - cluster.initial_master_nodes=es01      - cluster.name=es-sen      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms1536m -Xmx1536m&quot;    ulimits:      memlock:        soft: -1        hard: -1      nofile:        soft: 65536        hard: 65536    volumes:      - esdata01:/usr/share/elasticsearch/data      - /export/docker-compose-data/es/es01.yml:/usr/share/elasticsearch/config/elasticsearch.yml    ports:      - 9200:9200    networks:      - esnet    cpus: 0.5    mem_limit: 3G    memswap_limit: 3G    mem_reservation: 3G    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9200&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failure  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1    container_name: es02    environment:      - node.name=es02      - discovery.seed_hosts=es01      - cluster.initial_master_nodes=es01      - cluster.name=es-sen      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms1536m-Xmx1536m&quot;    ulimits:      memlock:        soft: -1        hard: -1      nofile:        soft: 65536        hard: 65536    volumes:      - esdata02:/usr/share/elasticsearch/data      - /export/docker-compose-data/es/es02.yml:/usr/share/elasticsearch/config/elasticsearch.yml    networks:      - esnet    cpus: 1    mem_limit: 3G    memswap_limit: 3G    mem_reservation: 3G    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9200&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failure  kibana:    image: docker.elastic.co/kibana/kibana:7.4.1    container_name: kibana    environment:      SERVER_NAME: kibana.sen-sdk.com      SERVER_HOST: 0.0.0.0      ELASTICSEARCH_HOSTS: http://es01:9200    volumes:      - /export/docker-compose-data/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml    ports:      - 5601:5601    networks:      - esnet    cpus: 0.5    mem_limit: 512m    memswap_limit: 512m    mem_reservation: 512m    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:5601&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failure  logstash:    image: docker.elastic.co/logstash/logstash:7.4.1    container_name: logstash    volumes:      - /export/docker-compose-data/logstash/pipeline/:/usr/share/logstash/pipeline/      - /export/docker-compose-data/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml      - logstash:/usr/share/logstash/data    ports:      - &quot;5044:5044&quot;      - &quot;9600:9600&quot;    networks:      - esnet    cpus: 0.5    mem_limit: 1g    memswap_limit: 1g    mem_reservation: 1g    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9600&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failurevolumes:  esdata01:    driver: local  esdata02:    driver: local  logstash:    driver: localnetworks:  esnet:EOF</code></pre><h4 id="启动-docker-compose">启动 docker-compose</h4><pre><code class="language-bash">cd /export/docker-compose-data/docker-compose up -d# docker-compose up -d --no-recreate</code></pre>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx ☞ 基本认证</title>
      <link href="posts/18de0eed/"/>
      <url>posts/18de0eed/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># 安装 htpasswd 工具yum install httpd-tools -y# 生成密码文件htpasswd -bc /usr/local/nginx/conf/.passwd usera pwd # 创建用户usera, 并写入 .passwdhtpasswd -b /usr/local/nginx/conf.passwd userb pwd  # 追加用户 userbhtpasswd -D /usr/local/nginx/conf/.passwd usera # 删除用户# nginx配置server &#123;    listen       80;    server_name  xxx.com;    index index.html;    location /auth &#123;        auth_basic &quot;nginx auth&quot;;        auth_basic_user_file /usr/local/nginx/conf/.passwd;        alias /export/webapps/xxx.com;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞软raid创建</title>
      <link href="posts/a7611bc1/"/>
      <url>posts/a7611bc1/</url>
      
        <content type="html"><![CDATA[<h2 id="linux-☞-软-raid-创建">linux ☞ 软 raid 创建</h2><ul><li>创建</li></ul><pre><code class="language-bash"># 磁盘分区fdisk /dev/sdafdisk /dev/sdb# 构建raid0# --level raid级别# --raid-devices 盘数# --chunk 条带深度，决定了数据分割的标准单位大小，数值越小，则数据越分散，性能越低(如若没有特殊优化需求，建议选默认值即可)mdadm -Cv /dev/md0 --level=0 --raid-devices=2 /dev/sda1 /dev/sdb1# 已上配置中, 也可以不分区, 直接进行 raid 构建mdadm --create --verbose /dev/md0 --level=0 --name=MY_RAID --raid-devices=number_of_volumes device_name1 device_name2# 观察和等待阵列初始化cat /proc/mdstat# 观察初始化后的阵列信息mdadm --detail /dev/md0# 格式化 （加卷标）mke2fs -t ext4 -L raid0 /dev/md0# 写入配置# 不同的操作系统 mdadm.conf 位置不同, 具体以 man mdadm.conf 为准mdadm --detail --scan | tee -a /etc/mdadm.conf# echo &quot;DEVICE /dev/sda1 /dev/sdb1 &quot; &gt;&gt; /etc/mdadm/mdadm.conf# mdadm -Ds &gt;&gt; /etc/mdadm/mdadm.conf# 创建新的 Ramdisk Image 以为新的 RAID 配置正确地预加载块储存设备模块sudo dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)# 写入挂载 （用卷标挂载，有些系统重启后，设备名会从md0变成md127）echo &quot;LABEL=raid0 /data ext4 defaults,nofail 0 2&quot; &gt;&gt; /etc/fstabmkdir /datamount -a# 确认挂载成功df -h</code></pre><ul><li>删除</li></ul><pre><code class="language-bash"># 删除/etc/fstab的挂载信息$ mdadm -S /dev/md0$ mdadm --misc --zero-superblock /dev/sda$ mdadm --misc --zero-superblock /dev/sdb# 删除/etc/mdadm/mdadm.conf文件中添加的DEVICE行和ARRAY行</code></pre><ul><li>额外信息</li></ul><pre><code class="language-bash">#2T以上大小分区parted /dev/sda mklabel gpt mkpart primary1 0% 100%partprobe</code></pre><ul><li>关于云</li></ul><pre><code class="language-bash">当使用云端磁盘构建raid的时候,且又想进行 raid 备份,则务必先停止io操作,停止io操作的方法最好是 umount 或者停机. 否则会导致 raid 数据完整性出现问题.</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> raid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞s3 初始化安装</title>
      <link href="posts/2c5a64ff/"/>
      <url>posts/2c5a64ff/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>脚本的目的：</p><ol><li>创建S3</li><li>添加生命周期</li><li>创建iam规则</li></ol><blockquote><p>s3官方授权示例：<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/example-policies-s3.html">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/example-policies-s3.html</a></p></blockquote><h3 id="主体脚本">主体脚本</h3><pre><code class="language-bash">#!/bin/bash# http://docs.amazonaws.cn/general/latest/gr/rande.html#s3_region# us-east-1 us-west-1 等# 需要s3权限和iam权限# 需要先编写 s3-lifecycle.json#桶名read -p &quot;输入s3桶名=&quot; S3BucketName#所属项目read -p &quot;输入项目名=&quot; TeamNameread -p &quot;输入AWS_ACCESS_KEY_ID=&quot; AWS_ACCESS_KEY_IDread -p &quot;输入AWS_SECRET_ACCESS_KEY=&quot; AWS_SECRET_ACCESS_KEYread -p &quot;输入AWS_DEFAULT_REGION=&quot; AWS_DEFAULT_REGIONexport AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_IDexport AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEYexport AWS_DEFAULT_REGION=$AWS_DEFAULT_REGIONS3LocationConstraint=$&#123;AWS_DEFAULT_REGION&#125;[[ $&#123;S3LocationConstraint&#125; == 'us-east-1' ]] &amp;&amp; aws s3api create-bucket --bucket $&#123;S3BucketName&#125; --region $&#123;S3LocationConstraint&#125; || aws s3api create-bucket --bucket $&#123;S3BucketName&#125; --region $&#123;S3LocationConstraint&#125; --create-bucket-configuration LocationConstraint=$&#123;S3LocationConstraint&#125;aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'conf/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'data/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'backup/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/7days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/15days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/30days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/60days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/90days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/longlasting/'aws s3api put-bucket-tagging --bucket $&#123;S3BucketName&#125; --tagging &quot;TagSet=[&#123;Key=Team,Value=$&#123;TeamName&#125;&#125;]&quot;aws s3api put-bucket-lifecycle-configuration --bucket $&#123;S3BucketName&#125; --lifecycle-configuration file://s3-lifecycle.jsoncat &gt; s3-program.rule &lt;&lt; EOF&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:GetObject&quot;,                &quot;s3:PutObject&quot;,                &quot;s3:DeleteObject&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/7days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/15days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/30days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/60days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/90days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/longlasting/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/conf/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/data/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/backup/*&quot;            ]        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListBucket&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;        &#125;    ]&#125;EOFcat &gt; s3-local.rule &lt;&lt; EOF&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListBucket&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:PutObject&quot;,                &quot;s3:GetObject&quot;,                &quot;s3:DeleteObject&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/7days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/15days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/30days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/60days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/90days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/longlasting/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/conf/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/data/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/backup/*&quot;            ],            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor2&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:GetBucketLocation&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor3&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListAllMyBuckets&quot;,            &quot;Resource&quot;: &quot;*&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;    ]&#125;EOFaws iam create-policy --policy-name s3-$&#123;S3BucketName&#125;-role --description &quot;For role use only!!!!!!!!!!!!&quot; --policy-document  file://s3-program.ruleaws iam create-policy --policy-name s3-$&#123;S3BucketName&#125;-local --description &quot;Limit the source IP!!!!!!!!!!!!&quot; --policy-document file://s3-local.ruleecho &quot;程序用户规则: s3-$&#123;S3BucketName&#125;-role 已生成&quot;echo &quot;本地用户规则：s3-$&#123;S3BucketName&#125;-local 已生成&quot;echo &quot;请将 local 规则关联到对应个人用户或组&quot;echo &quot;请将 role 规则关联到角色&quot;</code></pre><h3 id="生命周期规则-s3-lifecycle-json">生命周期规则 s3-lifecycle.json</h3><blockquote><p>规则说明：</p><ol><li>所有对象，30天之后转为ONEZONE_IA;</li><li>logs 前缀单独定义：<ul><li>days 路径下的对象保存对应的天数</li><li>longlasting 路径下的对象永久保存，但是 90 天之后的对象转换为 GLACIER</li></ul></li></ol></blockquote><pre><code class="language-shell">&#123;  &quot;Rules&quot;: [      &#123;          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;&quot;          &#125;,           &quot;Status&quot;: &quot;Enabled&quot;,           &quot;Transitions&quot;: [              &#123;                  &quot;Days&quot;: 30,                   &quot;StorageClass&quot;: &quot;ONEZONE_IA&quot;              &#125;          ],           &quot;NoncurrentVersionTransitions&quot;: [              &#123;                  &quot;NoncurrentDays&quot;: 30,                   &quot;StorageClass&quot;: &quot;ONEZONE_IA&quot;              &#125;          ],           &quot;ID&quot;: &quot;30days_onezone_ia&quot;      &#125;,      &#123;          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/longlasting/&quot;          &#125;,           &quot;Status&quot;: &quot;Enabled&quot;,           &quot;Transitions&quot;: [              &#123;                  &quot;Days&quot;: 90,                   &quot;StorageClass&quot;: &quot;GLACIER&quot;              &#125;          ],           &quot;ID&quot;: &quot;logs_90day_glacier&quot;      &#125;,       &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/7days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 7          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_7days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;:  1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/15days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 15          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_15days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;:  1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/30days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 30          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_30days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/60days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 60          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_60days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/90days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 90          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_90days_before&quot;      &#125;  ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> s3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker☞01安装</title>
      <link href="posts/b7cf7b1f/"/>
      <url>posts/b7cf7b1f/</url>
      
        <content type="html"><![CDATA[<h3 id="依赖">依赖</h3><pre><code class="language-bash">yum install -y yum-utils \  device-mapper-persistent-data \  lvm2</code></pre><h3 id="仓库">仓库</h3><pre><code class="language-bash:">yum-config-manager \    --add-repo \    https://download.docker.com/linux/centos/docker-ce.repo</code></pre><p>💁如果你用的是阿里云的镜像，那么则需要手动修改repo文件地址，例如：<a href="https://download.docker.com/linux/centos/7/x86_64/stable/">https://download.docker.com/linux/centos/7/x86_64/stable/</a></p><h3 id="安装">安装</h3><pre><code class="language-bash">yum install docker-ce -y### 安装 compose ， 可选yum install python3-pip -ypip3 install docker-compose 或者 pip install docker-compose### 开机自启动systemctl enable docker</code></pre><h3 id="修改-docker-默认数据目录">修改 docker 默认数据目录</h3><pre><code class="language-bash">[[ -f /etc/docker/daemon.json ]] &amp;&amp; mv /etc/docker/daemon.json /etc/docker/daemon.json.default || &#123; mkdir -p /etc/docker/ &amp;&amp;  touch /etc/docker/daemon.json &#125;cat &gt;  /etc/docker/daemon.json &lt;&lt; EOF&#123;  &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;],  &quot;data-root&quot;: &quot;/export/docker-data-root&quot;&#125;EOF</code></pre><h3 id="启动">启动</h3><pre><code class="language-bash">systemctl start docker</code></pre><h3 id="镜像库">镜像库</h3><pre><code class="language-bash">docker hubquay.io</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞01常见术语概念</title>
      <link href="posts/aaf9b7/"/>
      <url>posts/aaf9b7/</url>
      
        <content type="html"><![CDATA[<h2 id="物理">物理</h2><ol><li><p>master 是 k8s 的主节点，负责协调集群中的所有活动，例如调度应用程序、维护应用程序的所需状态、扩展应用程序和滚动更新。</p></li><li><p>node 是 k8s 的工作节点，也就是实际跑容器的节点。</p></li></ol><h2 id="服务">服务</h2><p>master 节点服务</p><ol><li>kube-apiserver 是 master 节点服务， 是k8s 的 api 服务，是集群入口，提供 http rset 服务。</li><li>kube-controller-manager 是 master 节点服务，维护集群状态（故障检测/滚动升级/node扩展等），是集群所有资源对象的自动化中心。</li><li>kube-scheduler 是负责 pod 的调度, 即如何将 pod 分发到 node</li><li>etcd 注册中心，保存集群状态。</li></ol><p>node 节点服务</p><ol><li><p>kubelet 是节点代理程序，部署在 node 上。它是k8s master和k8s node之间的纽带。处理 pod 创建/启动/监控/重启/销毁等工作。</p><p>开启 register-node = true 的情况下 会自动向 apiserver 服务注册自己。</p></li><li><p>kube-proxy 是实现node节点上各资源的通信</p></li></ol><h2 id="资源">资源</h2><ol><li><p>Namespace</p><p>命名空间是一个逻辑概念，起到了隔离作用。通过它，你可以找到它内部的其它资源对象，例如 pod，svc，deployment等</p></li><li><p>Pod</p><p>pod 内包含多个容器，所以多个容器共享以下资源。</p><ul><li>PID命名空间: pod内的进程能互相看到PID</li><li>网络命名空间: pod中的多个容器共享一个ip (唯一)</li><li>IPC命名空间: pod中的多个容器之间可以互相通信</li><li>UTS命名空间: pod中的多个容器共享一个主机名 (唯一)</li><li>存储卷: pod多个容器可以共同访问pod定义的存储卷</li></ul></li><li><p>Label</p><p>label用于表示资源，从而方便的被其它资源找到。它很重要。</p><p>标签定义 <code>key: value</code></p><p>选择器: <code>key &lt;= !=&gt; value</code>  <code>key &lt;not&gt; in (value1, value2)</code></p><p>RC 和 Service 通过 selector 选择器来选择 Pod 对象范围, 从而精细化的将 Pod 进行分组。</p></li><li><p>Deployment</p><p>无状态副本集。用来部署一个 pod 组，它通过 ReplicaSet 来进行缩放， 并需要 pod 模板创建 pod， 需要 Label 监控 pod。Deployment 通过 RC 严格执行配置所定义的pod副本数量。可以比喻为 aws 的目标组或者nginx的upstream组。</p></li><li><p>Satefulset</p><p>有状态副本集。</p></li><li><p>Service</p><p>svc 用来构建一个负载配置。可以比喻为aws的elb内网负载均衡器或者nginx的service proxy配置。</p><p>svc 通过 pod 定义的 label 发现一组 pod。这些 pod 本身有 endpoint 地址。</p><p>svc 创建后，会拿到一个集群内部ip和dns。kube-proxy 服务会将 svc 的 ip 和 pod 的 endpoint 地址关联起来。</p><p>最终，其它容器可以通过这个ip和dns来访问svc关联的pod资源。</p></li><li><p>Ingress</p><p>映射一个端口到宿主ip，提供服务的外部访问</p></li></ol><h2 id="服务组件流程图">服务组件流程图</h2><blockquote><ul><li>apiserver 负责 etcd 存储的所有操作，且只有 apiserver 才直接操作 etcd 集群</li><li>apiserver 对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件均通过 apiserver 进行通信<ul><li>controller manager、scheduler、kube-proxy 和 kubelet 等均通过 apiserver watch API 监测资源变化情况，并对资源作相应的操作</li><li>所有需要更新资源状态的操作均通过 apiserver 的 REST API 进行</li></ul></li><li>apiserver 也会直接调用 kubelet API（如 logs, exec, attach 等），默认不校验 kubelet 证书，但可以通过 <code>--kubelet-certificate-authority</code> 开启（而 GKE 通过 SSH 隧道保护它们之间的通信）</li></ul></blockquote><p><img src="/posts/aaf9b7/image-20200823102314477.png" alt="image-20200823102314477"></p><p><img src="/posts/aaf9b7/image-20200823103640732.png" alt="image-20200823103640732"></p><blockquote><p>以一个pod的构建为例:</p><ul><li>用户通过 REST API 创建一个 Pod</li><li>apiserver 将其写入 etcd</li><li>scheduluer 检测到未绑定 Node 的 Pod，开始调度并更新 Pod 的 Node 绑定</li><li>kubelet 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod</li><li>kubelet 通过 container runtime 取到 Pod 状态，并更新到 apiserver 中</li></ul></blockquote><p><img src="/posts/aaf9b7/image-20200823102430144.png" alt="image-20200823102430144"></p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞安装.</title>
      <link href="posts/b824edd8/"/>
      <url>posts/b824edd8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://hub.docker.com/_/mysql?tab=description">https://hub.docker.com/_/mysql?tab=description</a></p><pre><code class="language-bash">mkdir -p /export/docker-data-mysql/&#123;config,logs,mysql,mysql-files&#125;docker run --name mysql8 \-p 3306:3306 \--restart=always \--mount 'type=bind,src=/export/docker-data-mysql/config,dst=/etc/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/logs,dst=/var/log/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/mysql,dst=/var/lib/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/mysql-files,dst=/var/lib/mysql-files' \-e MYSQL_ROOT_PASSWORD=123456 \-d mysql:8 \--character-set-server=utf8mb4 \--collation-server=utf8mb4_general_ci</code></pre><blockquote><p>-d mysql:5.6</p></blockquote><p>开启远程访问</p><pre><code class="language-sql">alter user 'root'@'%' identified with mysql_native_password by '123456';</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞rds-mysql参数</title>
      <link href="posts/438aa722/"/>
      <url>posts/438aa722/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># 下列是非默认值character_set_client: utf8character_set_connection: utf8character_set_database: utf8character_set_filesystem: utf8character_set_results: utf8character_set_server: utf8collation_connection: utf8_general_cislow_query_log: 1long_query_time: 10log_output: FILE# 下列是默认值explicit_defaults_for_timestamp: '1'innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'innodb_file_per_table: '1'innodb_flush_method: O_DIRECTbinlog_cache_size: '32768'binlog_format: MIXED</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞EC2国内外之间迁移</title>
      <link href="posts/2e304ad6/"/>
      <url>posts/2e304ad6/</url>
      
        <content type="html"><![CDATA[<ul><li><p>aws 国内和国外之间无法直接复制 AMI，所以经咨询 aws 技术人员，就有了这么一道手工饭。。。</p></li><li><p>命令步骤如下：</p><pre><code class="language-shell">#源机器上：dd if=/dev/源根盘 of=/非根挂载点/文件A bs=1M#aws EC服务器上：dd if=/文件A of=/dev/盘B bs=1M oflag=direct#盘B打快照，再根据盘B快照生成AMI</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ec2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞利用cf服务初始化aws环境</title>
      <link href="posts/ddc4c969/"/>
      <url>posts/ddc4c969/</url>
      
        <content type="html"><![CDATA[<blockquote><p>因公司小项目多，账户多，每次构建aws初始化环境均很麻烦，且每个人操作的时候，想法都不一样，规则不统一，导致环境信息无法通用.</p><p>所以准备使用 cf 来构建一个通用模板，保持环境一致性.</p><p>之前一直没有使用，是因为 cf 本身编写不是太方便，然而我低估了 aws 的牛逼，仔细看了文档才知道， 有个 cf 本身有个堆栈，可以直接复刻当前环境.</p><p>这个工具就是 <code>cloudformer</code></p><p>官方文档是: <a href="https://docs.aws.amazon.com/zh_cn/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html#launch-cloudformer">https://docs.aws.amazon.com/zh_cn/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html#launch-cloudformer</a></p></blockquote><h2 id="那么步骤来了">那么步骤来了~</h2><ul><li><p>先通过 web 控制台生成一个适合本地化的通用环境，比如 vpc，子网，安全组，数据库子网组，参数组，s3以及相应的s3生命周期规则等等</p></li><li><p>构建 cloudformer 堆栈，并通过 cloudformer 服务生成当前环境模板</p></li><li><p>将当前环境模板的一些非通用内容，换成变量，并构建成 cf 参数或者脚本</p></li><li><p>最后通过执行脚本，并输入变量值，来生成新模板</p></li><li><p>在新环境里调用新模板来初始化环境</p></li></ul><h2 id="下面是我贴出的一个基础环境生成脚本">下面是我贴出的一个基础环境生成脚本</h2><ul><li><p>vpc: x.x.0.0/16</p></li><li><p>prod: x.x.128.0/17</p><ul><li>prod 子网默认有四个, 分别是 EC2 两个可用区, Database 两个可用区</li></ul></li><li><p>qa: x.x.0.0/19</p><ul><li>qa 子网默认有四个, 分别是 EC2 两个可用区, Database 两个可用区</li></ul></li><li><p>prod 和 qa 通过安全组来分割</p><ul><li><p>prod 规则示例如下(qa一样):</p></li><li><p>Prod-EC2-common:</p><table><thead><tr><th>规则名</th><th>协议</th><th>开放端口</th><th>源ip或者其它安全组ID</th><th></th></tr></thead><tbody><tr><td>所有流量</td><td>全部</td><td>全部</td><td>10.130.128.0/17</td><td></td></tr></tbody></table></li></ul><p>| SSH      | TCP  | 22       | ${src_allow_ssh_ip}  |      |</p><ul><li><p>Prod-Database-common:</p><table><thead><tr><th>规则名</th><th>协议</th><th>开放端口</th><th>源ip或者其它安全组ID</th><th></th></tr></thead><tbody><tr><td>自定义 TCP 规则</td><td>TCP</td><td>1025 - 65535</td><td>${src_allow_rds_ip}</td><td></td></tr><tr><td>自定义 TCP 规则</td><td>TCP</td><td>1025 - 65535</td><td>Prod-EC2-common-Group-ID</td><td></td></tr></tbody></table></li></ul></li><li><p>SNS 默认预警主题</p></li><li><p>mysql 子网组</p></li><li><p>redis 子网组</p></li><li><p>s3以及初始化生命周期规则</p></li><li><p>vpc 路由表不会添加默认路由, 还请自行添加(Internet 网关会自动生成)</p></li></ul><pre><code class="language-bash">#!/bin/bash# 本脚本用来生成一份基础环境的 cf 模板# 网络A和B段，例如 10.10 那么 vpc 就是 10.10.0.0/16VpcCidrBlockAB=# 大区区域, 例如 新加坡 那么区域就写 ap-southeast-1Region=# 项目名，用于区分资源以及添加 Team 标签 (只能是字母数字组合)TagTeamValue=# 允许访问EC2的22号端口的ip (指的是非vpc内网)，通常应该是堡垒机或者公司外网src_allow_ssh_ip=# 允许访问数据库端口的ip (指的是非vpc内网)，通常应该是堡垒机或者公司外网src_allow_rds_ip=# 默认 SNS 服务主体名 SNSThemeNameSNSThemeName=$&#123;TagTeamValue&#125;# 默认 SNS 服务信息接收用户邮箱, cf 模板执行完后，需要用户邮箱读取邮件自行确认消息SNSEmail=# 设置 vpc 任意两个可用区 字母标识, 比如 a bAvailabilityZoneOne=aAvailabilityZoneTwo=b######## 本模板, 不会添加 vpc 默认路由, 请自行加  #########cat &gt; cf-common.yml &lt;&lt; EOFAWSTemplateFormatVersion: 2010-09-09Resources:  vpc12345:    Type: 'AWS::EC2::VPC'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.0.0/16      InstanceTenancy: default      EnableDnsSupport: 'true'      EnableDnsHostnames: 'true'      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 84f85642-9390-4126-b347-95ca2ebf1c15  subnet0qa0database0$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.3.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-database-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 864ecf93-1ce6-4b4d-9b29-c3d89f07141b  subnet0prod0database0$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.131.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-database-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 7ef6daea-3bcd-4913-ab55-ba4f7bced319  subnet0qa0ec20$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.2.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-ec2-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: bc2b5a08-f7b8-4699-a7ba-5c9c6c021bd8  subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.4.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-database-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: b4738f0a-11a6-4d75-b9f0-a24072e53228  subnet0prod0ec20$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.129.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-ec2-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 2ffb24f6-49fd-4d3a-bdfe-69e90e228517  subnet0prod0ec20$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.128.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-ec2-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 9a2789fc-b7d2-4532-811b-be3455ab7d7d  subnet0qa0ec20$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.1.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-ec2-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 5c6c2638-a7b5-4185-a7a3-08d5901c252a  subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.132.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-database-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 00a78a8b-fd72-4613-bf09-8319b876ba2b  igw088b65cb430bc5ca0:    Type: 'AWS::EC2::InternetGateway'    Properties:      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-internet-gw    Metadata:      'AWS::CloudFormation::Designer':        id: e60ac13b-274d-46db-ac2a-5afaf975e1eb  dopt8128ece5:    Type: 'AWS::EC2::DHCPOptions'    Properties:      DomainName: $&#123;Region&#125;.compute.internal      DomainNameServers:        - AmazonProvidedDNS    Metadata:      'AWS::CloudFormation::Designer':        id: 909e6481-ccca-40fe-a1b1-4abe15a2ba18  acl03e81a4f65756520d:    Type: 'AWS::EC2::NetworkAcl'    Properties:      VpcId: !Ref vpc12345    Metadata:      'AWS::CloudFormation::Designer':        id: 93e06538-fe9e-49a7-8549-e2efb97dfab7  dbsubnet$&#123;TagTeamValue&#125;prod:    Type: 'AWS::RDS::DBSubnetGroup'    Properties:      DBSubnetGroupDescription: $&#123;TagTeamValue&#125;-prod      SubnetIds:        - !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 16096ba6-c5de-4a73-a083-07b2c0573362  dbsubnet$&#123;TagTeamValue&#125;qa:    Type: 'AWS::RDS::DBSubnetGroup'    Properties:      DBSubnetGroupDescription: $&#123;TagTeamValue&#125;-qa      SubnetIds:        - !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 186893f8-59ef-4d56-880f-d62294d25780  dbpg$&#123;TagTeamValue&#125;prod:    Type: 'AWS::RDS::DBParameterGroup'    Properties:      Description: $&#123;TagTeamValue&#125;-prod-mysql5.6      Family: mysql8.0      Parameters:        binlog_cache_size: '32768'        binlog_format: MIXED        character_set_client: utf8mb4        character_set_connection: utf8mb4        character_set_database: utf8mb4        character_set_filesystem: utf8mb4        character_set_results: utf8mb4        character_set_server: utf8mb4        collation_connection: utf8mb4_general_ci        explicit_defaults_for_timestamp: '1'        innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'        innodb_file_per_table: '1'        innodb_flush_method: O_DIRECT    Metadata:      'AWS::CloudFormation::Designer':        id: 87a954ed-d1b2-4626-87fe-f2515f4479b4  dbpg$&#123;TagTeamValue&#125;qa:    Type: 'AWS::RDS::DBParameterGroup'    Properties:      Description: $&#123;TagTeamValue&#125;-qa-mysql5.6      Family: mysql8.0      Parameters:        binlog_cache_size: '32768'        binlog_format: MIXED        character_set_client: utf8        character_set_connection: utf8        character_set_database: utf8        character_set_filesystem: utf8        character_set_results: utf8        character_set_server: utf8        collation_connection: utf8_general_ci        explicit_defaults_for_timestamp: '1'        innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'        innodb_file_per_table: '1'        innodb_flush_method: O_DIRECT    Metadata:      'AWS::CloudFormation::Designer':        id: 6c4c2e1e-ec99-4e7f-b254-0f09145a0d87  cachesubnet$&#123;TagTeamValue&#125;databaseprod:    Type: 'AWS::ElastiCache::SubnetGroup'    Properties:      Description: '$&#123;TagTeamValue&#125;_prod'      SubnetIds:        - !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: bdfb3d49-78d8-40df-84b6-63bf65616f5c  cachesubnet$&#123;TagTeamValue&#125;databaseqa:    Type: 'AWS::ElastiCache::SubnetGroup'    Properties:      Description: '$&#123;TagTeamValue&#125;_qa'      SubnetIds:        - !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;        - !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 70367616-0a11-4bc9-9373-a23be1690a59  topicIT:    Type: 'AWS::SNS::Topic'    Properties:      DisplayName: $&#123;SNSThemeName&#125;      Subscription:        - Endpoint: $&#123;SNSEmail&#125;          Protocol: email    Metadata:      'AWS::CloudFormation::Designer':        id: 343cc3f4-4380-47e5-850e-cc3fb560f22a  $&#123;TagTeamValue&#125;qacommon:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-qa-common      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  QA-EC2-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: d31db020-e702-41e5-8ccc-d71a5a26f565  $&#123;TagTeamValue&#125;prodcommon:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-prod-common      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  Prod-EC2-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: f54456ef-e984-4b00-b04d-64b16a2057ea  ITLinshi:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: IT-Linshi      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  I'm god    Metadata:      'AWS::CloudFormation::Designer':        id: 182b7e60-f895-4519-8845-4d32a8591340  $&#123;TagTeamValue&#125;proddatabase:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-prod-database      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  Prod-database-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: 44f0485a-2575-4eab-82b3-bc7507cd5820  $&#123;TagTeamValue&#125;qadatabase:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-qa-database      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  QA-database-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: f903102a-c394-4301-a08d-5625293ac23f  snspolicyIT:    Type: 'AWS::SNS::TopicPolicy'    Properties:      Topics:        - !Ref topicIT      PolicyDocument:        Version: 2008-10-17        Id: __default_policy_ID        Statement:          - Sid: __default_statement_ID            Effect: Allow            Principal:              AWS: '*'            Action:              - 'SNS:GetTopicAttributes'              - 'SNS:SetTopicAttributes'              - 'SNS:AddPermission'              - 'SNS:RemovePermission'              - 'SNS:DeleteTopic'              - 'SNS:Subscribe'              - 'SNS:ListSubscriptionsByTopic'              - 'SNS:Publish'              - 'SNS:Receive'            Resource: !Ref topicIT            Condition:              StringEquals:                'AWS:SourceOwner': '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 66a45d1a-3c99-4ef8-9c4e-db9ba717e17e  acl1:    Type: 'AWS::EC2::NetworkAclEntry'    Properties:      CidrBlock: 0.0.0.0/0      Egress: 'true'      Protocol: '-1'      RuleAction: allow      RuleNumber: '100'      NetworkAclId: !Ref acl03e81a4f65756520d    Metadata:      'AWS::CloudFormation::Designer':        id: de76822e-cddc-40d9-ba6f-4de20d1188ec  acl2:    Type: 'AWS::EC2::NetworkAclEntry'    Properties:      CidrBlock: 0.0.0.0/0      Protocol: '-1'      RuleAction: allow      RuleNumber: '100'      NetworkAclId: !Ref acl03e81a4f65756520d    Metadata:      'AWS::CloudFormation::Designer':        id: eb6e0bf9-3c84-4991-a037-64e2a368650b  subnetacl1:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0ec20$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 2801f8a7-901b-43b6-b4d9-9275d5639b86  subnetacl2:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0ec20$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 67641cac-39f6-4ee1-8410-60eaba963c27  subnetacl3:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0ec20$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 4b480f9d-e7b8-4034-b253-1c254fd83758  subnetacl4:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 1792de63-b1da-4c31-bab9-9925de98de6c  subnetacl5:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: e62f3e81-63f0-4f5e-b3e0-aa733ba6f323  subnetacl6:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0ec20$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 28228fcf-5649-4e5e-bf03-d636ca28bffe  subnetacl8:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 00ab0d37-3e37-427e-ac06-e94663719fcf  subnetacl9:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: ee93485d-3df4-416d-ab61-4f77c46441f1  subnetacl10:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: a4208016-e7cf-4acf-a718-104a25fef4c3  gw1:    Type: 'AWS::EC2::VPCGatewayAttachment'    Properties:      VpcId: !Ref vpc12345      InternetGatewayId: !Ref igw088b65cb430bc5ca0    Metadata:      'AWS::CloudFormation::Designer':        id: 3071b8a9-f4ab-4d61-83d8-d2792481e135  dchpassoc1:    Type: 'AWS::EC2::VPCDHCPOptionsAssociation'    Properties:      VpcId: !Ref vpc12345      DhcpOptionsId: !Ref dopt8128ece5    Metadata:      'AWS::CloudFormation::Designer':        id: 69a63971-39fa-45c9-8a17-cc730730083d  ingress1:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: '-1'      CidrIp: $&#123;VpcCidrBlockAB&#125;.0.0/19  ingress2:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: '-1'      CidrIp: $&#123;VpcCidrBlockAB&#125;.128.0/17  ingress3:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      SourceSecurityGroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      SourceSecurityGroupOwnerId: '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 8972689f-9301-4538-b493-bd1956559ecd  ingress4:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      SourceSecurityGroupId: !Ref $&#123;TagTeamValue&#125;qacommon      SourceSecurityGroupOwnerId: '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 236a8060-da9c-4be7-af29-fb6758a764a9  ingress5:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref ITLinshi      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  ingress6:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: tcp      FromPort: '22'      ToPort: '22'      CidrIp: $&#123;src_allow_ssh_ip&#125;  ingress8:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: tcp      FromPort: '22'      ToPort: '22'      CidrIp: $&#123;src_allow_ssh_ip&#125;  ingress11:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      CidrIp: $&#123;src_allow_rds_ip&#125;  ingress14:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      CidrIp: $&#123;src_allow_rds_ip&#125;  egress1:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress2:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress3:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref ITLinshi      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress4:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress5:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: '-1'      CidrIp: 0.0.0.0/0Description: ''EOF</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ec2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx ☞ 泛域名_变量截取</title>
      <link href="posts/78ceb386/"/>
      <url>posts/78ceb386/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">server &#123;    listen       80;    listen       [::]:80;    server_name ~^(?&lt;userName&gt;.*)\.apple\.com\.cn$;    root /export/webapps/apple.com/$userName;    access_log  /var/log/nginx/access.log  main;    #开启浏览器静态文件缓存    location ~ .*\.(html|htm|gif|jpg|jpeg|bmp|png|ico|txt|js|css)$ &#123;        expires      3h;    &#125;        # https://&lt;username&gt;.apple.com/api -&gt; /export/webapps/apple.com/api.php #####    location ~* ^/(api|event_api)$ &#123;        root /export/webapps/apple.com;        rewrite ^/(.*)$ /$1.php break;        fastcgi_pass     127.0.0.1:9001;        fastcgi_index    index.php;        include      fastcgi.conf;        fastcgi_connect_timeout    600s;        fastcgi_send_timeout       600s;        fastcgi_read_timeout       600s;        fastcgi_buffers 8 256k;        fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;        # https://&lt;username&gt;.apple.com/&lt;uri&gt; -&gt; /export/webapps/apple.com/&lt;username&gt;/&lt;uri&gt;.php    location ~* ^/[0-9a-zA-Z]+$ &#123;        rewrite ^/(.*)$ /$1.php break;        fastcgi_pass     127.0.0.1:9001;        fastcgi_index    index.php;        include      fastcgi.conf;        fastcgi_connect_timeout    600s;        fastcgi_send_timeout       600s;        fastcgi_read_timeout       600s;        fastcgi_buffers 8 256k;        fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;&#125;</code></pre><blockquote><p>location 优先级从上往下依次递减：<br>location =      仅匹配字符串自身<br>location ^~    匹配某个字符串开头的uri<br>location ~      正则匹配，区分大小写<br>location ~*    正则匹配，不区分大小写<br>location /      表示匹配“域名/之后的uri”，再比如localtion /images，表示匹配“域名/images之后的uri</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞安装-编译方式</title>
      <link href="posts/3321a2dc/"/>
      <url>posts/3321a2dc/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>用于编译安装php7.1</p><h4 id="目录结构">目录结构</h4><p>根目录：/usr/local/php<br>日志目录：/usr/local/php/var/log -&gt; /export/logs/php</p><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bash# 脚本, 适用于 php 7.1basedir=/usr/local/srccd $basedirrunuser=`whoami`[[ $runuser == 'root' ]] || &#123;        echo &quot;ERROR:执行用户不是$runuser&quot; &amp;&amp; exit&#125;#初始化服务器环境[[ -d /export/logs/php ]] || &#123;        echo &quot;/export/logs/php目录不存在&quot; &amp;&amp; exit&#125;yum install libmcrypt-devel ncurses-devel recode-devel aspell-devel curl-devel readline-devel openldap-devel enchant-devel pcre-devel net-snmp-devel libicu-devel libtool-ltdl-devel libjpeg-devel libpng-devel libxml2-devel bzip2-devel freetype-devel gcc-c++ mysql-develcp -p /usr/lib64/libldap* /usr/libln -s /usr/lib64/mysql /usr/lib/mysqlwget http://sg2.php.net/distributions/php-7.1.25.tar.gz -O php.tar.gzrm -rf php &amp;&amp; mkdir phptar xf php.tar.gz --strip-components 1 -C phpcd php &amp;&amp; ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-config-file-scan-dir=/usr/local/php/etc/php.d --with-curl --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-openssl --enable-mbstring --with-freetype-dir --with-jpeg-dir --with-png-dir --with-gd --with-libxml-dir --with-zlib --with-mcrypt --with-bz2 --enable-sysvshm --enable-sysvsem --enable-soap --with-recode --with-snmp --with-readline --enable-intl --enable-dba --enable-bcmath --with-enchant --with-pspell --enable-xml --enable-sockets --enable-exif --enable-inline-optimization --enable-fpm || exitmake &amp;&amp; make install || exitcp sapi/fpm/init.d.php-fpm /etc/rc.d/init.d/php-fpmchkconfig --add php-fpmchkconfig php-fpm offchmod u+x /etc/rc.d/init.d/php-fpmcp php.ini-production /usr/local/php/etc/php.inicd /usr/local/php/var &amp;&amp; rm -rf logln -s /export/logs/php logcat&gt;&gt;/usr/local/php/etc/php.ini&lt;&lt;EOF; 关闭php无用日志信息error_reporting = E_COMPILE_ERROR|E_RECOVERABLE_ERROR|E_ERROR|E_CORE_ERROR; 开启php opcache 缓存功能[opcache]zend_extension=opcache.so; 启动操作码缓存opcache.enable=1; 针对支持CLI版本PHP启动操作码缓存 一般被用来测试和调试opcache.enable_cli=0; 共享内存大小，单位为MBopcache.memory_consumption=128; 存储临时字符串缓存大小，单位为MB，PHP5.3.0以前会忽略此项配置opcache.interned_strings_buffer=8; 缓存文件数最大限制，命中率不到100%，可以试着提高这个值opcache.max_accelerated_files=4000; 一定时间内检查文件的修改时间, 这里设置检查的时间周期, 默认为 2, 单位为秒opcache.revalidate_freq=60; 开启快速停止续发事件，依赖于Zend引擎的内存管理模块，一次释放全部请求变量的内存，而不是依次释放内存块opcache.fast_shutdown=1;启用检查 PHP 脚本存在性和可读性的功能，无论文件是否已经被缓存，都会检查操作码缓存,可以提升性能。 但是如果禁用了 opcache.validate_timestamps选项， 可能存在返回过时数据的风险。opcache.enable_file_override=1EOFcat&gt;&gt;/usr/local/php/etc/php-fpm.conf&lt;&lt;EOF[global]log_level =errordaemonize = yesevents.mechanism = epollrlimit_files = 10240emergency_restart_threshold = 60emergency_restart_interval = 60s[fcgi]user = webappsgroup = webappslisten = 0.0.0.0:9000pm = staticpm.max_children = 100pm.max_requests = 1024request_slowlog_timeout = 1sslowlog = /usr/local/php/var/log/php-slow.logpm.status_path = /php-fpm_statusEOF</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab☞安装和备份</title>
      <link href="posts/d4de86f1/"/>
      <url>posts/d4de86f1/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://docs.gitlab.com/omnibus/docker/">https://docs.gitlab.com/omnibus/docker/</a></p></blockquote><h2 id="安装">安装</h2><pre><code class="language-bash"># 确保 /export 存在# createdomainName=docker run --detach   --hostname $&#123;domainName&#125;   --publish 443:443 --publish 80:80 --publish 2222:22   --name gitlab   --restart always   --volume /export/gitlab/config:/etc/gitlab   --volume /export/gitlab/logs:/var/log/gitlab   --volume /export/gitlab/data:/var/opt/gitlab   gitlab/gitlab-ce:latest# startdocker start gitlab# stopdocker stop gitlab# update (大版本升级可能失败，特别是数据库也升级的情况下)docker stop gitlabdocker rm gitlabdocker pull gitlab/gitlab-ce:latestdocker run --detach   --hostname $&#123;domainName&#125;  --publish 443:443 --publish 80:80 --publish 2222:22   --name gitlab   --restart always   --volume /export/gitlab/config:/etc/gitlab   --volume /export/gitlab/logs:/var/log/gitlab   --volume /export/gitlab/data:/var/opt/gitlab   gitlab/gitlab-ce:latest</code></pre><h2 id="基本配置">基本配置</h2><pre><code class="language-bash"># configure# https://docs.gitlab.com/omnibus/settings/README.htmlcp /export/gitlab/config/gitlab.rb /export/gitlab/config/gitlab.rb.bak</code></pre><pre><code class="language-bash">## /export/gitlab/config/gitlab.rb# 决定各个位置 url 链接内容external_url 'http://$&#123;domainName&#125;'# 决定各个位置 ssh 链接内容gitlab_rails['gitlab_shell_ssh_port'] = 2222gitlab_rails['gitlab_email_enabled'] = truegitlab_rails['gitlab_email_from'] = 'ew@xxx.com'gitlab_rails['gitlab_email_display_name'] = 'GitLab Admin'gitlab_rails['gitlab_email_reply_to'] = 'ew@xxx.com'gitlab_rails['gitlab_email_subject_suffix'] = 'GitLab'gitlab_rails['smtp_enable'] = truegitlab_rails['smtp_address'] = &quot;smtp.gmail.com&quot;gitlab_rails['smtp_port'] = 587gitlab_rails['smtp_user_name'] = &quot;ew@xxx.com&quot;gitlab_rails['smtp_password'] = &quot;&quot;gitlab_rails['smtp_domain'] = &quot;smtp.gmail.com&quot;gitlab_rails['smtp_authentication'] = &quot;login&quot;gitlab_rails['smtp_enable_starttls_auto'] = truegitlab_rails['smtp_tls'] = falsegitlab_rails['smtp_openssl_verify_mode'] = 'peer'</code></pre><h2 id="备份配置">备份配置</h2><p>备份到AWS-S3，role 授权</p><pre><code class="language-bash">## /export/gitlab/config/gitlab.rbgitlab_rails['backup_upload_connection'] = &#123;  'provider' =&gt; 'AWS',  'region' =&gt; '&lt;region-id&gt;',  #'aws_access_key_id' =&gt; 'AKIAKIAKI',  #'aws_secret_access_key' =&gt; 'secret123'  # If using an IAM Profile, don't configure aws_access_key_id &amp; aws_secret_access_key  'use_iam_profile' =&gt; true&#125;# 备份到S3上的根路径 bucket/Path gitlab_rails['backup_upload_remote_directory'] = '&lt;桶名&gt;/backup/gitlab'</code></pre><p>备份到阿里云-OSS，AK授权，暂不支持 role 授权</p><pre><code class="language-bash">## /export/gitlab/config/gitlab.rbgitlab_rails['backup_upload_connection'] = &#123;'provider' =&gt; 'aliyun','aliyun_accesskey_id' =&gt; 'AKIAKIAKI','aliyun_accesskey_secret' =&gt; 'secret123','aliyun_oss_bucket' =&gt; '&lt;桶名&gt;','aliyun_region_id' =&gt; '&lt;region-id&gt;','aliyun_oss_endpoint' =&gt; 'http://oss-cn-zhangjiakou-internal.aliyuncs.com'&#125;gitlab_rails['backup_upload_remote_directory'] = 'backup/gitlab'</code></pre><h2 id="备份计划，触发备份">备份计划，触发备份</h2><ol><li>阿里云 （role授权）</li></ol><pre><code class="language-bash"># 配置 role 到服务器上cat &gt; /etc/profile.d/ecs_role.sh &lt;&lt; EOFRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;EOF</code></pre><p>​添加脚本到crontab</p><pre><code class="language-bash">#!/bin/bashsource /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $basedirTime=`date +%Y%m%d`docker exec -t gitlab gitlab-backup createtar zcf $&#123;Time&#125;.config.tar.gz /export/gitlab/configaliyun oss cp $&#123;Time&#125;.config.tar.gz oss://桶名/backup/gitlab/ -e $&#123;Endpoint&#125; --force</code></pre><ol start="2"><li><p>aws (role授权)</p><p>添加脚本到crontab</p></li></ol><pre><code class="language-bash">#!/bin/bashbasedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $basedirTime=`date +%Y%m%d`docker exec -t gitlab gitlab-backup createtar zcf $&#123;Time&#125;.config.tar.gz /export/gitlab/configaws s3 cp $&#123;Time&#125;.config.tar.gz s3://桶名/backup/gitlab/</code></pre><h2 id="重载配置服务">重载配置服务</h2><p>docker exec -it gitlab gitlab-ctl reconfigure</p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞EC2-GPU实例安装显卡驱动</title>
      <link href="posts/2e07203a/"/>
      <url>posts/2e07203a/</url>
      
        <content type="html"><![CDATA[<blockquote><p>以下命令均在 ubuntu 用户中执行</p></blockquote><h2 id="安装-awscli-到-ubuntu-用户下">安装 awscli 到 ubuntu 用户下</h2><pre><code class="language-bash:">sudo apt-get update -ysudo apt-get install python3-pip -ypip3 install awscli --upgrade --user</code></pre><h2 id="更新系统内核和gcc，以及禁用开源驱动">更新系统内核和gcc，以及禁用开源驱动</h2><pre><code class="language-bash:">sudo apt-get upgrade -y linux-awssudo rebootsudo apt-get install -y gcc make linux-headers-$(uname -r)cat &lt;&lt; EOF | sudo tee --append /etc/modprobe.d/blacklist.confblacklist vga16fbblacklist nouveaublacklist rivafbblacklist nvidiafbblacklist rivatvEOFsudo vi /etc/default/grub     GRUB_CMDLINE_LINUX=&quot;rdblacklist=nouveau&quot;sudo update-grubsudo reboot</code></pre><h2 id="配置-aws-key-拉取-G3-系列驱动">配置 aws key 拉取 G3 系列驱动</h2><pre><code class="language-bash:">aws configure# 根据提示输入相关数据，这里 aws key 可以用任意 iam 用户的</code></pre><h2 id="下载-G3-驱动">下载 G3 驱动</h2><pre><code class="language-bash:"># G3实例直接从aws库里下载，其它需要自行去官网下载https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/install-nvidia-driver.html#obtain-nvidia-GRID-driver-linuxaws s3 cp --recursive s3://ec2-linux-nvidia-drivers/latest/ .</code></pre><h2 id="安装驱动">安装驱动</h2><pre><code class="language-bash:">sudo /bin/sh ./NVIDIA-Linux-x86_64*.run# 可能会出现 gcc 版本检测，忽略掉版本检测，直接安装sudo reboot</code></pre><h2 id="启用-GRUD-虚拟应用程序，默认启用-GRUD-虚拟工作站">启用 GRUD 虚拟应用程序，默认启用 GRUD 虚拟工作站</h2><pre><code class="language-bash:"># 具体步骤看文档https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/activate_grid.html</code></pre><h2 id="优化G3，开启全功率，关闭autoboost-（P3-实例上的-GPU-不支持-autoboost-功能。）">优化G3，开启全功率，关闭autoboost （P3 实例上的 GPU 不支持 autoboost 功能。）</h2><pre><code class="language-bash:">sudo nvidia-persistencedsudo nvidia-smi --auto-boost-default=0# P2 实例：sudo nvidia-smi -ac 2505,875# P3 实例：sudo nvidia-smi -ac 877,1530# G3 实例：sudo nvidia-smi -ac 2505,1177</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ubuntu </tag>
            
            <tag> gpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞静态配置</title>
      <link href="posts/d1d65c30/"/>
      <url>posts/d1d65c30/</url>
      
        <content type="html"><![CDATA[<blockquote><p>QPS 800-1000</p><p>2*4 机器</p></blockquote><pre><code class="language-ini">[www01]user = webappsgroup = webappslisten = /usr/local/php/var/log/php-fpm-www01.socklisten.owner = webappslisten.group = webappslisten.backlog = 10240listen.mode = 0666listen.allowed_clients = 127.0.0.1pm = staticpm.max_children = 300pm.max_requests = 1024request_slowlog_timeout = 1request_terminate_timeout = 1slowlog = /usr/local/php/var/log/php-slow.logpm.status_path = /php-fpm_status</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> php-fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>openvpn☞安装</title>
      <link href="posts/7c1abbe1/"/>
      <url>posts/7c1abbe1/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>openvpn 一般用于小公司远程连接公司办公网络环境，此脚本需要一个linux系统的主机。<br>至于土豪公司，可以无视。<br>文档包含安装脚本，创建用户脚本，删除用户脚本。使用的时候，安装脚本和创建用户脚本，需要自行修改一些变量。</p><blockquote><p>需要注意的是，发现移动网络连接其它运营商网络的时候，不稳定。<br>比如我这边，移动网络连接电信网络（openvpn所在网络），就容易丢包。</p></blockquote><h4 id="安装脚本">安装脚本</h4><pre><code class="language-bash:">yum install dockerdocker pull kylemanna/openvpnOVPN_DATA=&quot;/root/ovpn-data&quot;IP=&quot;机器外网ip或者nat后的内网ip&quot;  # 自行修改mkdir $&#123;OVPN_DATA&#125;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_genconfig -u tcp://$&#123;IP&#125;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn ovpn_initpkidocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn easyrsa build-client-full CLIENTNAME nopassdocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient CLIENTNAME &gt; $&#123;OVPN_DATA&#125;/CLIENTNAME.ovpndocker run --name openvpn -v $&#123;OVPN_DATA&#125;:/etc/openvpn -d -p 1194:1194 --privileged kylemanna/openvpn</code></pre><h4 id="创建用户脚本">创建用户脚本</h4><pre><code class="language-bash:">#!/bin/bash# 如果是nat后的内网ip，则需要修改配置文件里的ip为外网ip# bash xxx.sh &lt;用户名&gt;[[ -z $1 ]] &amp;&amp; exitCLIENTNAME=$1#read -p '输入用户名（字母组成）:' CLIENTNAMEOVPN_DATA=&quot;/root/ovpn-data&quot;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn easyrsa build-client-full $&#123;CLIENTNAME&#125; nopassdocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient $CLIENTNAME &gt; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '1a comp-lzo' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '2a tun-mtu 1500' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '3a auth-nocache' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i &quot;s/$&#123;本机ip&#125;/$&#123;外网ip&#125;/&quot; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn  # 若为nat环境，自行修改，若不是nat环境，注释掉本行mkdir -pv $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv /root/ovpn-data/pki/private/$&#123;CLIENTNAME&#125;.key $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv /root/ovpn-data/pki/issued/$&#123;CLIENTNAME&#125;.crt $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;cd $&#123;OVPN_DATA&#125;/users/tar zcf $&#123;CLIENTNAME&#125;.tar.gz $&#123;CLIENTNAME&#125;sz $&#123;CLIENTNAME&#125;.tar.gz</code></pre><h4 id="删除用户脚本">删除用户脚本</h4><pre><code class="language-bash:">#!/bin/bash# bash xxx.sh &lt;用户名&gt;[[ -z $1 ]] &amp;&amp; exitCLIENTNAME=$1#read -p '输入用户名（字母组成）:' CLIENTNAMEOVPN_DATA=&quot;/root/ovpn-data&quot;rm -rf /root/ovpn-data/pki/private/$&#123;CLIENTNAME&#125;.key*rm -rf /root/ovpn-data/pki/reqs/$&#123;CLIENTNAME&#125;.reqrm -rf /root/ovpn-data/pki/issued/$&#123;CLIENTNAME&#125;.crtsed -i &quot;/$&#123;CLIENTNAME&#125;/d&quot; /root/ovpn-data/pki/index.txtcd /root/ovpn-data/usersrm -rf $&#123;CLIENTNAME&#125;rm -f $&#123;CLIENTNAME&#125;.tar.gz</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> openvpn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞调整内核参数</title>
      <link href="posts/28361833/"/>
      <url>posts/28361833/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>redis 内存占用爆炸. 所以网上找了一个</p><pre><code class="language-python"># encoding: utf-8&quot;&quot;&quot;author: yangyi@youzan.comtime: 2018/4/26 下午4:34func: 获取数据库中没有设置ttl的 key&quot;&quot;&quot;import redisimport argparseimport timeimport sysclass ShowProcess:    &quot;&quot;&quot;    显示处理进度的类    调用该类相关函数即可实现处理进度的显示    &quot;&quot;&quot;    i = 0 # 当前的处理进度    max_steps = 0 # 总共需要处理的次数    max_arrow = 100 # 进度条的长度    # 初始化函数，需要知道总共的处理次数    def __init__(self, max_steps):        self.max_steps = max_steps        self.i = 0    # 显示函数，根据当前的处理进度i显示进度    # 效果为[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;]100.00%    def show_process(self, i = None):        if i is not None:            self.i = i        else:            self.i += 1        num_arrow = int(self.i * self.max_arrow / self.max_steps) # 计算显示多少个'&gt;'        num_line = self.max_arrow - num_arrow # 计算显示多少个'-'        percent = self.i * 100.0 / self.max_steps # 计算完成进度，格式为xx.xx%        process_bar = '[' + '&gt;' * num_arrow + ' ' * num_line + ']'\                      + '%.2f' % percent + '%' + '\r' # 带输出的字符串，'\r'表示不换行回到最左边        sys.stdout.write(process_bar) # 这两句打印字符到终端        sys.stdout.flush()    def close(self, words='done'):        print ''        print words        self.i = 0def check_ttl(redis_conn, no_ttl_file, dbindex):    start_time = time.time()    no_ttl_num = 0    keys_num = redis_conn.dbsize()    print &quot;there are &#123;num&#125; keys in db &#123;index&#125; &quot;.format(num=keys_num, index=dbindex)    process_bar = ShowProcess(keys_num)    with open(no_ttl_file, 'a') as f:        for key in redis_conn.scan_iter(count=1000):            process_bar.show_process()            if redis_conn.ttl(key) == -1:                no_ttl_num += 1    #            if no_ttl_num &lt; 1000:                f.write(key+'\n')            else:                continue    process_bar.close()    print &quot;cost time(s):&quot;, time.time() - start_time    print &quot;no ttl keys number:&quot;, no_ttl_num    print &quot;we write keys with no ttl to the file: %s&quot; % no_ttl_filedef main():    parser = argparse.ArgumentParser()    parser.add_argument('-p', type=int, dest='port', action='store', help='port of redis ')    parser.add_argument('-d', type=str, dest='db_list', action='store', default=0,                        help='ex : -d all / -d 1,2,3,4 ')    args = parser.parse_args()    port = args.port    if args.db_list == 'all':        db_list = [i for i in xrange(0, 16)]    else:        db_list = [int(i) for i in args.db_list.split(',')]    for index in db_list:        try:            pool = redis.ConnectionPool(host='127.0.0.1', port=port, db=index)            r = redis.StrictRedis(connection_pool=pool)        except redis.exceptions.ConnectionError as e:            print e        else:            no_ttl_keys_file = &quot;/tmp/&#123;port&#125;_&#123;db&#125;_no_ttl_keys.txt&quot;.format(port=port, db=index)            check_ttl(r, no_ttl_keys_file, index)if __name__ == '__main__':    main()</code></pre><ol><li>请勿在redis服务器上执行</li><li>修改脚本中 127.0.0.1 为 redis 服务器地址</li><li>脚本执行命令: <code>python nottlkey.py -d all -p 6379  </code></li><li>添加ttl命令：<code>cat no_ttl_keys.txt  |  xargs -i -t ./redis-cli -h &lt;redis_ip&gt; -n &lt;database_num&gt; expire &lt;ttl_time&gt;</code></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞调整内核参数</title>
      <link href="posts/28361833/"/>
      <url>posts/28361833/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>之前安装完redis，启动redis，总会告诉你让你默认修改一些参数，不知其原因，但每次都招办。<br>后来遇到一些redis内存使用紧张的时候，从而导致 redis-bgsave 失败，再一次想到了这个问题。<br>经过查阅资料，发现官方文档里告诉了原因，在此记录一下。如有不对之处，还请指出。</p><h4 id="我是问题">我是问题</h4><p>redis-bgsave与overcommit_memory的关系。<br>当剩余物理内存低于当前redis所用内存的时候，overcommit_memory=1的意义</p><h4 id="官方解释在此">官方解释在此</h4><blockquote><p><a href="https://redis.io/topics/faq">https://redis.io/topics/faq</a></p></blockquote><h2 id="Background-saving-fails-with-a-fork-error-under-Linux-even-if-I-have-a-lot-of-free-RAM">Background saving fails with a fork() error under Linux even if I have a lot of free RAM!</h2><p>Short answer: : <code>echo 1 &gt; /proc/sys/vm/overcommit_memory</code></p><p>And now the long one:</p><p>Redis background saving schema relies on the copy-on-write semantic of fork in modern operating systems: Redis forks (creates a child process) that is an exact copy of the parent. The child process dumps the DB on disk and finally exits. In theory the child should use as much memory as the parent being a copy, but actually thanks to the copy-on-write semantic implemented by most modern operating systems the parent and child process will <em>share</em> the common memory pages. A page will be duplicated only when it changes in the child or in the parent. Since in theory all the pages may change while the child process is saving, Linux can’t tell in advance how much memory the child will take, so if the setting is set to zero fork will fail unless there is as much free RAM as required to really duplicate all the parent memory pages, with the result that if you have a Redis dataset of 3 GB and just 2 GB of free memory it will fail.<code>overcommit_memory</code></p><p>Setting to 1 tells Linux to relax and perform the fork in a more optimistic allocation fashion, and this is indeed what you want for Redis.<code>overcommit_memory</code></p><p>A good source to understand how Linux Virtual Memory works and other alternatives for and is this classic from Red Hat Magazine, <a href="https://people.redhat.com/nhorman/papers/rhel3_vm.pdf">“Understanding Virtual Memory”</a>. You can also refer to the <a href="http://man7.org/linux/man-pages/man5/proc.5.html">proc(5)</a> man page for explanations of the available values.<code>overcommit_memory``overcommit_ratio</code></p><h4 id="翻译后的大致意思">翻译后的大致意思</h4><p>官方的FAQ，给人一种这么个意思。 如果你不设置overcommit_memory=1，那么COW机制将无法使用，所以当空余内存小于当前redis占用内存时，redis-bgsave 因为无法申请到足够的内存，将导致分配内存失败。<br>而COW机制的意思就是：父子进程公用内存页，因此只会copy变化的数据。因此，在COW机制下，redis-bgsave只需要申请到支撑变化数据的内存即可。<br>至于是否是因为启用 overcommit_memory=1 从而使得COW机制起作用，限于这是内核层面的东西</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞s3跨账户访问</title>
      <link href="posts/7cac5d21/"/>
      <url>posts/7cac5d21/</url>
      
        <content type="html"><![CDATA[<ul><li>规则如下：</li></ul><pre><code class="language-:">存储桶策略,授权其它账户的某个 iam 资源访问此存储桶arn:aws:iam::&lt;aws_account_id&gt;:&lt;type&gt;/&lt;name&gt;</code></pre><ul><li>示例配置如下：</li></ul><pre><code class="language-json:">&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Id&quot;: &quot;Policy1564021125924&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;object1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Principal&quot;: &#123;                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;role-name&gt;&quot;            &#125;,            &quot;Action&quot;: [                &quot;s3:Get*&quot;,                &quot;s3:List*&quot;            ],            &quot;Resource&quot;: &quot;arn:aws:s3:::&lt;open-bucket&gt;/&lt;open-path&gt;/*&quot;        &#125;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cloudformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞rds-mysql日志</title>
      <link href="posts/604c5952/"/>
      <url>posts/604c5952/</url>
      
        <content type="html"><![CDATA[<h2 id="常规-错误-慢-日志">常规/错误/慢/日志</h2><pre><code class="language-bash"># 修改参数组slow_query_log：要创建慢速查询日志，请设置为 1。默认值为 0。general_log(会产生大量日志)：要创建常规日志，请设置为 1。默认值为 0。</code></pre><p>long_query_time：要防止在慢速查询日志中记录快速运行的查询，请指定需要记录的最短查询执行时间值，以秒为单位。默认值为 10 秒；最小值为 0。<br>如果 log_output = FILE，则可以指定精确到微秒的浮点值, 即可记录 0.1s<br>如果 log_output = TABLE，则必须指定精确到秒的整数值。<br>系统只记录执行时间超过 long_query_time 值的查询。</p><p>log_queries_not_using_indexes(会产生大量日志)：要将所有不使用索引的查询记录到慢速查询日志，请设置为 1。默认值为 0。将记录不使用索引的查询，即使它们的执行时间小于 long_query_time 参数的值。</p><p>log_output option：您可为 log_output 参数指定下列选项之一。</p><p>TABLE（默认, 不建议, 影响数据库本身性能）– 将一般查询写入 mysql.general_log 表，将慢速查询写入 mysql.slow_log 表。</p><p>FILE(推荐,也是必须,否则无法推送到cloudwatch)– 将一般查询日志和慢速查询日志写入文件系统。日志文件每小时轮换一次。默认记录24小时.</p><p>NONE– 禁用日志记录</p><h2 id="审计日志">审计日志</h2><pre><code># 修改选项组, 添加额外选项MARIADB_AUDIT_PLUGIN </code></pre><h2 id="二进制日志">二进制日志</h2><blockquote><p>二进制日志一般我们用来恢复数据, 默认 rds 会尽量销毁二进制日志, 来保留磁盘可用空间.</p><p>二进制日志默认格式是   mixed</p></blockquote><p>配置二进制日志的方法:</p><pre><code class="language-bash"># 将二进制日志保留24小时call mysql.rds_set_configuration('binlog retention hours', 24);</code></pre><p>访问二进制日志的方法如下:</p><pre><code class="language-bash">mysqlbinlog \    --read-from-remote-server \    --host=&lt;mysql_rds_instance_address&gt; \    --port=&lt;mysql_port&gt; \    --user ReplUser \    --password \    --raw \    --result-file=/tmp/ \    binlog.00098</code></pre><blockquote><ul><li><p>指定  <code>--read-from-remote-server</code>  选项。</p></li><li><p><code>--host</code>：指定该实例所在的终端节点中的 DNS 名称。</p></li><li><p><code>--port</code>：指定该实例使用的端口。</p></li><li><p><code>--user</code>：指定已授予了复制从属实例权限的 MySQL 用户。</p></li><li><p><code>--password</code>：指定用户的密码，或忽略密码值以让实用程序提示您输入密码。</p></li><li><p>要按二进制格式下载文件，请指定  <code>--raw</code>  选项。</p></li><li><p><code>--result-file</code>：指定用于接收原始输出的本地文件。</p></li><li><p>指定一个或多个二进制日志文件的名称。要获取可用日志的列表，请使用 SQL 命令 SHOW BINARY LOGS。</p></li><li><p>要流式传输二进制日志文件，请指定  <code>--stop-never</code>  选项。</p></li></ul></blockquote><hr><h2 id="参考文档">参考文档:</h2><ol><li><a href="https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html#Appendix.MySQL.CommonDBATasks.Logs">https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html#Appendix.MySQL.CommonDBATasks.Logs</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>web ☞ 系统基本优化</title>
      <link href="posts/f41d0763/"/>
      <url>posts/f41d0763/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># 内核参数# 系统级别上限， 即整个系统所有进程单位时间可打开的文件描述符数量fs.file-max = 6553500# 三次握手请求频次net.ipv4.tcp_syn_retries = 5# 放弃回应一个TCP请求之前，需要尝试多少次net.ipv4.tcp_retries1 = 3# 三次握手应答频次net.ipv4.tcp_synack_retries = 2# 三次握手完毕， 没有数据沟通的情况下， 空连接存活时间net.ipv4.tcp_keepalive_time = 60# 探测消息发送次数net.ipv4.tcp_keepalive_probes = 3# 探测消息发送间隔时间net.ipv4.tcp_keepalive_intvl = 15net.ipv4.tcp_retries2 = 5net.ipv4.tcp_fin_timeout = 5# 系统处理不属于任何进程的TCP链接net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_max_orphans = 35000# 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。net.core.netdev_max_backlog = 10240# 对于还未获得对方确认的连接请求，可保存在队列中的最大数目net.ipv4.tcp_max_syn_backlog = 10240# 定义了系统中每一个端口最大的监听队列的长度 net.core.somaxconn=10240#最大timewait数net.ipv4.tcp_max_tw_buckets = 20000net.ipv4.ip_local_port_range=1024 65500# 开启时间戳net.ipv4.tcp_timestamps=1# 针对客户端有效，必须在开启时间戳的前提下net.ipv4.tcp_tw_reuse = 1# 开启 iptables 后， 链路追踪上限和超时时间, 若没有使用 iptables，则无效net.netfilter.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_tcp_timeout_established = 150## tcp栈内存使用， 单位是内存页， 一页=4KB#net.ipv4.tcp_mem = 524288 786432 1310720## socket读写缓冲区大小，单位是字节#net.ipv4.tcp_rmem = 4096 4096 16777216#net.ipv4.tcp_wmem = 4096 4096 16777216##最低内存和缓冲区回收倾向（此参数有一定风险）#vm.min_free_kbytes=409600#vm.vfs_cache_pressure=200</code></pre><pre><code class="language-bash">#修改/etc/security/limits.conf# 单会话级别，可打开的所有文件描述符上限* soft nofile 655350* hard nofile 655350# 单会话级别， 可打开的所有进程上限* soft nproc 10240* hard nproc 10240</code></pre><blockquote><h5 id="开启iptables后-查看当前链路表数量命令">开启iptables后, 查看当前链路表数量命令</h5><p>$ sysctl net.netfilter.nf_conntrack_count</p><p>net.netfilter.nf_conntrack_count = 601032</p><h5 id="查看连接数最高的10个IP：可以查封某个ip-或者判断是谁导致的">查看连接数最高的10个IP：可以查封某个ip, 或者判断是谁导致的</h5><p>$ awk -F’=’ ‘{c[$2]++}END{for ( i in c) print i,c[i]}’ /proc/net/nf_conntrack | head -10</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell☞if判定</title>
      <link href="posts/241ce96c/"/>
      <url>posts/241ce96c/</url>
      
        <content type="html"><![CDATA[<pre><code>-a FILE：存在则为真；否则则为假；-e FILE: 存在则为真；否则则为假；-f FILE: 存在并且为普通文件，则为真；否则为假；-d DIR: 存在并且为目录，则为真；否则为假；-L/-h FILE: 存在并且为符号链接文件，则为真；否则为假；-b: 存在并且为块设备，则为真；否则为假；-c: 存在并且为字符设备，则为真；否则为假-S: 存在并且为套接字文件，则为真；否则为假-p: 存在并且为命名管道，则为真；否则为假-s FILE: 存在并且为非空文件则为值，否则为假；-r FILE：文件可读为真，否则为假-w FILE：文件可写为真，否则为假</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞df与du数据不一致的原因</title>
      <link href="posts/a3dfc0e8/"/>
      <url>posts/a3dfc0e8/</url>
      
        <content type="html"><![CDATA[<h2 id="linux-☞-df与du数据不一致的原因">linux ☞ df与du数据不一致的原因</h2><ul><li><h3 id="问题表现：">问题表现：</h3></li></ul><p>df 数据比 du 数据小，而且是发生在删除文件之后。</p><hr><p>这个问题牵扯到 linux 系统是将如何确认和回收数据的.</p><ul><li><h3 id="空间是如何被判定占用的">空间是如何被判定占用的</h3></li></ul><p>linux文件系统判定空间是否被占用，是查看空间的 imap 是否为1，而 imap 是否为1，则取决于占用空间的文件的 inode 节点的 Links 是否为0。而每当程序调用文件且没有关闭，那么此文件的 inode 的 Links 就会加 1。</p><p>如果 Links 不为0，则 imap 就不会是 0 ，则这份空间就无法被再次调用.<br>另外，每一个文件，默认 inode  Links 是1</p><ul><li><h3 id="系统是如何删除一个文件的">系统是如何删除一个文件的</h3></li></ul><p>文件系统首先在父目录文件里找到所要删除的文件名，将此文件信息从父目录里清除掉，如若清除后，Links 为0 ，则删除 inode 节点，将 imap 就置为0，空间可以再次被利用。但是，如果父目录文件信息清除后，有程序在调用文件，则 Links 不为0，那么 inode 无法被删除， imap 也无法置为0，结果就是文件看着没了，但是空间还是没有释放。</p><hr><p>df 和 du 的最大区别就是：</p><p>df 是根据 inode 的 Links 来确认空间是否被占用，并进而统计</p><p>du 是根据目录的文件信息来确认空间是否被占用，并进而统计</p><ul><li><h3 id="如何释放被占用的空间">如何释放被占用的空间</h3></li></ul><p>从上面可知，我们只需要将被删除文件的 inode Links 降成 0 即可，也就是关闭此文件的调用程序.</p><p>如何查找调用程序？</p><pre><code class="language-bash">lsof -n | grep rm_file_name # 获取到程序 pid，并将其杀死即可</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-lookup插件</title>
      <link href="posts/25732c33/"/>
      <url>posts/25732c33/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>lookup 插件可以配合 loop 循环实现 with_x 循环</p><h2 id="lookup-查询插件">lookup 查询插件</h2><blockquote><p>query 插件与 lookup 插件一样，区别在于 query 默认情况下，启用了wantlist=true。<br>wantlist 的意思是将返回的字符串构成一个列表<br>另外，query 可以简写为q</p></blockquote><h5 id="loop-关键词">loop 关键词</h5><p>用于将一个列表进行循环，默认循环单体变量是 item. 官方用来替代 with_xxx</p><p>用来遍历 lookup 结果集</p><blockquote><p>自定义loop循环单体变量为 xxx</p><pre><code class="language-yaml">loop_control:  loop_var: xxx</code></pre><p>loop_control 还有其它控制，比如</p><ul><li>index_var: var_name 循环索引.</li><li>pause: time 循环间隔时间</li><li>label: var_name 去掉 var_name 之外的不相干信息输出</li></ul></blockquote><h2 id="常用参数示例">常用参数示例</h2><h4 id="file-参数">file 参数</h4><blockquote><p>多个文件内容合并在一起，或以字符串逗号分隔输出，或以列表形式输出</p></blockquote><h4 id="ini-参数">ini 参数</h4><blockquote><p>获取 ini 配置信息</p></blockquote><pre><code class="language-ini"># ~/test.ini[testA]a1=zhangsana2=lisi[testB]b1=wangwu</code></pre><pre><code class="language-yaml">---- hosts: localhost  gather_facts: no  remote_user: zyh  tasks:    - name: get ini      debug:        msg: &quot;&#123;&#123; q('ini', 'a1 section=testA file=~/test.ini') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;zhangsan&quot;    ]&#125;</code></pre><blockquote><p>当配置是 properties，可以追加 type=properties</p></blockquote><h4 id="dict参数">dict参数</h4><pre><code class="language-bash"># 结果集 [&#123;key: xxx, value: xxx&#125;, &#123;key: xxx, value: xxx&#125;]</code></pre><pre><code class="language-yaml">---- hosts: local  gather_facts: no  connection: local  vars:    users:      male: Bob      female: Maris  tasks:    - name: test vars 1      debug:        msg: &quot;&#123;&#123; item.key &#125;&#125;: &#123;&#123; item.value &#125;&#125;&quot;      loop: &quot;&#123;&#123; lookup('dict', users) &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=&#123;'key': u'male', 'value': u'Bob'&#125;) =&gt; &#123;    &quot;msg&quot;: &quot;male: Bob&quot;&#125;ok: [localhost] =&gt; (item=&#123;'key': u'female', 'value': u'Maris'&#125;) =&gt; &#123;    &quot;msg&quot;: &quot;female: Maris&quot;&#125;</code></pre><h4 id="subelements参数">subelements参数</h4><pre><code class="language-bash"># 写法：&#123;&#123; lookup('subelements',list,'content') &#125;&#125;# 一个由相同结构字典组成的列表list，将字典中某一个元素key（值必须是列表）与字典剩余的元素（剩余的元素作为一个整体新字典），构建笛卡尔积。从而形成 item。每一个字典拆分组合后的 item 构建结果集 items# 结果集items=[[&#123;list.0.剩余kv&#125;,  list.0.key.0], [&#123;list.0.剩余kv&#125;,  list.0.key.1], [&#123;list.1.剩余kv&#125;,  list.1.key.0], [&#123;list.1.剩余kv&#125;,  list.1.key.1], ]</code></pre><pre><code class="language-yaml">---- hosts: local  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                    age: 18                    content:                            - eating                            - sleeping                            - play ogre                  - name: Maris                    gender: female                    age: 20                    content:                            - eating                            - sleeping                            - shopping  tasks:          - name: test vars 2            debug:                    msg: &quot;&#123;&#123; item.0.name &#125;&#125; - &#123;&#123; item.0.gender &#125;&#125; - &#123;&#123; item.1 &#125;&#125;&quot;            loop: &quot;&#123;&#123; lookup('subelements',users,'content') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'eating']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - eating&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'sleeping']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - sleeping&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'play ogre']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - play ogre&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'eating']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - eating&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'sleeping']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - sleeping&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'shopping']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - shopping&quot;&#125;</code></pre><blockquote><p>若有多个key需要附加，需要 nested 与  include_tasks 的组合实现</p></blockquote><h4 id="nested参数">nested参数</h4><blockquote><p>将多个列表进行笛卡尔积运算</p><ol><li>test3.yml 中拿到 users循环单体 user</li><li>针对循环单体 user引入附加任务 test3_1.yml</li><li>通过lookup插件nested，将 user字典中各key的value相互遍历，构建新列表 item</li></ol></blockquote><pre><code class="language-yaml">################ test3.yml---- hosts: local  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                           age: 18                    content:                            - eating                               - sleeping                             - play ogre                    specialty:                            - english                              - game                       - name: Maris                            gender: female                         age: 20                    content:                            - eating                               - sleeping                             - shopping                     specialty:                            - history                              - cooking    tasks:          - include_tasks: test3_1.yml             loop: &quot;&#123;&#123; users &#125;&#125;&quot;            loop_control:                    loop_var: user ################ test3_1.yml- loop: &quot;&#123;&#123; lookup('nested',user.name, user.age, user.content, user.specialty) &#125;&#125;&quot;  debug:    msg: &quot;name:&#123;&#123; item.0 &#125;&#125;, age:&#123;&#123; item.1 &#125;&#125;, &#123;&#123; item.2 &#125;&#125;, &#123;&#123; item.3 &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=[u'Bob', 18, u'eating', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, eating, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'eating', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, eating, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'sleeping', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, sleeping, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'sleeping', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, sleeping, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'play ogre', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, play ogre, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'play ogre', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, play ogre, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'eating', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, eating, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'eating', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, eating, cooking&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'sleeping', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, sleeping, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'sleeping', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, sleeping, cooking&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'shopping', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, shopping, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'shopping', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, shopping, cooking&quot;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> lookup </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

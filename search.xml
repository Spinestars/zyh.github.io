<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>nginx☞location</title>
      <link href="posts/4c513678/"/>
      <url>posts/4c513678/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>开发有个小需求，因此需要添加一个额外的location</p><pre><code class="language-bash">location /game_log/v1/  ##新加的额外locationlocation ~* /v1/(.*)</code></pre><p>配置完后，发现不生效，查看日志，发现匹配到了第二个location。</p><h2 id="location写法">location写法</h2><p>优先级从高到低排序：</p><p>location =  /path  精确匹配<code>/path</code></p><p>location ^~ /path  匹配<code>/path</code>开头的</p><p>location ~  /path  匹配正则<code>/path</code>，区分大小写</p><p>location ~* /path  匹配正则<code>/path</code>，不区分大小写</p><p>location    /path  匹配<code>/path</code>开头的</p><p>location    /      匹配所有</p><h2 id="原因">原因</h2><ol><li>优先级</li></ol><p>因为 <code>~*</code>优先级大于<code>空</code>，因此先匹配 <code>location ~* /v1/(.*)</code></p><ol start="2"><li>正则</li></ol><p><code>/v1/(.*)</code> 并不是匹配以<code>/v1/</code>开头的，而是只要包含<code>/v1/</code>即可匹配</p><p>因<code>/game_log/v1/</code>包含<code>v1</code>，所以被 <code>location ~* /v1/(.*)</code>捕获</p><h2 id="解决">解决</h2><p>方法1:</p><p>将<code>location /game_log/v1/</code>改为<code>location ^~ /game_log/v1/</code></p><p>方法2:</p><p>将<code>location ~* /v1/(.*)</code>改为<code>location ~* ^/v1/(.*)</code></p>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞EFK简单部署-k8s</title>
      <link href="posts/95177ce7/"/>
      <url>posts/95177ce7/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch#running-efk-stack-in-production">kubernetes/cluster/addons/fluentd-elasticsearch at master · kubernetes/kubernetes (github.com)</a></p><p>EFK包含三个组件：</p><ol><li>fluentd 采集器</li><li>elasticsearch 搜索引擎</li><li>kibana 展示</li></ol><h2 id="数据过程">数据过程:</h2><p>fluentd ☞ Elasticsearch ☞ Kibana</p><h2 id="结构">结构</h2><p>k8s的集群日志收集结构，一般分为下面几种：</p><ol><li>集群模式，节点级收集器</li><li>边车容器模式，边车容器只负责将日志流式传输到stdout和stderr，节点级收集器进行收集</li><li>边车容器模式，边车容器直接从应用程序那收集日志，边车级收集器进行收集</li></ol><p>EFK采用第一种模式，这种模式目前也是官方比较推荐的。</p><p>其结构图如下：</p><p><img src="/posts/95177ce7/image-20210813111558385.png" alt="image-20210813111558385"></p><h2 id="安装步骤">安装步骤</h2><p>我们通过 helm 包管理器进行安装</p><p><a href="https://artifacthub.io/">Artifact Hub</a></p><p>⚠️需要注意的是，elasticsearch和kibana版本要保持一致</p><h3 id="elasticsearch"><a href="https://artifacthub.io/packages/helm/elastic/elasticsearch/7.14.0">elasticsearch</a></h3><p>下载chart包到本地，并解压</p><pre><code class="language-bash">mkdir kube-efk &amp;&amp; cd kube-efkhelm repo add elastic https://helm.elastic.cohelm fatch elastic/elasticsearchtar xf elasticsearch*.tgz &amp;&amp; cd elasticsearch</code></pre><p>编辑 values.yaml，自定义配置</p><p><a href="https://github.com/elastic/helm-charts/tree/master/elasticsearch#configuration">helm-charts/elasticsearch at master · elastic/helm-charts (github.com)</a></p><p>确认要安装的应用角色，默认开启了下列角色</p><p>[Node | Elasticsearch Guide <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#node-roles">7.14] | Elastic</a></p><pre><code class="language-yaml">roles:  master: &quot;true&quot;  ingest: &quot;true&quot;  data: &quot;true&quot;  remote_cluster_client: &quot;true&quot;  ml: &quot;true&quot;</code></pre><p>添加存储类，构建永久性存储</p><pre><code class="language-yaml">volumeClaimTemplate:  storageClassName: nfs-client  accessModes: [&quot;ReadWriteOnce&quot;]  resources:    requests:      storage: 30Gipersistence:  enabled: true</code></pre><p>添加污点容忍，如果有这个需求的话，例如允许调度到 dedicated=prod:NoSchedule 的节点</p><pre><code class="language-yaml">tolerations:  - key: &quot;dedicated&quot;    operator: &quot;Equal&quot;    value: &quot;prod&quot;    effect: &quot;NoSchedule&quot;</code></pre><p>开始安装</p><pre><code class="language-bash">kubectl create ns efkhelm install myes elastic/elasticsearch --version 7.14.0 --namespace efk -f values.yaml</code></pre><p>默认的配置里，es会创建三个节点</p><h3 id="fluentd-elasticsearch"><a href="https://artifacthub.io/packages/helm/kokuwa/fluentd-elasticsearch">fluentd-elasticsearch</a></h3><p>下载chart包到本地，并解压</p><pre><code class="language-bash">cd kube-efkhelm repo add kokuwa https://kokuwaio.github.io/helm-chartshelm fatch kokuwa/fluentd-elasticsearchtar xf fluentd-elasticsearch*.tgz &amp;&amp; cd fluentd-elasticsearch</code></pre><p>编辑 values.yaml，自定义配置</p><p>修改容器的数据目录（如果你不是默认路径的话）</p><pre><code class="language-yaml">hostLogDir:  varLog: /var/log  #dockerContainers: /var/lib/docker/containers  dockerContainers: /export/docker-data-root/containers  libSystemdDir: /usr/lib64</code></pre><p>修改elasticsearch的路径</p><pre><code class="language-yaml">elasticsearch:  auth:    enabled: false    user: null    password: null    existingSecret:      name: null      key: null  includeTagKey: true  setOutputHostEnvVar: true  # If setOutputHostEnvVar is false this value is ignored  #hosts: [&quot;elasticsearch-client:9200&quot;]  hosts: [&quot;elasticsearch-master.efk.svc.cluster.local:9200&quot;]</code></pre><blockquote><p>elasticsearch-master 是es创建完毕后的service_name</p></blockquote><p>添加master节点的容忍</p><pre><code class="language-yaml">tolerations:  - key: node-role.kubernetes.io/master    operator: Exists    effect: NoSchedule</code></pre><p>开始安装</p><pre><code class="language-bash">helm install myflu kokuwa/fluentd-elasticsearch -f values.yaml</code></pre><h3 id="校验-elasticsearch">校验 elasticsearch</h3><p>确认 elasticsearch 是否接收到来自 fluentd-elasticsearch 发送的数据</p><pre><code class="language-bash">kubectl run cirros-$RANDOM --image=cirros --rm -it -- /bin/shcurl elasticsearch-master.efk.svc.cluster.local:9200/_cat/indices===green open logstash-2021.07.03             YeQF97-WQZKvg6lieiXxRw 1 1  31045      0   6.7mb  3.4mbgreen open logstash-2021.07.02             qQAZg0rnSAW8FWxrNgwWhQ 1 1  42034      0   7.4mb  3.6mbgreen open logstash-2021.07.01             lJh5YdtySqukuITYrCmWcQ 1 1  30655      0   6.5mb  3.5mbgreen open logstash-2021.06.30             0vw7wc2tT9aNZJfXU2KLtw 1 1  27922      0   5.9mb  3.2mbgreen open logstash-2021.06.29             TiUBVT_MTC-mdP_fRbHoTQ 1 1  27914      0   5.9mb    3mbgreen open logstash-2021.06.28             54MQznz6Sr27Xn8JjSclsg 1 1  27921      0   5.9mb  3.1mbgreen open logstash-2021.06.27             VYcSLdq7QVO-p1rFdoZYRg 1 1  27920      0     6mb  3.1mbgreen open logstash-2021.06.26             C4HUoKhoTf67vcZJXW4H1A 1 1  44168      0   9.3mb  4.7mbgreen open logstash-2021.06.25             2TLEHI3TSiyEEALSWH3j3g 1 1  24310      0   5.4mb  2.8mbgreen open logstash-2021.06.24             Q8kF51zITB-G4xLGjveXEg 1 1  27338      0   6.1mb  2.9mb</code></pre><h3 id="kibana"><a href="https://artifacthub.io/packages/helm/elastic/kibana/7.14.0">kibana</a></h3><p>下载chart包到本地，并解压</p><pre><code class="language-bash">cd kube-efkhelm fatch elastic/kibanatar xf kibana*.tgz &amp;&amp; cd kibana</code></pre><p>编辑 values.yaml，自定义配置</p><p>修改elasticsearch地址，其实不用修改，因为这个chart和elasticsearch都是elastic出的，默认值就没问题</p><pre><code class="language-yaml">elasticsearchHosts: &quot;http://elasticsearch-master:9200&quot; </code></pre><p>修改kibana的service类型，从集群外访问</p><pre><code class="language-yaml">service:  type: LoadBalancer  loadBalancerIP: &quot;&quot;  port: 5601  nodePort: &quot;&quot;  labels: &#123;&#125;  annotations:    &#123;&#125;    # cloud.google.com/load-balancer-type: &quot;Internal&quot;    # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0    #service.beta.kubernetes.io/azure-load-balancer-internal: &quot;true&quot;    # service.beta.kubernetes.io/openstack-internal-load-balancer: &quot;true&quot;    # service.beta.kubernetes.io/cce-load-balancer-internal-vpc: &quot;true&quot;</code></pre><blockquote><p>根据你的环境修改，我这里环境用了metallb组件，用来模拟LB，所以annotations不需要添加任何注释</p></blockquote><h3 id="配置kibana">配置kibana</h3><p>添加模式分区</p><p>路径：Stack Management-&gt;Index patterns</p><p>Index pattern name: <code>logstash*</code></p><p>Time field: <code>@timestamp</code></p><p>配置一个展示k8s error错误的图标</p><p><img src="/posts/95177ce7/image-20210814122316905.png" alt="image-20210814122316905"></p><p><img src="/posts/95177ce7/image-20210814122130824.png" alt="image-20210814122130824"></p>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> k8s </tag>
            
            <tag> EFK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker☞非root用户执行程序</title>
      <link href="posts/604e367f/"/>
      <url>posts/604e367f/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>容器内以root用户运行，则就可以看到挂载点内root用户的文件，这并不安全。</p><p>但同时，容器里的用户需要与宿主机里的用户，拥有相同的ID，否则容器里的用户无法拥有足够的权限访问挂载点</p><h2 id="创建宿主机用户">创建宿主机用户</h2><p>对应容器里的用户</p><pre><code class="language-bash">groupadd --gid 60000 testuseradd --uid 60000 -g test --no-create-home test</code></pre><h2 id="容器使用方式">容器使用方式</h2><pre><code class="language-bash">FROM busybox:latestADD 1.txt /app/WORKDIR /appRUN addgroup --gid 60000 test &amp;&amp; adduser -u 60000 -G test -D -H testUSER test；；</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>harbor☞安装</title>
      <link href="posts/9573a87d/"/>
      <url>posts/9573a87d/</url>
      
        <content type="html"><![CDATA[<h2 id="硬件和依赖软件">硬件和依赖软件</h2><p><a href="https://goharbor.io/docs/2.3.0/install-config/installation-prereqs/">Harbor docs | Harbor Installation Prerequisites (goharbor.io)</a></p><h3 id="硬件依赖">硬件依赖</h3><table><thead><tr><th style="text-align:left">Resource</th><th style="text-align:left">Minimum</th><th style="text-align:left">Recommended</th></tr></thead><tbody><tr><td style="text-align:left">CPU</td><td style="text-align:left">2 CPU</td><td style="text-align:left">4 CPU</td></tr><tr><td style="text-align:left">Mem</td><td style="text-align:left">4 GB</td><td style="text-align:left">8 GB</td></tr><tr><td style="text-align:left">Disk</td><td style="text-align:left">40 GB</td><td style="text-align:left">160 GB</td></tr></tbody></table><h3 id="软件依赖">软件依赖</h3><table><thead><tr><th style="text-align:left">Software</th><th style="text-align:left">Version</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">Docker engine</td><td style="text-align:left">Version 17.06.0-ce+ or higher</td><td style="text-align:left">For installation instructions, see <a href="https://docs.docker.com/engine/installation/">Docker Engine documentation</a></td></tr><tr><td style="text-align:left">Docker Compose</td><td style="text-align:left">Version 1.18.0 or higher</td><td style="text-align:left">For installation instructions, see <a href="https://docs.docker.com/compose/install/">Docker Compose documentation</a></td></tr><tr><td style="text-align:left">Openssl</td><td style="text-align:left">Latest is preferred</td><td style="text-align:left">Used to generate certificate and keys for Harbor</td></tr></tbody></table><blockquote><p><a href="https://github.com/Spinestars/shell/blob/main/install/centos7_init.sh">shell/centos7_init.sh at main · Spinestars/shell (github.com)</a></p></blockquote><h3 id="网络依赖">网络依赖</h3><table><thead><tr><th style="text-align:left">Port</th><th style="text-align:left">Protocol</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left">443</td><td style="text-align:left">HTTPS</td><td style="text-align:left">Harbor portal and core API accept HTTPS requests on this port. You can change this port in the configuration file.</td></tr><tr><td style="text-align:left">4443</td><td style="text-align:left">HTTPS</td><td style="text-align:left">Connections to the Docker Content Trust service for Harbor. Only required if Notary is enabled. You can change this port in the configuration file.</td></tr><tr><td style="text-align:left">80</td><td style="text-align:left">HTTP</td><td style="text-align:left">Harbor portal and core API accept HTTP requests on this port. You can change this port in the configuration file.</td></tr></tbody></table><h2 id="下载-配置">下载/配置</h2><ul><li>下载在线安装包并解压</li></ul><p><a href="https://github.com/goharbor/harbor/releases">https://github.com/goharbor/harbor/releases</a></p><ul><li>修改解压后的配置文件</li></ul><pre><code class="language-bash">cp harbor.yml.tmpl harbor.ymlmkdir -p /export/docker-data-harbor/log</code></pre><p>主要的修改如下</p><blockquote><p>这里的配置并非 harbor 各组件直接调用的配置，而是 harbor 安装脚本所需的配置，安装脚本会根据这个配置，动态的生成之后 harbor 所需各项软件配置和数据</p></blockquote><pre><code class="language-yaml"># The IP address or hostname to access admin UI and registry service.# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.# &lt;域名&gt;hostname: &lt;域名&gt;# http related confighttp:  # port for http, default is 80. If https enabled, this port will redirect to https port  port: 10080# https related confighttps:  # https port for harbor, default is 443  port: 10443  # The path of cert and key files for nginx &lt;宿主机路径,不能是软链接&gt;  certificate:   private_key:   ...harbor_admin_password: &lt;web界面管理员密码&gt;# Harbor DB configurationdatabase:  # The password for the root user of Harbor DB. Change this before any production use.  password: &lt;数据库密码&gt;  # The maximum number of connections in the idle connection pool. If it &lt;=0, no idle connections are retained.  max_idle_conns: 100  # The maximum number of open connections to the database. If it &lt;= 0, then there is no limit on the number of open connections.  # Note: the default number of connections is 1024 for postgres of harbor.  max_open_conns: 900# The default data volume, default: /data  &lt;宿主机路径，所有软件的数据目录的根目录&gt;data_volume: /export/docker-data-harbor...# Log configurationslog:  # options are debug, info, warning, error, fatal  level: info  # configs for logs in local storage  local:    # Log files are rotated log_rotate_count times before being removed. If count is 0, old versions are removed rather than rotated.    rotate_count: 50    # Log files are rotated only if they grow bigger than log_rotate_size bytes. If size is followed by k, the size is assumed to be in kilobytes.    # If the M is used, the size is in megabytes, and if G is used, the size is in gigabytes. So size 100, size 100k, size 100M and size 100G    # are all valid.    rotate_size: 200M    # The directory on your host that store log,     location: /export/docker-data-harbor/log</code></pre><h3 id="https的配置">https的配置</h3><p>证书配置分为两部分，一个是nginx的，一个是docker的</p><pre><code class="language-bash">cp yourdomain.com.cert /etc/docker/certs.d/yourdomain.com/  # 如果你更换了默认端口，则复制到 /etc/docker/certs.d/yourdomain.com:port/cp yourdomain.com.key /etc/docker/certs.d/yourdomain.com/cp ca.crt /etc/docker/certs.d/yourdomain.com/</code></pre><h2 id="安装-启动-关闭">安装/启动/关闭</h2><h3 id="预安装，并启动">预安装，并启动</h3><p>生成 docker-compose 配置，并创建相关数据目录，以及容器所需的依赖数据</p><pre><code>./prepare &amp;&amp; docker-compose up -d</code></pre><h3 id="直接安装，并启动">直接安装，并启动</h3><pre><code>./install.sh</code></pre><h3 id="关闭">关闭</h3><pre><code class="language-bash">docker-compose down</code></pre><h2 id="其它">其它</h2><h3 id="证书更换">证书更换</h3><p>nginx需要需要更换 &lt;path_to_data_volume&gt;/secret/cert/ 下的文件</p><p>docker需要更换 /etc/docker/certs.d/yourdomain.com/ 下的文件</p><p>最后，重启容器服务</p><h2 id="web配置-策略">web配置-策略</h2><p>删除策略的具体需求：</p><ol><li>策略执行时，删除所有的含有 untag 镜像</li><li>策略执行时，保留最近7天含有 tag 镜像</li></ol><p><img src="/posts/9573a87d/image-20210823180632775.png" alt="image-20210823180632775"></p><p>策略的测试，可以通过【模拟运行】+运行后的【日志】来确定策略是否满足期望。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> harbor </category>
          
      </categories>
      
      
        <tags>
            
            <tag> harbor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vcsa☞存储设备有vmfs分区，但数据存储丢失</title>
      <link href="posts/eab024cf/"/>
      <url>posts/eab024cf/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>一个esxi老是断连，根据日志判断是主板硬盘接口有点问题，所以将硬盘换到了另一个主机。结果主机启动起来之后，vcsa里可以看到存储设备，就是数据存储不可访问，删除之后，再扫描也找不到。</p><h2 id="过程">过程</h2><p>开启esxi的ssh服务，进入之后，发现分区啥的都还在，看起来是没有挂载成功。</p><h3 id="检查分区">检查分区</h3><p>发现序号8就是没有挂载上的vmfs分区，且检查没有错误</p><p>voma -m vmfs -f check -d &lt;分区文件&gt;</p><pre><code>voma -m vmfs -f check -d /vmfs/devices/disks/naa.50014ee213cfd76d:8===Running VMFS Checker version 2.1 in check modeInitializing LVM metadata, Basic Checks will be doneChecking for filesystem activityPerforming filesystem liveness check..\Scanning for VMFS-6 host activity (4096 bytes/HB, 1024 HBs).         Scsi 2 reservation successfulPhase 1: Checking VMFS header and resource files   Detected VMFS-6 file system (labeled:'10-200-16-4-hdd') with UUID:60a7b864-03293672-7e51-8cdcd42142e2, Version 6:82Phase 2: Checking VMFS heartbeat regionMarking Journal addr (0, 0) in usePhase 3: Checking all file descriptors.   Found stale lock [type 10c00001 offset 7561216 v 24, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1109         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7577600 v 30, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1119         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7585792 v 87, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 999         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7602176 v 42, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1114         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7733248 v 47, hb offset 3899392         gen 77, mode 1, owner 60cb1a4b-1ed0702e-cebf-8cdcd42142e2 mtime 664         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7749632 v 49, hb offset 3899392         gen 77, mode 1, owner 60cb1a4b-1ed0702e-cebf-8cdcd42142e2 mtime 699         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7856128 v 70, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 986         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7864320 v 71, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 993         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7872512 v 72, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1004         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7880704 v 73, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1028         num 0 gblnum 0 gblgen 0 gblbrk 0]   Found stale lock [type 10c00001 offset 7888896 v 74, hb offset 3899392         gen 163, mode 1, owner 60d05066-92815c9b-65fa-8cdcd42142e2 mtime 1046         num 0 gblnum 0 gblgen 0 gblbrk 0]Phase 4: Checking pathname and connectivity.Phase 5: Checking resource reference counts.Total Errors Found:           0</code></pre><blockquote><p>输出信息里 Phase 1: 部分显示了vmfs分区的label和UUID</p></blockquote><h3 id="挂载vmfs分区，并加入开机启动">挂载vmfs分区，并加入开机启动</h3><p>esxcfg-volume -M &lt;分区UUID&gt;</p><pre><code class="language-bash">esxcfg-volume -M 60a7b864-03293672-7e51-8cdcd42142e2===Mounting volume 60a7b864-03293672-7e51-8cdcd42142e2</code></pre>]]></content>
      
      
      <categories>
          
          <category> 虚拟化 </category>
          
          <category> vmware </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vmware </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git☞常用命令</title>
      <link href="posts/f0c6f6c3/"/>
      <url>posts/f0c6f6c3/</url>
      
        <content type="html"><![CDATA[<h2 id="版本">版本</h2><pre><code class="language-bash"># 查看当前 commit idgit rev-parse HEAD# 放弃当前 commit id 之后的修改git reset --hard</code></pre><h2 id="暂存">暂存</h2><pre><code># 临时将修改暂存到堆栈列，并初始化到最后一个 commit id## 当存在多个暂存的时候，你需要自定义 messagegit stash save &lt;message&gt;# 取出堆栈列相应的暂存，并应用到暂存对应的 commit id git stash apply stash@&#123;X&#125;# 删除堆栈列相应的暂存git stash drop stash@&#123;X&#125;# 临时将修改暂存到堆栈列，添加一个默认名，并初始化到最后一个 commit id## 命名规范：stash@&#123;num&#125;: WIP on &lt;branch_name&gt; ： &lt;latest_commit_id&gt; &lt;latest_commit_message&gt;git stash# 取出堆栈列最上层的暂存，并应用到暂存对应的 commit id，同时删除暂存git stash pop</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>helm☞01入门</title>
      <link href="posts/51015304/"/>
      <url>posts/51015304/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>helm： k8s的包管理器，这里是v3版本</p><p>大致流程：Chart仓库&lt;-helm-&gt;存储到helm客户端本地-&gt;config-&gt;kubeconfig-&gt;Kube Apiserver-&gt;Release</p><p>chart仓库：存储chart程序包，不包含程序，是一个资源描述/模板 ，是一个http服务器</p><p>helm：是客户端</p><p>config：提供chart包所需的变量，构建特定的chart包配置，对应chart包里的values.yaml文件</p><p>Release：特定的chart包的实例化对象，部署于目标集群上的一个实例</p><p>因此，当chart更新后，helm可以自动滚动更新对应的Release实例</p><h2 id="部署">部署</h2><h3 id="下载Helm包">下载Helm包</h3><p><a href="https://helm.sh/zh/docs/intro/install/">Helm | 安装Helm</a></p><p><a href="https://github.com/helm/helm/releases">Releases · helm/helm (github.com)</a></p><p>Helm包是一个二进制程序，直接解压就可以用，将解压后的 helm 文件放置在 /usr/bin/ 下即可</p><p>你也可以通过脚本一键安装最新版本的helm: <code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash</code></p><h3 id></h3><h2 id="常用命令">常用命令</h2><h3 id="release命令-v3版本">release命令(v3版本)</h3><ul><li>install <code>helm install &lt;release_name&gt; &lt;chart_name&gt; &lt;flag&gt;</code> <code>helm install myelk elastic/elasticsearch --namespace elk -f values.yaml</code></li><li>delete</li><li>upgrade：更新release版本 <code>helm install &lt;release_name&gt; &lt;chart_name&gt; &lt;flag&gt;</code></li><li>rollback：回滚release版本</li><li>list</li><li>history：获取release历史版本</li><li>status：获取release状态信息</li></ul><h3 id="chart命令-v3版本">chart命令(v3版本)</h3><ul><li>create 创建一个chart，将包含一些必要的文件</li><li>fetch 下载并解压，如果你需要安装前，自定义配置，例如values.yaml</li><li>get 下载</li><li>inspect</li><li>package 打包一个chart<ul><li><code>helm package &lt;chart根目录&gt;</code></li></ul></li><li>verify 验证一个chart</li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> helm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> helm </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>helm☞02chart</title>
      <link href="posts/710e3e49/"/>
      <url>posts/710e3e49/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://helm.sh/docs/chart_template_guide/getting_started/">Helm | Getting Started</a></p><p>chart包构建</p><h2 id="创建框架">创建框架</h2><p>helm create &lt;chart包名&gt;</p><h2 id="必要文件">必要文件</h2><h3 id="Chart-yaml">Chart.yaml</h3><p>Chart.yaml 文件包含chart的描述。您可以从模板中访问它。 charts/ 目录可能包含其他chart（我们称之为subcharts）</p><h3 id="Templates目录">Templates目录</h3><p>包含以<code>.yaml</code>结尾的模板文件和以<code>.txt</code>结尾的NOTES.txt文件</p><h4 id="NOTES-txt">NOTES.txt</h4><p>Release安装过程中的输出文本</p><p>Release命令<code>helm status</code>信息</p><h4 id="模板文件">模板文件</h4><blockquote><p>go模板语法</p></blockquote><p><code>&#123;&#123; template "myapp.fullname" &#125;&#125;</code> 引用模板信息，这里的<code>myapp.fullname</code>即chart的名</p><p><code>&#123;&#123; .Values.replicaCount &#125;&#125;</code> 表示 Values.yaml 文件里的顶级 key: replicaCount</p><h2 id="验证">验证</h2><pre><code class="language-bash"># chart 根路径helm lint .</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> helm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> helm </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞17node污染和pod容忍</title>
      <link href="posts/932d1f08/"/>
      <url>posts/932d1f08/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/">污点和容忍度 | Kubernetes</a></p><p><a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint">Kubectl Reference Docs (kubernetes.io)</a></p><h2 id="规则">规则</h2><p>pod设定的容忍规则，如果匹配到了绑定在node上的污点规则，则可以被调度到污点节点上，否则不能被调度</p><blockquote><p>容忍规则和污点规则都可以添加多个</p></blockquote><h2 id="node污点命令">node污点命令</h2><pre><code class="language-bash"># 查看节点污染信息，若无污染则为 nonekubectl describe nodes | grep TaintsTaints:             &lt;none&gt;Taints:             node-role.kubernetes.io/master:NoScheduleTaints:             &lt;none&gt;</code></pre><p>上述例子中：</p><p>KEY是<code>node-role.kubernetes.io/master</code></p><p>VAL没有</p><p>TAINT_EFFECT是<code>NoSchedule</code></p><pre><code class="language-bash"># 添加污点## 标签对:污染关键词kubectl taint nodes &lt;NODE_NAME&gt; KEY_1=VAL_1:TAINT_EFFECT_1 ... KEY_N=VAL_N:TAINT_EFFECT_N# 移除污点kubectl taint nodes &lt;NODE_NAME&gt; KEY_1-</code></pre><h3 id="污染关键词-TAINT-EFFECT">污染关键词 TAINT_EFFECT</h3><p>TAINT_EFFECT 包含以下三种状态：</p><p><code>NoSchedule</code> 容忍规则没有匹配到此污点的pod绝对不会被调度拥有此污点的节点</p><blockquote><p>对于 NoSchedule，有一个例外，就是污点添加之前，pod已经在节点上了，则pod会继续运行。</p></blockquote><p><code>PreferNoSchedule</code>容忍规则没有匹配到此污点的pod尽量不会被调度到拥有此污点的节点</p><p><code>NoExecute</code> 容忍规则没有匹配到此污点的pod绝对不会被调度拥有此污点的节点，如果pod已经在污点添加之前就运行了，则会被立即驱逐</p><p>ℹ️ 特殊规则，就是容忍规则里包含了<code>tolerationSeconds</code>，则pod的容忍规则即使完全符合node的污染规则，也仅仅只能在<code>tolerationSeconds</code>宽恕期之内继续运行。</p><h2 id="pod容忍配置">pod容忍配置</h2><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  tolerations:  - key: &quot;example-key&quot;    operator: &quot;Exists&quot;    effect: &quot;NoExecute&quot;    tolerationSeconds: 3600</code></pre><p>上述配置表示，当 pod 所在节点被添加了一个<code>example-key:NoExecute</code> 污点的时候，pod 将不会被驱逐，而是可以继续存活3600秒，如果还未到3600秒，污点就被移除，则 pod 驱逐也会停止。</p><pre><code class="language-yaml">tolerations:- key: &quot;key1&quot;  operator: &quot;Equal&quot;  value: &quot;value1&quot;  effect: &quot;NoSchedule&quot;</code></pre><p>上述pod的容忍配置表示，当 pod 所在节点被添加了一个<code>key1=value1:NoSchedule</code>污点的时候，pod 将不会被驱逐。</p><h2 id="内置的污点">内置的污点</h2><p>当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：</p><ul><li><code>node.kubernetes.io/not-ready</code>：节点未准备好。这相当于节点状态 <code>Ready</code> 的值为 “<code>False</code>”。</li><li><code>node.kubernetes.io/unreachable</code>：节点控制器访问不到节点. 这相当于节点状态 <code>Ready</code> 的值为 “<code>Unknown</code>”。</li><li><code>node.kubernetes.io/memory-pressure</code>：节点存在内存压力。</li><li><code>node.kubernetes.io/disk-pressure</code>：节点存在磁盘压力。</li><li><code>node.kubernetes.io/pid-pressure</code>: 节点的 PID 压力。</li><li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li><li><code>node.kubernetes.io/unschedulable</code>: 节点不可调度。</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个 “外部” 云平台驱动， 它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li></ul><p>从上述状态来看，如果你想让某个pod在出现上述问题（例如节点网络故障）后，依然被调度在当前节点保持1小时，那么你可以添加下列容忍规则：</p><pre><code class="language-yaml">tolerations:- key: &quot;node.kubernetes.io/network-unavailable&quot;  operator: &quot;Exists&quot;  effect: &quot;NoExecute&quot;  tolerationSeconds: 3600</code></pre><h2 id="特殊情况">特殊情况</h2><p>DaemonSet 控制器自动为所有守护进程添加如下<code>标签</code>的 <code>NoSchedule</code> 容忍度以防 DaemonSet 崩溃：</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/pid-pressure</code> (1.14 或更高版本)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 或更高版本)</li><li><code>node.kubernetes.io/network-unavailable</code> (<em>只适合主机网络配置</em>)</li></ul><p>也就是说守护进程所在节点出现上述问题，从而导致自动添加污点的时候，守护进程也不会被驱逐。</p><h2 id="案例">案例</h2><h3 id="专属服务：专属节点">专属服务：专属节点</h3><p>线上服务器组专用</p><pre><code class="language-bash"># 添加污点## 仅允许拥有 dedicated=&lt;groupName&gt;:NoSchedule 容忍规则的 pod 可以被调度到此节点kubectl taint nodes k8s001 dedicated=prod:NoSchedule# 添加标签kubectl label nodes k8s001 dedicated=prod</code></pre><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    dedicated: prodspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: dedicated            operator: In            values:             - prod  tolerations:  - key: &quot;dedicated&quot;    operator: &quot;Equal&quot;    value: &quot;prod&quot;    effect: &quot;NoSchedule&quot;  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent</code></pre><p>tolerations 确保了 nginx pod 可以调度到拥有 dedicated=prod:NoSchedule 污点的节点，而 k8s001 拥有此污点。</p><p>affinity.nodeAffinity 节点亲和确保了必须调度到拥有 dedicated=prod 标签的节点，而 k8s001 拥有此标签。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞18日志收集系统</title>
      <link href="posts/3511df2/"/>
      <url>posts/3511df2/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞04redis监控</title>
      <link href="posts/978d0c1d/"/>
      <url>posts/978d0c1d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://awesome-prometheus-alerts.grep.to/rules#redis">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a></p><p><a href="https://github.com/oliver006/redis_exporter">oliver006/redis_exporter: Prometheus Exporter for Redis Metrics. Supports Redis 2.x, 3.x, 4.x, 5.x and 6.x (github.com)</a></p><p><a href="https://hub.docker.com/r/oliver006/redis_exporter/">oliver006/redis_exporter (docker.com)</a></p><h2 id="安装">安装</h2><pre><code class="language-bash">docker run -d --net promsnet --name redis_exporter -p 9121:9121 oliver006/redis_exporter -web.listen-address 0.0.0.0:9121 --redis.addr=</code></pre><blockquote><p>如果不添加 --redis.addr=，则 redis_exporter 会自动添加 redis://localhost:6379，如果 redis_exporter 部署的宿主机无法访问 redis://localhost:6379，则会导致 proms 报一个实例 down 掉。</p></blockquote><h2 id="prometheus-主配置">prometheus 主配置</h2><pre><code class="language-yaml">scrape_configs:  - job_name: 'redis'    metrics_path: /scrape    relabel_configs:    - source_labels: [__address__]      target_label: __param_target    - source_labels: [__param_target]      target_label: instance    - target_label: __address__      replacement: &lt;redis_exporter地址&gt;:9121    static_configs:    - targets:      - redis://&lt;被监控的redis地址01&gt;:6379      - redis://&lt;被监控的redis地址02&gt;:6379  - job_name: 'redis_exporter'    static_configs:      - targets:        - redis_exporter:9121</code></pre><h2 id="prometheus-告警配置">prometheus 告警配置</h2><pre><code class="language-yaml">groups:  - name: redis-alert    rules:    - alert: RedisDown      expr: redis_up == 0      for: 0m      labels:        severity: critical      annotations:        summary: Redis down (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis instance is down\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisOutOfSystemMemory      expr: redis_memory_used_bytes / redis_total_system_memory_bytes * 100 &gt; 90      for: 2m      labels:        severity: warning      annotations:        summary: Redis out of system memory (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis is running out of system memory (&gt; 90%)\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisOutOfConfiguredMaxmemory      expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 &gt; 90      for: 2m      labels:        severity: warning      annotations:        summary: Redis out of configured maxmemory (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis is running out of configured maxmemory (&gt; 90%)\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisTooManyConnections      expr: redis_connected_clients &gt; 100      for: 2m      labels:        severity: warning      annotations:        summary: Redis too many connections (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis instance has too many connections\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisNotEnoughConnections      expr: redis_connected_clients &lt; 5      for: 2m      labels:        severity: warning      annotations:        summary: Redis not enough connections (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Redis instance should have more connections (&gt; 5)\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: RedisRejectedConnections      expr: increase(redis_rejected_connections_total[1m]) &gt; 0      for: 0m      labels:        severity: critical      annotations:        summary: Redis rejected connections (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;Some connections to Redis has been rejected\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h2 id="grafana">grafana</h2><p><a href="http://www.eryajf.net/go?url=https://grafana.com/dashboards/763">https://grafana.com/dashboards/763</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞04mysql监控</title>
      <link href="posts/6178b477/"/>
      <url>posts/6178b477/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://awesome-prometheus-alerts.grep.to/rules#mysql">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a></p><p><a href="https://github.com/prometheus/mysqld_exporter">prometheus/mysqld_exporter: Exporter for MySQL server metrics (github.com)</a></p><p><a href="https://registry.hub.docker.com/r/prom/mysqld-exporter/">prom/mysqld-exporter (docker.com)</a></p><h2 id="安装">安装</h2><h3 id="添加mysql账户">添加mysql账户</h3><pre><code class="language-bash">CREATE USER 'exporter'@'&lt;局域网授信IP&gt;' IDENTIFIED BY 'exporter@123';GRANT PROCESS, REPLICATION CLIENT ON *.* TO 'exporter'@'&lt;局域网授信IP&gt;';GRANT SELECT ON performance_schema.* TO 'exporter'@'&lt;局域网授信IP&gt;';</code></pre><pre><code class="language-bash">docker run -d --net promsnet --name mysqld-exporter -p 9104:9104 --link=my_mysql_container:&lt;被监控的mysql容器名&gt;  \  -e DATA_SOURCE_NAME=&quot;exporter:exporter@123@(&lt;被监控的mysql容器名 or 被监控的mysql实例地址&gt;:3306)/&quot; prom/mysqld-exporter</code></pre><blockquote><p>–link 可选，关联被监控的容器数据库</p></blockquote><h2 id="prometheus-主配置">prometheus 主配置</h2><pre><code class="language-yaml">scrape_configs:  - job_name: 'mysql-001'    static_configs:      - targets: ['mysqld-exporter:9104'] # mysqld-exporter 采集器地址        labels:          instance: &lt;mysql_server_path&gt;:3306 # 变更采集后的标签instance为mysql实例地址</code></pre><h2 id="prometheus-告警配置">prometheus 告警配置</h2><pre><code class="language-yaml">groups:  - name: mysql-alert    rules:    - alert: MysqlDown      expr: mysql_up == 0      for: 0m      labels:        severity: critical      annotations:        summary: MySQL down (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;MySQL instance is down on &#123;&#123; $labels.instance &#125;&#125;\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: MysqlTooManyConnections(&gt;80%)      expr: avg by (instance) (rate(mysql_global_status_threads_connected[1m])) / avg by (instance) (mysql_global_variables_max_connections) * 100 &gt; 80      for: 2m      labels:        severity: warning      annotations:        summary: MySQL too many connections (&gt; 80%) (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;More than 80% of MySQL connections are in use on &#123;&#123; $labels.instance &#125;&#125;\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: MysqlRestarted      expr: mysql_global_status_uptime &lt; 60      for: 0m      labels:        severity: info      annotations:        summary: MySQL restarted (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;MySQL has just been restarted, less than one minute ago on &#123;&#123; $labels.instance &#125;&#125;.\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;    - alert: MysqlHighThreadsRunning      expr: avg by (instance) (rate(mysql_global_status_threads_running[1m])) / avg by (instance) (mysql_global_variables_max_connections) * 100 &gt; 60      for: 2m      labels:        severity: warning      annotations:        summary: MySQL high threads running (instance &#123;&#123; $labels.instance &#125;&#125;)        description: &quot;More than 60% of MySQL connections are in running state on &#123;&#123; $labels.instance &#125;&#125;\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h2 id="grafana">grafana</h2><p><a href="https://grafana.com/grafana/dashboards/7362">https://grafana.com/grafana/dashboards/7362</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞06rabbitmq监控</title>
      <link href="posts/7fd13722/"/>
      <url>posts/7fd13722/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://awesome-prometheus-alerts.grep.to/rules#rabbitmq">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a></p><p><a href="https://www.rabbitmq.com/prometheus.html">Monitoring with Prometheus &amp; Grafana — RabbitMQ</a></p><p><a href="https://grafana.com/grafana/dashboards/10991">RabbitMQ-Overview dashboard for Grafana | Grafana Labs</a></p><h2 id="安装">安装</h2><pre><code class="language-bash">rabbitmq-plugins enable rabbitmq_prometheusrabbitmqctl -q set_cluster_name test@rabbitmq</code></pre><p>15692 端口是 prometheus 采集器的暴露端口，docker方式默认采集插件已经开启。</p><p>宿主机里执行 <code>curl -s localhost:15692/metrics | head -n 3</code>确保可以返回采集器数据。</p><h2 id="prometheus-主配置">prometheus 主配置</h2><pre><code class="language-yaml">scrape_configs:  - job_name: 'rabbitmq-001'    static_configs:      - targets: ['&lt;rabbitmq地址&gt;:15692'] </code></pre><h2 id="prometheus-告警配置">prometheus 告警配置</h2><pre><code class="language-yaml">groups:  - name: mysql-alert    rules:      - alert: RabbitmqNodeDown        expr: sum(rabbitmq_build_info) &lt; 3        for: 0m        labels:          severity: critical        annotations:          summary: Rabbitmq node down (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Less than 3 nodes running in RabbitMQ cluster\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqNodeNotDistributed        expr: erlang_vm_dist_node_state &lt; 3        for: 0m        labels:          severity: critical        annotations:          summary: Rabbitmq node not distributed (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Distribution link state is not 'up'\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqInstancesDifferentVersions        expr: count(count(rabbitmq_build_info) by (rabbitmq_version)) &gt; 1        for: 1h        labels:          severity: warning        annotations:          summary: Rabbitmq instances different versions (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Running different version of Rabbitmq in the same cluster, can lead to failure.\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqMemoryHigh        expr: rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes * 100 &gt; 90        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq memory high (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A node use more than 90% of allocated RAM\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqFileDescriptorsUsage        expr: rabbitmq_process_open_fds / rabbitmq_process_max_fds * 100 &gt; 90        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq file descriptors usage (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A node use more than 90% of file descriptors\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqTooMuchUnack        expr: sum(rabbitmq_queue_messages_unacked) BY (queue) &gt; 1000        for: 1m        labels:          severity: warning        annotations:          summary: Rabbitmq too much unack (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;Too much unacknowledged messages\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqTooMuchConnections        expr: rabbitmq_connections &gt; 1000        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq too much connections (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;The total connections of a node is too high\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqNoQueueConsumer        expr: rabbitmq_queue_consumers &lt; 1        for: 1m        labels:          severity: warning        annotations:          summary: Rabbitmq no queue consumer (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A queue has less than 1 consumer\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;      - alert: RabbitmqUnroutableMessages        expr: increase(rabbitmq_channel_messages_unroutable_returned_total[1m]) &gt; 0 or increase(rabbitmq_channel_messages_unroutable_dropped_total[1m]) &gt; 0        for: 2m        labels:          severity: warning        annotations:          summary: Rabbitmq unroutable messages (instance &#123;&#123; $labels.instance &#125;&#125;)          description: &quot;A queue has unroutable messages\n  VALUE = &#123;&#123; $value &#125;&#125;\n  LABELS = &#123;&#123; $labels &#125;&#125;&quot;</code></pre><h2 id="grafana">grafana</h2><p><a href="https://grafana.com/grafana/dashboards/10991">https://grafana.com/grafana/dashboards/10991</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rabbitmq </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyuncli☞安装和基本调用</title>
      <link href="posts/6df9bb76/"/>
      <url>posts/6df9bb76/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>aliyun cli指令还不是太完善，并不是完全支持所有的阿里云产品，对标的是awscli</p><p>因此，建议时刻关注 <a href="https://github.com/aliyun/aliyun-cli/blob/master/CHANGELOG.md">aliyun-cli/CHANGELOG.md at master · aliyun/aliyun-cli (github.com)</a> 产品变化页面。</p><h1>安装</h1><pre><code class="language-bash">wget 'https://aliyuncli.alicdn.com/aliyun-cli-linux-latest-amd64.tgz'tar xf aliyun-cli-linux-latest-amd64.tgz</code></pre><p>💁近期的重要变化</p><p>3.0.40 - 3.0.42版本加入配置变量</p><p><code>ALIBABACLOUD_ACCESS_KEY_ID</code>, <code>ALICLOUD_ACCESS_KEY_ID</code></p><p><code>ALIBABACLOUD_ACCESS_KEY_SECRET</code>, <code>ALICLOUD_ACCESS_KEY_SECRET</code></p><p><code>ALIBABACLOUD_REGION_ID</code>, <code>ALICLOUD_REGION_ID</code></p><h1>配置</h1><pre><code class="language-bash"># 查看当前配置aliyun configure listProfile             | Credential         | Valid   | Region           | Language---------           | ------------------ | ------- | ---------------- | --------default             | AK:***             | Invalid |                  | enecsRamRoleProfile * | EcsRamRole:gitlab  | Valid   | cn-zhangjiakou   | en# 切换配置aliyun configure set --profile &lt;profile_name&gt;# 添加配置，例如 ecsramrole 模式# https://help.aliyun.com/document_detail/121193.html?spm=a2c4g.11186623.3.4.35af3ae51h8w8Maliyun configure set \   --profile ecsRamRoleProfile \  --mode EcsRamRole \  --ram-role-name RoleName \  --region cn-hangzhou</code></pre><h1>基本使用</h1><p>以 oss 为例：</p><pre><code class="language-bash">#对象存储访问方式变更## 添加EcsRamRole方式的配置，并切换到新配置上Region=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;## 设定 oss 的endpoint 地址，这里以内网地址为例Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;##查询## 默认查询是递归查询，-d 只查询一层aliyun oss ls oss://test/ -d -e $&#123;Endpoint&#125;##基本的上传或下载##上传文件 a.file 到 oss://test/ aliyun oss cp a.file oss://test/ -e $&#123;Endpoint&#125;##基本的递归上传##上传目录 abc 下的文件到 oss://test/ 下，如果有重复内容，则需要加入 --forcealiyun oss cp abc oss://test/ --recursive -e $&#123;Endpoint&#125;##复杂的递归上传##上传目录 abc 下的 .lzo 结尾的文件到 oss://test/ 下.##严禁在源目录里执行 --recursive 参数.##即禁止执行 aliyun oss cp . oss://test/ --recursive aliyun oss cp abc/ oss://test/ --include='*.lzo' --update --recursive -e $&#123;Endpoint&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_output##同步目录 sync 指令变更##同步目录 abc 下的文件到 oss://test/ 下，如有重复，则忽略;同时删除目标目录下本地没有的文件aliyun oss sync abc/ oss://test/ --update --delete --force -e $&#123;Endpoint&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_output</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
            <tag> cli </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>css☞背景图居中拉伸平铺</title>
      <link href="posts/fa5680b3/"/>
      <url>posts/fa5680b3/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-css">background: url('backend.jpg') center center no-repeat;background-attachment: fixed;  background-size: cover;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> css </category>
          
      </categories>
      
      
        <tags>
            
            <tag> css </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络☞链路聚合</title>
      <link href="posts/6f53a6b8/"/>
      <url>posts/6f53a6b8/</url>
      
        <content type="html"><![CDATA[<h2 id="目的">目的</h2><p>扩宽网络，突破单路</p><p>例如原本是单路千兆，扩成双路并发千兆</p><p>文档里的聚合类型均为动态聚合</p><h2 id="H3C交换机-副-其它交换机">H3C交换机(副) - 其它交换机</h2><pre><code class="language-bash"># 创建连接其它交换机的聚合接口# 聚合接口序号 1# 聚合接口设置为 trunk 模式， 并允许所有 vlan 通过# 聚合接口模式设置为动态聚合interface Bridge-Aggregation 1 port link-type trunk port trunk permit vlan all link-aggregation mode dynamic # 添加普通端口 1-4 到聚合接口# 设置普通端口为 trunk 模式，并允许所有 vlan 通过interface range GigabitEthernet1/0/1 to GigabitEthernet1/0/4 port link-type trunk port trunk permit vlan all port link-aggregation group 1</code></pre><blockquote><p>关于普通端口是否设置 link-type，很多文档里没有提及，但是我配置中，不设置网络就不通</p><p>若 H3C 连接的是终端，则将端口模式改为 access，并设置仅允许通过的 vlan</p></blockquote><h2 id="华为交换机-主-其它交换机">华为交换机(主) - 其它交换机</h2><pre><code class="language-bash"># 创建连接其它交换机的聚合接口# 聚合接口序号1# 聚合接口设置为 trunk 模式， 并允许所有 vlan 通过# 聚合接口模式设置为动态聚合interface Eth-Trunk 1 port link-type trunk port trunk allow-pass vlan 2 to 4094 mode lacp# 添加普通端口 1-4 到聚合接口# 设置普通端口的活动优先级为 100，默认端口优先级是30000+, 优先级数字越小，优先级越高interface GigabitEthernet0/0/1 eth-trunk 1 lacp priority 100interface GigabitEthernet0/0/2 eth-trunk 1 lacp priority 100interface GigabitEthernet0/0/3 eth-trunk 1 lacp priority 100interface GigabitEthernet0/0/4 eth-trunk 1 lacp priority 100</code></pre><blockquote><p>若华为连接的是终端，则将端口模式改为 access，并设置仅允许通过的 vlan</p></blockquote><h2 id="群辉">群辉</h2><p>前提：</p><p>交换机一侧已经配置好了动态聚合</p><p>控制面板-网络-网络界面-新增-创建 Bond</p><ol><li>设置聚合方式，选<code>IEEE 802.3ad 动态 Link Aggregationl</code></li><li>选择参与聚合的网络端口</li><li>设定聚合接口的ip地址</li></ol><p>保存设置后，群辉会重载。</p><p>如果不幸因配置故障，导致群辉无法访问，那么你可以通过群辉硬件上的 reset 按钮进行管理员账户和网络配置重置，需要注意的是，仅可按下4秒，在第一次听到滴声后，就立马松开。</p><h2 id="vsphere-vcenter">vsphere vcenter</h2><p>先看一个设置的拓扑，左边虚拟机16.50，默认只有一个虚拟网卡，右边lag1是一个上行聚合组</p><p><img src="/posts/6f53a6b8/image-20210527180252600.png" alt="image-20210527180252600"></p><p>登录 vsphere 网络控制台，点击<code>网络</code>，新建一个分布式虚拟交换机，期间所有属性全默认（名字你可以自己起，其它保持全默认就行）</p><p><img src="/posts/6f53a6b8/image-20210525145742419.png" alt="image-20210525145742419"></p><p>既然是一个虚拟交换机，那么肯定需要配置一个上联至物理交换机的聚合trunk链路（lag1）和下联虚拟机的聚合access链路（DportGroup-vlan2016）</p><h3 id="创建上联至物理交换机的聚合trunk链路">创建上联至物理交换机的聚合trunk链路</h3><p>选中所建立的 DSwitch - 配置 - LACP - 新建，创建虚拟交换机下的上联聚合接口lag1，lag1有4个端口，并且这4个端口对应4个宿主机网卡，负载平衡模式需要和物理交换机聚合接口设置的负载协议一致</p><p>假设这里都选择 src-dst-ip，即源和目标ip地址。</p><blockquote><p>需要注意的是，华为交换机普通型号不支持增强负载方案，也就是不支持sport和dspot负载。例如S5700EI就不支持。</p></blockquote><p>VLAN中继范围，我们选择中继所有，这个和物理交换机聚合接口设置保持一致即可。</p><p><img src="/posts/6f53a6b8/image-20210525151556021.png" alt="image-20210525151556021"></p><p>按照web页面的提示</p><p><img src="/posts/6f53a6b8/image-20210525155005563.png" alt="image-20210525155005563"></p><h3 id="创建一个下行端口组，用于关联虚拟机的网卡">创建一个下行端口组，用于关联虚拟机的网卡</h3><p>你有几个虚拟网卡，就开几个端口数，也可以多开，用不用看你自己。</p><p>VLAN ID就是指这个下行端口组允许通过的VLAN。</p><p><img src="/posts/6f53a6b8/image-20210527182054562.png" alt="image-20210527182054562"></p><h3 id="将exsi物理主机加入到DSwitch，并将DSwitch的聚合接口里的端口与exsi物理网卡绑定">将exsi物理主机加入到DSwitch，并将DSwitch的聚合接口里的端口与exsi物理网卡绑定</h3><p><img src="/posts/6f53a6b8/image-20210527175600067.png" alt="image-20210527175600067"></p><p>选中所建立的 DSwitch - 右键 - 添加和管理主机 - 添加主机</p><p>在添加和管理主机-管理物理适配器中，选择物理网卡并绑定到虚拟交换机的上联链路聚合组ige1里的虚拟端口，随你怎么绑定，反正把你的物理网卡都绑定上即可。</p><p>其余的迁移虚拟机网络之类的可以先不做。</p><p>到此，DSwitch和物理交换机之间的聚合应该已经打通了。</p><p>在物理交换机那边查看聚合接口，确保所有参与聚合的端口都是 selected 状态。</p><h3 id="将虚拟机网卡关联到虚拟交换机DSwitch的下行端口组">将虚拟机网卡关联到虚拟交换机DSwitch的下行端口组</h3><p>在本文档中，即将网络适配器关联到 DportGroup-vlan2016。</p><p>最后，在虚拟机网卡配置中，将网络适配器从VM Network改为下行端口组</p><p><img src="/posts/6f53a6b8/image-20210525160148265.png" alt="image-20210525160148265"></p><p>或者将虚拟机从 VM Network交换机迁出到 DSwitch 交换机</p><blockquote><p>需要注意的是，这种方式会将虚拟机下所有虚拟网卡全部迁移。</p></blockquote><p><img src="/posts/6f53a6b8/image-20210525160429071.png" alt="image-20210525160429071"></p><h3 id="测试虚拟机的网卡速度">测试虚拟机的网卡速度</h3><p>可以看到，已经突破了100MBps（聚合前，exsi宿主机到物理交换机之前是100MBps）</p><p><img src="/posts/6f53a6b8/image-20210527181349144.png" alt="image-20210527181349144"></p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 链路聚合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞k8s内置部署</title>
      <link href="posts/bced4977/"/>
      <url>posts/bced4977/</url>
      
        <content type="html"><![CDATA[<h2 id="部署文档">部署文档</h2><p><a href="https://prometheus-operator.dev/docs/prologue/quick-start/">https://prometheus-operator.dev/docs/prologue/quick-start/</a></p><p>如果采用的是kubeadm安装的k8s，或许会用到</p><p><a href="https://prometheus-operator.dev/docs/kube-prometheus-on-kubeadm/#kubeadm-pre-requisites">https://prometheus-operator.dev/docs/kube-prometheus-on-kubeadm/#kubeadm-pre-requisites</a></p><p>提到的信息。</p><h2 id="架构图">架构图</h2><p><img src="https://www.qikqiak.com/img/posts/prometheus-operator.png" alt="promtheus opeator"></p><p>这里的 servicemonitor 资源对象很关键</p><h2 id="监控的东西">监控的东西</h2><ul><li>cluster state via kube-state-metrics</li><li>nodes via the node_exporter</li><li>kubelets</li><li>apiserver</li><li>kube-scheduler</li><li>kube-controller-manager</li></ul><h2 id="基本步骤">基本步骤</h2><h3 id="拉取代码">拉取代码</h3><pre><code class="language-shell">git clone https://github.com/prometheus-operator/kube-prometheus.git</code></pre><h3 id="部署到k8s">部署到k8s</h3><p>ℹ️资源会部署在monitoring命名空间中</p><pre><code class="language-shell">kubectl create -f manifests/setup# 等待上述命令资源跑完kubectl create -f manifests/# 等待所有 pod 创建完毕kubectl get pod -n monitoring</code></pre><h3 id="添加ingress配置">添加ingress配置</h3><p>ℹ️需先部署完 ingress，例如</p><pre><code class="language-bash">kubectl get svc -n ingress-nginx===NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                      AGEingress-nginx-controller             LoadBalancer   10.96.210.139   10.200.16.11   80:32489/TCP,443:30936/TCP   88mingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;         443/TCP                      88m</code></pre><p>上述 ingress-nginx-controller 已经分到了 EXTERNAL-IP：10.200.16.11</p><p>部署下面的 ingress 配置</p><pre><code class="language-yaml">kind: IngressapiVersion: networking.k8s.io/v1metadata:  name: prometheus-ingress  namespace: monitoring  annotations:    kubernetes.io/ingress.class: &quot;nginx&quot;spec:  rules:  - host: grafana.it.local    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: grafana            port:              number: 3000  - host: proms.it.local    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: prometheus-k8s            port:              number: 9090  - host: alert.it.local    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: alertmanager-main            port:              number: 9093    </code></pre><p>解析 grafana.it.local 和 proms.it.local 到 svc 对象 ingress-nginx-controller 关联的 EXTERNAL-IP.</p><p>最后通过 <a href="http://grafana.it.local">http://grafana.it.local</a> 和 <a href="http://proms.it.local">http://proms.it.local</a> 访问</p><p>其中 grafana 的默认账户密码都是 admin，效果如图：</p><p><img src="/posts/bced4977/image-20210319171647029.png" alt="image-20210319171647029"></p><p>其中 prometheus 的效果如图：</p><p><img src="/posts/bced4977/image-20210319171728505.png" alt="image-20210319171728505"></p><h3 id="添加告警">添加告警</h3><p>配置相关可以在 kube-prometheus/manifests/alertmanager-secret.yaml 中找到</p><pre><code class="language-bash">apiVersion: v1kind: Secretmetadata:  labels:    alertmanager: main    app.kubernetes.io/component: alert-router    app.kubernetes.io/name: alertmanager    app.kubernetes.io/part-of: kube-prometheus    app.kubernetes.io/version: 0.21.0  name: alertmanager-main  namespace: monitoringstringData:  alertmanager.yaml: |-    global:      resolve_timeout: 5m      http_config: &#123;&#125;      smtp_hello: localhost      smtp_require_tls: true      pagerduty_url: https://events.pagerduty.com/v2/enqueue      opsgenie_api_url: https://api.opsgenie.com/      wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/      victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/    route:      receiver: Default      group_by:      - namespace      routes:      - receiver: Watchdog        match:          alertname: Watchdog      - receiver: Critical        match:          severity: critical      group_wait: 30s      group_interval: 5m      repeat_interval: 12h    inhibit_rules:    - source_match:        severity: critical      target_match_re:        severity: warning|info      equal:      - namespace      - alertname    - source_match:        severity: warning      target_match_re:        severity: info      equal:      - namespace      - alertname    receivers:    - name: Default      webhook_configs:      - url: &quot;...&quot;    - name: Critical      webhook_configs:      - url: &quot;...&quot;    - name: Watchdog    templates: []type: Opaque</code></pre><p>如上命令所示，添加 receivers ，这里均采用 webhook 方式</p><p>部署新配置，并reload alertmanager</p><pre><code class="language-bash">kubectl apply -f alertmanager-secret.yaml curl -X POST http://&lt;alertmanager_addr&gt;/-/reload</code></pre><h3 id="修改默认的prometheus规则">修改默认的prometheus规则</h3><p>默认的规则位于 prometheusrule 资源对象中</p><pre><code class="language-bash">kubectl get prometheusrule -n monitoringNAME                              AGEalertmanager-main-rules           6d22hkube-prometheus-rules             6d22hkube-state-metrics-rules          6d22hkubernetes-monitoring-rules       6d22hnode-exporter-rules               6d22hprometheus-k8s-prometheus-rules   6d22hprometheus-operator-rules         6d22h</code></pre><p>通过 kubectl edit 修改即可</p><h3 id="修正问题">修正问题</h3><p>prometheus页面中可能会看到有一些错误，两个核心组件kube-controller-manager和kube-scheduler是down</p><p><img src="/posts/bced4977/image-20210319174713804.png" alt="image-20210319174713804"></p><p>其原因在于，prometheus-operator的ServiceMonitor资源对象指定的svc不存在</p><pre><code class="language-bash">[root@k8s01 my-yaml]# kubectl get servicemonitor -n monitoringNAME                      AGEalertmanager              2d19hblackbox-exporter         2d19hcoredns                   2d19hgrafana                   2d19hkube-apiserver            2d19hkube-controller-manager   2d19hkube-scheduler            2d19hkube-state-metrics        2d19hkubelet                   2d19hnode-exporter             2d19hprometheus-adapter        2d19hprometheus-k8s            2d19hprometheus-operator       2d19h</code></pre><p>以kube-scheduler为例</p><pre><code class="language-bash">[root@k8s01 my-yaml]# kubectl get servicemonitor kube-scheduler -n monitoring -o yamlapiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:......spec:  endpoints:  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token    interval: 30s    port: https-metrics    scheme: https    tlsConfig:      insecureSkipVerify: true  jobLabel: app.kubernetes.io/name  namespaceSelector:    matchNames:    - kube-system  selector:    matchLabels:      app.kubernetes.io/name: kube-scheduler</code></pre><p>可以看到 <code>kube-scheduler</code> 的 <code>servicemonitor</code> 指向拥有 <code>app.kubernetes.io/name: kube-scheduler</code> 和 <code>ports.name: https-metrics</code> 的 svc</p><h4 id="建立servicemonitor所需的svc">建立servicemonitor所需的svc</h4><p>查看服务pod的标签</p><pre><code class="language-bash">[root@k8s01 my-yaml]# kubectl get pod -n kube-system | grep kube-schedulerkube-scheduler-k8s01            1/1     Running   0          36mkube-scheduler-k8s02            1/1     Running   0          18mkube-scheduler-k8s03            1/1     Running   0          16m[root@k8s01 my-yaml]# kubectl get pod kube-scheduler-k8s01  -n kube-system -o yaml | grep -A 2 labels  labels:    component: kube-scheduler    tier: control-plane--        f:labels:          .: &#123;&#125;          f:component: &#123;&#125;</code></pre><p>如上可以看到，在kubeadm安装的k8s中，kube-scheduler的 labels 是 component: kube-scheduler。</p><p>最后生成svc配置</p><pre><code class="language-bash">apiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-scheduler-prometheus  labels:    app.kubernetes.io/name: kube-scheduler  # 关键spec:  selector:    component: kube-scheduler  # 关键  ports:  - name: https-metrics  # 关键    port: 10259  # 关键    targetPort: 10259  # 关键    protocol: TCP---apiVersion: v1kind: Servicemetadata:  namespace: kube-system  name: kube-controller-manager-prometheus  labels:    app.kubernetes.io/name: kube-controller-managerspec:  selector:    component: kube-controller-manager  ports:  - name: https-metrics    port: 10257    targetPort: 10257    protocol: TCP</code></pre><h4 id="修改服务监听地址">修改服务监听地址</h4><p>默认kubeadm安装的kube-controller-manager和kube-scheduler监听地址都是127.0.0.1，这导致无法被采集，因此需要改成0.0.0.0。</p><p>修改 static pod 配置即可。</p><pre><code class="language-bash">sed -e &quot;s/- --address=127.0.0.1/- --address=0.0.0.0/&quot; -i /etc/kubernetes/manifests/kube-controller-manager.yamlsed -e &quot;s/- --address=127.0.0.1/- --address=0.0.0.0/&quot; -i /etc/kubernetes/manifests/kube-scheduler.yaml</code></pre><p>修改完，k8s会自动重建相应的pod。</p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞showdoc部署</title>
      <link href="posts/9a7bc037/"/>
      <url>posts/9a7bc037/</url>
      
        <content type="html"><![CDATA[<p>ℹ️ 配置引用的存储是 nfs</p><pre><code class="language-yaml">kind: IngressapiVersion: networking.k8s.io/v1metadata:  name: showdoc-ingress  namespace: it  annotations:    kubernetes.io/ingress.class: &quot;nginx&quot;spec:  rules:  - host: showdoc.xxx.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: showdoc-nginx-svc            port:              number: 80---kind: ServiceapiVersion: v1metadata:  name: showdoc-nginx-svc  namespace: itspec:  selector:    app: showdoc-nginx-pod  ports:  - protocol: TCP    port: 80    targetPort: 80    name: showdoc-nginx-svc---kind: DeploymentapiVersion: apps/v1metadata:  name: showdoc-nginx-dep  namespace: itspec:  replicas: 1  selector:    matchLabels:      app: showdoc-nginx-pod  template:    metadata:      labels:        app: showdoc-nginx-pod    spec:      containers:      - name: showdoc-nginx-pod        image: nginx        ports:        - containerPort: 80        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: showdoc-vol          subPath: showdoc.xxx.com/data          mountPath: /app        - name: showdoc-vc          subPath: nginx.conf          mountPath: /etc/nginx/nginx.conf        - name: showdoc-vc          subPath: showdoc.xxx.com.conf          mountPath: /etc/nginx/conf.d/showdoc.xxx.com.conf        - name: showdoc-vc          subPath: fastcgi.conf          mountPath: /etc/nginx/fastcgi.conf      volumes:      - name: showdoc-vol        persistentVolumeClaim:          claimName: showdoc-pvc      - name: showdoc-vc        configMap:          name: showdoc-cm---kind: ServiceapiVersion: v1metadata:  name: showdoc-php-svc  namespace: itspec:  selector:    app: showdoc-php-pod  ports:  - protocol: TCP    port: 9000    targetPort: 9000    name: showdoc-php-svc---kind: DeploymentapiVersion: apps/v1metadata:  name: showdoc-php-dep  namespace: itspec:  replicas: 1  selector:    matchLabels:      app: showdoc-php-pod  template:    metadata:      labels:        app: showdoc-php-pod    spec:      containers:      - name: showdoc-php-pod        #image: php:7.2-fpm        image: bitnami/php-fpm        ports:        - containerPort: 9000        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: showdoc-vol          subPath: showdoc.xxx.com/data          mountPath: /app      volumes:      - name: showdoc-vol        persistentVolumeClaim:          claimName: showdoc-pvc---kind: ConfigMapapiVersion: v1metadata:  name: showdoc-cm  namespace: itdata:  nginx.conf: |    user nginx;    worker_processes auto;    error_log /dev/stderr;    pid /run/nginx.pid;    include /usr/share/nginx/modules/*.conf;    events &#123;        worker_connections 1024;    &#125;    http &#123;        log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '                          '$status $body_bytes_sent &quot;$http_referer&quot; '                          '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';        access_log  /dev/stdout  main;        sendfile            on;        tcp_nopush          on;        tcp_nodelay         on;        keepalive_timeout   65;        types_hash_max_size 2048;        include             /etc/nginx/mime.types;        default_type        application/octet-stream;        include /etc/nginx/conf.d/*.conf;    &#125;  showdoc.xxx.com.conf: |    server        &#123;            listen 80;            server_name showdoc.xxx.com;            index index.html index.php;            root /app;            location ~ [^/]\.php(/|$)            &#123;                try_files $uri =404;                fastcgi_pass  showdoc-php-svc:9000;                fastcgi_index index.php;                include fastcgi.conf;            &#125;            location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$            &#123;                expires      1d;            &#125;            location ~ .*\.(js|css)?$            &#123;                expires      12h;            &#125;            location ~ /.well-known &#123;                allow all;            &#125;            location ~ /\.            &#123;                deny all;            &#125;        &#125;  fastcgi.conf: |    fastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;    fastcgi_param  QUERY_STRING       $query_string;    fastcgi_param  REQUEST_METHOD     $request_method;    fastcgi_param  CONTENT_TYPE       $content_type;    fastcgi_param  CONTENT_LENGTH     $content_length;    fastcgi_param  SCRIPT_NAME        $fastcgi_script_name;    fastcgi_param  REQUEST_URI        $request_uri;    fastcgi_param  DOCUMENT_URI       $document_uri;    fastcgi_param  DOCUMENT_ROOT      $document_root;    fastcgi_param  SERVER_PROTOCOL    $server_protocol;    fastcgi_param  REQUEST_SCHEME     $scheme;    fastcgi_param  HTTPS              $https if_not_empty;    fastcgi_param  GATEWAY_INTERFACE  CGI/1.1;    fastcgi_param  SERVER_SOFTWARE    nginx/$nginx_version;    fastcgi_param  REMOTE_ADDR        $remote_addr;    fastcgi_param  REMOTE_PORT        $remote_port;    fastcgi_param  SERVER_ADDR        $server_addr;    fastcgi_param  SERVER_PORT        $server_port;    fastcgi_param  SERVER_NAME        $server_name;    fastcgi_param  REDIRECT_STATUS    200;---kind: PersistentVolumeapiVersion: v1metadata:  name:  showdoc-pv  # 定义 pv 名字, 会被 pvc 引用  namespace: itspec:  claimRef:    name: showdoc-pvc    namespace: it  capacity:    storage: 10Gi  # 定义大小  accessModes:  - ReadWriteMany   # 定义访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /mnt/data001/nfs-k8s   # 定义 nfs 共享路径    server: 10.200.16.250        # 定义 nfs 服务器地址---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: showdoc-pvc  namespace: itspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 10Gi  volumeName: showdoc-pv</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> showdoc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>windows☞win10迁移数据</title>
      <link href="posts/88eb7b6/"/>
      <url>posts/88eb7b6/</url>
      
        <content type="html"><![CDATA[<ol><li><p>桌面文件备份</p></li><li><p>子系统备份</p><p>如果用的子系统是Ubuntu，那么目录是</p><p>C:\Users\&lt;家目录&gt;\AppData\Local\Packages\CanonicalGroupLimited.Ubuntu***</p><p>将整个文件夹备份</p></li><li><p>其它分区数据备份</p></li></ol><p>如果有必要，则清理当前电脑的无用资源，直接整个系统盘进行Ghost克隆还原也是可以的</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> windows </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ubuntu </tag>
            
            <tag> windows10 </tag>
            
            <tag> 系统 </tag>
            
            <tag> 迁移 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞02外置prometheus监控k8s</title>
      <link href="posts/e172b6e2/"/>
      <url>posts/e172b6e2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>prometheus 需要访问kubernetes获取指标，访问kubernetes需要授权。</p></blockquote><h2 id="授权认证">授权认证</h2><h3 id="创建服务账户">创建服务账户</h3><pre><code class="language-bash">kubectl create sa prometheus -n it</code></pre><h3 id="服务账户访问证书">服务账户访问证书</h3><p>申请prometheus访问k8s的用户证书</p><pre><code class="language-bash">openssl genrsa -out prometheus.key 2048openssl req -new -key prometheus.key -out prometheus.csr -subj &quot;/CN=prometheus/O=it&quot;RequestStr=`cat prometheus.csr | base64 | tr -d &quot;\n&quot;`# 提交申请到k8scat &lt;&lt;EOF | kubectl apply -f -apiVersion: certificates.k8s.io/v1beta1kind: CertificateSigningRequestmetadata:  name: prometheusspec:  groups:  - system:authenticated  request: $&#123;RequestStr&#125;  signerName: kubernetes.io/kube-apiserver-client  usages:  - client authEOF# 如果不出错，可以审批kubectl certificate approve prometheus# 导出证书kubectl get csr prometheus -o jsonpath='&#123;.status.certificate&#125;' | base64 --decode &gt; prometheus.crt# 检查证书有效期openssl x509 -in prometheus.crt -noout -dates</code></pre><blockquote><p>保存好证书文件 prometheus.crt，prometheus配置所需</p></blockquote><h3 id="授权服务账户角色">授权服务账户角色</h3><p>绑定服务账户prometheus到cluster-admin角色</p><pre><code class="language-bash">kubectl create clusterrolebinding prometheus --clusterrole cluster-admin --serviceaccount=it:prometheus</code></pre><p>因prometheus访问k8s需要拿到bearer_token，因此还需要获取服务账户prometheus的Tokens。</p><pre><code class="language-bash">kubectl get sa prometheus -n it -o jsonpath=&#123;'.secrets[0].name'&#125; | moreprometheus-token-s82mq</code></pre><p>将Token进行解密</p><pre><code class="language-bash">kubectl get secret prometheus-token-s82mq -n it -o jsonpath=&#123;'.data.token'&#125; | base64 -d &gt; prometheus.bearer_token</code></pre><blockquote><p>保存好解密后的 prometheus.bearer_token，prometheus配置所需</p></blockquote><h2 id="安装k8s指标采集服务">安装k8s指标采集服务</h2><h3 id="从kube-state-metrics获取指标">从<a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a>获取指标</h3><p>部署方式：<a href="https://github.com/kubernetes/kube-state-metrics/tree/master/examples/standard">点我</a></p><p>指标文档：<a href="https://github.com/kubernetes/kube-state-metrics/tree/master/docs">https://github.com/kubernetes/kube-state-metrics/tree/master/docs</a></p><pre><code class="language-bash">➜  standard git:(master) ✗ ls -ltotal 20-rw-r--r-- 1 zyh zyh  381 Mar 12 12:07 cluster-role-binding.yaml-rw-r--r-- 1 zyh zyh 1744 Mar 12 12:07 cluster-role.yaml-rw-r--r-- 1 zyh zyh 1134 Mar 12 17:29 deployment.yaml-rw-r--r-- 1 zyh zyh  197 Mar 12 12:07 service-account.yaml-rw-r--r-- 1 zyh zyh  410 Mar 12 12:07 service.yaml➜  standard git:(master) ✗ for i in *.yaml;do kubectl apply -f $i;done</code></pre><blockquote><p>需要注意的是，<a href="http://deployment.xn--yamlkube-state-metricsk8s-3b63b637k9tejna412ib11erhyf.gcr.io/kube-state-metrics/kube-state-metrics%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%9C%B0%E5%9D%80%E5%9B%BD%E5%86%85%E6%B2%A1%E6%B3%95%E8%AE%BF%E9%97%AE%E3%80%82%E5%9B%A0%E6%AD%A4%E9%9C%80%E8%A6%81%E6%9B%BF%E6%8D%A2%E4%B8%BA%E5%85%B6%E5%AE%83%E5%9C%B0%E5%9D%80%EF%BC%8C%E4%BE%8B%E5%A6%82%EF%BC%9Aquay.io/coreos/kube-state-metrics:v2.0.0-rc.0">deployment.yaml中kube-state-metrics的容器地址是k8s.gcr.io/kube-state-metrics/kube-state-metrics，这个地址国内没法访问。因此需要替换为其它地址，例如：quay.io/coreos/kube-state-metrics:v2.0.0-rc.0</a></p></blockquote><h3 id="检查服务是否正常">检查服务是否正常</h3><pre><code class="language-bash">curl ip:8080/metrics</code></pre><h2 id="Prometheus基于k8s的自动发现">Prometheus基于k8s的自动发现</h2><p><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config</a></p><h3 id="从apiservers获取cadvisor和kube-state-metrics的指标采集地址">从apiservers获取cadvisor和kube-state-metrics的指标采集地址</h3><pre><code class="language-yaml">  - job_name: 'k8s-cadvisor' # 抓容器, cadvisor被整合在kubelet中    scheme: https    metrics_path: /metrics    tls_config:      ca_file: /etc/prometheus/conf/prometheus.crt      insecure_skip_verify: true    bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    kubernetes_sd_configs:    - role: node      api_server: &quot;https://&lt;api_server_ip&gt;:6443&quot;      tls_config:        ca_file: /etc/prometheus/conf/prometheus.crt        insecure_skip_verify: true      bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    relabel_configs:    - action: labelmap # 标签映射：抓取regex正则匹配的标签名，复制为replacement，期间标签值不变（replacement为$1时可以不写）      regex: __meta_kubernetes_node_label_(.+)      replacement: $1    metric_relabel_configs:    - source_labels: [instance]      separator: ;      regex: (.+)      target_label: node      replacement: $1      action: replace    - source_labels: [pod_name] # 兼容老集群，老集群曾用标签 pod_name      separator: ;      regex: (.+)      target_label: pod      replacement: $1      action: replace    - source_labels: [container_name] # 兼容老集群，老集群曾用标签 container_name      separator: ;      regex: (.+)      target_label: container      replacement: $1      action: replace  - job_name: &quot;kube-state-metrics&quot; # 抓工作负载资源    scheme: http    tls_config:      ca_file: /etc/prometheus/conf/prometheus.crt      insecure_skip_verify: true    bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    kubernetes_sd_configs:    - role: endpoints      api_server: &quot;https://10.3.176.130:6443&quot;      tls_config:        ca_file: /etc/prometheus/conf/prometheus.crt        insecure_skip_verify: true      bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    relabel_configs:    - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name] # 将自动发现的endpoint只保留kube-state-metrics      regex: kube-state-metrics      replacement: $1      action: keep    - action: labelmap      regex: __meta_kubernetes_service_label_(.+)    - source_labels: [__meta_kubernetes_namespace]      action: replace      target_label: k8s_namespace    - source_labels: [__meta_kubernetes_service_name]      action: replace      target_label: k8s_sname        - job_name: 'kubernetes-apiservers'    scheme: https    tls_config:      ca_file: /etc/prometheus/conf/prometheus.crt      insecure_skip_verify: true    bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    kubernetes_sd_configs:    - role: endpoints      api_server: &quot;https://10.3.176.130:6443&quot;      tls_config:        ca_file: /etc/prometheus/conf/prometheus.crt        insecure_skip_verify: true      bearer_token_file: /etc/prometheus/conf/prometheus.bearer_token    relabel_configs:    - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]      action: keep      regex: default;kubernetes;https</code></pre><h2 id="添加grafana模板">添加grafana模板</h2><p><a href="https://grafana.com/grafana/dashboards/13105">https://grafana.com/grafana/dashboards/13105</a></p><p><img src="/posts/e172b6e2/image-20210313180815651.png" alt="image-20210313180815651"></p><p>上图是我在阿里云ASK中测试的最终效果图</p><h3 id="调整模板">调整模板</h3><p>如果你发现模板中，【微服务网络带宽】没有数据，则需要看下此板块SQL里的标签过滤是否有误。</p><p>在我的环境里，模板中过滤的 name=‘^k8s_.*’ 不存在。因此删除SQL里的这个过滤即可。</p><h2 id="添加预警">添加预警</h2><p>需要明确，我们需要监控什么</p><ol><li>k8s本身各组件状态</li><li>调度了多少个replicas？现在可用的有几个？</li><li>Pod是否启动成功</li><li>Pod重启了多少次？</li></ol><pre><code class="language-yaml">groups:- name: kubernetes  rules:  - alert: kube-state-metrics down    expr: (up&#123;job=&quot;kube-state-metrics&quot;&#125; or up&#123;job=&quot;kubernetes-apiservers&quot;&#125;) != 1  #0不正常，1正常    for: 5m  #持续时间 ，表示持续5分钟获取不到信息，则触发报警    labels:      severity: error      cluster: k8s    annotations:      summary: &quot;Job: &#123;&#123; $labels.job &#125;&#125; down&quot;      description: &quot;Instance:&#123;&#123; $labels.instance &#125;&#125;, Job &#123;&#123; $labels.job &#125;&#125; stop &quot;  - alert: JobFailed    expr: kube_job_status_failed == 1    for: 5m    labels:      severity: error      cluster: k8s    annotations:      summary: 'Job: &#123;&#123; $labels.job &#125;&#125; failed'      description: 'Namespace: &#123;&#123; $labels.namespace &#125;&#125;, Job: &#123;&#123; $labels.job &#125;&#125; run failed.'  - alert: PodDown    expr: kube_pod_container_status_running != 1    for: 2s    labels:      severity: warning      cluster: k8s    annotations:      summary: 'Container: &#123;&#123; $labels.container &#125;&#125; down'      description: 'Namespace: &#123;&#123; $labels.namespace &#125;&#125;, Pod: &#123;&#123; $labels.pod &#125;&#125; is not running'  - alert: PodReady    expr: kube_pod_container_status_ready != 1    for: 5m   #Ready持续5分钟，说明启动有问题    labels:      severity: warning      cluster: k8s    annotations:      summary: 'Container: &#123;&#123; $labels.container &#125;&#125; ready'      description: 'Namespace: &#123;&#123; $labels.namespace &#125;&#125;, Pod: &#123;&#123; $labels.pod &#125;&#125; always ready for 5m'  - alert: PodRestart    expr: changes(kube_pod_container_status_restarts_total[30m])&gt;0 #最近30分钟pod重启次数    for: 2s    labels:      severity: warning      cluster: k8s    annotations:      summary: 'Container: &#123;&#123; $labels.container &#125;&#125; restart'      description: 'namespace: &#123;&#123; $labels.namespace &#125;&#125;, pod: &#123;&#123; $labels.pod &#125;&#125; restart &#123;&#123; $value &#125;&#125; times'</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞利用acme.sh管理免费的ssl证书申请</title>
      <link href="posts/d2fed0bf/"/>
      <url>posts/d2fed0bf/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>申请免费的ssl证书途径有很多，例如阿里云的1年期，或者freessl这种站点。</p><p>如果你想自己部署一个服务来管理并自动更新，则可以使用 <a href="http://acme.sh">acme.sh</a></p><p><a href="http://acme.sh">acme.sh</a> 是一个开源程序，托管在github上。</p><h2 id="安装">安装</h2><p><a href="https://github.com/acmesh-official/acme.sh/wiki/%E8%AF%B4%E6%98%8E#1-%E5%AE%89%E8%A3%85-acmesh">https://github.com/acmesh-official/acme.sh/wiki/说明#1-安装-acmesh</a></p><p>参考官方文档即可，很简单，不过国内经常受限于网络问题导致无法下载，因此你可以在 gitee 上下载</p><p><a href="https://gitee.com/neilpang/acme.sh?_from=gitee_search#https://github.com/acmesh-official/acme.sh/wiki/%E8%AF%B4%E6%98%8E">https://gitee.com/neilpang/acme.sh?_from=gitee_search#https://github.com/acmesh-official/acme.sh/wiki/说明</a></p><p>不过 gitee 上看起来并不是最新的</p><h2 id="使用">使用</h2><h3 id="申请-ssl">申请 ssl</h3><p>以阿里云的dns托管解析为例，给abc.com创建免费ssl证书 <a href="https://letsencrypt.org/zh-cn/">Let’s Encrypt</a></p><p>创建访问阿里云dns解析所需的ram策略，请直接使用管理员策略，但是可以附加源ip限制</p><p>💁说实话我单独授权云解析策略就是不行，至于原因没有具体去验证。所以如果你授权了管理员策略，务必添加源ip限制</p><pre><code class="language-bash">DnsAccessKey=DnsAccessSecret=export  Ali_Key=$&#123;DnsAccessKey&#125;export  Ali_Secret=$&#123;DnsAccessSecret&#125;D1=abc.comDnsMode=dns_aliacme.sh --issue --dns $&#123;DnsMode&#125; -d $&#123;D1&#125; -d *.$&#123;D1&#125; </code></pre><blockquote><p>–dns 指定域名所在的托管解析商</p><p>-d 证书关联的域名</p></blockquote><p>这个步骤会给域名添加新的txt记录，用来校验域名是否归你所有</p><p>但是在写这篇文章的时候，有新的问题出现，就是acme.sh默认调用cloudflare和google的dns去验证添加的记录是否已生效，而国内一些服务商访问国际接口那是一个稀烂🤕 所以可能会无限的卡在校验这里。</p><p>根据<a href="https://github.com/acmesh-official/acme.sh/wiki/dnscheck">官方文档</a>，你可以添加 --dnssleep 300 去忽略掉。</p><h3 id="部署ssl">部署ssl</h3><p>将生成的密钥和证书放在指定位置，这里的key和cer分别对应nginx所需的key和cert</p><pre><code class="language-bash">acme.sh  --installcert  -d $&#123;D1&#125; --key-file $&#123;D1_DIR&#125;/$&#123;D1&#125;.key --fullchain-file $&#123;D1_DIR&#125;/fullchain.cer</code></pre><h3 id="更新">更新</h3><p><a href="http://acme.sh">acme.sh</a> 会在用户级别的crontab中添加自动更新指令。通过<code>crontab -l</code>可以看到。</p><p>如果你需要手动强制更新，则可以执行</p><pre><code class="language-bash"> acme.sh --renew -d $&#123;D1&#125; -d *.$&#123;D1&#125;</code></pre><h3 id="配置">配置</h3><p><a href="http://acme.sh">acme.sh</a> 的配置文件默认放在 ~/.acme.sh/account.conf 中，一般来说需要用户提供的环境变量，都会以 <code>SAVED_</code>开头。</p><h3 id="通告">通告</h3><p>如果你想接收 <a href="http://acme.sh">acme.sh</a> 处理后的消息，则需要配置，不同的消息体可以看<a href="https://github.com/acmesh-official/acme.sh/wiki/notify">官方文档</a></p><p>主要分为三个部分：</p><ul><li>消息级别</li><li>消息模式</li><li>消息钩子</li></ul><p>以自定义的 smtp 为例，你需要提供以下变量</p><pre><code class="language-bash">export SMTP_FROM=export SMTP_TO=export SMTP_HOST=&quot;smtp.feishu.cn&quot;export SMTP_SECURE=&quot;tls&quot;export SMTP_PORT=&quot;587&quot;export SMTP_USERNAME=export SMTP_PASSWORD=# export SMTP_BIN=&quot;/path/to/python_or_curl&quot;export SMTP_TIMEOUT=&quot;30&quot;</code></pre><p>并执行</p><pre><code class="language-bash">acme.sh --set-notify --notify-level 3 --notify-mode 0 --notify-hook smtp --accountconf ~/.acme.sh/account.conf</code></pre><p>如此以来，<a href="http://acme.sh">acme.sh</a> 会将你配置的环境变量追加存入 account.conf</p><h2 id="其它问题">其它问题</h2><h3 id="单一托管商多账户的问题">单一托管商多账户的问题</h3><p>在写这篇文章的时候，<a href="http://acme.sh">acme.sh</a> 当前并不支持（19年作者提出有时间会搞 🤺 ），但是你可以通过配置多个 account.conf，并在计划任务中使用 --accountconf 来写入多条更新计划命令来实现配置文件级别的轮询处理。就像下面这样:</p><pre><code class="language-bash">34 0 * * * &quot;/home/it/.acme.sh&quot;/acme.sh --cron --home &quot;/home/it/.acme.sh&quot; --accountconf /home/it/.acme.sh/account.conf.aliyun34 1 * * * &quot;/home/it/.acme.sh&quot;/acme.sh --cron --home &quot;/home/it/.acme.sh&quot; --accountconf /home/it/.acme.sh/account.conf.aws34 2 * * * &quot;/home/it/.acme.sh&quot;/acme.sh --cron --home &quot;/home/it/.acme.sh&quot; --accountconf /home/it/.acme.sh/account.conf.godaddy</code></pre><p>当然因为域名在生成ssl证书的时候，acme.sh并没有给域名关联某个配置的记录。因此每一条更新计划任务均会处理所有的域名，这必然会产生一些错误信息（即：xxx托管商找不到xxx域名），不过这无关紧要。</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> acme.sh </tag>
            
            <tag> ssl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞使用expect自动登录ssh后远程会话不会跟随本地会话缩放的问题</title>
      <link href="posts/7b3cf3d8/"/>
      <url>posts/7b3cf3d8/</url>
      
        <content type="html"><![CDATA[<h2 id="原因">原因</h2><p>expect 执行的时候，并不会将窗口大小改变的信号传递给远程会话。所以在脚本中添加即可。</p><h2 id="解决">解决</h2><pre><code class="language-bash">#!/usr/bin/env expect #trap sigwinch spawnedtrap &#123; set rows [stty rows] set cols [stty columns] stty rows $rows columns $cols &lt; $spawn_out(slave,name)&#125; WINCH</code></pre><blockquote><p>在脚本开头加入 trap</p></blockquote><h2 id="来源">来源</h2><p><a href="http://blog.sina.com.cn/s/blog_a73a649601018sgl.html">http://blog.sina.com.cn/s/blog_a73a649601018sgl.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ssh </tag>
            
            <tag> expect </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyun☞常见问题</title>
      <link href="posts/4cc38acc/"/>
      <url>posts/4cc38acc/</url>
      
        <content type="html"><![CDATA[<h2 id="adb-for-mysql">adb for mysql</h2><h3 id="adb-访问-oss-文件不存在">adb 访问 oss 文件不存在</h3><p>adb 是分布式集群，调用oss的时候，源ip并不是adb实例地址所解析的vpc ip。因此ram的权限不能添加源ip限制。</p><h2 id="emr">emr</h2><h3 id="hive-元数据使用外部rds的时候，构建表出错">hive 元数据使用外部rds的时候，构建表出错</h3><p>如果编码是utf8，变更为latin1</p><h3 id="hive删除表卡住，hive日志看不出异常">hive删除表卡住，hive日志看不出异常</h3><p>使用mysql8.0的时候出现，最终换成官方建议的5.7后，正常</p><h3 id="最新版emr采用role授权方式的时候，ecs-节点权限异常">最新版emr采用role授权方式的时候，ecs 节点权限异常</h3><p>role的规则必须是官方默认规则，任何缩小权限范围的规则都会异常。暂不知原因。</p><blockquote><p>说实话，官方默认的规则权限好大</p></blockquote><h3 id="HUE-服务不太正常">HUE 服务不太正常</h3><p>确认zookeeper是否安装</p><h2 id="CDN">CDN</h2><h3 id="非简单跨域-OPTIONS-无法通过">非简单跨域 OPTIONS 无法通过</h3><p>产生OPTIONS的原因是客户端请求存在自定义header，因此跨域的配置需要允许额外的自定义header</p><pre><code class="language-bash">Access-Control-Allow-Origin=&lt;允许的域&gt;Access-Control-Allow-Methods=GET,POST,PUT,OPTIONSAccess-Control-Allow-Headers=&lt;允许的自定义header&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rabbitmq☞安装</title>
      <link href="posts/b43013ef/"/>
      <url>posts/b43013ef/</url>
      
        <content type="html"><![CDATA[<h3 id="docker方式">docker方式</h3><pre><code class="language-bash">mkdir -p /export/docker-data-rabbitmq/&#123;data,log,conf.d&#125;chown -R 999:999 /export/docker-data-rabbitmq/&#123;data,log,conf.d&#125;</code></pre><p>⚠️<code>999</code>是rabbitmq官方镜像里的<code>rabbitmq</code>的<code>uid</code></p><h4 id="添加额外的日志配置">添加额外的日志配置</h4><p>/export/docker-data-rabbitmq/conf.d/log.conf</p><pre><code>#log.file = rabbit.loglog.dir = /var/log/rabbitmqlog.file.level = info# rotate every night at midnightlog.file.rotation.date = $D0# keep up to 5 archived log files in addition to the current onelog.file.rotation.count = 5</code></pre><pre><code class="language-bash"># https://hub.docker.com/_/rabbitmq, 然后搜索 management 版本 [docker pull rabbitmq:management]pwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`echo mq@$&#123;pwd&#125;;container_name=rabbitmqnetwork=cmsdocker run --name $&#123;container_name&#125; \--hostname $&#123;container_name&#125; \--network $&#123;network&#125; \--restart always \-p 5672:5672 -p 15672:15672 -p 15692:15692 \--cpus=1 \--memory=1G --memory-swap=1G \--ulimit nofile=204800 \--mount &quot;type=bind,src=/export/docker-data-rabbitmq/data,dst=/var/lib/rabbitmq&quot; \--mount &quot;type=bind,src=/export/docker-data-rabbitmq/conf.d,dst=/etc/rabbitmq/conf.d&quot; \--mount &quot;type=bind,src=/export/docker-data-rabbitmq/log,dst=/var/log/rabbitmq&quot; \-e RABBITMQ_DEFAULT_VHOST=rabbitmq \-e RABBITMQ_DEFAULT_USER=admin \-e RABBITMQ_DEFAULT_PASS=mq@$&#123;pwd&#125; \-e RABBITMQ_LOGS=rabbitmq.log \-d rabbitmq:3.8.13-management</code></pre><p>需要注意的是，docker方式，<code>RABBITMQ_LOGS</code>默认值是<code>-</code>也就是输出到<code>stdout</code>, 这里我改成了具体文件</p><p>⚠️ 任何配置，环境变量的优先级大于文本配置</p><p>安装logrotate，rabbitmq的日志轮转依托它</p><pre><code class="language-bash">docker exec -it rabbitmq /bin/bashapt updateapt install logrotateexit</code></pre><p>重启 rabbitmq</p><pre><code class="language-bash">docker restart rabbitmq</code></pre><h4 id="Prometheus-监控">Prometheus 监控</h4><p>15692 端口是 prometheus 采集器的暴露端口，docker方式默认采集插件已经开启。</p><p>若 prometheus 采集器插件没有启动，可以进容器里通过 rabbitmq-plugins enable rabbitmq_prometheus 启动</p><p>宿主机里执行 <code>curl -s localhost:15692/metrics | head -n 3</code>确保可以返回采集器数据。</p><p>prometheus的告警规则，可以访问 <a href="https://awesome-prometheus-alerts.grep.to/rules#rabbitmq">Awesome Prometheus alerts | Collection of alerting rules (grep.to)</a> 获取</p><p>grafana的模板，可以访问 <a href="https://grafana.com/grafana/dashboards/10991">RabbitMQ-Overview dashboard for Grafana | Grafana Labs</a></p><h3 id="rpm方式-过时">rpm方式(过时)</h3><pre><code># 安装 erlang 的 rabbitmq 兼容版方式 (卸载命令 yum erase erlang-*)yum install -y https://github.com/rabbitmq/erlang-rpm/releases/download/v22.0.7/erlang-22.0.7-1.el6.x86_64.rpm# 安装 rabbitmq-serveryum install -y https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.15/rabbitmq-server-3.7.15-1.el6.noarch.rpm;## 开启管理rabbitmq-plugins enable rabbitmq_management;# 修改一些小配置vim /etc/rabbitmq/rabbitmq.conflog.file.level = warninglog.file = rabbitmq.logvim /etc/rabbitmq/rabbitmq-env.confRABBITMQ_LOG_BASE=/export/rabbitmq/logsmkdir -p /export/rabbitmq/logs &amp;&amp; chown rabbitmq:rabbitmq /export/rabbitmq/logschown rabbitmq.rabbitmq /var/lib/rabbitmq/.erlang.cookie;echo '* soft nofile 202400' &gt;&gt; /etc/security/limits.confecho '* hard nofile 202400' &gt;&gt; /etc/security/limits.conf/etc/init.d/rabbitmq-server restart;</code></pre><h3 id="添加用户和虚拟主机">添加用户和虚拟主机</h3><pre><code class="language-bash">#输入http://ip:15672可以登录管理界面,# rpm 安装后，只有默认账户guest/guest.# docker 安装后，有 admin/admin 用户# 添加一个 vhostpro_vhost=rabbitmqctl add_vhost $&#123;pro_vhost&#125;# 添加一个新用户user_name=pwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`rabbitmqctl add_user $&#123;user_name&#125; mq@$&#123;pwd&#125;;echo &quot;$&#123;user_name&#125;用户密码是：mq@$&#123;pwd&#125;&quot;#该命令使用户 user_name 具有 pro_vhost 中所有资源的配置、写、读权限以便管理其中的资源# rabbitmqctl set_permissions -p vhost User ConfP WriteP ReadPrabbitmqctl set_permissions -p $&#123;pro_vhost&#125; $&#123;user_name&#125; &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;; # 用户绑定 tags# tags决定了用户更高级的权限，## 超级管理员(administrator)：可登陆管理控制台(启用management plugin的情况下)，可查看所有的信息，并且可以对用户，策略(policy)进行操作。## 监控者(monitoring)：可登陆管理控制台(启用management plugin的情况下)，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)## 策略制定者(policymaker)：可登陆管理控制台(启用management plugin的情况下), 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)## 普通管理者(management)：仅可登陆管理控制台(启用management plugin的情况下)，无法看到节点信息，也无法对策略进行管理。## 其他(other)：无法登陆管理控制台，通常就是普通的生产者和消费者。user_tag=administratorrabbitmqctl set_user_tags $&#123;user_name&#125; $&#123;user_tag&#125;;#查看所有用户rabbitmqctl list_users;</code></pre><h3 id="状态">状态</h3><pre><code class="language-bash">rabbitmqctl statusrabbitmqctl list_queues -p &lt;vhost_name&gt;</code></pre><h3 id="队列">队列</h3><pre><code class="language-bash">rabbitmqctl list_queues --vhost &lt;&gt;</code></pre><h2 id="php-语言使用">php 语言使用</h2><pre><code class="language-bash">wget https://github.com/alanxz/rabbitmq-c/releases/download/v0.7.1/rabbitmq-c-0.7.1.tar.gz;tar xf rabbitmq-c-0.7.1.tar.gz;cd rabbitmq-c-0.7.1;./configure --prefix=/usr/local/rabbitmq-c-0.7.1;make &amp;&amp; make install;cd ..;wget https://pecl.php.net/get/amqp-1.9.3.tgz;tar xf amqp-1.9.3.tgz;cd amqp-1.9.3;/usr/local/php/bin/phpize;./configure --with-php-config=/usr/local/php/bin/php-config --with-amqp --with-librabbitmq-dir=/usr/local/rabbitmq-c-0.7.1;make &amp;&amp; make install;php.ini里添加生成的amqp.so的路径信息extension = /usr/local/php/lib/php/extensions/no-debug-non-zts-20160303/amqp.so</code></pre>]]></content>
      
      
      <categories>
          
          <category> 中间件 </category>
          
          <category> rabbitmq </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> rabbitmq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞动态agent</title>
      <link href="posts/3b5cd3e1/"/>
      <url>posts/3b5cd3e1/</url>
      
        <content type="html"><![CDATA[<h2 id="流程">流程</h2><p>jenkins-master -&gt; pipeline -&gt; kubernetes 插件 -&gt; kubernetes 集群 -&gt; jenkins-agent【k8s pod】 -&gt; jenkins-master</p><h2 id="凭证信息">凭证信息</h2><p>通过 kube config 里的加密信息获取凭据所需的证书</p><pre><code class="language-bash">certificate-authority-data=client-certificate-data=client-key-data=echo &quot;$&#123;certificate-authority-data&#125;&quot; | base64 -d &gt; ca.crtecho &quot;$&#123;client-certificate-data&#125;&quot; | base64 -d &gt; client.crtecho &quot;$&#123;client-key-data&#125;&quot; | base64 -d &gt; client.keyopenssl pkcs12 -export -out cert.pfx -inkey client.key -in client.crt -certfile ca.crt</code></pre><p>将 pkcs12 传输到 jenkins 凭据中</p><p><img src="/posts/3b5cd3e1/image-20210116152344875.png" alt="image-20210116152344875"></p><p>💁你需要在凭据【密码】位置中输入你生成 pkcs12 证书时输入的密码.</p><h2 id="安装插件">安装插件</h2><ol><li>安装 kubernetes 插件</li><li>添加一个新的插件配置</li></ol><p><img src="/posts/3b5cd3e1/image-20210116152033813.png" alt="image-20210116152033813"></p><ol start="3"><li>确保插件配置【连接测试】显示成功连接</li></ol><h2 id="配置-pipeline-动态生成-agent">配置 pipeline 动态生成 agent</h2><p>关于 agent 的 pod 模板，默认插件已经提供了一个，只不过你看不到。因此哪怕你什么都不做，你 pipeline stages 也会在默认的 agent pod 里执行。</p><p>我们一般选择给 agent pod 添加一个额外的容器，这个额外的容器里包含了我们执行 stages 所需要的环境，例如叫 jenkinstools。</p><p>即最终，整个pod包含两个容器，分别是沟通 jenkins-master 的 jenkins-agent 服务和执行我们工作流步骤的 jenkinstools 容器。</p><p>需要注意的是：</p><p>多容器 pod 模板有诸多的硬性限制：</p><p>Multiple containers can be defined in a pod. One of them is automatically created with name <code>jnlp</code>, and runs the Jenkins JNLP agent service, with args <code>$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;</code>, and will be the container acting as Jenkins agent.</p><p>Other containers must run a long running process, so the container does not exit. If the default entrypoint or command just runs something and exit then it should be overridden with something like <code>cat</code> with <code>ttyEnabled: true</code>.</p><p><strong>WARNING</strong> If you want to provide your own Docker image for the JNLP agent, you <strong>must</strong> name the container <code>jnlp</code> so it overrides the default one. Failing to do so will result in two agents trying to concurrently connect to the master.</p><ol><li>jnlp agent 容器会自动创建，哪怕你没有定义。</li><li>如果你自定义了 JNLP 服务所在的容器，那么负责连接 master 的 jnlp agent 容器名必须叫 jnlp。</li><li>其它容器，也就是我们用来执行 stages 的容器，必须运行一个持久的程序，以便于容器不会自动退出。例如 cat 命令并附加一个伪终端。</li><li>stages 阶段执行的时候，必须明确的指定额外容器，否则会默认在 jnlp 容器里执行. （这个不是很确定，但是我测试是这样）</li></ol><p>下图是我的 pod 模板配置（部分截图）</p><p><img src="/posts/3b5cd3e1/image-20210126182630608.png" alt="image-20210126182630608"></p><p>在 pipeline 里调用插件配置</p><pre><code>pipeline&#123;    agent&#123;        kubernetes&#123;            label &quot;jenkins-agent&quot; // pod 模板标签            cloud 'kubernetes'  // 插件配置名        &#125;    &#125;    stages &#123;        stage('Start Command On Remote Machine') &#123;            steps &#123;                container('jenkinstools') &#123;  // 明确指定 pod 里的执行容器名                    script&#123;                        mytools.getRemoteIP(serverIP)                        mytools.ansible(ansibleRemoteUser,&quot;$&#123;ansibleShellCommand&#125;&quot;)                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;</code></pre><h2 id="问题点">问题点</h2><h3 id="额外的自定义-pod-属性">额外的自定义 pod 属性</h3><p>默认 pod 模板的 web 界面，并没有提供所有的属性输入框，例如 spec.hostAliases 指定自定义域名解析</p><p>因此，当需要一些自定义的属性时，就需要编写自定义的 yaml，并将其填入 【Raw YAML for the Pod】，并将【Yaml merge strategy】设置为 【Merge】</p><p>以添加自定义域名解析为例：</p><pre><code class="language-yaml">apiVersion: v1kind: Podspec:  hostAliases:  - ip: &quot;1.1.1.1&quot;    hostnames:    - &quot;test.com&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aliyun☞oss</title>
      <link href="posts/b5d7d6ee/"/>
      <url>posts/b5d7d6ee/</url>
      
        <content type="html"><![CDATA[<h2 id="构建-oss-存储桶">构建 oss 存储桶</h2><ul><li>执行脚本（执行本脚本跑到阿里云的云端cli去执行）</li></ul><pre><code class="language-python">#!/usr/bin/python3.6# -*- coding: utf-8 -*-###  Usage：https://help.aliyun.com/document_detail/32027.html##  Github：https://github.com/aliyun/aliyun-oss-python-sdk##  author: zyhimport oss2, osfrom oss2.models import (LifecycleExpiration, LifecycleRule,                        BucketLifecycle,AbortMultipartUpload,                        TaggingRule, Tagging, StorageTransition,                        NoncurrentVersionStorageTransition,                        NoncurrentVersionExpiration)from oss2.models import Tagging, TaggingRule#from aliyunsdkcore.client import AcsClientfrom aliyunsdkcore.acs_exception.exceptions import ClientExceptionfrom aliyunsdkcore.acs_exception.exceptions import ServerExceptionfrom aliyunsdkram.request.v20150501.CreatePolicyRequest import CreatePolicyRequest####################################region = '我是区域ID'bucketName = '我是桶名'project = '我是标签project的值'akey = skey = ###################################endpoint = 'http://oss-&#123;0&#125;.aliyuncs.com'.format(region)auth = oss2.Auth(akey,skey)bucket = oss2.Bucket(auth, endpoint, bucketName)# create bucketbucket.create_bucket()# add tagrule = TaggingRule()rule.add('project', project)tagging = Tagging(rule)bucket.put_bucket_tagging(tagging)# init dirsbucket.put_object('conf/README','我是存放配置的目录')bucket.put_object('data/README','我是存放数据的目录')bucket.put_object('hive/README','我是存放hive的目录')bucket.put_object('backup/README','我是存放备份的目录')bucket.put_object('logs/7days/README','我是存放保留7天的日志目录')bucket.put_object('logs/15days/README','我是存放保留15天的日志目录')bucket.put_object('logs/30days/README','我是存放保留30天的日志目录')bucket.put_object('logs/60days/README','我是存放保留60天的日志目录')bucket.put_object('logs/90days/README','我是存放保留90天的日志目录')bucket.put_object('logs/180days/README','我是存放永久保留的日志目录')# add lifecyclerule1 = LifecycleRule('rule1', 'logs/7days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=7))rule2 = LifecycleRule('rule2', 'logs/15days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=15))rule3 = LifecycleRule('rule3', 'logs/30days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=30))rule4 = LifecycleRule('rule4', 'logs/60days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=60))rule5 = LifecycleRule('rule5', 'logs/90days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=90))rule6 = LifecycleRule('rule6', 'logs/180days/',                      status=LifecycleRule.ENABLED,                      expiration=LifecycleExpiration(days=180))rule7 = LifecycleRule('rule7', 'logs/longlasting/',                      status=LifecycleRule.ENABLED,                      storage_transitions=[StorageTransition(days=60,storage_class=oss2.BUCKET_STORAGE_CLASS_IA),                          StorageTransition(days=180,storage_class=oss2.BUCKET_STORAGE_CLASS_ARCHIVE)])lifecycle = BucketLifecycle([rule1, rule2, rule3, rule4, rule5, rule6, rule7])bucket.put_bucket_lifecycle(lifecycle)os.system(&quot;sed 's#ossBucketName#&#123;0&#125;#g' local.policy.default &gt; local_&#123;0&#125;.policy&quot;.format(bucketName))os.system(&quot;sed 's#ossBucketName#&#123;0&#125;#g' role.policy.default &gt; role_&#123;0&#125;.policy&quot;.format(bucketName))os.system(&quot;aliyun ram CreatePolicy --PolicyName oss-&#123;0&#125;-local --PolicyDocument \&quot;`cat local_&#123;0&#125;.policy`\&quot;&quot;.format(bucketName))os.system(&quot;aliyun ram CreatePolicy --PolicyName oss-&#123;0&#125;-role --PolicyDocument \&quot;`cat role_&#123;0&#125;.policy`\&quot;&quot;.format(bucketName))</code></pre><ul><li>远程用户策略（执行脚本前需要添加的）</li></ul><pre><code class="language-json">&#123;  &quot;Version&quot;: &quot;1&quot;,  &quot;Statement&quot;: [    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [                &quot;oss:GetObject&quot;,                &quot;oss:PutObject&quot;,                &quot;oss:DeleteObject&quot;,                &quot;oss:ListObject&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName/logs/7days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/15days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/30days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/60days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/90days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/180days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/longlasting/*&quot;,        &quot;acs:oss:*:*:ossBucketName/conf/*&quot;,        &quot;acs:oss:*:*:ossBucketName/data/*&quot;,        &quot;acs:oss:*:*:ossBucketName/backup/*&quot;,        &quot;acs:oss:*:*:ossBucketName/hive/*&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;1.1.1.1&quot;,            &quot;2.2.2.2&quot;          ]        &#125;      &#125;    &#125;,    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;oss:List*&quot;,        &quot;oss:GetBucketLocation&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName&quot;      ],      &quot;Condition&quot;: &#123;        &quot;IpAddress&quot;: &#123;          &quot;acs:SourceIp&quot;: [            &quot;1.1.1.1&quot;,            &quot;2.2.2.2&quot;          ]        &#125;      &#125;    &#125;  ]&#125;</code></pre><ul><li>角色用户策略（执行脚本前需要添加的）</li></ul><pre><code class="language-json">&#123;  &quot;Version&quot;: &quot;1&quot;,  &quot;Statement&quot;: [    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [                &quot;oss:GetObject&quot;,                &quot;oss:PutObject&quot;,                &quot;oss:DeleteObject&quot;,                &quot;oss:ListObject&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName/logs/7days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/15days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/30days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/60days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/90days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/180days/*&quot;,        &quot;acs:oss:*:*:ossBucketName/logs/longlasting/*&quot;,        &quot;acs:oss:*:*:ossBucketName/conf/*&quot;,        &quot;acs:oss:*:*:ossBucketName/data/*&quot;,        &quot;acs:oss:*:*:ossBucketName/backup/*&quot;,        &quot;acs:oss:*:*:ossBucketName/hive/*&quot;      ]    &#125;,    &#123;      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;oss:List*&quot;      ],      &quot;Resource&quot;: [        &quot;acs:oss:*:*:ossBucketName&quot;      ]    &#125;  ]&#125;</code></pre><h2 id="使用">使用</h2><p>以角色授权方式+aliyun cli命令方式走起.</p><p>😔，aliyun cli 的文档和使用一言难尽=。=</p><p><strong>使用前请确保角色已经关联了对应权限以及角色已经绑定到了ECS上</strong></p><pre><code class="language-bash"># /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;</code></pre><blockquote><p>本脚本，会让任何一个会话登陆的时候就拿到角色拥有的权限</p></blockquote><pre><code class="language-bash">##导入角色source /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;BucketName=test#需要注意的是，如果请求端资源与oss不在一个大区，则endpoint地址需要用下述外网地址： 关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！Endpoint=&quot;http://oss-$&#123;Region&#125;.aliyuncs.com&quot;##查询## 默认查询是递归查询，-d 只查询一层aliyun oss ls oss://$&#123;BucketName&#125;/ -d -e $&#123;Endpoint&#125;##基本的上传或下载##上传文件 a.file 到 oss://test/ aliyun oss cp a.file oss://$&#123;BucketName&#125;/ -e $&#123;Endpoint&#125;##基本的递归上传##上传目录 abc 下的文件到 oss://test/ 下，如果有重复内容，则需要加入 --force：关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！aliyun oss cp abc oss://$&#123;BucketName&#125;/ --recursive -e $&#123;Endpoint&#125;##复杂的递归上传##上传目录 abc 下的 .lzo 结尾的文件到 oss://test/ 下.##严禁在源目录里执行 --recursive 参数.  关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！关键！##即禁止执行 aliyun oss cp . oss://$&#123;BucketName&#125;/ --recursive aliyun oss cp abc/ oss://$&#123;BucketName&#125;/ --include='*.lzo' --recursive --force -e $&#123;Endpoint&#125;##同步目录 sync 指令变更##同步目录 abc 下的文件到 oss://$&#123;BucketName&#125;/ 下，如有重复，则忽略aliyun oss cp abc oss://$&#123;BucketName&#125;/ --recursive -u -e $&#123;Endpoint&#125;</code></pre><h2 id="开发向-sdk">开发向 sdk</h2><p>关于php sdk访问对象存储的文档</p><p>php oss 对象 sdk<br><a href="https://packagist.org/packages/aliyuncs/oss-sdk-php?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6">https://packagist.org/packages/aliyuncs/oss-sdk-php?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6</a><br>php ram role sdk<br><a href="https://packagist.org/packages/alibabacloud/credentials?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6">https://packagist.org/packages/alibabacloud/credentials?spm=a2c6h.13321295.0.0.4f765c2dMbvzR6</a></p>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aliyun </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aliyun </tag>
            
            <tag> oss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞012常用模块的记录</title>
      <link href="posts/d1503f4a/"/>
      <url>posts/d1503f4a/</url>
      
        <content type="html"><![CDATA[<h2 id="os">os</h2><p><a href="https://pkg.go.dev/os#File">os package - os - pkg.go.dev</a></p><p>os.OpenFile</p><pre><code>func OpenFile(name string, flag int, perm FileMode) (*File, error)</code></pre><p><code>os.OpenFile</code>根据<code>flag</code>模式来创建/打开一个文件</p><p>这里的返回值<code>*File</code>其实也是<code>io.Reader</code>，都是textual I/O这一类. 因此，<code>*File</code>也可以被用于<code>bufio</code>和<code>io</code>模块</p><p>常用的flag组合：</p><p><code>os.O_CREATE|os.O_RDWR</code> 创建并可读可写</p><p><code>os.O_CREATE|os.O_RDWR|os.O_APPEND</code>创建并可读可写可追加</p><p>⚠️需要注意的是，<code>os.O_APPEND</code>将始终如一的将指针定位到尾部，因此<code>*File.Seek</code>无法使用，所以你也无法实现计算文件行数之类的方法(因为你无法定位到首部)。</p><h2 id="bufio">bufio</h2><p>实现带缓存的I/O</p><h2 id="mysql">mysql</h2><p><a href="https://pkg.go.dev/database/sql#Rows.Next">sql package - database/sql - pkg.go.dev</a></p><pre><code class="language-go">import (    &quot;database/sql&quot;_ &quot;github.com/go-sql-driver/mysql&quot;)func main()&#123;mysqlConnect := fmt.Sprintf(&quot;%s:%s@tcp(%s)/mysql&quot;, conf.Database.Username, conf.Database.Password, conf.Database.Host)    if err != nil &#123;        errors.New(&quot;Error mysql connect&quot;)        return    &#125;    db.SetConnMaxLifetime(100*time.Second)  //最大连接周期，超过时间的连接就close    db.SetMaxOpenConns(10) //设置最大连接数    db.SetMaxIdleConns(5) //设置闲置连接    rows, err := db.Query(&quot;show full processlist&quot;)    if err != nil &#123;        errors.New(&quot;Error db Query&quot;)        return    &#125;      var Id,Time int64    var User, Host, Command, State string    var Db, Info sql.NullString    for rows.Next() &#123;        rows.Scan(&amp;Id, &amp;User, &amp;Host, &amp;Db, &amp;Command, &amp;Time, &amp;State, &amp;Info)    fmt.Printf(&quot;%d,%s,%s,%s,%s,%d,%s,\&quot;%s\&quot;\n&quot;, Id, User, Host, Db.String, Command, Time, State, Info.String&#125;</code></pre><p><code>rows.Next()</code>读取一行，并指向行尾</p><p><code>rows.Scan(dest ...interface&#123;&#125;)</code>将当前行的值逐个字段的复制给dest，dest是指针类型。dest顺序和当前行的字段顺序需要一一对应。</p><h2 id="gopkg-in-gomail-v2"><a href="http://gopkg.in/gomail.v2">gopkg.in/gomail.v2</a></h2><p><a href="https://pkg.go.dev/gopkg.in/gomail.v2">gomail package - gopkg.in/gomail.v2 - pkg.go.dev</a></p><pre><code class="language-go">func sendEmail(smtpServer string, smtpPort int, smtpUser string, smtpPassword string, receiveUser []string, subject string, content string, attach string) error &#123;    m := gomail.NewMessage()    m.SetHeader(&quot;From&quot;, smtpUser)    m.SetHeader(&quot;To&quot;, receiveUser...)    m.SetHeader(&quot;Subject&quot;, subject)    m.SetBody(&quot;text/html&quot;, content)    m.Attach(attach)    d := gomail.NewDialer(smtpServer, smtpPort, smtpUser, smtpPassword)    err := d.DialAndSend(m)    return err&#125;</code></pre><h2 id="gopkg-in-yaml-v3"><a href="http://gopkg.in/yaml.v3">gopkg.in/yaml.v3</a></h2><p><a href="https://pkg.go.dev/gopkg.in/yaml.v3">yaml package - gopkg.in/yaml.v3 - pkg.go.dev</a></p><p>配置文件示例</p><pre><code class="language-yaml">email:  smtpserver: smtp.feishu.cn  smtpport: 465  smtpuser:   smtppassword:   receiveuser:     - zhangsan@abc.com  subject:   content: database:  host:   username:   password:   timeout: </code></pre><pre><code class="language-go">//映射配置文件的结构体type Config struct &#123;    Email struct &#123;        Smtpserver string `json:&quot;smtpserver&quot;`        Smtpport int `yaml:&quot;smtpport&quot;`        Smtpuser string `json:&quot;smtpuser&quot;`        Smtppassword string `yaml:&quot;smtppassword&quot;`        Receiveuser []string `yaml:&quot;receiveuser&quot;`        Subject string `yaml:&quot;subject&quot;`        Content string `yaml:&quot;content&quot;`    &#125;    Database struct &#123;        Host string `yaml:&quot;host&quot;`        Username string `yaml:&quot;username&quot;`        Password string `yaml:&quot;password&quot;`        Timeout int64 `yaml:&quot;timeout&quot;`    &#125;&#125;//实现结构体方法 getConffunc (config *Config) getConf(confPath string) (*Config) &#123;    yamlFile, err := ioutil.ReadFile(confPath)    if err != nil &#123;        fmt.Println(&quot;Error reading config&quot;)    &#125;    err = yaml.Unmarshal(yamlFile, config)    if err != nil &#123;        fmt.Println(&quot;Error unmarshaling config&quot;)    &#125;    return config&#125;func main()&#123;    var config Config    conf := config.getConf(&quot;/to_path/config.yaml&quot;)    fmt.Println(conf.Email.Smtpserver)&#125;</code></pre><p>⚠️定义配置文件映射的结构体的时候，属性首字母要大写</p><h2 id="time">time</h2><p><a href="https://pkg.go.dev/time#Now">time package - time - pkg.go.dev</a></p><p>go采用一种很独特的方式来格式化时间，即：</p><p>2006数字代表年，如同常见的<code>%Y</code></p><p>01数字代表月，如同常见的<code>%m</code></p><p>02数字代表日</p><p>15数字代表时（24小时的下午三点）</p><p>04数字代表分</p><p>05数字代表秒</p><p>Z07数字代表时区(-07也可以)</p><p>整体记忆方法就是：01、02、15(03)、04、05、2006(06)、Z07(07)</p><pre><code class="language-go">time.Now().Format(&quot;2006.01.02 15:04:05 Z07&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络☞S5700配置攻击朔源</title>
      <link href="posts/5290b79d/"/>
      <url>posts/5290b79d/</url>
      
        <content type="html"><![CDATA[<h2 id="目的">目的</h2><p>判断网络攻击来源</p><h2 id="设置管理员ip为白名单">设置管理员ip为白名单</h2><pre><code class="language-bash">acl 3666rule 1 permit ip source 10.200.15.1 0.0.0.0</code></pre><h2 id="根据IP判断">根据IP判断</h2><pre><code class="language-bash">cpu-defend policy 1auto-defend enableauto-defend attack-packet sample 5auto-defend trace-type source-mac source-ip source-portvlanauto-defend protocol arp icmp dhcp telnetauto-defend threshold 60auto-defend alarm enableauto-defend alarm threshold 60auto-defend whitelist 1 acl 3666</code></pre><h2 id="根据端口判断">根据端口判断</h2><pre><code class="language-bash">cpu-defend policy 1auto-port-defend enableauto-port-defend attack-packet sample 5auto-port-defend protocol arp-request arp-reply dhcp icmpauto-port-defend protocol arp-request threshold 30auto-port-defend protocol arp-reply threshold 30auto-port-defend protocol dhcp threshold 30auto-port-defend protocol icmp threshold 30auto-port-defend alarm enableauto-port-defend whitelist 1 acl 3666</code></pre><h2 id="应用策略">应用策略</h2><pre><code class="language-bash">cpu-defend-policy 1 global</code></pre><h2 id="查看攻击来源ip">查看攻击来源ip</h2><pre><code class="language-bash">display auto-defend attack-source</code></pre><h2 id="查看攻击来源端口">查看攻击来源端口</h2><pre><code class="language-bash">display auto-port-defend attack-source</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> 网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 交换机 </tag>
            
            <tag> S5700 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞ubuntu20.04纯命令行网卡配置</title>
      <link href="posts/e4f7ce4c/"/>
      <url>posts/e4f7ce4c/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>netplan 用于屏蔽各种网络管理器，它可以让你用一份配置，就帮你把网络给配置好，而无需你考虑网络控制器如何使用。</p><p>20.04 已经默认安装了 netplan.</p><h2 id="记录当前网卡标识名">记录当前网卡标识名</h2><pre><code class="language-bash">ip addr show # ===1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: enp0s31f6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000    link/ether d8:9e:f3:31:83:c8 brd ff:ff:ff:ff:ff:ff</code></pre><p>例如上述中的 <code>enp0s31f6</code></p><h2 id="编辑配置文件">编辑配置文件</h2><pre><code class="language-bash">cat /etc/netplan/00-installer-config.yaml# ===# This is the network config written by 'subiquity'network:  ethernets:    enp0s31f6:      addresses: [10.200.10.2/24]      dhcp4: no      optional: true      gateway4: 10.200.10.1      nameservers:        addresses: [10.200.10.1]  version: 2  renderer: networkd</code></pre><h2 id="加载配置">加载配置</h2><pre><code class="language-bash">netplan applysystemctl restart networkd-dispatcher.service</code></pre><blockquote><p>之后如果再修改 ip， 再次执行 <code>netplay apply</code> 即可</p></blockquote><h2 id="参考">参考</h2><p><a href="https://netplan.io/examples/#configuration">https://netplan.io/examples/#configuration</a></p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ubuntu </tag>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞mysql慢sql预警</title>
      <link href="posts/ab1247fb/"/>
      <url>posts/ab1247fb/</url>
      
        <content type="html"><![CDATA[<h2 id="用途">用途</h2><p>检测数据库中执行缓慢的sql，并记录预警</p><h2 id="脚本">脚本</h2><pre><code class="language-bash">#!/bin/bash# by zyh# 检查数据库耗时过长的语句# 所需配置文件:# 1. userfile.txt 用于判断程序用户的sql是否超时，超时了杀死  ##用户名      密码    超时时间# 2. database.conf 需要show full processlist权限  ##Username=  ##Password=  ##Mysqlhostname=  ##Database=  ##Port=# bash start.sh database.conf 放到计划任务里# -------------------------------------------------------------------------------#--变量--basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`currtime=`date &quot;+%Y%m%d_%H%M%S&quot;`#Username=#Password=#Mysqlhostname=#Database=#Port=source $basedir/$1datadir=$basedir/data/$Database-$1/$currtime[[ -d $datadir ]] || mkdir -p $datadir#--main--mysql -s -r -u$Username -p$Password -h$Mysqlhostname -P$Port -e 'show full processlist' | grep -v 'Sleep' |grep -v 'system' | egrep '(Query|Execute)' | grep -i $'\tselect' &gt; $datadir/$Database.allawk -F'\t' 'BEGIN&#123;OFS=&quot;\t&quot;&#125;ARGIND==1&#123;warntime[$1]=$3;warnpwd[$1]=$2&#125; \                ARGIND==2&#123; \                        for( nametime in warntime ) &#123; \                                if( $2 == nametime &amp;&amp; $6 &gt; warntime[$2] ) &#123; \                                        print warnpwd[$2],$0;break \                                &#125; \                        &#125; \                &#125;' $basedir/userfile.txt $datadir/$Database.all &gt; $datadir/$Database.warncat $datadir/$Database.warn | while read warningpwd id warningname other;do        echo &quot;# $&#123;warningname&#125;: $id $other&quot;        echo &quot;mysql -s -r -u$warningname -p$warningpwd -h$Mysqlhostname -P$Port -e 'kill '&quot;$id&quot;'&quot;done &gt; $datadir/kill.sql[[ -s $datadir/kill.sql ]] &amp;&amp; python $&#123;basedir&#125;/sendmail.py '收件人邮箱地址' &quot;[SQL-TimeOut]-$Database&quot; &quot;($Mysqlhostname)The attachment content is timeout SQL information&quot; '抄送邮箱地址'  &quot;$datadir/kill.sql&quot; || rm -rf $datadir</code></pre><pre><code class="language-python">#!/usr/bin/python#coding:utf-8# by zyh# python sendmail.py '收件人邮箱地址'  '主题' '邮件内容' '抄送邮箱地址' '附件绝对路径'import smtplibfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartimport sysreload(sys)sys.setdefaultencoding('utf8')mail_host = 'smtp服务器地址'  # 例如 smtp.gmail.com:587mail_user = '发件人邮箱'      # 例如 it@gmail.commail_pass = '发件人密码'def send_mail(*args):    argsNum=len(args[0])    if argsNum == 4:        filename,to_list, subject, content=args[0]    elif argsNum == 5:        filename,to_list, subject, content, cc_list=args[0]    elif argsNum == 6:        filename,to_list, subject, content, cc_list, subfile=args[0]    me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;    #初始化邮件对象 msg    msg = MIMEMultipart()    msg['From'] = me    msg['to'] = to_list    emailList=to_list    # 主题    msg['Subject'] = Header(subject, 'utf-8').encode()    # 内容    msg.attach(MIMEText(content, 'plain', 'utf-8'))    # 抄送    if 'cc_list' in vars():        msg['Cc'] = cc_list        emailstring = to_list+','+cc_list        emailList=emailstring.split(&quot;,&quot;)    # 附件    if 'subfile' in vars():        att1 = MIMEText(open((&quot;%s&quot;%subfile),'rb').read(), 'base64', 'utf8')        att1[&quot;Content-Type&quot;] = 'text/plain'        att1[&quot;Content-Disposition&quot;] = 'attachment; filename='+subfile.split('/')[-1] # 这里的filename可以任意写，写什么名字，邮件中显示什么名字        msg.attach(att1)    # 发送    try:        print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)        print &quot;connect mail server suesscc&quot;        s.starttls()        s.login(mail_user,mail_pass)        print &quot;login mail server suesscc&quot;        s.sendmail(me,emailList,msg.as_string())        s.close()        return True    except Exception,e:        print &quot;%s\t%s&quot;%(to_list,str(e))        return Falseif __name__ == &quot;__main__&quot;:    if len(sys.argv) &lt; 4:        exit()    send_mail(sys.argv)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>web☞服务器承载能力计算</title>
      <link href="posts/7c44c2e8/"/>
      <url>posts/7c44c2e8/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>我们在评估一个系统需要多少个服务器来支撑的时候，一般会涉及到多个点。</p><ul><li>pv</li><li>qps</li><li>并发</li><li>平均响应时间</li></ul><p>如果你已经搜索过相关站点，你会看到一个公式：并发=qps*平均响应时间</p><p>说实话，这个公式很蛋疼，猛地一看让人一脸懵逼。这里我按照我的方式来说一下。</p><p>首先将公式变形， QPS = 工作进程数 / 平均响应时间</p><p>其次，换算两边时间单位都为秒级， QPS = （1秒 / 秒级平均响应时间）* 工作进程数</p><p>再次，将<code>并发</code>理解为系统的<code>工作进程</code>，也就是如果并发是100，那么你可以简单的理解为理想情况下100个工作进程同一时间只能处理100个请求。</p><p>那么，<code>1秒 / 秒级平均响应时间</code>就是指每个工作进程1秒能处理多少个请求。</p><p>所以，最终得到 <code>QPS</code>就是系统每秒可以处理的请求数。</p><p>如果你日PV是 300W，那么一般情况下，每天80%的请求量都集中在每天20%的时间里。因此我们可以计算出高峰时间的秒级请求量是 3000000 * 0.8  / （3600 * 24 * 0.2） = 139</p><p>因此，如果你的系统根据 <code>QPS = 工作进程数 / 平均响应时间</code> 得到的 QPS 大于 139，那么你的系统就可以顶住。</p><p>如果小于，那么你有两个选择：</p><ol><li>需要增加工作进程数，也就是增加服务器数量或者服务器里的工作进程</li><li>优化系统，降低平均响应时间</li></ol><blockquote><p>⚠️增加服务器的工作进程，并不一定会产生正向效应，因为工作进程越多，那么进程就可能频繁切换，反而导致工作进程不干活。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> web </tag>
            
            <tag> 承载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞16kubeadm集群升级</title>
      <link href="posts/7538d100/"/>
      <url>posts/7538d100/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/</a></p><p>分为两部分，升级核心节点和升级工作节点</p><h1>升级控制平面节点 Control Plane</h1><p>控制平面核心组件包含：</p><ul><li>kube-apiserver</li><li>etcd</li><li>kube-scheduler</li><li>kube-controller-manager</li><li>cloud-controller-manager</li></ul><h2 id="列出想升级的kube版本">列出想升级的kube版本</h2><p>这里以1.20版本为例</p><p>在核心节点上执行</p><pre><code class="language-bash">master_verions=1.20yum list --showduplicates kubeadm --disableexcludes=kubernetes | grep $&#123;master_verions&#125;===kubeadm.x86_64                       1.20.8-0                        @kuberneteskubeadm.x86_64                       1.20.0-0                        kuberneteskubeadm.x86_64                       1.20.1-0                        kuberneteskubeadm.x86_64                       1.20.2-0                        kuberneteskubeadm.x86_64                       1.20.4-0                        kuberneteskubeadm.x86_64                       1.20.5-0                        kuberneteskubeadm.x86_64                       1.20.6-0                        kuberneteskubeadm.x86_64                       1.20.7-0                        kuberneteskubeadm.x86_64                       1.20.8-0                        kubernetes</code></pre><p>–disableexcludes=kubernetes 只允许kubernetes库</p><h2 id="所有核心节点：提前下载好升级所需的镜像">所有核心节点：提前下载好升级所需的镜像</h2><blockquote><p>所有核心节点执行</p></blockquote><pre><code class="language-bash"># 选择主版本号对应的最新稳定版本full_version=1.20.8</code></pre><h3 id="列出版本所需的包">列出版本所需的包</h3><pre><code class="language-bash">kubeadm config images list --kubernetes-version=$&#123;full_version&#125;===k8s.gcr.io/kube-apiserver:v1.20.8k8s.gcr.io/kube-controller-manager:v1.20.8k8s.gcr.io/kube-scheduler:v1.20.8k8s.gcr.io/kube-proxy:v1.20.8k8s.gcr.io/pause:3.2k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns:1.7.0</code></pre><h3 id="构建包拉取脚本">构建包拉取脚本</h3><p>根据上面的输出版本，修改下面脚本中 <code>pause</code> <code>etcd</code> <code>coredns</code> 的版本号</p><pre><code class="language-bash">cat&gt;images-pull.sh&lt;&lt;EOF#!/bin/bash# kubeadm config images listimages=(kube-apiserver:v$&#123;full_version&#125;kube-controller-manager:v$&#123;full_version&#125;kube-scheduler:v$&#123;full_version&#125;kube-proxy:v$&#123;full_version&#125;pause:3.2etcd:3.4.13-0coredns:1.7.0)for imageName in \$&#123;images[@]&#125;;do    docker pull registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;    docker tag registry.aliyuncs.com/google_containers/\$&#123;imageName&#125; k8s.gcr.io/\$&#123;imageName&#125;    docker rmi registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;doneEOF</code></pre><pre><code class="language-bash">bash images-pull.sh</code></pre><blockquote><p>如果某些包无法下载，则需自行去 dockerhub 上找，并在下载完毕后，添加 k8s 自己的 tags，例如 coredns<br>docker pull coredns/coredns:1.6.7<br>docker tag coredns/coredns:1.6.7 <a href="http://k8s.gcr.io/coredns:1.6.7">k8s.gcr.io/coredns:1.6.7</a><br>docker rmi coredns/coredns:1.6.7</p></blockquote><h2 id="第一个核心节点：安装目标版本的kubeadm">第一个核心节点：安装目标版本的kubeadm</h2><p>升级计划是以kubeadm的版本为基准的，例如你当前安装的 kubeadm 的版本是 x，那么之后通过 kubeadm 命令列出的升级计划就是升级到 x 的最新稳定版。</p><p>因此，我们需要将 kubeadm 升级到目标版本</p><pre><code class="language-bash">yum install -y kubeadm-$&#123;full_version&#125; --disableexcludes=kubernetes</code></pre><h2 id="第一个核心节点：停止调度">第一个核心节点：停止调度</h2><p>假设核心节点有三个，分别是k8s01，k8s02，k8s03，选其一k8s01进行升级</p><h3 id="驱逐升级的节点">驱逐升级的节点</h3><p>在k8s01执行，将其驱逐</p><pre><code class="language-bash:">kubectl drain k8s01 --ignore-daemonsets===node/k8s01 already cordonedWARNING: ignoring DaemonSet-managed Pods: default/lxcfs-jcclq, kube-system/kube-flannel-ds-9nfkf, kube-system/kube-proxy-xrxb4, metallb-system/speaker-gz4wpevicting pod kube-system/coredns-6f5c7bbdfb-q7lbnpod/coredns-6f5c7bbdfb-q7lbn evictednode/k8s01 evicted</code></pre><p>–ignore-daemonsets 忽略 daemonsets，因为 daemonsets 会在驱逐节点上重建 pod 从而导致驱逐失败。</p><pre><code class="language-bash">kubectl get node===NAME    STATUS                     ROLES    AGE   VERSIONk8s01   Ready,SchedulingDisabled   master   40h   v1.18.6</code></pre><h2 id="第一个核心节点：列出升级计划">第一个核心节点：列出升级计划</h2><p>在k8s01列出升级计划</p><pre><code class="language-bash">kubeadm upgrade plan</code></pre><p>会输出三部分内容，第一步是当前集群信息 v1.19.12 和当前kubeadm版本对应的最新稳定版 v1.20.8</p><pre><code>[upgrade/config] Making sure the configuration is correct:[upgrade/config] Reading configuration from the cluster...[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'[preflight] Running pre-flight checks.[upgrade] Running cluster health checks[upgrade] Fetching available versions to upgrade to[upgrade/versions] Cluster version: v1.19.3[upgrade/versions] kubeadm version: v1.20.8I0623 14:13:26.566059   21021 version.go:254] remote version is much newer: v1.21.2; falling back to: stable-1.20[upgrade/versions] Latest stable version: v1.20.8[upgrade/versions] Latest stable version: v1.20.8[upgrade/versions] Latest version in the v1.19 series: v1.19.12[upgrade/versions] Latest version in the v1.19 series: v1.19.12</code></pre><p>第二部分是升级信息，升级信息又分为两部分。</p><ul><li>升级到当前版本 v1.19.3 的最新稳定版本 v1.19.12</li></ul><pre><code>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT   CURRENT       AVAILABLEkubelet     3 x v1.19.3   v1.19.12Upgrade to the latest version in the v1.19 series:COMPONENT                 CURRENT    AVAILABLEkube-apiserver            v1.19.3    v1.19.12kube-controller-manager   v1.19.3    v1.19.12kube-scheduler            v1.19.3    v1.19.12kube-proxy                v1.19.3    v1.19.12CoreDNS                   1.7.0      1.7.0etcd                      3.4.13-0   3.4.13-0You can now apply the upgrade by executing the following command:        kubeadm upgrade apply v1.19.12</code></pre><ul><li>升级到当前 kubeadm v1.20.8 版本对应的 kubernetes 的最新稳定版本 v1.20.8</li></ul><pre><code>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':COMPONENT   CURRENT       AVAILABLEkubelet     3 x v1.19.3   v1.20.8Upgrade to the latest stable version:COMPONENT                 CURRENT    AVAILABLEkube-apiserver            v1.19.3    v1.20.8kube-controller-manager   v1.19.3    v1.20.8kube-scheduler            v1.19.3    v1.20.8kube-proxy                v1.19.3    v1.20.8CoreDNS                   1.7.0      1.7.0etcd                      3.4.13-0   3.4.13-0You can now apply the upgrade by executing the following command:        kubeadm upgrade apply v1.20.8</code></pre><p>第三部分是手动更新部分</p><pre><code class="language-:">The table below shows the current state of component configs as understood by this version of kubeadm.Configs that have a &quot;yes&quot; mark in the &quot;MANUAL UPGRADE REQUIRED&quot; column require manual config upgrade orresetting to kubeadm defaults before a successful upgrade can be performed. The version to manuallyupgrade to is denoted in the &quot;PREFERRED VERSION&quot; column.API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIREDkubeproxy.config.k8s.io   v1alpha1          v1alpha1            nokubelet.config.k8s.io     v1beta1           v1beta1             no</code></pre><p>⭐️如果<code>MANUAL UPGRADE REQUIRED</code>标记是<code>yes</code>，则你需要手动进行升级。手动升级的前提是需要自行提供配置文件</p><pre><code class="language-bash">kubeadm upgrade apply --config &lt;配置文件&gt;</code></pre><p>另外，kubeadm upgrade 总是会刷新证书。</p><h2 id="第一个核心节点：升级">第一个核心节点：升级</h2><pre><code class="language-bash">kubeadm upgrade apply v1.20.8=== 期间会有一次确认[upgrade/version] You have chosen to change the cluster version to &quot;v1.20.8&quot;[upgrade/versions] Cluster version: v1.19.3[upgrade/versions] kubeadm version: v1.20.8[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y=== 静态pod升级，并备份的信息[upgrade/staticpods] Moved new manifest to &quot;/etc/kubernetes/manifests/etcd.yaml&quot; and backed up old manifest to &quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-06-23-14-21-18/etcd.yaml&quot;[apiclient] Found 3 Pods for label selector component=etcd[upgrade/staticpods] Component &quot;etcd&quot; upgraded successfully![upgrade/staticpods] Moved new manifest to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; and backed up old manifest to &quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-06-23-14-21-18/kube-apiserver.yaml&quot;[upgrade/staticpods] Component &quot;kube-apiserver&quot; upgraded successfully![upgrade/staticpods] Moved new manifest to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; and backed up old manifest to &quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-06-23-14-21-18/kube-controller-manager.yaml&quot;[upgrade/staticpods] Component &quot;kube-controller-manager&quot; upgraded successfully![upgrade/staticpods] Moved new manifest to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; and backed up old manifest to &quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2021-06-23-14-21-18/kube-scheduler.yaml&quot;[upgrade/staticpods] Component &quot;kube-scheduler&quot; upgraded successfully!===[upgrade/postupgrade] Applying label node-role.kubernetes.io/control-plane='' to Nodes with label node-role.kubernetes.io/master='' (deprecated)[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.20&quot; in namespace kube-system with the configuration for the kubelets in the cluster[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[addons] Applied essential addon: CoreDNS[endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address[addons] Applied essential addon: kube-proxy=== 提醒升级成功[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.20.8&quot;. Enjoy!=== 提醒若没有升级 kubelets 则需要继续升级[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.</code></pre><h2 id="第一个核心节点：升级网络插件">第一个核心节点：升级网络插件</h2><p>这里我用的是 flannel. 所以我只是简单的重新执行一边</p><pre><code class="language-bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre><blockquote><p>修改里面的 “Network”: “10.244.0.0/16”, 变更为你自己的 pod 网段，即 cm对象 kubeadm-config 里的 podSubnet 或者 kubeadm 初始化安装参数 pod-network-cidr</p></blockquote><pre><code class="language-bash">kubectl describe cm kubeadm-config -n kube-system===ClusterConfiguration:----apiServer:  extraArgs:    authorization-mode: Node,RBAC  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: k8sapi:8443controllerManager: &#123;&#125;dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: v1.20.8networking:  dnsDomain: cluster.local  podSubnet: 10.97.0.0/16  serviceSubnet: 10.96.0.0/16scheduler: &#123;&#125;</code></pre><pre><code class="language-bash">kubectl apply -f  kube-flannel.yml</code></pre><h2 id="第一个核心节点：解锁使其可以调度">第一个核心节点：解锁使其可以调度</h2><pre><code class="language-bash">kubectl uncordon k8s01kubectl get node===NAME    STATUS   ROLES                  AGE    VERSIONk8s01   Ready    control-plane,master   246d   v1.19.3k8s02   Ready    control-plane,master   246d   v1.19.3k8s03   Ready    control-plane,master   120d   v1.19.3</code></pre><p>确保k8s01处于Ready状态，这里暂时版本还是旧版本，因为你服务没有重启</p><h2 id="剩余核心节点：依次升级">剩余核心节点：依次升级</h2><p>分别在 k8s02 和 k8s03 上执行升级命令</p><blockquote><p>需要依次执行，不可并发执行</p></blockquote><pre><code class="language-bash">nodeHostname=yum install -y kubeadm-$&#123;full_version&#125; --disableexcludes=kuberneteskubectl drain $&#123;nodeHostname&#125; --ignore-daemonsetskubeadm upgrade nodekubectl uncordon $&#123;nodeHostname&#125;</code></pre><p>你只需执行上述命令，无需执行其它命令，除非升级出了问题。。。</p><h2 id="所有核心节点：升级-kubelet-和-kubectl">所有核心节点：升级 kubelet 和 kubectl</h2><pre><code class="language-bash">yum install -y kubelet-$&#123;full_version&#125; kubectl-$&#123;full_version&#125; --disableexcludes=kubernetes</code></pre><p>重启两个组件</p><pre><code class="language-bash">systemctl daemon-reloadsystemctl restart kubelet</code></pre><h2 id="所有核心节点：开启-kube-scheduler-和-kube-controller-manager-端口">所有核心节点：开启 kube-scheduler 和 kube-controller-manager 端口</h2><p>若发现<code>kubectl get cs</code> 两个服务无法连接，则可以看下配置端口是否为0，如果是则执行下列命令</p><pre><code class="language-bash">sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml</code></pre><p>🌟不一定需要执行，具体以实际环境为准</p><h2 id="检查升级结果">检查升级结果</h2><pre><code class="language-bash">kubectl get node===NAME    STATUS   ROLES                  AGE    VERSIONk8s01   Ready    control-plane,master   246d   v1.20.8k8s02   Ready    control-plane,master   246d   v1.20.8k8s03   Ready    control-plane,master   120d   v1.20.8</code></pre><pre><code class="language-bash">kubectl get cs===Warning: v1 ComponentStatus is deprecated in v1.19+NAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</code></pre><pre><code class="language-bash">kubectl get pod -n kube-system===NAME                            READY   STATUS    RESTARTS   AGEcoredns-74ff55c5b-2rc7x         1/1     Running   0          40mcoredns-74ff55c5b-tndb5         1/1     Running   0          4m56setcd-k8s01                      1/1     Running   0          40metcd-k8s02                      1/1     Running   0          10metcd-k8s03                      1/1     Running   0          4m35skube-apiserver-k8s01            1/1     Running   0          40mkube-apiserver-k8s02            1/1     Running   0          10mkube-apiserver-k8s03            1/1     Running   0          4m26skube-controller-manager-k8s01   1/1     Running   0          100skube-controller-manager-k8s02   1/1     Running   0          101skube-controller-manager-k8s03   1/1     Running   0          98skube-flannel-ds-42hng           1/1     Running   0          12mkube-flannel-ds-b42qj           1/1     Running   0          13mkube-flannel-ds-ss8w5           1/1     Running   0          11mkube-proxy-2xxpn                1/1     Running   0          40mkube-proxy-pg7j7                1/1     Running   0          40mkube-proxy-txh2t                1/1     Running   0          39mkube-scheduler-k8s01            1/1     Running   0          103skube-scheduler-k8s02            1/1     Running   0          98skube-scheduler-k8s03            1/1     Running   0          98s</code></pre><h1>升级工作节点 node</h1><p>我这里测试环境，核心节点和工作节点是重叠的，所以无需再升级工作节点。</p><pre><code class="language-bash">full_version=1.20.8nodeName=# 升级kubeadmyum install -y kubeadm-$&#123;appVersion&#125; --disableexcludes=kubernetes# 锁定nodekubectl drain $&#123;nodeName&#125; --ignore-daemonsets# 升级nodekubeadm upgrade node# 升级 kubelet 和 kubectlyum install -y kubelet-$&#123;appVersion&#125; kubectl-$&#123;appVersion&#125; --disableexcludes=kubernetes# 重启systemctl daemon-reloadsystemctl restart kubelet# 解锁nodekubectl uncordon $&#123;nodeName&#125;</code></pre><h1>升级失败</h1><blockquote><p>控制平面节点升级失败，且自动回滚也失败了的话，尝试下面的操作</p></blockquote><h2 id="控制平面节点：尝试强制升级">控制平面节点：尝试强制升级</h2><pre><code class="language-bash">full_version=kubeadm upgrade apply --force $&#123;appVersion&#125;</code></pre><h2 id="无论如何都失败，手动恢复">无论如何都失败，手动恢复</h2><p>升级前，kubernetes 会备份 etcd 和 静态pod的内容</p><p>对应关系如下：</p><p><code>/etc/kubernetes/tmp/kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt;/etcd</code>= <code>/var/lib/etcd/</code></p><p><code>/etc/kubernetes/tmp/kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;</code>= <code> /etc/kubernetes/manifests/</code></p><p>将等号左边的路径内容覆盖到等号右边，以便于静态pod重建</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> kubeadm </tag>
            
            <tag> 升级 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞15证书管理</title>
      <link href="posts/55da993d/"/>
      <url>posts/55da993d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>k8s默认的CA证书有效期是10年。默认签发的服务证书有效期是1年。</p><p>k8s支持自动轮换证书。</p><p>k8s支持轮换时候的证书有效期。不过最长是10年。（我这边设置为100年，没效果）</p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/</a></p><h2 id="检查当前控制面节点里证书">检查当前控制面节点里证书</h2><pre><code class="language-bash">kubeadm alpha certs check-expiration</code></pre><p>你可以通过此命令查看各类证书的有效期</p><h2 id="触发自动轮换证书的行为">触发自动轮换证书的行为</h2><p>当你通过kubeadm upgrade node命令进行升级或者通过control plane进行升级的时候，k8s会自动轮换证书。</p><h2 id="通过命令手动触发证书轮换">通过命令手动触发证书轮换</h2><p><code>kubeadm alpha certs renew all</code>你可以通过此命令进行所有证书轮换</p><p>⭐️请注意，如果你运行了高可用集群，则需要在所有的control-plane节点上运行此命令，也就是说主节点上运行。</p><h2 id="为kubelet配置证书自动轮转">为kubelet配置证书自动轮转</h2><p><a href="https://kubernetes.io/docs/tasks/tls/certificate-rotation/">https://kubernetes.io/docs/tasks/tls/certificate-rotation/</a></p><p>⭐️1.18和1.19有差异。1.18是已经默认开启的，不过默认的轮转的证书有效期是1年</p><p>使用自动轮转，需要开启两个服务的配置：</p><ul><li><p>开启kubelet启动标记：<code>--rotate-certificates=true</code></p><p>⭐️开启确认命令：<code>kubectl describe cm/kubelet-config-1.18 -n kube-system | grep 'rotateCertificates'</code></p><p>⭐️动态的修改kubelet <a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#reconfiguring-the-kubelet-on-a-running-node-in-your-cluster">https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/#reconfiguring-the-kubelet-on-a-running-node-in-your-cluster</a></p><pre><code class="language-bash"># 请先安装 jq 命令. 他是 epel 里的# 开启代理监听kubectl proxy --port=8001 # 导出kubelet配置NODE_NAME=&quot;节点名&quot;; curl -sSL &quot;http://localhost:8001/api/v1/nodes/$&#123;NODE_NAME&#125;/proxy/configz&quot; | jq '.kubeletconfig|.kind=&quot;KubeletConfiguration&quot;|.apiVersion=&quot;kubelet.config.k8s.io/v1beta1&quot;' &gt; kubelet_configz_$&#123;NODE_NAME&#125;# 编辑完配置，重新导入，并确认新配置对象名kubectl -n kube-system create configmap node-config-new --from-file=kubelet=kubelet_configz_$&#123;NODE_NAME&#125; --append-hash -o yaml# 调整每一个node对象，添加新的配置对象kubectl edit node $&#123;NODE_NAME&#125;===configSource:    configMap:        name: CONFIG_MAP_NAME # 将这个名字替换成新的配置对象名        namespace: kube-system        kubeletConfigKey: kubelet# 检查配置生效kubectl get node $&#123;NODE_NAME&#125; -o json | jq '.status.config'</code></pre></li><li><p>调整默认证书有效期</p><p>开启<code>kube-controller-manager</code>启动标记：<code>--cluster-signing-duration</code>，这个标记默认是1年，你可以改成10年 <code>87600h0m0s</code></p><p>修改位置：<code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code></p><p>修改生效：kube-controller-manager pod 将自动重建</p><p>⚠️如果是1.18，则启动标记是<code>--experimental-cluster-signing-duration</code></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> 证书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞14认证授权</title>
      <link href="posts/a5ed67a6/"/>
      <url>posts/a5ed67a6/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><h3 id="流程介绍">流程介绍</h3><p>这里是一个认证的一个基本流程：</p><p><img src="/posts/a5ed67a6/image-20201016114248549.png" alt="image-20201016114248549"></p><p>即： kubectl =&gt; 用户认证 =&gt; 用户授权 =&gt; 入口控制 -&gt; 资源对象</p><p>用户认证：检查递交信息包含的证书，用户名。确定是否是正常用户。</p><p>用户授权：给用户授权权限策略，授权方式有多种：ABAC mode, RBAC Mode, and Webhook mode。确定是否有权限。</p><p>入口控制器：具有特殊功能的过滤器，他们会把请求拦截下来，如果请求违反了过滤器的配置，则请求会被拒绝。确定是否有更精细的粒度控制。</p><blockquote><p>以上三阶段都是通过插件实现的。当某个阶段的某个插件授权通过后，就不会再需要此阶段的其它插件进行校验。</p></blockquote><h2 id="用户认证方式">用户认证方式</h2><h3 id="令牌认证">令牌认证</h3><p>kubectl 通过提交令牌给用户认证</p><h3 id="TSL认证">TSL认证</h3><p>kubectl 和 Api server 双向认证</p><h3 id="username-password认证">username/password认证</h3><h2 id="用户类型">用户类型</h2><p>当你从使用者角度来看的时候，k8s把用户分为<code>普通用户</code>和<code>服务用户</code>。普通用户对外（也就是上图里的Human），服务用户对内（也就是上图里的Pod）。</p><p>请记住，k8s并没有普通用户的<strong>实体对象</strong>。因此，只要你递交的用户拥有k8s集群内部CA所签发的证书，那么这个用户就会被rbac子系统认为是有效的。</p><h3 id="普通用户-useraccount">普通用户 useraccount</h3><h4 id="如何创建普通用户">如何创建普通用户</h4><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user">https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user</a></p><ol><li><p>创建私钥key, 证书请求csr</p><p>创建私钥会让你填写一些属性，这里有两个属性特别重要，分别是指定用户名的属性CN和指定用户组的属性O</p><p>例如创建用户<code>zyh</code>，群组<code>it</code></p><pre><code>openssl genrsa -out zyh.key 2048openssl req -new -key zyh.key -out zyh.csr -subj &quot;/CN=zyh/O=it&quot;</code></pre></li><li><p>构建k8s的CertificateSigningRequest对象</p><ul><li>csr文件进行base64编码</li></ul><pre><code class="language-bash">RequestStr=`cat zyh.csr | base64 | tr -d &quot;\n&quot;`</code></pre><ul><li>将编码后的内容写入 request 字段</li></ul><pre><code class="language-yaml">cat &lt;&lt;EOF | kubectl apply -f -apiVersion: certificates.k8s.io/v1kind: CertificateSigningRequestmetadata:  name: zyhspec:  groups:  - system:authenticated  request: $&#123;RequestStr&#125;  signerName: kubernetes.io/kube-apiserver-client  usages:  - client authEOF</code></pre><p>⭐️usages字段必须是<code>client auth</code></p><p>⭐️需要注意的是，证书签发默认只有1年有效期</p><p>🌟确保当前 csr 没有重名申请</p></li><li><p>批准证书请求，并获取证书</p><p>批准：</p><pre><code class="language-bash">kubectl get csrkubectl certificate approve zyh</code></pre><p>获取：</p><pre><code class="language-bash">kubectl get csr zyh -o jsonpath='&#123;.status.certificate&#125;' | base64 --decode &gt; zyh.crt</code></pre><p>⭐️检查证书有效期：​<code>openssl x509 -in zyh.crt -noout -dates</code></p><p>至此，一个普通用户就创建完毕了。但是它没有任何权限，还需要进行授权。</p></li></ol><h4 id="如何使用普通用户">如何使用普通用户</h4><ol><li><p>添加用户证书到kubectl配置</p><pre><code class="language-bash">kubectl config set-credentials zyh --client-key=zyh.key --client-certificate=zyh.crt --embed-certs=true</code></pre></li><li><p>设置用户上下文，方便进行用户切换</p><pre><code class="language-bash">kubectl config set-context zyh --cluster=kubernetes --user=zyh</code></pre></li><li><p>通过用户上下文进行用户切换</p><pre><code class="language-bash">kubectl config use-context zyh</code></pre></li></ol><p>用户的有效期，取决于你证书的有效期。</p><h3 id="服务用户-serviceaccount">服务用户 serviceaccount</h3><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#service-account-automation">https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#service-account-automation</a></p><p>用于Pod认证</p><h4 id="如何创建服务用户">如何创建服务用户</h4><pre><code class="language-bash">kubectl create serviceaccount admin</code></pre><blockquote><p>创建完毕后，对应的 secret.namespace 会生成一个新的对象：&lt;sa_name&gt;-token-$RANDOM，用来进行资源对象的认证，请注意这里是认证，不是授权</p></blockquote><h3 id="如何使用">如何使用</h3><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  labels:    run: cirros-6365  name: cirros-6365  namespace: defaultspec:  serviceAccountName: &lt;serviceAccount_name&gt;</code></pre><h2 id="用户授权">用户授权</h2><h3 id="用户授权方式">用户授权方式</h3><p>RBAC授权机制：基于角色的授权机制.</p><h3 id="授权步骤">授权步骤</h3><p>授权就是给这个用户绑定一个具体的角色。角色代表了一种权限集合。</p><ol><li><p>创建 role 角色对象资源，创建一个<code>developer</code>角色，权限是拥有 pod 资源的增删改查，且<strong>命名空间是developer</strong></p><pre><code class="language-bash">kubectl create ns developerkubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods --namespace=developer</code></pre></li><li><p>创建 rolebinding 角色绑定对象资源，且<strong>命名空间是developer</strong></p><pre><code class="language-bash">kubectl create rolebinding developer-binding-zyh --role=developer --user=zyh --namespace=developer</code></pre></li><li><p>测试权限</p><pre><code class="language-bash">kubectl get pod -n developer===No resources found in developer namespace.kubectl get svc -n developer===Error from server (Forbidden): services is forbidden: User &quot;zyh&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; in the namespace &quot;developer&quot;</code></pre></li></ol><p>通过上述创建过程，你可以发现用户创建和用户授权两个阶段是分离的。也就是说，你可以创建多个用户，然后用同一个角色绑定多个用户。</p><h3 id="对象总结">对象总结</h3><p>role 和 rolebinding 是 namespace 级别对象，clusterrole 和 clusterrolebinding 是集群级别对象。</p><p>你可以通过 rolebinding 将 user 和 role 关联起来，实现对一个 user 添加 namespace 级别的 role 权限。</p><p>你可以通过 clusterrolebinding 将 user 和 clusterrole 关联起来，实现对一个 user 添加是集群级别的 clusterrole 权限。</p><p>最特殊的是，你还可以通过 rolebinding 将 user 和 clusterrole 关联起来，实现对一个 user 添加 namespace 级别的 clusterrole 权限。</p><p>这种情况下，因为 clusterrole 是集群级别，因此你可以将多个 namespace 下的 rolebinding 和 user 关联到一个 clusterrole。此时，rolebinding 和 user 拥有的权限将自动被约束在各自的 namespace 下。</p><blockquote><p>当你把 user 替换为 group 的时候，将会授权某个组</p><p>当你把 user 替换为 serviceaccount 的时候，将会授权调用 spec.serviceAccountName 的 pod</p></blockquote><h2 id="请求的实际转化">请求的实际转化</h2><p>客户端对 api server 发起的请求，都会转化为 http 请求</p><p>例如发起一个针对 deployment.default 对象的请求，</p><p>其 request path 是：</p><p>http://<apiserver>:6443/apis/apps/v1/namespaces/default/deployments/my-deployment/</apiserver></p><blockquote><p>这里 /apps/v1 指的是 kubectl api-versions 列出的 versions。</p></blockquote><p>可以针对对象发起的http请求动作是：</p><p>GET/POST/PUT/DELETE</p><p>转化为API的动作是：（即你用kubectl调用的动作）</p><p>get/list/create/update/patch/watch/proxy/redirect/delete/deletecollection</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> authorization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞011异常处理</title>
      <link href="posts/2dab3d00/"/>
      <url>posts/2dab3d00/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>三个奇特的关键字<code>defer</code>/<code>recover</code>/<code>Panic</code></p><h2 id="defer">defer</h2><p><code>defer</code>用于当前函数作用域结束后，执行定义的代码</p><h3 id="写法">写法</h3><pre><code class="language-go">defer func()  // func来自其它函数defer func()&#123;&#125;  // func为新建函数defer func(args1)&#123;&#125;(args2) // args2是传入变量，args1是接受变量</code></pre><h3 id="规则">规则</h3><p>若当前函数存在return，则<code>defer</code>有可能修改return的值.</p><p>情况1：<code>defer</code>中的变量与函数返回值变量一致。</p><pre><code class="language-go">func test01()(p int)&#123;    defer func()&#123;        p++    &#125;()    return 0&#125;func main()&#123;    f := test01()&#125;</code></pre><p>上述例子中，函数体执行完毕后，p=0;</p><p>然后执行<code>defer</code>函数，因此，p=1.</p><p>所以，最终结果，f=1</p><p>情况2:<code>defer</code>与当前函数构成闭包。</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func test01(x int)(p int)&#123;    defer func(y int)&#123;        y++        p=x+y    &#125;(x)    return 0&#125;func main()&#123;    f := test01(2)    fmt.Println(f)&#125;</code></pre><p>上述例子中，函数体执行完毕后:</p><p>x=2</p><p>y=2</p><p>p=0</p><p>然后执行<code>defer</code>函数，因为<code>defer</code>占用了<code>test01</code>的形参<code>x</code>，所以导致当前作用域中，<code>x</code>没有被释放，因此，p=5</p><p>所以，最终结果，f=5</p><p><code>defer</code>后面代码涉及到的<code>形参</code>的赋值，在<code>defer</code>声明的时候就被确定.</p><p>多个defer存在的时候，按照<code>后进先出</code>原则，即倒叙执行</p><pre><code class="language-go">func main() &#123;    i := 0    defer fmt.Println(&quot;a:&quot;, i)    defer func(i int) &#123;        fmt.Println(&quot;b:&quot;, i)    &#125;(i)    defer func() &#123;        fmt.Println(&quot;c:&quot;, i)    &#125;()    i++&#125;</code></pre><p>上述例子中，main在执行的时候：</p><p>defer fmt.Println(“a:”, i) 被声明的同时，i赋值为0</p><p>defer func(i int) {}(i) 被声明的同时，i赋值为0</p><p>defer func() {}() 被声明的同时，i没有被赋值，因为这个i不是defer的形参，而是引用，所以这个i的生命周期在main执行完毕后依然存在</p><p>main执行完毕后：由<code>i++</code>得到i=1</p><p>先执行defer func() {}() ，得到 c:1</p><p>然后执行defer func(i int) {}(i)，需注意这里的i是形参，不是引用，得到b:0</p><p>最后执行defer fmt.Println(“a:”, i) ，需注意这里的i是形参，不是引用，得到a:0</p><p>所以，最终结果是：</p><pre><code>c: 1b: 0a: 0</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞010错误处理</title>
      <link href="posts/336ec0ac/"/>
      <url>posts/336ec0ac/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>Go语言检测和报告错误情况的方法是</p><ul><li>可能导致错误的函数将返回两个变量：一个值和一个错误代码，如果成功，则为nil；如果错误条件，则为== nil。</li><li>在函数调用之后检查错误。如果发生错误（ if error != nil），则停止执行实际功能（或必要时整个程序）。</li></ul><h2 id="例子">例子</h2><pre><code class="language-go">package mainimport (    &quot;fmt&quot;    &quot;errors&quot;    &quot;math&quot;)func Sqrt(i float64) (float64, error) &#123;    if i&lt;0 &#123;        return 0,errors.New(&quot;错误：负数的平方根&quot;)    &#125;    return math.Sqrt(i), nil&#125;func main()&#123;    var i float64    i=-64    q, err := Sqrt(i)    if err != nil &#123;        fmt.Println(err)    &#125;    fmt.Println(q)&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞009接口</title>
      <link href="posts/77a46e91/"/>
      <url>posts/77a46e91/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>接口和结构体一样，也是一种自定义数据类型，用于给某个实体对象提供一系列方法，</p><p>只不过接口本身不实现这些方法，而是由其它类型（例如结构体）来实现, 你必须实现接口里的所有方法，否则你将无法实现这个接口</p><p>接口类型变量，可以赋值为任何其它数据<code>类型</code>,例如结构体类型</p><h2 id="写法">写法</h2><p>定义方法结合的接口</p><blockquote><p>方法名必须唯一，且不能为空</p></blockquote><pre><code class="language-go">type &lt;inter_name&gt; interface &#123;    &lt;method_name1&gt; [return_type]    &lt;method_name2&gt; [return_type]    ...&#125;</code></pre><p>定义需要方法的结构体</p><pre><code class="language-go">type &lt;struct_name&gt; struct &#123;    var var_name1 var_type&#125;</code></pre><p>定义方法，因为结构体<code>需要</code>实现方法，所以<code>接收者</code>是结构体</p><pre><code class="language-go">func (s &lt;struct_name&gt;) &lt;method_name1&gt;() [return_type]&#123;    ...&#125;func (s &lt;struct_name&gt;) &lt;method_name2&gt;() [return_type]&#123;    ...&#125;</code></pre><blockquote><p>如果有额外的接口也拥有相同的方法，则结构体将一并实现.</p></blockquote><p>最后，接口可以嵌套</p><pre><code class="language-go">package mainimport &quot;fmt&quot;type zuofan interface &#123;    zhongcan()    xican()&#125;type kaiche interface &#123;    qiche()    diandongche()&#125;type jineng interface &#123;    zuofan    kaiche&#125;type ren struct &#123;    xingming string    xingbie string    nianling int&#125;func (r ren) zhongcan() &#123;    fmt.Printf(&quot;%s,%s,%d岁,学会了中餐\n&quot;, r.xingming, r.xingbie, r.nianling)&#125;func (r ren) xican() &#123;    fmt.Printf(&quot;%s,%s,%d岁,学会了西餐\n&quot;, r.xingming, r.xingbie, r.nianling)&#125;func (r ren) qiche() &#123;    fmt.Printf(&quot;%s,%s,%d岁,学会了汽车\n&quot;, r.xingming, r.xingbie, r.nianling)&#125;func (r ren) diandongche() &#123;    fmt.Printf(&quot;%s,%s,%d岁,学会了电动车\n&quot;, r.xingming, r.xingbie, r.nianling)&#125;func main()&#123;    var jn jineng    jn = ren&#123;xingming:&quot;张三&quot;, xingbie:&quot;男&quot;, nianling:20&#125;    jn.zhongcan()    jn.xican()    jn.qiche()    jn.diandongche()&#125;</code></pre><p>上述例子：</p><ol><li>声明接口<code>jineng</code>的变量<code>jn</code></li><li>给<code>jn</code>赋值类型，这里是<code>ren</code></li><li>赋值完毕后，<code>ren</code>的实体对象就与<code>jn</code>绑定在了一起</li><li>通过<code>jn</code>接口调用方法</li></ol>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞008结构体</title>
      <link href="posts/a705a91a/"/>
      <url>posts/a705a91a/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>结构体是多种类型的集合，它本身就是类型的一种，因此需要用<code>type</code>关键字来声明</p><p>鉴于上述的性质，常用来描述一个具有多种属性的类。</p><p>例如，人类，书本等。</p><h2 id="写法和使用">写法和使用</h2><p>当定义好一个struct后，就可以通过var声明struct类型了</p><pre><code class="language-go">package mainimport &quot;fmt&quot;type Person struct &#123;    name string    age int&#125;type Book struct &#123;    title string    author string&#125;func readBook(b *Book) &#123;    fmt.Printf(&quot;他们读取的书籍：%s\n&quot;, (*b).title)&#125;func main()&#123;    p1 := Person&#123;&quot;zhangsan&quot;, 20&#125;    p2 := Person&#123;name:&quot;lisi&quot;, age:10&#125;    b := Book&#123;title:&quot;魔兽世界&quot;, author:&quot;wangwu&quot;&#125;    fmt.Println(p1.name, p2.name)    readBook(&amp;b)&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞007函数-方法</title>
      <link href="posts/f8eb4b1b/"/>
      <url>posts/f8eb4b1b/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>一个函数是没有所有者的。</p><p>当一个函数有了接收的人后，它就叫<code>方法</code></p><p>在python中，一个类（接收者）定义方法，直接在类里写方法即可，只不过有一个<code>self</code>指向了它本身。</p><p>在go中，没有这个<code>self</code>，因此定义方法需要指出谁是<code>接收者</code></p><h2 id="定义">定义</h2><pre><code class="language-go">func (&lt;接收者&gt; &lt;接收者类型&gt;) 函数名() 函数返回类型&#123;    ...&#125;</code></pre><p>上面可以看出，方法和函数定义的区别，就是<code>函数名</code>前面添加了一个<code>(接收者 接收者类型)</code></p><h2 id="例子">例子</h2><pre><code class="language-go">package mainimport &quot;fmt&quot;type person struct &#123;    name string&#125;func (self person) change()&#123;    self.name = &quot;凡尔赛.&quot;+self.name    fmt.Printf(&quot;你的新名字可能变成了：%s\n&quot;, self.name)&#125;func main()&#123;    p := person&#123;name:&quot;张三&quot;&#125;    p.change()    fmt.Printf(&quot;你的新名字变更失败，它依然是：%s\n&quot;, p.name)&#125;</code></pre><p>上面的例子，有一个现象，就是<code>change()</code>并不能变更<code>person.name</code></p><p>因为方法的接收者是<code>值</code>接收者，所以<code>p.change()</code>其实是将p复制了一份传递给了方法的<code>self</code>.那么p和self的<code>内存地址</code>不一样，修改<code>self.name</code>当然就没有意义了。</p><p>那么将方法的接收者变更为指针，就没得问题了。</p><pre><code class="language-go">func (self *person) change()&#123;    self.name = &quot;凡尔赛.&quot;+self.name    fmt.Printf(&quot;你的新名字变成了：%s\n&quot;, self.name)&#125;func main()&#123;    p := person&#123;name:&quot;张三&quot;&#125;    (&amp;p).change()    fmt.Printf(&quot;你的新名字变更成功，它是：%s\n&quot;, p.name)&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞006函数-闭包</title>
      <link href="posts/318675c1/"/>
      <url>posts/318675c1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>构建闭包原则：</p><ol><li>函数A中还有一个函数B，并且函数A返回了函数B</li><li>函数B调用了函数A的形参</li><li>main函数里变量C调用函数A</li></ol><h2 id="基本原理">基本原理</h2><p>正常情况下，函数被调用执行完毕后，函数的形参将被释放回收。</p><p>但当函数是一个闭包后，这里假设是函数A。</p><p>此时<code>C=函数A</code>, 而函数A返还函数B，因此<code>C=函数B</code>，函数A已被调用完毕，按理说函数A的形参A应该被回收。</p><p>因为<code>C还存在</code>，故而函数B中调用的函数A的形参A无法被回收，因此，形参A就可以一直存活下来。</p><p>那么，每当你执行一次C，函数B就执行一次，形参A因为无法被回收，也会一直保持最新的数据。</p><h2 id="举例">举例</h2><pre><code class="language-go">package mainimport &quot;fmt&quot;func fca(x int) func(y int) int&#123;    return func(y int) int &#123;        x+=y        return x    &#125;&#125;func fcb(startNum,step,time int) &#123;    fmt.Printf(&quot;初始数(startNum)：%d, 步长(step)%d, 迈步次数(time)：%d\n&quot;,startNum, step, time)    fmt.Printf(&quot;nextNum被赋值，从而等同于函数func(y int) int&#123;&#125;, 此时func中引用了形参x:1\n&quot;)    nextNum := fca(startNum)    for i:=1;i&lt;=time;i++ &#123;        fmt.Printf(&quot;执行nextNum(step) &#123;x+=y&#125;, 返回x:%d\n&quot;,nextNum(step))        if i == 3 &#123;            fmt.Printf(&quot;nextNum释放，系统回收x, 从新赋值函数func(y int) int&#123;&#125;,\n&quot;)            nextNum = fca(startNum)        &#125;    &#125;&#125;func main()&#123;    fcb(1,2,5)&#125;===初始数(startNum)：1, 步长(step)2, 迈步次数(time)：5nextNum被赋值，从而等同于函数func(y int) int&#123;&#125;, 此时func中引用了形参x:1执行nextNum(step) &#123;x+=y&#125;, 返回x:3执行nextNum(step) &#123;x+=y&#125;, 返回x:5执行nextNum(step) &#123;x+=y&#125;, 返回x:7nextNum释放，系统回收x, 从新赋值函数func(y int) int&#123;&#125;,执行nextNum(step) &#123;x+=y&#125;, 返回x:3执行nextNum(step) &#123;x+=y&#125;, 返回x:5</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞004条件语句</title>
      <link href="posts/8a6b5455/"/>
      <url>posts/8a6b5455/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>常见的if就不说了</p><h2 id="switch">switch</h2><p>case后面的值匹配了变量，就执行对应代码，否则执行default代码</p><pre><code class="language-go">switch 变量/常量 &#123;    case 值1:        ...    case 值2, 值3:        ...    default:        ...&#125;</code></pre><p>条件为真，就执行对应代码，否则执行default代码</p><pre><code class="language-go">switch &#123;    case 条件1:        ...    case 条件2:        ...    default:        ...&#125;</code></pre><h3 id="switch特殊">switch特殊</h3><p><code>fallthrough</code>关键词</p><p>当某个case执行语句中包含<code>fallthrough</code>的话，则紧邻的下一条<code>case</code>不需要判断条件就会直接执行</p><h2 id="select">select</h2><p>select语句与switch类似，但它仅用来监听和channel有关的IO操作，当 IO 操作发生时，触发相应的动作。</p><p>那么既然case条件与channel有关，则case条件必然伴随着写入和读取chan类型变量</p><p>子句执行规则：</p><p>select判断所有case条件是否与chan有关：</p><p>与chan有关：</p><p>​判断case条件是否可完成chan的I/O操作：</p><p>​可完成：</p><p>​<code>随机</code>执行一个完成case条件的语句</p><p>​不可完成：</p><p>​有default子句：执行default子句</p><p>​无default子句：阻塞，直到出现一个可完成的case条件</p><p>与chan无关：</p><p>​直接报错</p><pre><code class="language-go">package mainimport (    &quot;fmt&quot;)func main() &#123;    ch1 := make(chan int,1)    for i := 0; i &lt; 10; i++&#123;        select &#123;        case x := &lt;- ch1:            fmt.Printf(&quot;ch1取出数据%d\n&quot;,x)        case ch1 &lt;- i:            fmt.Printf(&quot;ch1插入数据%d\n&quot;,i)        &#125;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞005函数</title>
      <link href="posts/98f86e16/"/>
      <url>posts/98f86e16/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>如果有参数，则需提供参数类型</p><p>如果有返回值，则需提供返回值类型</p><h2 id="结构">结构</h2><pre><code class="language-go">func &lt;func_name&gt; (&lt;args&gt; args_type) &lt;rtn_value_type&gt; &#123;    ...    return rtn_value&#125;</code></pre><h2 id="函数参数">函数参数</h2><p>函数参数是形参，调用函数并传递给函数的参数是实参，形参和实参类型要<code>一致</code></p><p>函数参数（形参）仅在函数体内生效</p><p>在函数没有被调用的时候，函数参数（形参）不会分配内存单元</p><p>当函数参数（形参）接收了一个实参<code>内存值</code>的时候，函数体内的<code>函数参数</code>将和<code>实参</code>没有任何关联</p><p>当函数参数（形参）接收了一个实参<code>内存地址</code>的时候，函数体内的<code>&amp;函数参数</code>将受到<code>实参</code>影响</p><h3 id="参数个数">参数个数</h3><p>参数可以有多个，只需要在形参类型前添加<code>...</code></p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main()&#123;    print(1,2,3,4,5)&#125;func print (a ...int) &#123;    for _,v := range a&#123;        fmt.Println(v)    &#125;&#125;</code></pre><blockquote><p><code>_</code> 表示接收无用的值</p></blockquote><h2 id="用例">用例</h2><pre><code class="language-go">package mainimport &quot;fmt&quot;func fcLen(yp *string) int &#123;    fmt.Printf(&quot;传递的形参是：%p,%s&quot;,yp, *yp) // %p标识指针变量    return len(*yp)&#125;func main()&#123;    a := &quot;abc&quot;    b := fcLen(&amp;a)    fmt.Println(b)                                            &#125;</code></pre><h2 id="递归函数">递归函数</h2><p>递归函数在嵌套的时候，需要一个退出条件，从而防止死循环</p><p>例如</p><pre><code class="language-go">func jiecheng(i int)int &#123;    if i &gt; 1 &#123;        result = i * jiecheng(i-1)        return result    &#125;    return 1&#125;</code></pre><h2 id="特殊函数">特殊函数</h2><p>make()用于chan、map以及切片创建</p><p>new(<type>)用于创建一个指向type内存地址的指针</type></p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞003运算符</title>
      <link href="posts/1a034363/"/>
      <url>posts/1a034363/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>常见的运算符就不说了，学过数学的都会。</p><h2 id="逻辑运算符">逻辑运算符</h2><p><code>A &amp;&amp; B</code> A和B都为真，则真</p><p><code>A || B</code> A和B任意为真，则为真</p><p><code>!A</code> A为真，则为假</p><p>当上述条件为真时，则执行下一步代码</p><h2 id="位运算符">位运算符</h2><p>将整数转为二进制后进行运算</p><p>二进制数：0为假，1为真</p><p>例如：</p><pre><code class="language-go">a,b := 2,4</code></pre><p>若<code>c := a &amp; b</code>按位与，按照<code>位数</code>分别进行<code>与</code>运算</p><pre><code>a： 0 1 0b： 1 0 0c： 0 0 0</code></pre><p>因此，最终 c = 0</p><p>若<code>c := a | b</code>按位或，按照<code>位数</code>分别进行<code>或</code>运算</p><pre><code>a： 0 1 0b： 1 0 0c： 1 1 0</code></pre><p>因此，最终 c = 6</p><p>若<code>c := a ^ b</code>按位异或，按照<code>位数</code>分别进行<code>异或</code>运算</p><p>异或：相同为0，不同为1</p><pre><code>a： 0 1 0b： 1 0 0c： 1 1 0</code></pre><p>因此，最终 c = 6</p><p>若<code>c := a &amp;^ b</code>，</p><p>则c等于将a中ab都为1的位写为0后的新a</p><pre><code>a： 0 1 0b： 1 0 0c： 0 1 0  # a中没有ab都为1的位，因此c的值依然是a</code></pre><p>若<code>c := a&lt;&lt;b</code>左移运算，即<code>a*2的b次方</code>。c等于32</p><p>若<code>c := a&gt;&gt;b</code>右移运算，即<code>a/2的b次方</code>，c等于0</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞002常量</title>
      <link href="posts/8c3e952e/"/>
      <url>posts/8c3e952e/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>常量只有一个元数据，就是内存地址存储的数据。</p><p>常量声明的同时必须赋值</p><h2 id="声明和赋值">声明和赋值</h2><blockquote><p>不能省略 const 关键词，省略了就没法判断是常量还是变量了</p></blockquote><h3 id="单类型">单类型</h3><pre><code class="language-go">const &lt;常量1&gt;,&lt;常量2&gt; &lt;常量类型&gt; = &lt;常量1的值&gt;, &lt;常量2的值&gt;</code></pre><h3 id="多类型">多类型</h3><p>根据变量值，自动判断变量类型</p><pre><code class="language-go">const &lt;常量1&gt;, &lt;常量2&gt; = &lt;常量1的值&gt;, &lt;常量2的值&gt;</code></pre><h3 id="单类型和多类型混合">单类型和多类型混合</h3><pre><code class="language-go">const (    &lt;常量1&gt;,&lt;常量11&gt; &lt;类型1&gt; = &lt;常量1的值&gt;, &lt;常量11的值&gt;    &lt;常量2&gt; &lt;类型2&gt; = &lt;常量2的值&gt;)</code></pre><p>上述写法中，如果常量2和常量1一样，则可以直接写常量2，省略其余。</p><pre><code class="language-go">const (    &lt;常量1&gt;,&lt;常量11&gt; &lt;类型1&gt; = &lt;常量1的值&gt;, &lt;常量11的值&gt;    &lt;常量2&gt;,&lt;常量22&gt;)</code></pre><p>需要注意的是，前两两行声明的常量数量需要对齐一致。</p><p>⚠️常量声明赋值的时候，常量的值只能是<code>string</code>,<code>int</code>,<code>bool</code>类型，如果你的值是<code>函数</code>返回，则这个<code>函数</code>必须是<code>内置函数</code></p><p>例如：</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func rtnlen(p string) (int) &#123;    return len(p)&#125;var (    code03 = &quot;abc&quot;    code04 = rtnlen(code03)    // 这里不会报错，因为变量没有 “必须为内置函数” 这个限制)const (    code06 = &quot;abc&quot;    code05 = rtnlen(code06)    // 这里会报错，因为 rtnlen() 不是内置函数，将其更换为 len() 就不会报错)func main()&#123;    fmt.Println(code05)    fmt.Println(code03, code04)&#125;</code></pre><h2 id="特殊常量iota">特殊常量iota</h2><p>有一个内置的特殊常量iota，每声明一个常量，它会+1，初始值是0</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>go☞001变量</title>
      <link href="posts/759b68d8/"/>
      <url>posts/759b68d8/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>变量有两个元数据，一是内存地址，一是内存地址存储的数据。</p><blockquote><p>内存地址也是一串数</p></blockquote><h2 id="声明类型">声明类型</h2><p>一些变量类型:</p><p>int</p><p>string</p><p>bool</p><p>chan  管道</p><p>[数组大小]数组值类型 数组</p><p>[]数组值类型 切片</p><p>map   字典</p><h2 id="变量类型">变量类型</h2><p>如果变量内存地址里存储的是<code>非内存地址</code>,则这类变量称之为<code>值类型</code>变量，也就是<code>普通</code>变量</p><p>如果变量内存地址里存储的是<code>另一个变量的内存地址</code>，则这类变量称之为<code>引用类型</code>变量，也叫<code>指针</code>变量</p><p>💁 变量的内存地址里不能存储变量自己的内存地址</p><h3 id="值变量：普通变量">值变量：普通变量</h3><p>方法1：</p><p>仅可以定义一种变量类型</p><pre><code class="language-go">var &lt;变量1&gt;,&lt;变量2&gt; &lt;变量类型&gt; = &lt;变量1的值&gt;, &lt;变量2的值&gt;</code></pre><blockquote><p>若变量的值没有提供，则系统会根据变量类型自动提供</p><p>例如:</p><p>int类型，默认为0</p><p>bool类型，默认为false</p><p>字符串类型，默认为&quot;&quot;</p><p>其余类型，基本都是nil  (nil和python中的NULL是一个意思)</p></blockquote><p>方法2:</p><p>可以同时定义多种类型</p><p>根据变量值，自动判断变量类型</p><pre><code class="language-go">var &lt;变量1&gt;, &lt;变量2&gt; = &lt;变量1的值&gt;, &lt;变量2的值&gt;</code></pre><p>方法3:</p><p>可以同时定义多种类型</p><p>根据变量值，自动判断变量类型</p><p>省略 var 声明关键词</p><p>⚠️只能用于函数体内!!!</p><pre><code class="language-go">&lt;变量1&gt;, &lt;变量2&gt; := &lt;变量1的值&gt;,&lt;变量2的值&gt;</code></pre><p>方法4：</p><p>因式分解，常用来在函数体外声明全局变量</p><p>当然，也可以在函数体内定义局部变量</p><pre><code class="language-go">var (    &lt;变量1&gt; &lt;类型1&gt; = &lt;变量1的值&gt;    &lt;变量2&gt; &lt;类型2&gt; = &lt;变量2的值&gt;)</code></pre><h3 id="引用变量：指针变量">引用变量：指针变量</h3><p>指针变量就是变量类型前加<code>*</code>，其它不变</p><p>变量的值必须是<code>内存地址</code></p><p><code>内存地址</code>的获取方法是<code>&amp;普通变量</code></p><p>例如:</p><pre><code class="language-go">var02 := &amp;var01</code></pre><h2 id="普通变量和指针变量的使用区别">普通变量和指针变量的使用区别</h2><p>通过老的普通变量赋值生成的新普通变量，两者之间因为内存地址不一样，因此没有任何关联</p><p>例如：</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;var var01 int = 123var var02 int = var01fmt.Printf(&quot;var01的值是:%d\nvar02的值是:%d\n&quot;, var01, var02)fmt.Printf(&quot;var01的内存地址是:%d\nvar02的内存地址是:%d\n&quot;, &amp;var01, &amp;var02)var01 = 456    fmt.Printf(&quot;var01的新值是:%d\nvar02的值不变:%d\n&quot;, var01, var02) &#125;===var01的值是:123var02的值是:123var01的内存地址是:824634941440var02的内存地址是:824634941448var01的新值是:456var02的值不变:123</code></pre><p>通过老的普通变量赋值生成的新指针变量，两者之间是引用关系</p><p>当普通变量被赋予新值后，对应的指针变量也可以输出新值，因为普通变量的内存地址始终没变</p><pre><code class="language-go">package mainimport &quot;fmt&quot;func main() &#123;var var01 int = 123var var02 *int = &amp;var01fmt.Printf(&quot;var01的值是:%d\nvar01的内存地址是:%d\n&quot;, var01, var02)fmt.Printf(&quot;var01的的值是（通过*var02获取）:%d\n&quot;, *var02)&#125;===var01的值是:123var01的内存地址（通过var02获取）:824633802928var01的值是（通过*var02获取）:123var01的新值是:456var01的内存地址（通过var02获取）:824633802928var01的新值是（通过*var02获取）:456</code></pre><h2 id="作用范围">作用范围</h2><p>函数内定义的是局部变量，生命周期仅在函数被调用期间</p><p>函数外定义的是全局变量，可以在任何地方使用</p><p>当两种变量名一致时，局部变量优先级更高</p><h2 id="数组">数组</h2><p>一维数组：<code>var &lt;var_name&gt; []&lt;type&gt;&#123;&#125;</code></p><p>二维数组：<code>var &lt;var_name&gt; [][]&lt;type&gt;&#123;&#125;</code></p><p>[] 表示数组长度，填入数字代表数组大小，填入<code>...</code>代表不确定数组大小由系统自动计算大小</p><p>不管是一维数组，还是二维数组，只能存在一种类型</p><p>数组一旦定义则不可变的</p><h3 id="例子">例子</h3><pre><code class="language-go">package mainimport &quot;fmt&quot;func main()&#123;    array01 := [3]int&#123;1,2,3&#125;    array02 := [3]int&#123;4,5,6&#125;    array03 := [][3]int&#123;&#125;                    array03 = append(array03, array01)    array03 = append(array03, array02)    for _,x := range(array03)&#123;        for _,y := range(x)&#123;            fmt.Println(y)        &#125;    &#125;&#125;</code></pre><p><code>array03</code>定义了一个不限行，限3列的数组，这里不限行<code>[]</code>的写法叫<code>切片</code></p><h2 id="切片">切片</h2><p>切片是可变的数组，可以通过<code>append(切片,追加数组)</code>来扩展<code>切片</code></p><h3 id="写法">写法</h3><p>基本写法</p><pre><code class="language-go">s1 := []int&#123;1,2,3&#125; // 初始化一个内置3元素，容量3的切片s1</code></pre><p>添加容量大小的写法</p><pre><code class="language-go">s1 := make([]int, &lt;len&gt;, &lt;cap&gt;) // &lt;len&gt;是定义切片内元素的长度，&lt;cap&gt;是定义切片容纳元素的上限数</code></pre><h3 id="常用函数">常用函数</h3><p><code>append(&lt;目标切片&gt;, &lt;填充切片&gt;)</code>，用来扩展目标切片</p><p><code>copy(&lt;目标切片&gt;, &lt;需复制切片&gt;)</code>，用来将切片复制到目标切片</p><h3 id="切片截取">切片截取</h3><pre><code class="language-go">s[:3] //截取0-2构建新切片, 忽略首位数字s[3:] //截取3-最后，构建新切片，忽略末尾数字</code></pre><h3 id="遍历输出">遍历输出</h3><p>用<code>range</code>关键字</p><p>如果是数组或者切片，它返回(元素序号，元素值)</p><p>如果是集合，它返回(key, value)</p><pre><code class="language-go">for _, i := range &lt;切片&gt;&#123;    fmt.Println(i)&#125;</code></pre><h2 id="集合">集合</h2><p>map 类似于 pythhon 的字典，由kv对组成，它是无序的，因此，在使用 range 返回的时候，无法决定返回顺序。</p><h3 id="写法-2">写法</h3><pre><code class="language-go">var m1 map[&lt;key_type&gt;]value_type</code></pre><p>例子：</p><pre><code class="language-go">m1 := map[string]int&#123;&quot;河南&quot;:1, &quot;郑州&quot;:2&#125;fmt.Println(m1[&quot;河南&quot;])fmt.Println(m1[&quot;郑州&quot;])===12</code></pre><h3 id="删除">删除</h3><p>delete(<map>, <key>) 从map中删除对应的kv</key></map></p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> go </category>
          
      </categories>
      
      
        <tags>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞09存储资源</title>
      <link href="posts/1080c6b6/"/>
      <url>posts/1080c6b6/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>在使用云服务的时候,我们创建磁盘,需要在存储服务中提交一个请求,里面包含了磁盘类型,磁盘大小.之后,我们再把这个申请的磁盘挂载到所需的计算资源上即可.</p><p>而在k8s中. 当使用存储的时候，我们会涉及到以下主要概念</p><p>构建存储的概念：</p><ul><li><p>volume # 这个概念指的是提供的存储服务，例如云商的云盘（aws-ebs，阿里云-云盘），也可以是nfs/cephfs。</p></li><li><p>pv 或者 storageclass # 这个概念你可以认为是云服务商的存储服务的接口（例如web控制台操作界面）。当然，你需要自己提供接口所需要的存储信息，否则，k8s就不知道应该如何使用 volume。</p></li><li><p>pvc # 这个概念你可以认为是你向云服务商存储接口提交的创建请求。通过 pvc，你就可以拿到一块可用的存储盘了。</p></li></ul><p>使用存储的概念：</p><ul><li>podtemplate.spec.volumes  # 这个概念你可以认为是云服务商中你将磁盘绑定到计算资源</li><li>podtemplate.spec.containers.volumeMounts  # 这个概念你可以认为是你登陆到云服务商中的计算资源中,并使用mount命令进行实际挂载.</li></ul><p>你可能会发现，从一个正常逻辑来说，一个volume如果要在计算资源中使用，它是需要格式化一个文件系统的，但是我上面的概念是直接就挂载成了一个目录。这是因为k8s已经在你通过pv资源帮你创建好了文件系统。当然你也可以通过将pv的模式改为块设备，不过这样一来，程序就需要确认是否可以识别块设备了。我想一般的应用程序是用不上这个类型的。<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode</a></p><p>最后，pv或者storageclass是一个集群范围的对象，pvc则是一个namespace级别的对象</p><h2 id="pv">pv</h2><p>pod 与 pv 与 pvc 之间是强依赖关系。在资源对象被调用的过程中，任何一个删除操作都不会立即执行，而是在生命周期结束之后才会执行。</p><p>分为静态和动态</p><p>静态pv: 需要先创建一定数量的pv, 才能通过pvc申请成功.</p><p>动态pv: 需要先构建供应商.主流的云商存储基本都有供应商, 区别在于是内置了,还是需要自行外部创建.然后再通过pvc去申请.</p><p>内置供应商列表: <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></p><p>外置供应商部署: <a href="https://github.com/kubernetes-retired/external-storage">https://github.com/kubernetes-retired/external-storage</a></p><h3 id="静态PV">静态PV</h3><p>静态PV, 用户无法自行通过pvc去申请, 因为存储管理员需要先创建好匹配容量的pv才可以.就如同你想构建一个云服务器，但需要先创建好一个云盘。</p><p>一个pv的例子, 这里以 nfs 为例:</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name:  nas-it-local-pv  # 定义 pv 名字, 会被 pvc 引用  namespace: itspec:  claimRef:    name: nas-it-local-pvc    namespace: it  capacity:    storage: 100Gi  # 定义大小  accessModes:  - ReadWriteMany   # 定义访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /mnt/data001/nfs-k8s   # 定义 nfs 共享路径    server: 10.200.16.250        # 定义 nfs 服务器地址</code></pre><ul><li><p>accessModes 决定了pod可以如何使用pv. 它有三种模式:</p><ol><li><p><strong>ReadWriteOnce</strong>(单pod读写)</p></li><li><p><strong>ReadOnlyMany</strong>(多pod只读)</p></li><li><p><strong>ReadWriteMany</strong>(多pod读写)</p><p>这里是官方列举的各种存储类型支持的模式: <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes</a></p></li></ol></li><li><p>persistentVolumeReclaimPolicy 有三种模式:</p><ol><li><p><strong>Retain</strong>(只删pvc)</p></li><li><p><strong>Delete</strong>(同时删除pv和pvc)</p></li><li><p><strong>Recycle</strong>(同时删除pv和pvc,并清空pv存储的资源).</p><p>详情见 <a href="#persistentVolumeReclaimPolicy">persistentVolumeReclaimPolicy</a></p></li></ol></li></ul><h3 id="静态PV对应的PVC">静态PV对应的PVC</h3><pre><code class="language-yaml">kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-it-local-pvc  namespace: itspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi  volumeName: nas-it-local-pv</code></pre><h3 id="可能会用到的卷类型：本地">可能会用到的卷类型：本地</h3><p>本地卷是一种静态卷，因为它强依赖具体的node，所以POD无需进行节点亲和性绑定。</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name: k8s01-pv-localspec:  capacity:    storage: 100Gi  volumeMode: Filesystem  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Delete  storageClassName: local-storage  local:    path: /mnt/disks/ssd1  nodeAffinity:    required:      nodeSelectorTerms:      - matchExpressions:        - key: kubernetes.io/hostname          operator: In          values:          - k8s01</code></pre><h3 id="动态PV">动态PV</h3><p>动态PV, 用户可以自行通过PVC向StroageClass申请资源. 就如同你在构建阿里云的ECS的时候，只需要告知使用多大的磁盘，云服务器创建的时候会自动申请。</p><p>动态PV的构建需要一个新的资源对象StroageClass插件。插件在设计的时候，都会根据volume类型拥有自己的一些特定参数。例如云服务的云盘一般都可以设置请求的磁盘类型。你可以在这里找到各种支持的volume类型以及它们的属性字段：<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner">https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner</a></p><p>不过，有些volume类型的供应商已经有对应的内置插件，有些则没有。</p><p>以网络存储nfs为例. nfs属于外置供应商，因此没有内置插件。所以需要先构建<strong>nfs-client</strong>插件。</p><p><a href="https://artifacthub.io/packages/helm/ckotzbauer/nfs-client-provisioner">nfs-client-provisioner 1.0.2 · helm/ckotzbauer (artifacthub.io)</a></p><p>⚠️ 需要注意的是，nfs-client正常运行需要两个条件：</p><ul><li><code>yum install nfs-utils -y</code></li><li>从 k8s 1.20版本开始，编辑 /etc/kubernetes/manifests/kube-apiserver.yaml 添加 <code>- --feature-gates=RemoveSelfLink=false</code></li></ul><p>安装命令：</p><pre><code class="language-yaml">## 国外源# helm repo add ckotzbauer https://ckotzbauer.github.io/helm-charts# helm repo update# helm install nfs-client ckotzbauer/nfs-client-provisioner --set storageClass.name=nfs-client --set nfs.server=10.200.10.4 --set nfs.path=/volume1/k8s --set storageClass.reclaimPolicy=Delete --set storageClass.archiveOnDelete=true --set storageClass.allowVolumeExpansion=true --set storageClass.defaultClass=true## 阿里源helm repo add apphub https://apphub.aliyuncs.comhelm repo updatehelm install nfs-client apphub/nfs-client-provisioner --set storageClass.name=nfs-client --set nfs.server=10.200.10.4 --set nfs.path=/volume1/k8s --set storageClass.reclaimPolicy=Delete --set storageClass.archiveOnDelete=true --set storageClass.allowVolumeExpansion=true --set storageClass.defaultClass=true</code></pre><blockquote><p>nfs.server是nfs服务器地址</p><p>nfs.path是nfs共享目录</p><p>parameters.archiveOnDelete 当回收策略在被执行的时候（<a href="#persistentVolumeReclaimPolicy">persistentVolumeReclaimPolicy</a>），pv删除的同时申请的存储资源是否要归档。false就是不归档直接删除。这里归档的意思就是将申请的存储资源的物理路径前加一个archived前缀，即：<code>archived-$&#123;NS_NAME&#125;-$&#123;PVC_NAME&#125;-$&#123;PV_NAME&#125;</code></p><p>storageClass.allowVolumeExpansion 开启卷扩展。这个选项可以让你动态的调整 pvc 请求的对象大小。</p></blockquote><p>关于nfs-client的各种配置参数,可以查看文档https://github.com/helm/charts/tree/master/stable/nfs-client-provisioner#configuration</p><p>nfs-client会自动帮你创建一个 StroageClass。</p><pre><code class="language-yaml">kubectl get StorageClass/nfs-client -o yamlallowVolumeExpansion: trueapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  annotations:    meta.helm.sh/release-name: nfs-client    meta.helm.sh/release-namespace: default  labels:    app: nfs-client-provisioner    app.kubernetes.io/managed-by: Helm    chart: nfs-client-provisioner-1.2.9    heritage: Helm    release: nfs-client  managedFields:  - apiVersion: storage.k8s.io/v1  ......  name: nfs-client  resourceVersion: &quot;9314040&quot;  selfLink: /apis/storage.k8s.io/v1/storageclasses/nfs-client  uid: 3d695640-0cba-4660-9c92-53d870649b2cparameters:  archiveOnDelete: &quot;false&quot;provisioner: cluster.local/nfs-client-nfs-client-provisionerreclaimPolicy: DeletevolumeBindingMode: Immediate</code></pre><p>你可以通过添加 <code>metadata.annotations.storageclass.kubernetes.io/is-default-class: true</code> 来将此存储类设置为集群默认存储类。这样，当pvc中没有显式的指定存储类的时候，将会用默认存储类。</p><p>在此例子中，你无法添加，不过你可以在创建之前通过 storageClass.defaultClass 选项设置。因为上述例子是通过helm创建的，创建完后存储类配置不可被修改。</p><h3 id="动态PV对应的PVC">动态PV对应的PVC</h3><pre><code class="language-yaml">kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-it-local-pvcspec:  storageClassName: nfs-client  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi</code></pre><p>在这里,我们通过 <code>spec.storageClassName</code>字段来调用名字叫 nfs-client 存储类。</p><h3 id="PV回收策略-persistentVolumeReclaimPolicy">PV回收策略 persistentVolumeReclaimPolicy</h3><p><strong>Retain</strong> 只删除 PVC, PV保留, PV不能被用于其它Pod. 如果想复用存储资源,则需重新构建PV和PVC。假设以aws角度来看，就是EC2删了，web控制台还能看到EBS。</p><ul><li>在静态PV下,PV的名字是固定的,因此删除PVC之后,可以采用原有的PV配置重建PV继而分配到原有的资源.</li><li>在动态PV下,PV的名字是动态生成的, 因此删除PVC之后, 当PVC重新申请资源的时候, PV默认将会分配到一个新的资源路径.</li></ul><p>⭐️对于Retain策略，如果你想精准的将新pvc绑定到某个残留的pv，则可以参照[保留一个静态PV, 并告知pvc强制请求pv](#保留一个静态PV, 并告知pvc强制请求pv)</p><p><strong>Delete</strong> 同时删除PV和PVC. 存储资源是否删除取决于每一个StorageClass中parameters的定义。</p><ul><li>当StorageClass是nfs-client时，开启 <code>parameters.archiveOnDelete=true</code> ，就可以定义不删除而是将其归档。假设以aws角度来看，就是web控制台里的EC2和EBS都没了，但是aws在后台还偷偷的帮你将EBS的里的数据归档保存了。</li><li>当StorageClass是微软/aws/阿里云的云盘的时候，一般默认是删除云盘。从而避免产生大量闲置云盘。</li></ul><p><strong>Recycle</strong> 删除创建的所有对象和存储资源.</p><ul><li>当前只有nfs和hostPath支持.</li></ul><h3 id="存储资源物理层面回收">存储资源物理层面回收</h3><p>k8s官方建议不要使用回收策略 <strong>Recycle</strong>, 因为它无法反悔。k8s建议管理员自己去删除. 例如通过一个pod去删除.pod例子如下:</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: pv-recycler  namespace: defaultspec:  restartPolicy: Never  volumes:  - name: vol    hostPath:      path: /any/path/it/will/be/replaced   # 这里应该填写需要删除的内容根路径. 根路径应该是node本地可以访问的物理资源地址.  containers:  - name: pv-recycler    image: &quot;k8s.gcr.io/busybox&quot;    command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&quot;$(ls -A /scrub)\&quot; || exit 1&quot;]    volumeMounts:    - name: vol      mountPath: /scrub</code></pre><p>上述例子会构建一个 pod，pod会将本地的 /any/path/it/will/be/replaced 路径里的内容都删除掉。</p><h3 id="如何保留一个静态PV-并告知pvc强制请求pv">如何保留一个静态PV, 并告知pvc强制请求pv</h3><p>默认情况下，pvc找静态pv是自动的，例如会自动找大小匹配的pv。这可能有时候不符合我们的需求。例如我们想给一个数据库构建一个固定强依赖的pv。</p><p>此时需要在构建pv的时候，添加 <code>spec.claimRef</code>。例如：</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumemetadata:  name: foo-pvspec:  claimRef:    name: foo-pvc    namespace: foo</code></pre><p>这个例子是将 foo-pv 强制保留给 foo-pvc. 也就是说其它 pvc 无法使用 foo-pv。</p><p>之后，我们在 pvc 中添加 <code>spec: volumeName</code> 。例如：</p><pre><code class="language-yaml">apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: foo-pvc  namespace: foospec:  volumeName: foo-pv</code></pre><h3 id="在线扩缩（仅支持动态pv）">在线扩缩（仅支持动态pv）</h3><p>要实现在线调整大小，需要满足下列条件：</p><ol><li><p>feature-gates的<code>ExpandInUsePersistentVolumes</code>被开启。它在1.15版本以后默认开启。</p></li><li><p>StorageClass的<code>allowVolumeExpansion</code>被开启。这意味着必须是动态pv。</p></li><li><p>文件系统是XFS, Ext3, or Ext4。这个需要StorageClass的支持。一般来说volume属于块设备类型的资源才支持，比如aws的ebs。</p><blockquote><p>你可以在这里确认StorageClass是否支持fstype属性。<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters">https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters</a></p></blockquote></li></ol><p>满足上述条件后，你就可以通过 pvc 修改存储大小了。即时 pvc 正在被使用。</p><h3 id="局限性">局限性</h3><p>当一个 pod 同时申请多个 volume 的时候，可能会出现 volume-A 申请成功，volume-B 因物理容量不够申请失败的问题，此时 pod 将卡住。这种情况下，需要手动介入去清理。</p><h3 id="手动介入">手动介入</h3><p>需要我们手动的时候，都是资源申请失败或者不小心删除了PVC之类的。而不管是什么，我们第一目的是数据不丢。</p><p>因此，我们在清理故障对象资源的时候，应该遵循下列步骤：</p><ol><li>将pv的回收策略<code>persistentVolumeReclaimPolicy</code>定义为<code>Retain</code></li><li>删除pvc，这个时候 pv 对象将会保留，但是 pv 状态会变成 Released，这个状态下 pv 无法再被使用</li><li>删除pv对象字段<code>spec.claimRef</code>，从而将pv与pvc解绑，从而使 pv 状态变为 Available</li><li>重建pvc，并将pvc对象字段<code>volumeName</code>设置为pv的名字</li><li>恢复pv的回收策略。</li></ol><h2 id="PVC如何与POD结合使用">PVC如何与POD结合使用</h2><p>这里以上述例子中的 nfs 卷类型为例。</p><pre><code class="language-yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: nas-it-local-dep  namespace: it  labels:    app: nas-it-localspec:  serviceName: nas-it-local-vsftpd  revisionHistoryLimit: 10  replicas: 1  selector:    matchLabels:      app: nas-it-local  template:    metadata:      labels:        app: nas-it-local    spec:      hostNetwork: true      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io/hostname                operator: In                values:  - 容器                - k8s01      containers:      - name: vsftp        image: fauria/vsftpd        ports:        - containerPort: 20        - containerPort: 21        env:        - name: LOG_STDOUT          value: STDOUT        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: nas-it-local-vol          subPath: nas.it.local/data          mountPath: /home/vsftpd        - name: nas-it-local-vc          subPath: vsftpd.conf          mountPath: /etc/vsftpd/vsftpd.conf          readOnly: true        - name: nas-it-local-vc          subPath: virtual_users.txt          mountPath: /etc/vsftpd/virtual_users.txt          readOnly: true        - name: nas-it-local-vc          subPath: admin          mountPath: /etc/vsftpd/virtual/admin          readOnly: true        - name: nas-it-local-vc          subPath: yuangong          mountPath: /etc/vsftpd/virtual/yuangong          readOnly: true      volumes:      - name: nas-it-local-vol        persistentVolumeClaim:          claimName: nas-it-local-pvc      - name: nas-it-local-vc        configMap:          name: nas-it-local-cm</code></pre><ol><li><p>创建 <code>volumes：nas-it-local-vol</code>，并绑定 <code>PVC：nas-it-local-pvc</code></p><pre><code class="language-yaml">      volumes:      - name: nas-it-local-vol        persistentVolumeClaim:          claimName: nas-it-local-pvc</code></pre></li><li><p>创建 <code>volumeMounts</code>挂载 <code>volumes：nas-it-local-vol</code></p><p>这里将 nfs 共享目录中<code>nas.it.local/data</code>里的数据挂载到容器里的 <code>/home/vsftpd</code>路径</p><pre><code class="language-yaml">        volumeMounts:        - name: nas-it-local-vol          subPath: nas.it.local/data          mountPath: /home/vsftpd</code></pre><p>⭐️subPath不存在的时候，nfs 共享目录会自动创建。</p></li></ol><hr><h2 id="持久卷与临时卷">持久卷与临时卷</h2><p>以上方式都是持久卷方式，k8s还有一种临时卷，用来进行非必须的数据存储。例如缓存/会话一类的。临时卷数据将随着POD生命周期的终止而终止。</p><p>k8s支持的临时卷主要是这两种：</p><ul><li><p>emptyDir</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: test-pdspec:  containers:  - image: k8s.gcr.io/test-webserver    name: test-container    volumeMounts:    - mountPath: /cache      name: cache-volume  volumes:  - name: cache-volume    emptyDir: &#123;&#125;</code></pre><p>emptyDir还支持构建一个内存层缓存，也就是 tmpfs。你可以通过<code>emptyDir.medium: &quot;Memory&quot;</code>来开启。</p></li><li><p>generic ephemeral volumes 这一种是更加灵活的emptyDir，可以是网络存储，如果插件支持，则各种持久卷的特性它都有可能支持。当前1.18不支持，从1.19开始支持，1.19也是alpha，默认不开启。</p><p>具体使用看https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装库☞vsftpd</title>
      <link href="posts/894f9595/"/>
      <url>posts/894f9595/</url>
      
        <content type="html"><![CDATA[<h2 id="配置文件">配置文件</h2><pre><code class="language-yaml">apiVersion: apps/v1kind: StatefulSetmetadata:  name: nas-it-local-dep  namespace: it  labels:    app: nas-it-localspec:  serviceName: nas-it-local-vsftpd  revisionHistoryLimit: 10  replicas: 1  selector:    matchLabels:      app: nas-it-local  template:    metadata:      labels:        app: nas-it-local    spec:      hostNetwork: true      affinity:        nodeAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            nodeSelectorTerms:            - matchExpressions:              - key: kubernetes.io/hostname                operator: In                values:                - k8s01  # 这里绑定 pod 所在节点      containers:      - name: vsftp        image: fauria/vsftpd        ports:        - containerPort: 20        - containerPort: 21        env:        - name: LOG_STDOUT          value: STDOUT        resources:          requests:            memory: &quot;128Mi&quot;            cpu: &quot;125m&quot;          limits:            memory: &quot;512Mi&quot;            cpu: &quot;250m&quot;        volumeMounts:        - name: nas-it-local-vol          subPath: nas.it.local/data          mountPath: /home/vsftpd        - name: nas-it-local-vc          subPath: vsftpd.conf          mountPath: /etc/vsftpd/vsftpd.conf          readOnly: true        - name: nas-it-local-vc          subPath: virtual_users.txt          mountPath: /etc/vsftpd/virtual_users.txt          readOnly: true        - name: nas-it-local-vc          subPath: admin          mountPath: /etc/vsftpd/virtual/admin          readOnly: true        - name: nas-it-local-vc          subPath: yuangong          mountPath: /etc/vsftpd/virtual/yuangong          readOnly: true      volumes:      - name: nas-it-local-vol        persistentVolumeClaim:          claimName: nas-it-local-pvc      - name: nas-it-local-vc        configMap:          name: nas-it-local-cm---kind: ConfigMapapiVersion: v1metadata:  name: nas-it-local-cm  namespace: itdata:  vsftpd.conf: |    # Run in the foreground to keep the container running:    background=NO    # Allow anonymous FTP? (Beware - allowed by default if you comment this out).    anonymous_enable=NO    # Uncomment this to allow local users to log in.    local_enable=YES    ## Enable virtual users    guest_enable=YES    ## Virtual users will use the same permissions as anonymous    virtual_use_local_privs=YES    # Uncomment this to enable any form of FTP write command.    write_enable=YES    ## PAM file name    pam_service_name=vsftpd_virtual    ## Home Directory for virtual users    user_sub_token=$USER    local_root=/home/vsftpd/$USER    user_config_dir=/etc/vsftpd/virtual    # You may specify an explicit list of local users to chroot() to their home    # directory. If chroot_local_user is YES, then this list becomes a list of    # users to NOT chroot().    chroot_local_user=YES    # Workaround chroot check.    # See https://www.benscobie.com/fixing-500-oops-vsftpd-refusing-to-run-with-writable-root-inside-chroot/    # and http://serverfault.com/questions/362619/why-is-the-chroot-local-user-of-vsftpd-insecure    allow_writeable_chroot=YES    ## Hide ids from user    hide_ids=YES    ## Enable logging    xferlog_enable=YES    xferlog_file=/var/log/vsftpd/vsftpd.log    ## Enable active mode    port_enable=YES    connect_from_port_20=YES    ftp_data_port=20    ## Disable seccomp filter sanboxing    seccomp_sandbox=NO    ### Variables set at container runtime    pasv_address=10.200.16.10    pasv_max_port=21110    pasv_min_port=21100    pasv_addr_resolve=NO    pasv_enable=YES    file_open_mode=0666    local_umask=077    xferlog_std_format=NO    reverse_lookup_enable=YES    pasv_promiscuous=NO    port_promiscuous=NO  virtual_users.txt: |    admin    admin    yuangong    yuangong  admin: |    local_root=/home/vsftpd    anon_world_readable_only=NO    write_enable=YES    anon_mkdir_write_enable=YES    anon_upload_enable=YES    anon_other_write_enable=YES  yuangong: |    local_root=/home/vsftpd/yuangong    anon_world_readable_only=NO    write_enable=NO    anon_mkdir_write_enable=NO    anon_upload_enable=NO    anon_other_write_enable=NO---apiVersion: v1kind: PersistentVolumemetadata:  name:  nas-it-local-pv  # 定义 pv 名字, 会被 pvc 引用  namespace: itspec:  claimRef:    name: nas-it-local-pvc    namespace: it  capacity:    storage: 100Gi  # 定义大小  accessModes:  - ReadWriteMany   # 定义访问模式  persistentVolumeReclaimPolicy: Retain   # 定义pvc删除后的策略  nfs:    path: /mnt/data001/nfs-k8s   # 定义 nfs 共享路径    server: 10.200.16.250        # 定义 nfs 服务器地址---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: nas-it-local-pvc  namespace: itspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 100Gi  volumeName: nas-it-local-pv</code></pre><ul><li>这是一个<strong>主动模式</strong>的 ftp. 鉴于 k8s 特殊的网络环境.以及端口的繁琐配置. 所以采用主动模式, 可以让我们更舒服一些.</li><li>pv和pvc方面根据你自己的环境自行调整.</li><li>你需要自行将pod强制绑定到某个物理节点的ip上. 避免因pod重构时漂移到其它ip. 配置文件里绑定的是k8s01</li></ul><h2 id="用户创建">用户创建</h2><p>上述配置会创建admin和yuangong两个用户。</p><p>admin用户拥有所有权。根目录是/home/vsftpd</p><p>yuangong用户只有下载权限。根目录是/home/vsftpd/yuangong</p><h3 id="添加新用户">添加新用户</h3><ul><li><p>添加用户名和密码到 virtual_users.txt 配置</p></li><li><p>添加用户配置到cm对象中</p><pre><code class="language-yaml">  我是用户名: |    local_root=/home/vsftpd/&lt;我是用户名&gt;    anon_world_readable_only=NO    write_enable=YES    anon_mkdir_write_enable=YES    anon_upload_enable=YES    anon_other_write_enable=YES</code></pre></li><li><p>创建用户目录 /home/vsftpd/&lt;我是用户名&gt;</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vsftpd </tag>
            
            <tag> 安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞monit</title>
      <link href="posts/ae41ae33/"/>
      <url>posts/ae41ae33/</url>
      
        <content type="html"><![CDATA[<h2 id="安装">安装</h2><pre><code class="language-bash">yum install monit -y</code></pre><h2 id="主配置">主配置</h2><pre><code class="language-bash"># vim /etc/monitrcset daemon  10 # 调整周期</code></pre><p>⭐️官方配置文档: <a href="https://mmonit.com/monit/documentation/monit.html">https://mmonit.com/monit/documentation/monit.html</a></p><h2 id="程序保活配置">程序保活配置</h2><p>默认存放位置: /etc/monit.d/</p><p>这里以 pidfile 检测方式为例</p><p>nginx的配置:</p><pre><code class="language-bash">check process nginx with pidfile /var/run/nginx.pid  start program = &quot;/usr/sbin/nginx&quot;  stop program = &quot;/usr/bin/killall nginx&quot;if changed pid then alert</code></pre><p>上述nginx配置的意思:</p><ol><li><p>检测方式: pidfile</p></li><li><p>开关程序路径</p></li><li><p>发现pid改变,则触发动作: 预警</p></li></ol><p>tomcat的配置:</p><pre><code class="language-bash">check process tomcat-8080 with pidfile /opt/tomcat/tomcat-8080/bin/tomcat.pid  start program = &quot;/usr/bin/bash /opt/tomcat/tomcat-8080/bin/startup.sh&quot;    as uid &quot;root&quot; and gid &quot;root&quot;  stop program = &quot;/usr/bin/ps aux | /usr/bin/grep '/opt/tomcat/tomcat-8080/bin/bootstrap.jar' | /usr/bin/awk '&#123;print &quot;kill -9 &quot;$2&#125;' | /usr/bin/bash&quot;    as uid &quot;root&quot; and gid &quot;root&quot;if failed port 8080 then alert</code></pre><p>上述tomcat配置的意思:</p><ol><li>检测方式: pidfile</li><li>开关程序路径, 并且执行的时候需要以 root 权限执行</li><li>发现 8080 不通, 则触发动作: 预警</li></ol><p>🌟需要注意的是, 所有的文件路径都需要是绝对路径, 包括命令.</p><h2 id="启动-关闭">启动/关闭</h2><pre><code class="language-bash">systemctl start/stop/reload monit</code></pre><p>🌟当monit启动之后, 它就会加载保活配置, 并进行检测.</p><h2 id="日志">日志</h2><pre><code>/var/log/monit.log</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> monit </category>
          
      </categories>
      
      
        <tags>
            
            <tag> monit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞13高级版负载均衡ingress</title>
      <link href="posts/7627a41d/"/>
      <url>posts/7627a41d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>ingress 可以提供负载均衡/SSL管理/虚拟命名主机/path路由. 它的目的是增加7层流量分发等负载功能. 你可以通过 Ingress 将请求转发到各个 Service 对象.</p><p>但是ingress本身并不是规则的执行者. 它只是记录规则, 规则执行者由ingress-controller实现.</p><p>ingress-controller 的实现有很多，具体可以查看页面</p><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers/">https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers/</a></p><p>例如：</p><ul><li>nginx ingress 基于 nginx 的 ingress 控制器</li><li>istio ingress 基于 istio 的 ingress 控制器</li><li>traefik ingress 基于 traefik 的 ingress 控制器</li></ul><h2 id="特点">特点</h2><ul><li><p>ingress 提供规则, 它需要和后端业务位于同一个ns中.</p></li><li><p>ingress controller 实现规则, 它可以单独是一个ns.因为它会自动去找那些注解含有自己的ingress.</p></li><li><p>ingress 针对的主要是 http 和 https. 这些之外的端口暴漏你应该使用 Service.Type=NodePort 或者 Service.Type=LoadBalancer</p></li></ul><h2 id="流量路径">流量路径</h2><p>internet =&gt; Ingress =&gt; Service</p><p><img src="/posts/7627a41d/image-20201023110559218.png" alt="image-20201023110559218"></p><h2 id="ingress">ingress</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></p><h3 id="以nginx-ingress控制器来构建一个例子">以nginx ingress控制器来构建一个例子</h3><p>client =&gt; <a href="http://one.foo.com/one">http://one.foo.com/one</a> =&gt; Ingress = &gt; Service(one):8001 =&gt; pod</p><p>client =&gt; http://*.foo.com/other =&gt; Ingress  =&gt; Service(other):8002 =&gt; pod</p><p>需要注意的是，在下列 ingress 配置中，通过 annotations 注解来调用 nginx ingress 控制器。不同的 ingress 控制器的注解配置有所不同，具体以控制器本身说明文档为基准。</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: minimal-ingress  annotations:    nginx.ingress.kubernetes.io/rewrite-target: /spec:  rules:  - host: one.foo.com    http:      paths:      - path: /one        pathType: Prefix        backend:          service:            name: one            port:              number: 8001  - host: &quot;*.foo.com&quot;    http:      paths:      - pathType: Prefix        path: &quot;/other&quot;        backend:          service:            name: other            port:              number: 8002</code></pre><p>🌟请注意, <code>networking.k8s.io/v1</code>必须是v1.19+才可以使用, 如果你不是,则应该使用<code>networking.k8s.io/v1beta1</code></p><p>为了方便理解 ingress 的配置项，我参考 nginx 配置，来进行了一些横向对比：</p><p>nginx.servername:  这里是 spec.rules[].host</p><p>nginx.location: 这里是 spec.rules.http.paths[].path</p><p>nginx.upstream: 这里是 spec.rules.http.paths[].backend</p><h3 id="关于-spec-rules-host">关于 spec.rules[].host</h3><p>这里有一些注意事项. host 支持统配匹配. 不过 <code>*</code>不能跨级匹配. 例如,</p><p>*.abc.com 可以匹配 <a href="http://one.abc.com">one.abc.com</a> 但是不能匹配 <a href="http://two.one.abc.com">two.one.abc.com</a>.</p><h3 id="关于-spec-rules-http-paths-path">关于 spec.rules.http.paths[].path</h3><p>这里有一些注意事项.如上例子所述. pathType 定义了 path 的类型. 它有三个类型:</p><ul><li><p>ImplementationSpecific: 已ingress class为基准. 具体如何匹配,需要看选用的ingress class.这是默认的.</p></li><li><p>Exact: 严格匹配. 不能有一点不同.</p></li><li><p>Prefix: 基于&quot;/&quot;分段进行前缀匹配. 例如,</p><ul><li>/aaa/bbb 可以匹配任何 /aaa/bbb/ 开头的, 或者 /aaa/bbb.  因此 /aaa/bbbb 是不能匹配的.</li><li>/aaa 可以匹配任何 /aaa/ 开头的, 或者 /aaa. 因此, /aaaa 是不能匹配的.</li><li>/ 可以匹配任何 / 开头的. 因此, 匹配所有.</li></ul><p>⭐️请注意, Prefix 类型下.你写的匹配字符串首尾都需要加<code>/</code>, 如果你尾部没有加<code>/</code>, 那么 ingress 在匹配的时候, 也会帮你加上<code>/</code>.</p></li></ul><p>当Exact和Prefix混用的时候, 如果一个请求同时匹配了这两个类型下的path, 那么Exact 优先级更高.</p><h3 id="构建TLS">构建TLS</h3><ol><li><h4 id="创建-Secret-对象-导入证书文件">创建 Secret 对象, 导入证书文件</h4><pre><code class="language-yaml">apiVersion: v1kind: Secretmetadata:  name: foo-com-tls  namespace: defaultdata:  tls.crt: base64 encoded cert # 这里需要填写 base64 编码后的 cert 内容. tls.crt 不可更名  tls.key: base64 encoded key  # 这里需要填写 base64 编码后的 key 内容. tls.key 不可更名type: kubernetes.io/tls</code></pre></li><li><h4 id="在Ingress对象中调用-Secret">在Ingress对象中调用 Secret</h4><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: tls-example-ingressspec:  tls:  - hosts:      - https-example.foo.com    secretName: foo-com-tls  rules:  - host: https-example.foo.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: service1            port:              number: 80</code></pre></li></ol><p>通过配置文件, 可以看出来加密到Ingress就停止了(后端Service对象service1的端口是80).这也符合我们在非k8s环境中的流程.</p><p>另外, 不同的Ingress控制器对于TLS这块也有一定差异.具体以控制器本身说明为准.</p><h2 id="Ingress-controller">Ingress controller</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/</a></p><p>Ingress 想要正常运作, 需要Ingress控制器的支持.</p><p>你可以在k8s中部署多个Ingress控制器, 但是你需要通过在 annotations 中用 ingress.class 注解强调你想用哪一款.</p><p>就像下面这样</p><pre><code class="language-yaml">apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata:  annotations:    kubernetes.io/ingress.class: nginx  name: example  namespace: foospec:  rules:    - host: www.example.com      http:        paths:          - backend:              serviceName: exampleService              servicePort: 80            path: /  # This section is only required if TLS is to be enabled for the Ingress  tls:      - hosts:          - www.example.com        secretName: example-tls---apiVersion: v1kind: Secretmetadata:  name: example-tls  namespace: foodata:  tls.crt: &lt;base64 encoded cert&gt;  tls.key: &lt;base64 encoded key&gt;type: kubernetes.io/tls</code></pre><p>这里<code>backend</code>是旧写法，你应该使用新的写法：</p><pre><code class="language-yaml">  backend:    service:      name: exampleService      port:        number: 80</code></pre><p>这里我们选用 nginx ingress 控制器。它的部署文档是: <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md</a> 或者 <a href="https://kubernetes.github.io/ingress-nginx/deploy/">https://kubernetes.github.io/ingress-nginx/deploy/</a></p><p>这里，我将通过helm来安装ingress-nginx. (你也可以选择其它安装方式. 在 <a href="https://kubernetes.github.io/ingress-nginx/deploy/">https://kubernetes.github.io/ingress-nginx/deploy/</a> 中可以找到很多不同环境的安装方式. )</p><h3 id="helm-安装">helm 安装</h3><p>这里是 helm 的安装文档: <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a></p><p>当前它的安装方式应该是:</p><pre><code class="language-bash">curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3chmod 700 get_helm.sh./get_helm.sh</code></pre><h3 id="nginx-ingress-安装">nginx ingress 安装</h3><p>当前 ingress-nginx 的仓库地址是: <a href="https://hub.helm.sh/charts/ingress-nginx/ingress-nginx">https://hub.helm.sh/charts/ingress-nginx/ingress-nginx</a> 你可以在这里直接找到安装命令</p><pre><code class="language-bash">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginxhelm repo add stable https://kubernetes-charts.storage.googleapis.com/helm repo updatekubectl create namespace ingress-nginx## 安装helm install ingress-nginx ingress-nginx/ingress-nginx -n ingress-nginx## 升级helm upgrade ingress-nginx ingress-nginx/ingress-nginx -n ingress-nginx --install## 自定义配置安装helm show values ingress-nginx/ingress-nginx &gt; ingress-nginx.yamlhelm install ingress-nginx ingress-nginx/ingress-nginx -f ingress-nginx.yaml -n ingress-nginx</code></pre><pre><code class="language-bash">kubectl get all -n ingress-nginx===NAME                                            READY   STATUS    RESTARTS   AGEpod/ingress-nginx-controller-5886685d54-hf6l6   1/1     Running   0          68mNAME                                         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGEservice/ingress-nginx-controller             LoadBalancer   10.96.210.139   &lt;pending&gt;     80:32489/TCP,443:30936/TCP   68mservice/ingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;        443/TCP                      68mNAME                                       READY   UP-TO-DATE   AVAILABLE   AGEdeployment.apps/ingress-nginx-controller   1/1     1            1           68mNAME                                                  DESIRED   CURRENT   READY   AGEreplicaset.apps/ingress-nginx-controller-5886685d54   1         1         1       68m</code></pre><p>我们可以看到 ingress-nginx 控制器创建了一个 pod, 这个 pod 其实就是一个 nginx. 它通过接收 ingress 定义的规则, 来动态的变更nginx配置,从而正确的将流量转发给后端应用的service对象.</p><pre><code class="language-bash">kubectl describe pod/ingress-nginx exec -it pod/ingress-nginx-controller-5886685d54-hf6l6===    Requests:      cpu:      100m      memory:   90Mi</code></pre><p>当前配置默认没有做limit限制. 所以需要关注ingress-nginx创建的pod所在物理节点的性能是否可以满足. 建议强制绑定到多个node节点, 这批节点专门跑用来进行ingress控制器.</p><pre><code class="language-bash">kubectl --namespace ingress-nginx exec -it pod/ingress-nginx-controller-5886685d54-hf6l6 -- /bin/bash==cat /etc/nginx/nginx.conf</code></pre><p>这里我们可以看到ingress-nginx会动态的加载ingress定义的路由规则.</p><p>最后, 我们可能最需要关注的是 <code>service/ingress-nginx-controller</code> 对象. 因为它的类型决定了, 我们如何解析将要暴漏的服务域名.</p><p>我们可以看到通过helm默认安装的 ingress crotroller 的 Service 对象是LoadBalancer类型. 这种类型我们一般是用于云端环境的. 而我这里测试环境是裸机,因此你可以看到这里它一直无法获取到<code>EXTERNAL-IP</code>.</p><p>如何解决这个问题.k8s官方文档这里提供了多种裸机安装下的解决方案. <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/</a></p><p>我们这里通过 <a href="https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb">https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#a-pure-software-solution-metallb</a> 方案来解决 LB 类型的 <code>EXTERNAL-IP</code>获取问题. 其它方案并不是采用的 LB 类型, 因此这里不再叙说.</p><p>⚠️metallb 方案在当前并不是一个成熟的方案,当然它应该是可用的😄 所以用于本地测试环境是可以的. 线上环境建议直接用云服务的LB即可.</p><h2 id="metallb">metallb</h2><p><a href="https://metallb.universe.tf/installation/#installation-with-helm">https://metallb.universe.tf/installation/#installation-with-helm</a></p><p>metallb 模拟了云环境中的负载均衡器功能.</p><p>🌟If you’re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode.</p><p>如果你是ipvs 且 k8s 版本在1.14.2以上, 那么你需要启动严格arp模式.</p><pre><code class="language-bash">kubectl edit configmap -n kube-system kube-proxy===ipvs:  strictARP: true</code></pre><p>开始安装</p><pre><code class="language-bash">helm repo add metallb https://metallb.github.io/metallbhelm fetch metallb metallb/metallb</code></pre><p>修改 metalla/values.yaml, 添加地址池，用来给svc下发ip。请记住, 这些地址必须是没有被其它资源占用的.</p><pre><code class="language-bash">configInline:  address-pools:   - name: default     protocol: layer2     addresses:     - 10.200.16.11 - 10.200.16.19</code></pre><p>通过 helm 安装</p><pre><code class="language-bash">kubectl create ns metallb-systemhelm install metallb --namespace metallb-system -f metallb/values.yaml metallb/metallb# 下面这个命令仅在你初始安装的时候需要运行kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=&quot;$(openssl rand -base64 128)&quot;</code></pre><p>下面,让我们再看看 ingress crotroller 的 svc 对象信息.</p><pre><code class="language-bash">kubectl get svc -n ingress-nginx===NAME                                 TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)                      AGEingress-nginx-controller             LoadBalancer   10.96.210.139   10.200.16.11   80:32489/TCP,443:30936/TCP   88mingress-nginx-controller-admission   ClusterIP      10.96.128.101   &lt;none&gt;         443/TCP                      88m</code></pre><p>终于<code>svc/ingress-nginx-controller</code>拿到了一个<code>EXTERNAL-IP</code>, 现在你就可以将域名解析到<code>EXTERNAL-IP</code>, 并通过这个域名来访问你的应用了.(而且它必须是能通过80访问的.)😄</p><p>同时, 让我们来看看 ingress 对象信息.</p><p>⭐️我暴漏的应用在ns: it中.</p><pre><code class="language-bash">kubectl get ingress -n it===NAME                   CLASS    HOSTS          ADDRESS        PORTS   AGE test-it-local-ingress   &lt;none&gt;   test.it.local   10.200.16.11   80      106m</code></pre><p>可以看到. ingress 的信息也更新了, 并拿到了 metallb 的下发的ip.</p><p>你可以在任意一个节点上通过 ipvs 规则来看到转发情况.</p><pre><code class="language-bash">ipvsadm -L -n===TCP  10.200.16.11:80 rr  -&gt; 10.97.2.57:80                Masq    1      0          0TCP  10.200.16.11:443 rr  -&gt; 10.97.2.57:443               Masq    1      0          0</code></pre><p>这里 10.97.2.57 是 ingress 控制器的 pod ip.</p><p>升级</p><p><a href="https://metallb.universe.tf/installation/#upgrade">https://metallb.universe.tf/installation/#upgrade</a></p><h2 id="nginx-ingress-控制器暴漏非80和非443端口">nginx ingress 控制器暴漏非80和非443端口</h2><p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/">https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/</a></p><p>默认你会发现ingress无法暴漏80和443之外的端口.这是因为默认ingress-nginx并没有开启4层转发.</p><h3 id="开启ingress-nginx中pod的4层转发">开启ingress-nginx中pod的4层转发</h3><p>检查 deployment: ingress-nginx 中 pod 模板是否开启了下列参数</p><pre><code class="language-bash">deploymentName=`kubectl get all -n ingress-nginx | grep deployment | awk '&#123;print $1&#125;'`kubectl edit $&#123;deploymentName&#125; -n ingress-nginx===- args:    - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services    - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</code></pre><h3 id="创建暴漏端口的配置文件">创建暴漏端口的配置文件</h3><pre><code class="language-yaml">apiVersion: v1kind: ConfigMapmetadata:  name: tcp-services  namespace: ingress-nginxdata:  8080: &quot;it/test-it-local-svc:8080&quot;</code></pre><p>这里8080: &quot;it/test-it-local-svc:8080&quot;<code>第一个8080指的是ingress-nginx的pod对象里需要监听的端口, </code>it/test-it-local-svc:8080` 指的是ns:it下的svc对象test-it-local-svc的8080端口</p><h3 id="添加暴漏端口到ingress-nginx中svc对象">添加暴漏端口到ingress-nginx中svc对象</h3><pre><code class="language-bash">kubectl get all -n ingress-nginx | grep service | awk '&#123;print $1&#125;' kubectl edit service/ingress-nginx-controller -n ingress-nginx===  - name: test-8080    port: 8080    protocol: TCP    targetPort: 8080</code></pre><p>三步过后, 不出意外, 你将可以通过ingress-nginx的pod对象中的nginx.conf文件看到tcp的转发配置.</p><pre><code class="language-bash">        # TCP services        server &#123;                preread_by_lua_block &#123;                        ngx.var.proxy_upstream_name=&quot;tcp-it-test-it-local-svc-8080&quot;;                &#125;                listen                  8080;                proxy_timeout           600s;                proxy_pass              upstream_balancer;        &#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> ingress </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞12有状态服务StatefulSet</title>
      <link href="posts/4bf29cf0/"/>
      <url>posts/4bf29cf0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>当我们构建一个有状态的应用的时候, 例如 mysql / ftp / 包含session的管理界面等. 我们可能会有下列预期:</p><ul><li>稳定的、唯一的网络标识符。</li><li>稳定的、持久的存储。</li><li>有序的、优雅的部署和缩放。</li><li>有序的、自动的滚动更新。</li></ul><p>StatefulSet 就是干这个的.</p><h2 id="一个例子">一个例子</h2><pre><code class="language-yaml:">apiVersion: v1kind: Servicemetadata:  name: nginx  labels:    app: nginxspec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata:  name: webspec:  serviceName: &quot;nginx&quot;  replicas: 2  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: k8s.gcr.io/nginx-slim:0.8        ports:        - containerPort: 80          name: web        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html  volumeClaimTemplates:  - metadata:      name: www    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      resources:        requests:          storage: 1Gi</code></pre><h2 id="特点">特点</h2><ul><li>pod 拥有唯一的顺序索引和稳定的网络身份(不管怎么删, 名字都不变, ip 会变). pod 被部署的时候其名字是按照<code>&lt;statefulset name&gt;-&lt;ordinal index&gt;</code>创建的(web-0, web-1). 这个名字就是他们的hostname.(这是默认行为)</li><li>pod 不管怎么删, pod与pvc之间的关系都不会混乱.</li><li>pod 之间可以通过固定的域名互相访问.</li><li>启动 pod 的时候, 按照索引, 从 0 开始. 一个一个启动. 不会同时启动多个. (这是默认行为)</li><li>删除 pod 的时候, 按照索引倒序删除, 从最后一个开始, 一个一个删除. 不会同时删除多个. (这是默认行为)</li><li>删除 pod 的时候, pvc 和 pv 并不会删除.</li><li>Service 必须是 headless .</li></ul><blockquote><p>你可以通过设置statefulset管理策略, 来改变上述默认行为</p><p><a href="https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy">https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy</a></p></blockquote><h2 id="扩展-缩放">扩展/缩放</h2><p>扩展: <code>kubectl scale sts web --replicas=5</code></p><p>缩放: <code>kubectl patch sts web -p '&#123;&quot;spec&quot;:&#123;&quot;replicas&quot;:3&#125;&#125;'</code></p><p>当然你也可以通过修改配置文件里 replicas 的值来进行变更.</p><h2 id="更新">更新</h2><p>有状态应用的更新和无状态应用的更新有些差异.</p><p>有状态应用的更新策略(spec.updateStrategy)是 RollingUpdate 和 OnDelete. 其中 RollingUpdate 是默认策略, 这个是自动滚动更新. 而OnDelete的意思是只有你手动删除了pod才会更新.</p><p>更新是针对pod模板的. 例如:</p><pre><code class="language-bash">kubectl patch statefulset web --type='json' -p='[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;gcr.io/google_containers/nginx-slim:0.8&quot;&#125;]'</code></pre><blockquote><p>这里更新了nginx的镜像.</p></blockquote><p>更新遵循以下原则:</p><ul><li><p>所有的 pod 需要是就绪状态.</p></li><li><p>更新采用索引倒序进行(即从索引最后一个号码开始更新), 并且是一个一个的更新.</p></li><li><p>当更新失败的时候, 已经收到更新的pod将保持更新后的版本, 没有开始更新的pod将恢复到更新前的版本.</p></li></ul><p>🌟需要注意的是, 如果你的更新文件(二进制文件故障或者配置文件)有问题从而导致更新失败, 你可能需要强制回滚. 以下是说明信息:</p><p>即你恢复了更新前的模板,却发现statefulset依然不正常, 则你需要<strong>手动删除</strong>所有由错误模板产出的pod.</p><p>在这之后, statefulset将会采用更新前的模板重建pod.</p><p>(<a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#forced-rollback">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#forced-rollback</a>)</p><h2 id="阶段更新-staging-an-update">阶段更新(staging an update)</h2><p>有时候我们可能并不想一次更新所有, 此时可以进行阶段更新.</p><p>阶段更新的意思就是通过在索引上设置一个点. 当pod的索引大于等于这个点的时候, 才会更新. (默认点是索引0)</p><pre><code class="language-bash">kubectl patch statefulset web -p '&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:3&#125;&#125;&#125;&#125;'</code></pre><p>这个意思是设置索引3为阶段分割点.</p><p>如果你想回到默认更新, 则只需要调整分割点, 重新执行上述命令即可.</p><h2 id="删除">删除</h2><p>删除分为联级删除和非联级删除.</p><p>联级删除就是 statefulset 和 pod 都删除. 这是默认行为.</p><p>非联级删除, 只会删除 statefulset. 你可以通过删除的时候附加<code>--cascade=false</code>开启它.</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> StatefulSet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞12守护进程</title>
      <link href="posts/63d8d3d4/"/>
      <url>posts/63d8d3d4/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>DaemonSet 确保指定的node上都存在一份 pod. 常用来构建一些批量代理性质的app. 例如监控节点app, 或者日志节点app</p><h2 id="特点">特点</h2><ul><li>当一个节点添加到集群, 或从集群中删除, 则 pod 会被添加/删除</li><li>当DaemonSet被删除, 则 pod 都会被删除.</li><li>pod模板中的 RestartPolicy 必须是默认值, 也就是 Always.</li><li>创建后.spec.selector 不可修改.</li><li>默认调度在所有符合条件的node上.</li></ul><h2 id="一个例子">一个例子</h2><pre><code class="language-yaml">apiVersion: apps/v1kind: DaemonSetmetadata:  name: fluentd-elasticsearch  namespace: kube-system  labels:    k8s-app: fluentd-loggingspec:  selector:    matchLabels:      name: fluentd-elasticsearch  template:    metadata:      labels:        name: fluentd-elasticsearch    spec:      tolerations:      # this toleration is to have the daemonset runnable on master nodes      # remove it if your masters can't run pods      - key: node-role.kubernetes.io/master        effect: NoSchedule      containers:      - name: fluentd-elasticsearch        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2        resources:          limits:            memory: 200Mi          requests:            cpu: 100m            memory: 200Mi        volumeMounts:        - name: varlog          mountPath: /var/log        - name: varlibdockercontainers          mountPath: /var/lib/docker/containers          readOnly: true      terminationGracePeriodSeconds: 30      volumes:      - name: varlog        hostPath:          path: /var/log      - name: varlibdockercontainers        hostPath:          path: /var/lib/docker/containers</code></pre><p>这是一个日志采集pod. 它将k8s集群节点的 /var/log 和 /var/lib/docker/containers 挂载到 pod 中. 这样, elasticsearch 就可以获取到k8s集群所有节点的信息了.</p><h2 id="pod运行在部分node上">pod运行在部分node上</h2><p>你有两种方式处理pod的部分调度.</p><h3 id="通过节点标签-nodeSelector-来选择">通过节点标签(nodeSelector)来选择.</h3><p><code>.spec.template.spec.nodeSelector</code>可以让你仅在匹配的node上创建pod.</p><p>例如: 仅在拥有ssd磁盘的节点上构建</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: nginx  labels:    env: testspec:  containers:  - name: nginx    image: nginx    imagePullPolicy: IfNotPresent  nodeSelector:    disktype: ssd</code></pre><h3 id="通过节点亲和性-nodeAffinity-来选择">通过节点亲和性(nodeAffinity)来选择</h3><p><code>.spec.affinity.nodeAffinity</code> , 它有以下属性: (每一个属性由两部分意思组成)</p><p>requiredDuringScheduling<strong>IgnoredDuringExecution</strong><br>表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。</p><p>requiredDuringScheduling<strong>RequiredDuringExecution</strong><br>表示pod必须部署到满足条件的节点上，如果没有满足条件的节点，就不停重试。其中RequiredDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，则重新选择符合要求的节点。</p><p>preferredDuringScheduling<strong>IgnoredDuringExecution</strong><br>表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中IgnoreDuringExecution表示pod部署之后运行的时候，如果节点标签发生了变化，不再满足pod指定的条件，pod也会继续运行。</p><p>preferredDuringScheduling<strong>RequiredDuringExecution</strong><br>表示优先部署到满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。其中RequiredDuringExecution表示如果后面节点标签发生了变化，满足了条件，则重新调度到满足条件的节点。</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: with-node-affinityspec:  affinity:    nodeAffinity:      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: kubernetes.io/e2e-az-name            operator: In            values:            - e2e-az1            - e2e-az2      preferredDuringSchedulingIgnoredDuringExecution:      - weight: 1        preference:          matchExpressions:          - key: another-node-label-key            operator: In            values:            - another-node-label-value  containers:  - name: with-node-affinity    image: gcr.io/google_containers/pause:2.0</code></pre><p>这个配置的意思是pod<strong>必须被调度</strong><a href="http://xn--kubernetes-m11qu01s34wc.io/e2e-az-name=e2e-az1%E6%88%96%E8%80%85kubernetes.io/e2e-az-name=e2e-az2">到标签kubernetes.io/e2e-az-name=e2e-az1或者kubernetes.io/e2e-az-name=e2e-az2</a> 的节点。与此同时, 还将在上述条件的基础上, <strong>优先调度</strong>到额外拥有标签为another-node-label-key=another-node-label-value的节点上。</p><h2 id="污点和容忍度Taint-and-toleration">污点和容忍度Taint-and-toleration</h2><p><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</a></p><p>k8s通过污点来剔除某些故障节点, 或构建一些专属节点.</p><p>k8s通过容忍度来确保某些pod可以忽略污点.</p><p>在正常情况下. 没有任何容忍度的pod不会被调度到存在污点的node上. (当出现节点故障时候, 默认会附加一个容忍度, 容忍度失效300秒)</p><p>但DaemonSet却需要忽略这些污点, 以便于pod长存. (例如你构建的监控pod)</p><p>以下是DaemonSet自动附加的容忍度, 它将忽略这些污点. (截至到v1.19)</p><pre><code class="language-bash">node.kubernetes.io/not-readynode.kubernetes.io/unreachablenode.kubernetes.io/disk-pressurenode.kubernetes.io/memory-pressurenode.kubernetes.io/unschedulablenode.kubernetes.io/network-unavailable</code></pre><p>上述情况基本保证了DaemonSet的pod不会因各种意外情况导致pod被驱逐.</p><h2 id="访问DaemonSet-pod">访问DaemonSet pod</h2><p>你可以通过构建一个 Headless Service 并通过 endpoint  来访问各个 pod.</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> DaemonSet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞11-2namespace资源约束</title>
      <link href="posts/ed4c29d5/"/>
      <url>posts/ed4c29d5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://kubernetes.io/docs/concepts/policy/limit-range/">https://kubernetes.io/docs/concepts/policy/limit-range/</a></p><p>namespace 资源约束(LimitRange)防止pod的配置超出预期.从而让我们可以更方便的管理资源.</p><h2 id="特点">特点</h2><ul><li>提供 pod 的默认资源约束.</li><li>限制pod的资源约束.</li><li>LimitRange配置策略的构建和变更仅影响之后的pod</li></ul><h2 id="流程">流程</h2><h3 id="构建LimitRange">构建LimitRange</h3><pre><code class="language-yaml">apiVersion: v1kind: LimitRangemetadata:  name: cpu-mem-storage-min-max-defaultspec:  limits:  - type: Container    max:      cpu: &quot;1000m&quot;      memory: &quot;1G&quot;    min:      cpu: &quot;50m&quot;      memory: &quot;50M&quot;    default:      cpu: &quot;250m&quot;      memory: &quot;250M&quot;    defaultRequest:      cpu: &quot;50m&quot;      memory: &quot;50M&quot;  - type: PersistentVolumeClaim    max:      storage: 30G    min:      storage: 8G</code></pre><pre><code class="language-bash">kubectl apply -f limitrange-default.yaml --namespace=default</code></pre><p>上述例子为 default 命名空间加了一个资源策略.效果如下:</p><ul><li><p>当pod没有任何限制的时候, pod 规则如下遵循 default 和 defaultRequest 的属性</p><ul><li><p>limit: cpu=250m mem=250M</p></li><li><p>request: cpu=50m mem=50M</p></li><li><p>8G &lt; pvc &lt; 30G</p><p>会全部走limitrange的默认值</p></li></ul></li><li><p>当pod设置了limit: cpu=1000m mem=1G的时候, pod规则如下</p><ul><li><p>limit: cpu=1000m mem=1G</p></li><li><p>request: cpu=1000m mem=1G</p></li><li><p>8G &lt; pvc &lt; 30G</p><p>这里比较奇葩, 官方就是这么设定的. 虽然你没设置request, 但是request 不会走默认值, 会继承 limit 的自定义值</p></li></ul></li><li><p>当pod只设置了request: cpu=100m mem=150M的时候, pod规则如下</p><ul><li><p>limit: cpu=250m mem=250M</p></li><li><p>request: cpu=100m mem=150M</p></li><li><p>8G &lt; pvc &lt; 30G</p><p>这里符合大家的逻辑…没设置就是走默认.</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> namespace </tag>
            
            <tag> 资源约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞11-1pod资源约束</title>
      <link href="posts/64fa9c84/"/>
      <url>posts/64fa9c84/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a></p><p>资源约束在我们平时使用的时候主要是cpu和内存层面, 以及本地临时存储(emptyDir)</p><p>k8s资源约束分为两种: request(软约束) 和 limits(硬约束)</p><p>request(软约束)确保了pod中的容器至少可以使用这么多资源. 不过当节点如果没有其它容器,则此容器可以突破request限制.</p><p>limits(硬约束) 则使硬性的限制了容器资源上限. 如果容器请求的内存大于了limits,则会收到oom错误. 如果容器请求的cpu大于了limits, 则不会产出错误, 因为cpu对于程序来说,它不是一个硬性指标.如果cpu资源不够,只会导致程序卡顿到死…</p><p>最后,k8s会严格按照pod定义的资源限制进行调度,即时某个节点上有大量空闲资源,但只要空闲资源不能满足pod的资源定义,就不能调度到这个节点上.</p><h2 id="写法">写法</h2><ul><li><code>spec.containers[].resources.limits.cpu </code>cpu限制</li><li><code>spec.containers[].resources.limits.memory</code> 内存限制</li><li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code>   不常用, 可以忽略</li><li><code>spec.containers[].resources.limits.ephemeral-storage</code> empty临时存储限制</li><li><code>spec.containers[].resources.requests.cpu</code></li><li><code>spec.containers[].resources.requests.memory</code></li><li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt; </code> 不常用, 可以忽略</li><li><code>spec.containers[].resources.requests.ephemeral-storage</code></li></ul><h2 id="单位">单位</h2><p>k8s将一个超线程称为一个vcpu. 1vcpu=1000m. 我们在定义资源限制时, 应该始终用 m 作为单位.假设你限制0.5个vcpu,则填写500m.</p><p>k8s的内存和临时存储单位和平时我们所用的没什么区别. 你只需要记住 K/M/G/T/P/E 这些即可. 例如, 100M就是100兆</p><p>一个例子:</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: frontendspec:  containers:  - name: app    image: images.my-company.example/app:v4    resources:      requests:        memory: &quot;64M&quot;        cpu: &quot;250m&quot;        ephemeral-storage: &quot;2Gi&quot;      limits:        memory: &quot;128M&quot;        cpu: &quot;500m&quot;        ephemeral-storage: &quot;4Gi&quot;  - name: log-aggregator    image: images.my-company.example/log-aggregator:v6    resources:      requests:        memory: &quot;64M&quot;        cpu: &quot;250m&quot;        ephemeral-storage: &quot;2Gi&quot;      limits:        memory: &quot;128M&quot;        cpu: &quot;500m&quot;        ephemeral-storage: &quot;4Gi&quot;</code></pre><h2 id="资源限制如何运作">资源限制如何运作</h2><p>k8s会通过kubelet将pod定义的资源限制传递给容器.</p><p>如果你容器使用的是docker.</p><p>cpu软限制将对标docker的–cpu-shares. 而cpu硬限制会告诉容器每100ms可以使用的CPU时间总量是  limits.cpu * 100.</p><blockquote><p>关于docker的–cpu-shares, 可以参考https://docs.docker.com/engine/reference/run/#cpu-share-constraint</p><p>总的来说, --cpu-shares 会让容器按照所设定的分值比例去使用cpu.不过, 在多核心节点上, 这个规则又不是很适用. 按照官方的说法, 当多核心cpu的时候,它的规则是:</p><p>On a multi-core system, the shares of CPU time are distributed over all CPU cores. Even if a container is limited to less than 100% of CPU time, it can use 100% of each individual CPU core.</p><p>For example, consider a system with more than three cores. If you start one container <code>&#123;C0&#125;</code> with <code>-c=512</code> running one process, and another container <code>&#123;C1&#125;</code> with <code>-c=1024</code> running two processes, this can result in the following division of CPU shares:</p><pre><code>PID    containerCPUCPU share100    &#123;C0&#125;0100% of CPU0101    &#123;C1&#125;1100% of CPU1102    &#123;C1&#125;2100% of CPU2</code></pre><p>这里三个容器,都是单核心程序</p></blockquote><p>内存的限制没有什么特别需要注意的.</p><h2 id="容器中的可见资源">容器中的可见资源</h2><p>默认情况下,不管你如何设置资源限制,容器里的可见资源都等于节点资源.也就是说你在容器里查看/proc/meminfo显示的资源并不是你设置的.这个问题会带来很多麻烦.</p><p>因为很多程序的参数都是根据可见资源来设定的.例如nginx的auto, 或者jvm里的内存参数.</p><blockquote><p>从 Java 8u191开始, jvm 已经默认实现了容器感知( -XX:+UseContainerSupport). 因此无需安装下面的 lxcfs 方案.</p><p>并且, 在容器中建议只设置如下内存参数:</p><p>-XX:MaxRAMPercentage  最大堆内存=物理内存百分比, 建议为容器限制的50%-75% . 毕竟还需要预留其它内存.</p></blockquote><p>而社区常见的作法是用<code>lxcfs</code> 来提升资源的可见性.<code>lxcfs</code> 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器, 它也可以支持Docker容器.</p><p>LXCFS通过用户态文件系统, 在容器中提供下列 <code>procfs</code> 的文件.</p><pre><code class="language-bash">/proc/cpuinfo/proc/diskstats/proc/meminfo/proc/stat/proc/swaps/proc/uptime</code></pre><p>与我们资源限制有关的, 主要是 cpuinfo 和 meminfo.</p><p>目前社区的做法如下:</p><ol><li><p>所有节点安装 fuse-libs 包.</p><pre><code class="language-bash">yum install -y fuse-libs</code></pre></li><li><p>安装部署lxcfs</p><pre><code class="language-bash">git clone https://github.com/denverdino/lxcfs-admission-webhook.gitcd lxcfs-admission-webhookvim deployment/lxcfs-daemonset.yaml=== 修正配置里的 apiVersion=== 当前git里的代码是陈旧的...代码里的版本在 1.18 已经被废弃). 具体归属于什么版本, 请参考k8s官方api文档 === https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#daemonset-v1-appsapiVersion:apps/v1  kubectl apply -f deployment/lxcfs-daemonset.yamlkubectl get pod#等待 lxcfs pod 状态成 running#如果你发现 pod 始终出错，那么执行下列命令：kubectl delete -f deployment/lxcfs-daemonset.yamlrm -rf /var/lib/lxcfskubectl apply -f deployment/lxcfs-daemonset.yamldeployment/install.shkubectl get secrets,pods,svc,mutatingwebhookconfigurations=== 查看各个对象状态</code></pre></li><li><p>给相关namespace注入lxcfs，例如default</p><pre><code class="language-bash">kubectl label namespace default lxcfs-admission-webhook=enabled</code></pre></li><li><p>重启添加了资源限制的pod, 此时 /proc/cpuinfo 和 /proc/meminfo 将会正确显示.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> pod </tag>
            
            <tag> 资源约束 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞10应用配置与密码与信息提供</title>
      <link href="posts/4b0e4a5/"/>
      <url>posts/4b0e4a5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>kubernetes 可以以环境变量或者以文件的方式提供信息给容器。这些信息存放在 secret 和 configmap 对象里。</p><p>所以步骤一般是这样的。</p><ol><li>创建secret 和 configmap 对象</li></ol><h2 id="以文件方式提供">以文件方式提供</h2><p>kubernetes 提供了一种卷类型 projected volume。</p><p>写法：</p><pre><code class="language-yaml:">        volumeMounts:        - name: all-in-one          mountPath: /etc/allinfo          readonly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: user      - secret:          name: pass      - configmap:          name: content      - downwardAPI:          items:            - path: &quot;labels&quot;              fieldRef:                fieldPath: metadata.labels            - path: &quot;annotations&quot;              fieldRef:                fieldPath: metadata.annotations</code></pre><blockquote><p>user, pass, content, labels, annotations 将会以文件形式存在于容器挂载点 /etc/allinfo 内。</p></blockquote><p>创建 secret: user 和 secret: pass</p><pre><code class="language-bash">echo -n &quot;admin&quot; &gt; ./username.txtkubectl create secret generic user --from-file=./username.txtecho -n &quot;1f2d1e2e67df&quot; &gt; ./username.txtkubectl create secret generic pass --from-file=./password.txt</code></pre><p>创建 configmap: content</p><pre><code class="language-bash">echo -n &quot;userinfo&quot; &gt; ./content.txtkubectl create configmap content --from-file=./content.txt</code></pre><p>downwardAPI 内容直接写入</p><h2 id="配置ConfigMap">配置ConfigMap</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/configmap/">https://kubernetes.io/docs/concepts/configuration/configmap/</a></p><p>ConfigMap 常用在两种情况中. 第一种是提供环境变量.第二种是提供配置文件</p><p>一个简单配置例子如下</p><pre><code class="language-yaml">kind: ConfigMapapiVersion: v1metadata:  name: cm-demo  namespace: defaultdata:  data.1: hello  data.2: world  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>上述例子中, 包含的内容有</p><p>环境变量方式:</p><pre><code class="language-yaml">  data.1: hello  data.2: world</code></pre><p>配置文件方式:</p><pre><code class="language-yaml">  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>nginx.config 是文件名, 管道符 | 下面是文件内容.</p><p>需要注意的是, 内容依然要遵循 yaml 的缩进规则</p><p>使用方式如下:</p><pre><code class="language-yaml">    spec:      containers:      - name: nginx        image: nginx        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $&#123;DATA.1&#125; $&#123;DATA.2&#125;&quot;]        ports:        - containerPort: 80        env:        - name: DATA1          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.1        - name: DATA2          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.2        volumeMounts:        - name: cm-demo-vc          subPath: nginx.conf          mountPath: /etc/nginx/nginx.conf      volumes:      - name: cm-demo-vc        configMap:          name: cm-demo</code></pre><p>配置文件数据卷方式:</p><ul><li>通过在 spec.volumes 中定义一个关联声明(cm.demo-vc). 它一端被 spec.containers.volumeMounts 调用, 另一端关联 configMap. 然后通过 spec.containers.volumeMounts 绑定 configMap 后, 就可以使用 subPath 调用配置文件, 并使用 mountPath 挂载到容器里的具体路径上.</li><li>这种方式下,configMap更新后,pod内挂载的也会同时更新.</li></ul><p>环境变量方式:</p><ul><li>通过在 spec.containers.env 中定义. 例子中, DATA1 是环境变量的键, DATA1的值通过valueFrom定义.最终,你可以在容器中使用环境变量DATA1和DATA2</li></ul><h2 id="密码Secret">密码Secret</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a></p><p>secret对象里的值需要写入加密后的.</p><p>secret对象也可以被当作文件挂载或环境变量注入</p><p>一个简单例子:</p><p>我们定义一个用户密码对,分别时username和password</p><p>secret对象要求值必须进行base64编码加密(当type为Opaque的时候).</p><pre><code class="language-bash">[root@k8s00 test-yaml]# echo -n &quot;admin&quot; | base64YWRtaW4=[root@k8s00 test-yaml]# echo -n &quot;admin321&quot; | base64YWRtaW4zMjE=</code></pre><pre><code class="language-yaml"># 创建 secret 对象 mysecretapiVersion: v1kind: Secretmetadata:  name: mysecret  namespace: defaulttype: Opaquedata:  username: YWRtaW4=  password: MWYyZDFlMmU2N2Rm---apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    command:    - sleep    - &quot;3600&quot;    env:      - name: SECRET_USERNAME        valueFrom:          secretKeyRef:            name: mysecret            key: username      - name: SECRET_PASSWORD        valueFrom:          secretKeyRef:            name: mysecret            key: password    volumeMounts:    - name: foo      mountPath: &quot;/etc/foo&quot;      readOnly: true  volumes:  - name: foo    secret:      secretName: mysecret</code></pre><p>在通过上述配置创建好资源后,我们进入pod,进行测试.</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl exec -it mypod -- /bin/sh===可以看到两个加密信息生成了两个软连接,并指向了隐藏文件/ # ls -l /etc/foototal 0lrwxrwxrwx    1 root     root            15 Sep 17 07:18 password -&gt; ..data/passwordlrwxrwxrwx    1 root     root            15 Sep 17 07:18 username -&gt; ..data/username===环境变量/ # echo $&#123;SECRET_USERNAME&#125;admin===文件方式/ # cat /etc/foo/usernameadmin/ # </code></pre><h2 id="限制">限制</h2><ul><li>机密资源限制在同一命名空间</li><li>数据大小不能超过1MB</li><li>当pod引用不存在的机密时,pod无法启动</li></ul><h2 id="其它">其它</h2><ol><li><p>不可变的机密(k8s v1.19以上)</p><pre><code class="language-yaml">secret.immutable: true</code></pre><p>这种方式用来构建永久不可变的密码,且构建后无法更改,且构建后使用它的pod也无法卸载掉它.</p><p>因此,一旦构建后想更改,则它和它关联的pod都要删除重建.</p><p>优点:</p><ul><li>避免错误的更新</li><li>显著提高集群性能</li></ul></li><li><p>命令方式创建</p><p>kubectl create configmap/secret <name> xxx</name></p><p>这里xxx可以用两种方式:</p><ul><li>–from-literal=<key>=<value> 指定kv</value></key></li><li>–from-file=&lt;文件/目录&gt; 当为目录的时候,会递归将目录里的文件都写入对象中</li></ul></li><li><p>隐藏的密码信息文件</p><p>你可以将 secret.data.<key> 写成 secret.data.&lt;.key&gt; 来隐藏它.</key></p></li></ol><h2 id="信息提供Downward-API">信息提供Downward API</h2><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></p><p>一个比较合适的解决方案是采用Downward API传递信息到容器. 然后程序根据信息来设置.</p><p>一个简单的例子: 将资源限制数据挂载到容器/etc/podinfo</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: kubernetes-downwardapi-volume-example-2spec:  containers:    - name: client-container      image: k8s.gcr.io/busybox:1.24      command: [&quot;sh&quot;, &quot;-c&quot;]      args:      - while true; do          echo -en '\n';          if [[ -e /etc/podinfo/cpu_limit ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_limit; fi;          if [[ -e /etc/podinfo/cpu_request ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_request; fi;          if [[ -e /etc/podinfo/mem_limit ]]; then            echo -en '\n'; cat /etc/podinfo/mem_limit; fi;          if [[ -e /etc/podinfo/mem_request ]]; then            echo -en '\n'; cat /etc/podinfo/mem_request; fi;          sleep 5;        done;      resources:        requests:          memory: &quot;32Mi&quot;          cpu: &quot;125m&quot;        limits:          memory: &quot;64Mi&quot;          cpu: &quot;250m&quot;      volumeMounts:        - name: podinfo          mountPath: /etc/podinfo  volumes:    - name: podinfo      downwardAPI:        items:          - path: &quot;cpu_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.cpu              divisor: 1m          - path: &quot;cpu_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.cpu              divisor: 1m          - path: &quot;mem_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.memory              divisor: 1Mi          - path: &quot;mem_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.memory              divisor: 1Mi</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> ConfigMap </tag>
            
            <tag> Secret </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞09应用配置与密码与信息提供</title>
      <link href="posts/4b0e4a5/"/>
      <url>posts/4b0e4a5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>kubernetes 可以以环境变量或者以文件的方式提供信息给容器。这些信息存放在 secret 和 configmap 对象里。</p><p>所以步骤一般是这样的。</p><ol><li>创建secret 和 configmap 对象</li></ol><h2 id="以文件方式提供">以文件方式提供</h2><p>kubernetes 提供了一种卷类型 projected volume。</p><p>写法：</p><pre><code class="language-yaml:">        volumeMounts:        - name: all-in-one          mountPath: /etc/allinfo          readonly: true  volumes:  - name: all-in-one    projected:      sources:      - secret:          name: user      - secret:          name: pass      - configmap:          name: content      - downwardAPI:          items:            - path: &quot;labels&quot;              fieldRef:                fieldPath: metadata.labels            - path: &quot;annotations&quot;              fieldRef:                fieldPath: metadata.annotations</code></pre><blockquote><p>user, pass, content, labels, annotations 将会以文件形式存在于容器挂载点 /etc/allinfo 内。</p></blockquote><p>创建 secret: user 和 secret: pass</p><pre><code class="language-bash">echo -n &quot;admin&quot; &gt; ./username.txtkubectl create secret generic user --from-file=./username.txtecho -n &quot;1f2d1e2e67df&quot; &gt; ./username.txtkubectl create secret generic pass --from-file=./password.txt</code></pre><p>创建 configmap: content</p><pre><code class="language-bash">echo -n &quot;userinfo&quot; &gt; ./content.txtkubectl create configmap content --from-file=./content.txt</code></pre><p>downwardAPI 内容直接写入</p><h2 id="配置ConfigMap">配置ConfigMap</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/configmap/">https://kubernetes.io/docs/concepts/configuration/configmap/</a></p><p>ConfigMap 常用在两种情况中. 第一种是提供环境变量.第二种是提供配置文件</p><p>一个简单配置例子如下</p><pre><code class="language-yaml">kind: ConfigMapapiVersion: v1metadata:  name: cm-demo  namespace: defaultdata:  data.1: hello  data.2: world  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>上述例子中, 包含的内容有</p><p>环境变量方式:</p><pre><code class="language-yaml">  data.1: hello  data.2: world</code></pre><p>配置文件方式:</p><pre><code class="language-yaml">  nginx.conf: |    property.1=value-1    property.2=value-2    property.3=value-3</code></pre><p>nginx.config 是文件名, 管道符 | 下面是文件内容.</p><p>需要注意的是, 内容依然要遵循 yaml 的缩进规则</p><p>使用方式如下:</p><pre><code class="language-yaml">    spec:      containers:      - name: nginx        image: nginx        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $&#123;DATA.1&#125; $&#123;DATA.2&#125;&quot;]        ports:        - containerPort: 80        env:        - name: DATA1          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.1        - name: DATA2          valueFrom:            configMapKeyRef:              name: cm-demo-vc              key: data.2        volumeMounts:        - name: cm-demo-vc          subPath: nginx.conf          mountPath: /etc/nginx/nginx.conf      volumes:      - name: cm-demo-vc        configMap:          name: cm-demo</code></pre><p>配置文件数据卷方式:</p><ul><li>通过在 spec.volumes 中定义一个关联声明(cm.demo-vc). 它一端被 spec.containers.volumeMounts 调用, 另一端关联 configMap. 然后通过 spec.containers.volumeMounts 绑定 configMap 后, 就可以使用 subPath 调用配置文件, 并使用 mountPath 挂载到容器里的具体路径上.</li><li>这种方式下,configMap更新后,pod内挂载的也会同时更新.</li></ul><p>环境变量方式:</p><ul><li>通过在 spec.containers.env 中定义. 例子中, DATA1 是环境变量的键, DATA1的值通过valueFrom定义.最终,你可以在容器中使用环境变量DATA1和DATA2</li></ul><h2 id="密码Secret">密码Secret</h2><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/">https://kubernetes.io/docs/concepts/configuration/secret/</a></p><p>secret对象里的值需要写入加密后的.</p><p>secret对象也可以被当作文件挂载或环境变量注入</p><p>一个简单例子:</p><p>我们定义一个用户密码对,分别时username和password</p><p>secret对象要求值必须进行base64编码加密(当type为Opaque的时候).</p><pre><code class="language-bash">[root@k8s00 test-yaml]# echo -n &quot;admin&quot; | base64YWRtaW4=[root@k8s00 test-yaml]# echo -n &quot;admin321&quot; | base64YWRtaW4zMjE=</code></pre><pre><code class="language-yaml"># 创建 secret 对象 mysecretapiVersion: v1kind: Secretmetadata:  name: mysecret  namespace: defaulttype: Opaquedata:  username: YWRtaW4=  password: MWYyZDFlMmU2N2Rm---apiVersion: v1kind: Podmetadata:  name: mypodspec:  containers:  - name: mypod    image: busybox    command:    - sleep    - &quot;3600&quot;    env:      - name: SECRET_USERNAME        valueFrom:          secretKeyRef:            name: mysecret            key: username      - name: SECRET_PASSWORD        valueFrom:          secretKeyRef:            name: mysecret            key: password    volumeMounts:    - name: foo      mountPath: &quot;/etc/foo&quot;      readOnly: true  volumes:  - name: foo    secret:      secretName: mysecret</code></pre><p>在通过上述配置创建好资源后,我们进入pod,进行测试.</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl exec -it mypod -- /bin/sh===可以看到两个加密信息生成了两个软连接,并指向了隐藏文件/ # ls -l /etc/foototal 0lrwxrwxrwx    1 root     root            15 Sep 17 07:18 password -&gt; ..data/passwordlrwxrwxrwx    1 root     root            15 Sep 17 07:18 username -&gt; ..data/username===环境变量/ # echo $&#123;SECRET_USERNAME&#125;admin===文件方式/ # cat /etc/foo/usernameadmin/ # </code></pre><h2 id="限制">限制</h2><ul><li>机密资源限制在同一命名空间</li><li>数据大小不能超过1MB</li><li>当pod引用不存在的机密时,pod无法启动</li></ul><h2 id="其它">其它</h2><ol><li><p>不可变的机密(k8s v1.19以上)</p><pre><code class="language-yaml">secret.immutable: true</code></pre><p>这种方式用来构建永久不可变的密码,且构建后无法更改,且构建后使用它的pod也无法卸载掉它.</p><p>因此,一旦构建后想更改,则它和它关联的pod都要删除重建.</p><p>优点:</p><ul><li>避免错误的更新</li><li>显著提高集群性能</li></ul></li><li><p>命令方式创建</p><p>kubectl create configmap/secret <name> xxx</name></p><p>这里xxx可以用两种方式:</p><ul><li>–from-literal=<key>=<value> 指定kv</value></key></li><li>–from-file=&lt;文件/目录&gt; 当为目录的时候,会递归将目录里的文件都写入对象中</li></ul></li><li><p>隐藏的密码信息文件</p><p>你可以将 secret.data.<key> 写成 secret.data.&lt;.key&gt; 来隐藏它.</key></p></li></ol><h2 id="信息提供Downward-API">信息提供Downward API</h2><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/</a></p><p>一个比较合适的解决方案是采用Downward API传递信息到容器. 然后程序根据信息来设置.</p><p>一个简单的例子: 将资源限制数据挂载到容器/etc/podinfo</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: kubernetes-downwardapi-volume-example-2spec:  containers:    - name: client-container      image: k8s.gcr.io/busybox:1.24      command: [&quot;sh&quot;, &quot;-c&quot;]      args:      - while true; do          echo -en '\n';          if [[ -e /etc/podinfo/cpu_limit ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_limit; fi;          if [[ -e /etc/podinfo/cpu_request ]]; then            echo -en '\n'; cat /etc/podinfo/cpu_request; fi;          if [[ -e /etc/podinfo/mem_limit ]]; then            echo -en '\n'; cat /etc/podinfo/mem_limit; fi;          if [[ -e /etc/podinfo/mem_request ]]; then            echo -en '\n'; cat /etc/podinfo/mem_request; fi;          sleep 5;        done;      resources:        requests:          memory: &quot;32Mi&quot;          cpu: &quot;125m&quot;        limits:          memory: &quot;64Mi&quot;          cpu: &quot;250m&quot;      volumeMounts:        - name: podinfo          mountPath: /etc/podinfo  volumes:    - name: podinfo      downwardAPI:        items:          - path: &quot;cpu_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.cpu              divisor: 1m          - path: &quot;cpu_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.cpu              divisor: 1m          - path: &quot;mem_limit&quot;            resourceFieldRef:              containerName: client-container              resource: limits.memory              divisor: 1Mi          - path: &quot;mem_request&quot;            resourceFieldRef:              containerName: client-container              resource: requests.memory              divisor: 1Mi</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> ConfigMap </tag>
            
            <tag> Secret </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞08负载均衡service</title>
      <link href="posts/d3b80b5f/"/>
      <url>posts/d3b80b5f/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>在实际业务中,因业务压力问题,经常会有多个后端服务副本,它们共同承担请求.我们使用云服务的时候,可以购买阿里云的slb或者aws的elb/alb等负载均衡器向这些后端副本分发流量. 并通过这些负载均衡向公网暴漏内网这些后端程序.</p><p>k8s设计了一个service对象来实现这一目的.</p><h2 id="k8sIP">k8sIP</h2><p>在提Service对象之前,需要先知道k8s中存在的三种IP.即 NodeIP, ClusterIP, PodIP</p><p>NodeIP 就是物理节点ip, 这个没得说, 玩家自己定义</p><p>ClusterIP 是k8s的一个虚拟ip, 本身没有任何实体, 也就是VIP</p><p>PodIP 是容器共享的一个网络命名空间对应的ip, 一个pod里的容器共用</p><h2 id="Service">Service</h2><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/">https://kubernetes.io/zh/docs/concepts/services-networking/service/</a></p><p>Service对象通过servicespec.type来设定类型, 共计4个类型: ClusterIP, NodePort, LoadBalancer, ExternalName. 这四个类型可以分为两类</p><ul><li>创建Service对象供集群内部访问</li></ul><p>ClusterIP(默认类型): 反向代理集群内的pod. 供集群内其它服务访问. 流量过程是: 集群内部其它服务=&gt;ClusterIP:端口=&gt;Pod</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  name: myservicespec:  selector:    app: myapp  ports:  - protocol: TCP    port: 80    targetPort: 8080    name: myapp-http</code></pre><blockquote><p>上述例子中, 集群内部访问myservice.default的时候,此时会访问到app=myapp的pod</p><p>这里port是Service暴露端口, targetPort是pod暴露端口. 默认情况下, targetPort将等于port</p><p>spec.type没有定义是因为ClusterIP是Service对象的默认类型</p></blockquote><p>ExternalName: 构建一个CNAME解析(service对象-CNAME-其它域名).  我能想到的主要是让集群内部服务可以访问到集群外部服务.</p><p>例如:</p><pre><code class="language-yaml">kind: ServiceapiVersion: v1metadata:  name: my-service  namespace: prodspec:  type: ExternalName  externalName: my.database.example.com</code></pre><blockquote><p>集群内部访问my-service.prod的时候, <a href="http://xn--k8sdnsmy-vq8m536awioci6an95by58d3far6b.database.example.com">将通过k8s的dns服务返回my.database.example.com</a></p></blockquote><ul><li>供集群外部访问</li></ul><p>NodePort: 反向代理集群内的pod(使用NAT在每一个集群node上的相同端口上公开Service, 是ClusterIP类型的超集). 供集群外服务访问. 流量过程是: 集群外=&gt;任意节点ip:端口=&gt;ClusterIP:端口=&gt;Pod</p><pre><code class="language-yaml">apiVersion: v1kind: Servicemetadata:  name: myservicespec:  selector:    app: myapp  type: NodePort  ports:  - protocol: TCP    nodePort: 31000    port: 80    targetPort: 80    name: myapp-http</code></pre><blockquote><p>集群外部此时可以访问任意物理节点ip:31000, 此时可以访问到集群内部app=myapp的pod.</p><p>这里nodePort是物理节点暴露端口, port是Service暴露端口, targetPort是pod暴露端口.</p><p>nodePort可以不定义, 则此时会自动从30000-32767随机分配.</p></blockquote><p>LoadBalancer: 对接云商的负载均衡服务, 给Service分配一个固定IP. 方便云服务商的LB服务绑定. (是NodePort类型的超集). 流量过程是: 集群外=&gt;云服务LB=&gt;任意物理节点ip:端口=&gt;ClusterIP:端口=&gt;Pod</p><p>这种类型建议直接参考官方文档: <a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer</a></p><h2 id="Headless-Service">Headless Service</h2><p>这是一种特殊的对象, 从配置上来看,它和ClusterIP类型的区别就是<code>service.spec.clusterIP: None</code>.</p><p>它具有以下特点:</p><ul><li>结合StatefulSet对象使用, 从而创建一连串名称有序的有状态应用副本.</li><li>pod拥有整个生命周期内唯一且不变的域名. 因此 pod 之间可以通过域名访问.它的格式应该是这样的:<code>&lt;podname&gt;-&lt;index&gt;.&lt;svcname&gt;.&lt;nsname&gt;.svc.cluster.local</code></li></ul><p>一个关于zk集群构建的k8s官方例子: <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/">https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/</a></p><h2 id="粘性会话">粘性会话</h2><p>有些时候,我们需要会话黏性,你可以通过service.spec.sessionAffinity=ClientIP来设置.并同时可以通过service.spec.sessionAffinityConfig.clientIP.timeoutSeconds来设置会话保持时间,它默认是3小时.</p><h2 id="多端口">多端口</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services">https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services</a></p><p>有些时候,某个服务可能同时存在多个端口,例如web服务,会同时有80和443.这种情况下你需要针对两个端口分别设定一个端口名称.即 <a href="http://service.spec.ports.name">service.spec.ports.name</a></p><h2 id="环境变量">环境变量</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables">https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables</a></p><p>当service创建的时候,kubelet会生成一批环境变量.你可以在其它pod中使用这些环境变量访问service.不过前提是service对象必须提前创建.</p><p>不过,我觉得一般来说,有dns方式,都会通过dns去访问.而且dns也没有先创建service这一个限制.</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞06无状态服务deployment-pod更新</title>
      <link href="posts/18e4fa73/"/>
      <url>posts/18e4fa73/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>对于更新,我们一般会采用滚动更新,即:</p><ul><li>先扩展一部分新版本</li><li>删除一部分老版本</li><li>重复上述行为,直至所有老版本被替换.</li></ul><p>对于回滚,我们想回滚到任何一个之前的版本.</p><p>而deployment对象可以帮我们实现这些.</p><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></p><p>一个基本的deployment例子</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80</code></pre><p>例子需要注意的点：</p><ul><li>metadata： 这里会定义name，labels，namespace，annotations。其中 annotations （注解）是特殊的，你可以用它来定义一些工具集，例如在阿里云的k8s服务中，你可以添加镜像快照注解信息，从而让 pod 无需从镜像仓库下载启动，而是直接通过快照加速启动。</li><li>selector： deployment使用selector指定的label关联pod。</li></ul><h2 id="构建deployment">构建deployment</h2><p>创建一个nginx的deployment</p><pre><code class="language-bash">kubectl apply -f nginx-dep.yaml===apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-dep  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        ports:        - containerPort: 80</code></pre><blockquote><p>这里dep的配置中spec.template.spec.container.image=nginx，我没有设置具体版本</p></blockquote><h2 id="确认当前版本">确认当前版本</h2><p>查看版本历史，使用kubectl rollout history</p><blockquote><p>这里涉及到 rollout 指令，回滚的时候也会用到.</p></blockquote><pre><code class="language-bash">kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         &lt;none&gt;</code></pre><p>上述结果中，版本1没有添加 CHANGE-CAUSE 信息</p><p>添加注释信息，使用kubectl annotate deployment/&lt;&gt; <a href="http://kubernetes.io/change-cause=%22first%22">kubernetes.io/change-cause=“first”</a></p><pre><code class="language-bash">kubectl annotate deployment/nginx-dep kubernetes.io/change-cause=&quot;first&quot;kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         first</code></pre><blockquote><p>你也可以在执行命令的时候，追加 --record 来将执行的命令写入到当前change-cause</p><p>如果你不小心写错了，你可以通过添加<code>-</code> 来删除注解，就像下面这样，命令最后有一个<code>-</code></p><p>kubectl annotate deployment/nginx-dep &lt;注解key&gt;-</p></blockquote><h2 id="更新版本">更新版本</h2><p>发布版本1.19.2，将deployment中所有nginx的image版本从默认变更为1.19.2。</p><pre><code class="language-bash">kubectl set image deployment/nginx-dep nginx=nginx:1.19.2kubectl annotate deployment/nginx-dep kubernetes.io/change-cause=&quot;nginx image to 1.19.2&quot;kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         first2         nginx image to 1.19.2</code></pre><blockquote><p>除了使用set指令修改，你也可以通过edit修改yaml配置的方式进行</p><p>kubectl edit deployment/nginx-dep</p></blockquote><p>更新状态查看，可运行：</p><pre><code class="language-bash">kubectl rollout status deployment/nginx-dep</code></pre><p>更深层的rs状态查看，可运行：</p><pre><code class="language-bash">kubectl get rs -l app=nginx</code></pre><blockquote><p>-l 是调用标签过滤</p></blockquote><p>再发布一个版本1.18，将deployment中所有nginx的image版本从1.19.2变更为1.18。</p><pre><code class="language-bash">kubectl set image deployment/nginx-dep nginx=nginx:1.18kubectl annotate deployment/nginx-dep kubernetes.io/change-cause=&quot;nginx image to 1.18&quot;kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE1         first2         nginx image to 1.19.23         nginx image to 1.18</code></pre><p>这样,我们总共有三个版本，你可以通过追加版本号来看具体的版本内容</p><pre><code class="language-bash">kubectl rollout history deployment/nginx-dep --revision=&lt;num&gt;</code></pre><h2 id="回滚版本">回滚版本</h2><p>我们选择将nginx版本回滚到first第一个版本.</p><pre><code class="language-bash">kubectl rollout undo deployment/nginx-dep --to-revision=1kubectl rollout history deployment/nginx-dep#---deployment.apps/nginx-dep REVISION  CHANGE-CAUSE2         nginx image to 1.19.23         nginx image to 1.184         first</code></pre><h2 id="更新策略">更新策略</h2><p>deployment 的 .spec 中可以添加一些策略.</p><pre><code class="language-bash">spec:    minReadySeconds: 3  # pod 就绪时间，在此时间之前，deployment 认为 pod 还没有准备好. 默认0  revisionHistoryLimit: 10 # 最大版本保留次数. 默认10   strategy:    type: rollingUpdate # 定义变更策略. 除了 rollingUpdate，还可以是Recreate.Recreate指的是先删除所有pod,再创建.    rollingUpdate:      maxUnavailable: 30% # 变更期间最多存在多少个不可用的pod,可以是具体数字      maxSurge: 30% # 变更期间最多存在多少个超出已定义副本的pod数量,可以是具体数字</code></pre><blockquote><p>maxUnavailable 和 maxSurge 设置相等即可. 即开多少个新的，就关多少个老的</p></blockquote><h2 id="最终样子">最终样子</h2><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-dep  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  minReadySeconds: 3  # pod 就绪时间，在此时间之前，deployment 认为 pod 还没有准备好. 默认0  revisionHistoryLimit: 10 # 最大版本保留次数. 默认10   strategy:    type: rollingUpdate # 定义变更策略. 除了 rollingUpdate，还可以是Recreate.Recreate指的是先删除所有pod,再创建.    rollingUpdate:      maxUnavailable: 30% # 变更期间最多存在多少个不可用的pod,可以是具体数字      maxSurge: 30% # 变更期间最多存在多少个超出已定义副本的pod数量,可以是具体数字  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx        ports:        - containerPort: 80</code></pre><h2 id="暂停服务">暂停服务</h2><pre><code class="language-yaml">kubectl rollout pause deployment/nginx-dep</code></pre><p>启用暂停后，再启用更新，则新的更新在暂停期间不会产生任何效果</p><h2 id="恢复服务">恢复服务</h2><pre><code class="language-yaml">kubectl rollout resume deployment/nginx-dep</code></pre><p>启用恢复后，则暂停期间启用的更新将开始发挥作用</p><h2 id="小结">小结</h2><pre><code class="language-bash">kubectl rollout history deployment/&lt;&gt; # 查看对象历史kubectl rollout undo deployment/&lt;&gt; --to-revision=&lt;&gt; # 对象版本回退kubectl rollout status deployment/&lt;&gt;# 展示执行状态kubectl rollout pause deployment/&lt;&gt; # 暂停此次版本操作行为kubectl rollout resume deployment/&lt;&gt; # 恢复此次版本操作行为kubectl set image # 修改镜像配置kubectl annotate # 添加一个注释</code></pre><p>更新过程：</p><ul><li>创建新的replicaset</li><li>将新的replicaset扩展</li><li>将旧的replicaset缩容</li><li>新的replicaset对应的pod已就绪或可用</li></ul><p>当<code>kubectl rollout status deployment/&lt;&gt;</code>命令执行完后，$? 状态为 0，则说明 deployment 处于 complete。</p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞07无状态服务deployment-pod扩缩</title>
      <link href="posts/f6e328b5/"/>
      <url>posts/f6e328b5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>k8s的pod缩放功能,和aws的auto scaling功能是一回事。虽然可能没有aws的auto scaling功能强大。</p><p>k8s的pod自动缩放功能称之为HPA(Horizontal Pod Autoscaler)，它可以基于设定的cpu阈值，来自动调整deployment中的pod数量。</p><h2 id="手动缩放">手动缩放</h2><pre><code class="language-yaml">kubectl scale deployment.v1.apps/nginx-dep --replicas=10</code></pre><h2 id="自动缩放">自动缩放</h2><p>k8s的自动缩放首先要安装监控服务，毕竟没有监控指标，就无法根据指标进行自动缩放。</p><h3 id="metrics-server">metrics-server</h3><p><a href="https://github.com/kubernetes-sigs/metrics-server">https://github.com/kubernetes-sigs/metrics-server</a></p><p>metrics 服务器可以通过资源度量值 API 对外提供度量数据，Horizontal Pod Autoscaler 正是根据此 API 来获取度量数据.如果没有此服务,HPA将无法工作.</p><pre><code class="language-bash">wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml</code></pre><p>默认 metrics-server 的 deployment 无法直接使用, 我们需要添加几个参数,  来禁止 ca 认证和开通 dns</p><p>在 deployment.spec.template.spec.containers 中新加入下列配置:</p><pre><code class="language-yaml">        command:          - /metrics-server          - --kubelet-insecure-tls          - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname</code></pre><pre><code class="language-bash">kubectl apply -f components.yaml</code></pre><h3 id="hpa">hpa</h3><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</a></p><p>创建一个用于测试的web服务, hpa-test.yaml</p><pre><code class="language-yaml">apiVersion: apps/v1kind: Deploymentmetadata:  name: php-apachespec:  selector:    matchLabels:      run: php-apache  replicas: 1  template:    metadata:      labels:        run: php-apache    spec:      containers:      - name: php-apache        image: k8s.gcr.io/hpa-example        ports:        - containerPort: 80        resources:          limits:            cpu: 500m          requests:            cpu: 200m---apiVersion: v1kind: Servicemetadata:  name: php-apache  labels:    run: php-apachespec:  ports:  - port: 80  selector:    run: php-apache</code></pre><blockquote><p>Service 类型给一组pod提供访问接口</p></blockquote><h3 id="基于CPU指标开启">基于CPU指标开启</h3><p>命令方式:</p><pre><code class="language-bash">kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=5</code></pre><p>声明方式:</p><pre><code class="language-yaml">apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: php-apachespec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: php-apache  minReplicas: 1  maxReplicas: 5  targetCPUUtilizationPercentage: 50</code></pre><blockquote><p>这里的意思是, cpu 阈值50%, 最小pod数1, 最大pod数5. hpa会将所有pod的平均cpu利用率维持在50%, 并且pod数量在1~5的范围内波动</p></blockquote><p>查看当前cpu使用率</p><pre><code class="language-bash">kubectl get hpa===NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          57m</code></pre><p>添加测试用的客户端, 访问web服务, 增加web服务的cpu使用率</p><pre><code class="language-bash">kubectl run -it --rm load-generator --image=busybox /bin/shwhile true; do wget -q -O- http://php-apache; done=== 会输出大量OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!</code></pre><p>在经过一段时间等待后(不会超过1分钟, 默认监控抓取数据间隔时间是1分钟),我们可以看到 pod 数量发生变化</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          60m[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   0%/50%    1         5         1          60m[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   250%/50%   1         5         1          61m[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   250%/50%   1         5         1          61m[root@k8s00 test-yaml]# kubectl get deployment/php-apacheNAME         READY   UP-TO-DATE   AVAILABLE   AGEphp-apache   5/5     5            5           68m[root@k8s00 test-yaml]# </code></pre><p>现在,关闭客户端请求,等待1分钟以上.再次看hpa, 这时候cpu利用率已经下降</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl get hpaNAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGEphp-apache   Deployment/php-apache   11%/50%   1         5         5          64m</code></pre><p>这时候,再看pod数量,它应该已经开始缩减,但是这个缩减并不是在cpu使用率下降之后就立即执行,而是内部有一个算法.它避免因立即执行从而导致资源出现反复波动.</p><pre><code class="language-bash">[root@k8s00 test-yaml]# kubectl get deployment php-apacheNAME         READY   UP-TO-DATE   AVAILABLE   AGEphp-apache   5/5     5            5           70m[root@k8s00 test-yaml]# kubectl describe deployment/php-apacheName:                   php-apacheNamespace:              defaultCreationTimestamp:      Wed, 16 Sep 2020 10:22:32 +0800Labels:                 &lt;none&gt;Annotations:            deployment.kubernetes.io/revision: 1Selector:               run=php-apacheReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailableStrategyType:           RollingUpdateMinReadySeconds:        0RollingUpdateStrategy:  25% max unavailable, 25% max surgePod Template:  Labels:  run=php-apache  Containers:   php-apache:    Image:      k8s.gcr.io/hpa-example    Port:       80/TCP    Host Port:  0/TCP    Limits:      cpu:  500m    Requests:      cpu:        200m    Environment:  &lt;none&gt;    Mounts:       &lt;none&gt;  Volumes:        &lt;none&gt;Conditions:  Type           Status  Reason  ----           ------  ------  Progressing    True    NewReplicaSetAvailable  Available      True    MinimumReplicasAvailableOldReplicaSets:  &lt;none&gt;NewReplicaSet:   php-apache-5c4f475bf5 (1/1 replicas created)Events:  Type    Reason             Age    From                   Message  ----    ------             ----   ----                   -------  Normal  ScalingReplicaSet  14m    deployment-controller  Scaled up replica set php-apache-5c4f475bf5 to 4  Normal  ScalingReplicaSet  14m    deployment-controller  Scaled up replica set php-apache-5c4f475bf5 to 5  Normal  ScalingReplicaSet  6m50s  deployment-controller  Scaled down replica set php-apache-5c4f475bf5 to 4  Normal  ScalingReplicaSet  4m49s  deployment-controller  Scaled down replica set php-apache-5c4f475bf5 to 1</code></pre><h3 id="基于其它非资源类型的指标">基于其它非资源类型的指标</h3><p>当前hpa的api版本是autoscaling/v1，它只能抓取cpu指标。</p><p>如果你想抓取内存指标(当前也只支持内存)和非资源类型指标,那么需要将版本变更为autoscaling/v2beta2。</p><p>autoscaling/v2beta2版本的配置有了一些变化,且可能随时会有新的变化.你可以在</p><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics</a> 找到它的例子。</p><pre><code class="language-yaml">apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata:  name: php-apachespec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: php-apache  minReplicas: 1  maxReplicas: 10  metrics:  - type: Resource    resource:      name: cpu      target:        type: AverageUtilization        averageUtilization: 50  - type: Pods    pods:      metric:        name: packets-per-second      target:        type: AverageValue        averageValue: 1k  - type: Object    object:      metric:        name: requests-per-second      describedObject:        apiVersion: networking.k8s.io/v1beta1        kind: Ingress        name: main-route      target:        kind: Value        value: 10k</code></pre><p>上面的例子意思是：</p><ol><li>保持每个pod的cpu使用率不超过50%，并且每秒能承受1000个数据包。</li><li>所有位于ingress后面的pod，每秒能处理10000个请求。</li></ol><blockquote><p>请注意：</p><ol><li>targetCPUUtilizationPercentage被metrics数组代替，metrics数组包含了多种度量指标。</li><li>target中数据由两种组成：<ul><li>averageValue：每一个pod都需要满足此指标值</li><li>value：所有pod供需满足此指标</li></ul></li></ol></blockquote><p>这些指标可以分为三类，第一类 Resource，第二类 Pods，第三类 Object。</p><h3 id="基于标签的k8s对象指标">基于标签的k8s对象指标</h3><pre><code class="language-yaml">- type: Object  object:    metric:      name: `http_requests`      selector: `verb=GET`    target:      type: AverageValue      averageValue: 30</code></pre><blockquote><p>从监控系统里获取标签verb=GET的值</p></blockquote><p>上述例子的意思是，确保每一个pod都能满足标签为verb=GET的http请求数量。</p><h3 id="基于非k8s对象指标">基于非k8s对象指标</h3><pre><code class="language-yaml">- type: External  external:    metric:      name: queue_messages_ready      selector: &quot;queue=worker_tasks&quot;    target:      type: AverageValue      averageValue: 30</code></pre><p>当类型变为External的时候，就可以指定监控系统里存储的其它监控指标。</p><p>例如，监控系统从消息队列服务里拿到了消息数量的指标，并附加queue=worker_tasks。那么就可以和上述例子一样，根据消息数量指标进行缩放。</p><h2 id="度量">度量</h2><p>hpa和度量指标API里的数据均使用k8s中称为量纲的特殊整数表示。</p><p>量纲的意思就是：</p><ul><li>数字太小就加上后缀单位m，例如1.5写成1500m。</li><li>数字刚好就是整数显示。</li><li>数字大就加上后缀单位，比如k，M。</li><li>总之没有小数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞常用命令</title>
      <link href="posts/e6ae9474/"/>
      <url>posts/e6ae9474/</url>
      
        <content type="html"><![CDATA[<h2 id="查询库表占用空间">查询库表占用空间</h2><pre><code class="language-mysql">SELECT    table_schema AS '数据库',    table_name AS '表名',    table_rows AS '记录数',    TRUNCATE (data_length / 1024 / 1024 / 1024, 3) AS '数据容量(GB)',    TRUNCATE (index_length / 1024 / 1024 / 1024 , 3) AS '索引容量(GB)',TRUNCATE (DATA_FREE / 1024 / 1024 / 1024, 3) AS '已占用空间但未使用(GB)'FROM    information_schema. TABLESWHERE    table_schema in ('new_data','megable_main','megable_active')ORDER BY    data_length DESC;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞05pod探测器</title>
      <link href="posts/9aa66899/"/>
      <url>posts/9aa66899/</url>
      
        <content type="html"><![CDATA[<h2 id="探测器">探测器</h2><blockquote><p><a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes">https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes</a></p></blockquote><p>探测器有三种：启动探测器startupProbe、存活探测器livenessProbe，就绪探测器readinessProbe</p><p>探测器有三种探测方式，命令方式，http方式，tcp方式。</p><p>例如命令方式：</p><pre><code class="language-yaml">    livenessProbe:      exec:        command:        - cat        - /tmp/healthy      initialDelaySeconds: 5      periodSeconds: 5</code></pre><p>kubelet 使用启动探测器可以知道应用程序容器什么时候启动了。 如果配置了这类探测器，就可以控制容器在启动成功后再进行存活性和就绪检查， 确保这些存活、就绪探测器不会影响应用程序的启动。 这可以用于对慢启动容器进行存活性检测，避免它们在启动运行之前就被杀掉。</p><p>kubelet 使用就绪探测器可以知道容器什么时候准备好了并可以开始接受请求流量， 当一个 Pod 内的所有容器都准备好了，才能把这个 Pod 看作就绪了。 这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端。 在 Pod 还没有准备好的时候，会从 Service 的负载均衡器中被剔除的。</p><p>kubelet 使用存活探测器来知道什么时候要重启容器。 例如，存活探测器可以捕捉到死锁程序（进程在，但程序已无法进一步执行）。 这样的情况下重启容器有助于让应用程序在有问题的情况下更可用。</p><h2 id="探测器的一些配置">探测器的一些配置</h2><ul><li><code>initialDelaySeconds</code>：容器启动后要等待多少秒后存活和就绪探测器才被初始化，默认是 0 秒，最小值是 0。</li><li><code>periodSeconds</code>：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。</li><li><code>timeoutSeconds</code>：探测的超时后等待多少秒。默认值是 1 秒。最小值是 1。</li><li><code>successThreshold</code>：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。 存活和启动探测的这个值必须是 1。最小值是 1。</li><li><code>failureThreshold</code>：当探测失败时，Kubernetes 的重试次数。 存活探测情况下的放弃就意味着重新启动容器。 就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。</li></ul><h2 id="启动探测器-startupProbe">启动探测器 startupProbe</h2><p>目的在于设置一个容器启动宽容期。当启动探测器确认容器没问题后，后续的健康状态将交给存活探测器。</p><p>它仅在容器启动初期运行。</p><p>例如：</p><pre><code class="language-yaml">ports:- name: liveness-port  containerPort: 8080  hostPort: 8080startupProbe:  httpGet:    path: /healthz    port: liveness-port  failureThreshold: 30  # 错误次数阈值  periodSeconds: 10  # 检测间隔周期  # 启动超时时间=failureThreshold*periodSeconds</code></pre><blockquote><p>kubelet 检测总时间超过 300 秒后，如果还未检测成功，认为启动失败。</p></blockquote><h2 id="就绪探测器-readinessProbe">就绪探测器 readinessProbe</h2><p>目的在于仅确认容器是否健康，但是并不删除容器，不健康时候不提供接收svc流量。适用于下列场景：</p><ol><li>容器启动后，加载数据多，加载完之前无法提供服务</li></ol><p>它将在容器整个生命周期中运行。</p><p>例如：</p><pre><code class="language-yaml:">apiVersion: v1kind: Podmetadata:  name: goproxy  labels:    app: goproxyspec:  containers:  - name: goproxy    image: k8s.gcr.io/goproxy:0.1    ports:    - containerPort: 8080    readinessProbe:      tcpSocket:        port: 8080      initialDelaySeconds: 5      periodSeconds: 10</code></pre><blockquote><p>kubelet 等待 initialDelaySeconds 后开始第一次检测，每次检测间隔10秒，检测成功，认为可以提供服务</p></blockquote><h2 id="存活探测器-livenessProbe">存活探测器 livenessProbe</h2><p>目的在于确认容器是否健康，并在不健康的时候重启容器。</p><p>它将在容器整个生命周期中运行。</p><p>它与就绪探测器最大区别就是，kubelet检测失败后会重启容器。</p><p>例子如下：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  labels:    test: liveness  name: liveness-httpspec:  containers:  - name: liveness    image: k8s.gcr.io/liveness    args:    - /server    livenessProbe:      httpGet:        path: /healthz        port: 8080        httpHeaders:        - name: Custom-Header          value: Awesome      initialDelaySeconds: 3      periodSeconds: 3</code></pre><blockquote><p>探测器会认为 200&lt;=x&lt;400 的状态码为正常。</p></blockquote><h2 id="最佳实践">最佳实践</h2><ol><li>存活和就绪探针的健康状态接口应该是两个互相独立的接口</li><li>存活探针的健康状态接口应该是直接返回状态结果，不应该有任何逻辑</li><li>就绪探针的健康状态接口应该提供就绪所需要的逻辑。但是不应该添加重新构建就绪的逻辑。例如：处理http请求的程序需要一个数据库连接的就绪状态，那么就应该在就绪探针的健康状态接口里添加数据库连接的检查逻辑。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞预警</title>
      <link href="posts/4ebbd3a8/"/>
      <url>posts/4ebbd3a8/</url>
      
        <content type="html"><![CDATA[<p>定义预警分为几个步骤</p><ol><li>定义预警媒介</li><li>定义预警用户</li><li>定义预警动作</li></ol><p>触发器达到阈值，调用预警动作，预警动作调用预警用户，预警用户调用预警媒介</p><h2 id="预警媒介">预警媒介</h2><p>配置-报警媒介类型，这里有两个要素</p><h3 id="报警媒介类型">报警媒介类型</h3><p>类型有很多种，比如邮件，自定义脚本，这里我们选自定义脚本，来定义一个企业微信预警</p><p><img src="/posts/4ebbd3a8/image-20200902144015196.png" alt="image-20200902144015196"></p><p>脚本需要接收三个参数：</p><p>{ALERT.SENDTO}：收件人</p><p>{ALERT.SUBJECT}：主题</p><p>{ALERT.MESSAGE}：内容</p><p>我们一般只用{ALERT.SENDTO}和{ALERT.MESSAGE}即可</p><h3 id="信息模板">信息模板</h3><p>定义信息模板，一般我们定义告警模板/恢复模板/自动注册模板</p><p>告警模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144408513.png" alt="image-20200902144408513"></p><pre><code>【[骷髅]】=》&#123;TRIGGER.SEVERITY&#125; 事件 : &#123;EVENT.ID&#125;【问题时间】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125; 【问题名字】: &#123;EVENT.NAME&#125;【问题主机】: &#123;HOST.NAME&#125;【问题级别】: &#123;EVENT.SEVERITY&#125;【阈值信息】: &#123;TRIGGER.NAME&#125;【阈值项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;【Operational data】: &#123;EVENT.OPDATA&#125;&#123;TRIGGER.URL&#125;</code></pre><p>恢复模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144504865.png" alt="image-20200902144504865"></p><pre><code>【[OK]】=》&#123;TRIGGER.SEVERITY&#125; 事件 : &#123;EVENT.ID&#125;【问题时间】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125; 【问题恢复时间】: &#123;EVENT.RECOVERY.DATE&#125;  &#123;EVENT.RECOVERY.TIME&#125;【问题名字】: &#123;EVENT.NAME&#125;【问题主机】: &#123;HOST.NAME&#125;【问题级别】: &#123;EVENT.SEVERITY&#125;【阈值信息】: &#123;TRIGGER.NAME&#125;【阈值项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;【Operational data】: &#123;EVENT.OPDATA&#125;&#123;TRIGGER.URL&#125;</code></pre><p>自动注册模板如图：</p><p><img src="/posts/4ebbd3a8/image-20200902144535568.png" alt="image-20200902144535568"></p><pre><code>自动注册: 主机名: &#123;HOST.HOST&#125;主机ip: &#123;HOST.IP&#125;代理端口: &#123;HOST.PORT&#125;</code></pre><h3 id="放置发送信息的脚本">放置发送信息的脚本</h3><p>具体位置以zb server的安装和配置为基准.</p><pre><code class="language-python">#!/usr/bin/env python3&quot;&quot;&quot;@author: zyh@contact: aaa103439@hotmail.com@software: vscode@file: sendchat.py@time: 2020/02/05@use: pip3 install requests configparser&quot;&quot;&quot;import sys, os, requests, pathlib, json, configparserimport loggingimport logging.handlersfrom datetime import datetimeclass PySendchat():    def __init__(self, corpid, agentid, secret, touser, content):        self.corpid=corpid        self.agentid=agentid        self.secret=secret        self.touser=touser        self.content=content        LOG_PATHDIR=os.path.dirname(os.path.abspath(__file__))        LOG_FILENAME = '&#123;0&#125;/sendchat.log'.format(LOG_PATHDIR)        self.my_logger = logging.getLogger('SendChat')        self.my_logger.setLevel(logging.INFO)        # Add the log message handler to the logger        handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=102400000, backupCount=5)        # create formatter and add it to the handlers        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')        handler.setFormatter(formatter)        self.my_logger.addHandler(handler)    def gettoken(self):        self.my_logger.info('-----------------------------------------------------------------------------------------')        pwd=os.path.dirname(os.path.abspath(__file__))        tokenfile='&#123;0&#125;/wechat.&#123;1&#125;'.format(pwd,self.agentid)        if pathlib.Path(tokenfile).exists():            tokenfilectime=os.path.getctime(tokenfile)            currenttime=datetime.now().timestamp()            dtime=currenttime-tokenfilectime            self.my_logger.info('&#123;0&#125; lived &#123;1&#125;s.'.format(tokenfile, dtime))            if dtime &gt;= 7200:                try:                    os.remove(tokenfile)                    self.my_logger.info('Token file &#123;0&#125;: delete success'.format(tokenfile))                except Exception as e:                    self.my_logger.error('Token file:&#123;0&#125; delete error.Reason:&#123;1&#125;'.format(tokenfile,e))                    exit        # check token file        try:            tokensize = os.path.getsize(tokenfile)        except Exception as e:            self.my_logger.info('Token file is not exist.Reason:&#123;0&#125;'.format(e))            tokensize = 0        # get token from token file        if tokensize != 0:            with open(tokenfile, 'rb') as fd:                token = fd.read() # get token success                self.my_logger.info('Get token from token file.')            jsonObject = json.loads(token.decode(encoding='utf8'))            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token        # get token from weixin api        else:            try:                self.my_logger.info('New Token Create.')                f = requests.get('https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&#123;0&#125;&amp;corpsecret=&#123;1&#125;'.format(self.corpid, self.secret))                token = f.content                self.my_logger.info('Get token from weixin api.')                jsonObject = json.loads(token.decode(encoding='utf8'))                errcode=int(jsonObject.get(&quot;errcode&quot;))                if errcode != 0:                    errmsg=jsonObject.get(&quot;errmsg&quot;)                    self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(errmsg))                    exit()            except Exception as e:                self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(e))                exit()            try:                self.my_logger.info('Write token to &#123;0&#125;.'.format(tokenfile))                with open(tokenfile, 'wb') as fd:                    fd.write(token)            except Exception as e:                self.my_logger.error('Write &#123;0&#125; error!Reason:&#123;1&#125;'.format(tokenfile,e))                exit()            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token    def sendmsg(self):        accessToken = self.gettoken()        self.my_logger.info('Token:&#123;0&#125;'.format(accessToken))        sendMapDirectroy = &#123;&#125;        sendMapDirectroy[&quot;agentid&quot;] = self.agentid        sendMapDirectroy[&quot;touser&quot;] = self.touser        sendMapDirectroy[&quot;msgtype&quot;] = &quot;text&quot;        sendMapDirectroy[&quot;safe&quot;] = &quot;0&quot;        contentDirectory = &#123;&#125;        sendMapDirectroy[&quot;text&quot;] = contentDirectory        contentDirectory[&quot;content&quot;] = self.content        bodyStr = json.dumps(sendMapDirectroy, ensure_ascii=False).encode(encoding=&quot;utf-8&quot;)        try:            f = requests.post(url=&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s&quot; % accessToken,                          data=bodyStr, timeout=5)            self.my_logger.info(f.content)        except Exception as e:            self.my_logger.error('Send chat network error!Reason:&#123;0&#125;'.format(e))if __name__ == '__main__':    appname = sys.argv[1]    content = sys.argv[3]    # read conf.ini    conf = configparser.ConfigParser()    conf_path = os.path.dirname(os.path.abspath(__file__))    conf_ini = &quot;&#123;0&#125;/conf.ini&quot;.format(conf_path)    if pathlib.Path(conf_ini).exists():        conf.read(conf_ini)        corpid = conf.get(&quot;wechat&quot;, &quot;corpid&quot;)        appinfo = conf.get(&quot;app&quot;, appname)        agentid = appinfo.split(':')[0]        secret = appinfo.split(':')[1]        groupname = conf.get(&quot;group&quot;, appname)        touser = groupname.split(':')[0]        chatobj = PySendchat(corpid, agentid, secret, touser, content)    else:        print('conf.ini error')        exit()    try:        chatobj.sendmsg()    except:        chatobj.my_logger.error(&quot;Send chat failure!&quot;)</code></pre><pre><code class="language-ini"># conf.ini[wechat]corpid = [app]it = &lt;app_agent_id&gt;:&lt;app_secret&gt;[group]it = usera|userb</code></pre><p>eg:</p><p>python3 <a href="http://wechat.py">wechat.py</a> it ‘’ &lt;预警内容&gt;</p><p>因 zabbix 调用脚本并非 root 用户，所以给脚本附加 x 权限，避免权限问题。</p><pre><code class="language-bash">chmod a+x wechat.py# 因脚本会生成其它文件# 所以还需要给脚本所在的目录添加其它用户的写入权限chmod 757 alertscripts</code></pre><h2 id="预警用户">预警用户</h2><p>管理-用户-添加用户，这里有三要素</p><h3 id="用户">用户</h3><p>你无需登陆这个用户，所以密码可以尽量的复杂</p><p><img src="/posts/4ebbd3a8/image-20200902150131147.png" alt="image-20200902150131147"></p><h3 id="报警媒介">报警媒介</h3><p><img src="/posts/4ebbd3a8/image-20200902150145022.png" alt="image-20200902150145022"></p><blockquote><p>我们定义这个用户被调用的时候，将会发送信息给it组，至于it组包含哪些人员，则由脚本定义。</p></blockquote><h3 id="权限">权限</h3><p><img src="/posts/4ebbd3a8/image-20200902150153011.png" alt="image-20200902150153011"></p><h2 id="预警动作">预警动作</h2><p>配置-动作-左上角（触发器动作Trigger actions），这里我们需要定义两个要素</p><ol><li>什么条件下执行动作</li><li>动作的实际行为</li></ol><h3 id="条件">条件</h3><p>需要注意的是计算方式</p><blockquote><p>问题没有被制止的意思：没有人为的关闭预警</p></blockquote><p><img src="/posts/4ebbd3a8/image-20200902150806009.png" alt="image-20200902150806009"></p><h3 id="操作">操作</h3><p><img src="/posts/4ebbd3a8/image-20200902151037597.png" alt="image-20200902151037597"></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zabbix </tag>
            
            <tag> wechat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞容器搭建</title>
      <link href="posts/25f36846/"/>
      <url>posts/25f36846/</url>
      
        <content type="html"><![CDATA[<h2 id="创建网络">创建网络</h2><pre><code class="language-bash">docker network create -d bridge zbnet</code></pre><h2 id="mysql">mysql</h2><pre><code class="language-bash">mkdir -p /export/docker-data-mysql/&#123;config,logs,data&#125;docker run --name mysql57 \-p 3306:3306 \--network zbnet \--restart=always \--mount 'type=bind,src=/export/docker-data-mysql/config,dst=/etc/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/logs,dst=/var/log/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/data,dst=/var/lib/mysql' \-e MYSQL_ROOT_PASSWORD=123456 \-d mysql:5.7</code></pre><h2 id="zabbix-server">zabbix-server</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-server-mysql">https://hub.docker.com/r/zabbix/zabbix-server-mysql</a></p></blockquote><p>安装 centos + server + mysql 版本</p><p>默认，环境变量 <code>MYSQL_USER</code> and <code>MYSQL_PASSWORD</code> are <code>zabbix</code>, <code>zabbix</code>.</p><pre><code>mkdir -p /export/docker-data-zbserver/alertscriptsdocker volume create zabbixServerEtcdocker run --name=zabbix_server \-p 10051:10051 \--network zbnet \--restart=always \--mount 'type=volume,src=zabbixServerEtc,dst=/etc/zabbix' \--mount 'type=bind,src=/export/docker-data-zbserver/alertscripts,dst=/usr/lib/zabbix/alertscripts' \-e DB_SERVER_HOST=&quot;mysql57&quot; \-e MYSQL_DATABASE=&quot;zabbixserver&quot; \-e MYSQL_USER=&quot;zabbix&quot; \-e MYSQL_PASSWORD=&quot;zabbix&quot; \-e MYSQL_ROOT_PASSWORD=&quot;123456&quot; \-d zabbix/zabbix-server-mysql:centos-latest</code></pre><ol><li><p>默认安装完之后，容器内缺少一些开发环境，这会导致我们的告警脚本执行失败，比如我是用 python3 写的微信告警，而环境里没有。因此需要额外加装。</p><pre><code class="language-bash">docker logs -f zabbix_server # 可以看到如下错误203:20200831:030724.713 Failed to execute command &quot;/usr/lib/zabbix/alertscripts/wechat.py 'it' '自动注册: xxx-use-001-10-240-128-100' '主机名: xxx-use-001-10-240-128-100主机ip: xxx代理端口: 10050'&quot;: env: 'python3': No such file or directory</code></pre><pre><code class="language-bash"># 安装 python3 和模块docker exec -it -u root zabbix_server yum install python3docker exec -it -u root zabbix_server pip3 install requests configparser</code></pre></li><li><p>alertscripts 目录需要有 other 可执行权限</p><pre><code class="language-bash">chmod o+w /export/docker-data-zbserver/alertscripts</code></pre><p>因为 zabbix_server 在执行告警脚本的时候，用的是普通用户</p></li></ol><h2 id="zabbix-server-web">zabbix-server-web</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-web-nginx-mysql">https://hub.docker.com/r/zabbix/zabbix-web-nginx-mysql</a></p></blockquote><pre><code class="language-bash"># 安装字体yum install google-noto-sans-simplified-chinese-fonts.noarch -ydocker run --name zabbix_web \-p 80:8080 \-p 443:8443 \--restart=always \--network zbnet \--mount 'type=bind,src=/usr/share/fonts/google-noto/NotoSansSC-Regular.otf,dst=/usr/share/zabbix/assets/fonts/DejaVuSans.ttf' \-e DB_SERVER_HOST=&quot;mysql57&quot; \-e MYSQL_DATABASE=&quot;zabbixserver&quot; \-e MYSQL_USER=&quot;zabbix&quot; \-e MYSQL_PASSWORD=&quot;zabbix&quot; \-e MYSQL_ROOT_PASSWORD=&quot;123456&quot; \-e ZBX_SERVER_HOST=&quot;zabbix_server&quot; \-e PHP_TZ=&quot;Asia/Shanghai&quot; \-d zabbix/zabbix-web-nginx-mysql:latest</code></pre><blockquote><p><a href="http://ip">http://ip</a> 即可</p><p>初始账户密码是 Admin/zabbix</p></blockquote><p>到这里，你就可以访问zabbix了，不过这时候你会发现zabbix提示agent不可达</p><p><img src="/posts/25f36846/image-20200901172913999.png" alt="image-20200901172913999"></p><p>其原因是我们还没创建agent</p><p>需要注意的是，因为默认zabbix server的主机配置项监听的是 127.0.0.1:10050, 并且主机名配置的是</p><p><code>zabbix server</code>. 这和我们本文档有一些冲突，所以需要修改一些配置。</p><ol start="2"><li>修改主机配置中的 hostname 为 agent 容器中的环境变量 ZBX_HOSTNAME。这里我们为 zabbix_server</li><li>修改主机配置中的监听地址为 zabbix_server，且监听方式为 dns</li></ol><p>最终修改完如下：</p><p><img src="/posts/25f36846/image-20200901175159400.png" alt="image-20200901175159400"></p><h2 id="zabbix-agent">zabbix-agent</h2><blockquote><p><a href="https://hub.docker.com/r/zabbix/zabbix-agent">https://hub.docker.com/r/zabbix/zabbix-agent</a></p></blockquote><pre><code class="language-bash">docker volume create zbAgentEtcdocker run --name zabbix_agent \--network=container:zabbix_server \--mount 'type=volume,src=zbAgentEtc,dst=/etc/zabbix/zabbix_agentd.d' \-e ZBX_DEBUGLEVEL=&quot;3&quot; \-e ZBX_HOSTNAME=&quot;zabbix_server&quot; \-e ZBX_SERVER_HOST=&quot;zabbix_server&quot; \-d zabbix/zabbix-agent:latest</code></pre><p>这里 zabbix_agent 采用网络模式为容器模式，并加入到 zabbix_server 容器中，以便于 zabbix_server 可以找到 zabbix_agent</p><p>其它配置变量详见上面官方文档</p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞离线迁移脚本</title>
      <link href="posts/c83ca575/"/>
      <url>posts/c83ca575/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">databases=&quot;&quot;mysqltonew()&#123;    olddatabasehost=    olddatabaseport=    olddatabase=$1    oldUserName=    oldpassword=    newdatabasehost=    newolddatabaseport=    newdatabase=$&#123;olddatabase&#125;    newUserName=    newpassword=    # utf8 / utf8mb4    read -p &quot;character [utf8/utf8mb4/latin1]:&quot; Character    # 导出老库    mysqldump -u$&#123;oldUserName&#125; -p$&#123;oldpassword&#125; -h$&#123;olddatabasehost&#125; -P$&#123;olddatabaseport&#125; --default-character-set=$&#123;Character&#125; --single-transaction $&#123;olddatabase&#125; &gt; $&#123;olddatabase&#125;.sql    # 创建新库    mysql -u$&#123;newUserName&#125; -p$&#123;newpassword&#125; -h$&#123;newdatabasehost&#125; -P$&#123;newolddatabaseport&#125; -e &quot;CREATE DATABASE $&#123;newdatabase&#125; DEFAULT CHARSET $&#123;Character&#125; COLLATE $&#123;Character&#125;_general_ci;&quot;    # 导入新库    mysql -u$&#123;newUserName&#125; -p$&#123;newpassword&#125; -h$&#123;newdatabasehost&#125; -P$&#123;newolddatabaseport&#125; --default-character-set=$&#123;Character&#125; $&#123;newdatabase&#125; &lt; $&#123;olddatabase&#125;.sql&#125;for i in $&#123;databases&#125;;do        echo &quot;start $i&quot;        mysqltonew $idone</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
            <tag> mysqldump </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞04pod钩子函数</title>
      <link href="posts/3ec72f7e/"/>
      <url>posts/3ec72f7e/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>我们在实际工作中，可能会遇到容器程序的准备工作，以及容器结束之前的处理工作。比如，容器开始之前下载一些包，或者容器结束之前上传日志等。</p><p>k8s 给我们提供了两个函数用于处理这些工作，分别是 postStart和 preStop</p><h2 id="postStart">postStart</h2><ol><li>它将在容器创建后立即运行，但不保证在 ENTRYPOINT 之前运行</li><li>它是同步状态，如果它卡住，pod 无法达到 running 状态</li><li>它位于容器 lifecycle 中</li></ol><p>示例：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: poststart-testspec:  containers:  - name: poststart-test    image: nginx    lifecycle:      postStart:        exec:          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo This is postStart-test &gt; /usr/share/nginx/html/index.html&quot;]</code></pre><pre><code class="language-bash">[root@k8s00 ~]# kubectl apply -f poststart-test.yamlpod/poststart-test created[root@k8s00 ~]# kubectl get pod -o wideNAME             READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATESpoststart-test   0/1     ContainerCreating   0          7s    &lt;none&gt;   k8s02   &lt;none&gt;           &lt;none&gt;[root@k8s00 ~]# kubectl get pod -o wideNAME             READY   STATUS    RESTARTS   AGE   IP          NODE    NOMINATED NODE   READINESS GATESpoststart-test   1/1     Running   0          10s   10.97.2.6   k8s02   &lt;none&gt;           &lt;none2[root@k8s00 ~]# curl 10.97.2.6This is postStart-test</code></pre><h2 id="preStop">preStop</h2><ol><li>它将在容器结束之前运行</li><li>它是同步的，如果它卡住，pod 将停留在 running 状态，无法达到 failed 状态</li><li>它位于容器 lifecycle 中</li></ol><p>示例：</p><pre><code class="language-yaml">apiVersion: v1kind: Podmetadata:  name: prestop-testspec:  containers:  - name: prestop-test    image: nginx    volumeMounts:    - name: prestop-test-tmp      mountPath: /usr/share    lifecycle:      preStop:        exec:          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo This is preStop &gt; /usr/share/message&quot;]  volumes:  - name: prestop-test-tmp    hostPath:      path: /tmp</code></pre><pre><code class="language-bash">[root@k8s00 ~]# kubectl delete pod prestop-test# 这里 prestop-test 被调度到 k8s02[root@k8s02 ~]# cat /tmp/messageThis is preStop</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞03pod</title>
      <link href="posts/94518966/"/>
      <url>posts/94518966/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>pod 可以说是 k8s 的基础单元. 我觉得可以类比云环境的ecs/ec2这一类的基本计算单元.而 pod 上运行的容器, 可以类比为ecs/ec2上的app程序.</p><p>你总能在k8s的各类资源中找到云环境对应的资源影子. 如果你用过GCP,你会更有这种感觉.</p><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/">https://kubernetes.io/docs/concepts/workloads/pods/</a></p><h2 id="Pod-与-工作负载资源对象">Pod 与 工作负载资源对象</h2><p>pod 一般不单独使用, 因为单独使用意味着没有高可用.k8s建议 pod 要始终和工作负载资源对象一起使用. 你可以认为工作负载资源对象就如同云环境中的资源目标组.这个概念的实际对象差不多是负载均衡服务的缩放组概念. 例如aws的auto scaling.</p><p>k8s将工作负载资源对象主要分为三种:</p><ul><li>Deployment 无状态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</a></li><li>StatefulSet 有状态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a></li><li>DaemonSet 守护态 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a></li></ul><p>还有CronJob这类计划任务类型的.</p><p>工作负载资源对象既然类同云服务的缩放组概念. 那么它必然需要依托于镜像模板和缩放规则.</p><p>镜像模板在k8s中叫 pod 模板(pod template).</p><h2 id="Pod-模板">Pod 模板</h2><p>特点:</p><ul><li>模板修改并应用后，会重建新的pod, 并终止现有pod.</li></ul><p>例子:</p><pre><code class="language-yaml">apiVersion: batch/v1beta1kind: CronJobmetadata:  name: hellospec:  schedule: &quot;*/1 * * * *&quot;  jobTemplate:    spec:      template:   ## 从这里开始, 以下内容就是 Pod 模板.        spec:          containers:          - name: hello            image: busybox            args:            - /bin/sh            - -c            - date; echo Hello from the Kubernetes cluster          restartPolicy: OnFailure</code></pre><blockquote><p>这是一个计划任务，它会遵循 “*/1 * * * *” 时间规则来启动 hello</p></blockquote><h3 id="如何寻找最合适的kind所属的apiVersion">如何寻找最合适的kind所属的apiVersion</h3><p>在这里可能有人不知道如何选择apiVersion。你可以通过kubectl api-versions来找到kind所属的apiGroup，然后再通过</p><p>kubectl get --raw “/apis” 的输出找 preferredVersion。</p><pre><code class="language-bash">➜  kubectl api-resources | grep jobscronjobs                          cj           batch                          true         CronJobjobs                                           batch                          true         Job➜  kubectl get --raw &quot;/apis&quot; |python -mjson.tool | grep -B 1 batch        &#123;            &quot;name&quot;: &quot;batch&quot;,--                &#123;                    &quot;groupVersion&quot;: &quot;batch/v1&quot;,--                &#123;                    &quot;groupVersion&quot;: &quot;batch/v1beta1&quot;,--            &quot;preferredVersion&quot;: &#123;                &quot;groupVersion&quot;: &quot;batch/v1&quot;,</code></pre><p>如上命令所示，在我的k8s版本中 kind: jobs 的最合适apiVersion是 batch/v1</p><h2 id="Pod-存储-网络">Pod 存储/网络</h2><h3 id="pod-存储">pod 存储</h3><p>这是一个大问题. 如果你想真正的使用k8s的pod资源, 那么需要先看这一部分的内容.</p><p>简单来说, 存储资源主要分网络和本地两大类.</p><p>本地这一类一般用于临时或者特殊环境. 就如同云服务中的存储类节点里的那种本地盘, 它不可靠. 因为pod本身默认是不强制绑定某个节点的,因此如果你pod异常了,那么它有可能重建的时候漂移到其它节点.此时你如果用本地盘，那么数据将丢失。</p><p>网络这一类,可以对接的有云服务厂家的存储资源,也可以对接自建的nfs这一类网络存储。</p><p>细致的说明, 参考官方文档https://kubernetes.io/docs/concepts/storage/</p><h3 id="pod-网络">pod 网络</h3><p>鉴于前面提到的pod类同于ecs/ec2. 因此. pod中的容器就如同ecs/ec2里的app一样，都有相同的ip, 端口范围, 主机名.</p><p>k8s的网络基于各种插件.每一种插件的实现详情见官网. <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model">https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model</a></p><p>如果你是本地搭建, 那么常用的插件是Flannel. 如果你是在云服务上搭建,那么建议使用云服务已有的k8s服务.</p><p>如果你必须在云服务上自己搭建,那么aws/azure/gcp都有对应的网络插件.它可以让你在k8s中结合使用云服务的网络组件.</p><h2 id="静态pod">静态pod</h2><p>特点：</p><ol><li>永远运行在固定节点</li><li>由所在节点的kubelet管理，但只负责保活，即pod崩溃重生</li><li>kubelet会让apiserver创建一个镜像pod，便于可以通过kubectl查询到静态pod</li></ol><p>配置：</p><ol><li>存放在 /etc/kubernetes/manifests 当采用kubeadm安装的时候，一般位于此目录。具体需要去看kubelet配置。</li><li>配置本身可以按照标准pod方式来创建</li></ol><p>检测：</p><ol><li>kubelet会定期检测配置目录加载配置创建/重建pod</li></ol><blockquote><p>当你通过kubeadm创建的时候，那么k8s的几个重要组件均会以静态pod的方式在master节点上创建，你可以在/etc/kubernetes/manifests/这里找到他们的配置</p></blockquote><pre><code class="language-bash">[root@k8s00 ~]# ll /etc/kubernetes/manifests/total 16-rw------- 1 root root 1848 Aug 25 16:28 etcd.yaml-rw------- 1 root root 2709 Aug 25 16:28 kube-apiserver.yaml-rw------- 1 root root 2564 Aug 25 16:33 kube-controller-manager.yaml-rw------- 1 root root 1120 Aug 25 16:33 kube-scheduler.yaml</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
            <tag> pod </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞kubectl多集群管理</title>
      <link href="posts/c15432fa/"/>
      <url>posts/c15432fa/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://github.com/sunny0826/kubecm">sunny0826/kubecm: Easier management of kubeconfig. (github.com)</a></p><h2 id="下载、安装">下载、安装</h2><pre><code class="language-bash">wget -O kubecm_release.tgz https://github.com/sunny0826/kubecm/releases/download/v0.15.3/kubecm_0.15.3_Linux_x86_64.tar.gztar xf kubecm_release.tgz &amp;&amp; rm -rf kubecm_release.tgzmv kubecm /usr/local/bin/</code></pre><h2 id="基本使用">基本使用</h2><blockquote><p><a href="https://kubecm.cloud/#/en-us/cli/kubecm_add">add (kubecm.cloud)</a></p></blockquote><p>添加配置</p><pre><code class="language-bash">kubecm add -f qa.yaml # 合并 qa.yaml 到 ~/.kube/config==选择 true ，就会将名字叫 qa 的配置追加到 ~/.kube/config</code></pre><p>切换配置</p><pre><code class="language-bash">kubecm switch qa</code></pre><p>删除配置</p><pre><code class="language-bash">kubecm delete qa</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞02安装</title>
      <link href="posts/9602dad/"/>
      <url>posts/9602dad/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/">使用 kubeadm 引导集群 | Kubernetes</a></p><h2 id="结构">结构</h2><p><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/ha-topology/">高可用拓扑选项 | Kubernetes</a></p><p>我们选堆叠结构</p><p><img src="https://d33wubrfki0l68.cloudfront.net/d1411cded83856552f37911eb4522d9887ca4e83/b94b2/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg" alt="堆叠的 etcd 拓扑"></p><p>不过因为我环境里没有 LB 组件，因此选用 haproxy 来代替，又因为要为 haproxy 做高可用，因此为 haproxy 再外套一层 keepalived。</p><p>如果在阿里云构建，可以认为 LB 是健壮的。</p><h2 id="机器">机器</h2><p>机器系统都是 centos7,</p><p>三个节点，均为控制平面节点，同时也是Node节点，并利用 keepalived + haproxy 进行 <code>控制平面组件：apiserver</code> 高可用</p><table><thead><tr><th>hostname</th><th>ip</th><th>type</th></tr></thead><tbody><tr><td>k8sapi</td><td>10.200.16.100</td><td>keepalived vip</td></tr><tr><td>k8s01</td><td>10.200.16.101</td><td>master keepalived(主) haproxy</td></tr><tr><td>k8s02</td><td>10.200.16.102</td><td>master keepalived(备) haproxy</td></tr><tr><td>k8s03</td><td>10.200.16.103</td><td>master</td></tr></tbody></table><p>请务必确保内网可以通过表格里的 <code>hostname</code> 解析到对应的 <code>ip</code></p><p>请务必将系统的 hostname 改为上述表里的 hostname</p><p>数据走向：</p><p>client-&gt;keepalived(vip:8443)-&gt;haproxy(vip:8443)-&gt; all:6443</p><h2 id="所有节点">所有节点</h2><h3 id="时间同步">时间同步</h3><pre><code class="language-bash">yum install chrony -ysed -i  '1a server cn.pool.ntp.org prefer iburst' /etc/chrony.confsystemctl restart chronydsystemctl enable chronydchronyc activity</code></pre><h3 id="系统配置">系统配置</h3><pre><code class="language-bash"># 加载模块modprobe overlaymodprobe br_netfilter# 添加配置cat &gt; /etc/sysctl.d/99-kubernetes-cri.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-iptables  = 1net.ipv4.ip_forward                 = 1net.bridge.bridge-nf-call-ip6tables = 1EOF# 配置生效sysctl --system# 清空防火墙systemctl stop firewalld.service iptables.servicesystemctl disable firewalld.servicesystemctl disable iptables.service;iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X# 关闭selinuxsetenforce 0sed -i 's@^\(SELINUX=\).*@\1disabled@' /etc/sysconfig/selinuxsed -i 's@^\(SELINUX=\).*@\1disabled@' /etc/selinux/config# 关闭 swap，kubelet 1.18 要求.如果你fstab也有，请一并注释swapoff -a# 安装 ipvsyum install ipvsadm -yipvsadm --clearcat&gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt; 'EOF'ipvs_mods_dir=&quot;/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs&quot;for i in $(ls $ipvs_mods_dir | grep -o &quot;^[^.]*&quot;); do    /sbin/modinfo -F filename $i  &amp;&gt; /dev/null    if [ $? -eq 0 ]; then        /sbin/modprobe $i    fidoneEOFchmod +x /etc/sysconfig/modules/ipvs.modules/etc/sysconfig/modules/ipvs.modules</code></pre><h3 id="安装容器">安装容器</h3><pre><code class="language-bash">yum install -y yum-utils \  device-mapper-persistent-data \  lvm2yum-config-manager \    --add-repo \    https://download.docker.com/linux/centos/docker-ce.repo</code></pre><h4 id="选择版本">选择版本</h4><p>一定要选择所安装的 k8s 版本兼容的最新容器版本</p><blockquote><p><a href="https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker">https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker</a></p></blockquote><pre><code class="language-bash">yum list docker-ce --showduplicates | sort -r===docker-ce.x86_64            3:20.10.7-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.6-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.5-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.4-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.3-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.2-3.el7                    docker-ce-stabledocker-ce.x86_64            3:20.10.1-3.el7                    docker-ce-stable#################### 选择好所要安装的版本 ####################docker_version=20.10.7  # 例如选择20.10.7</code></pre><h4 id="安装并启动">安装并启动</h4><pre><code class="language-bash"># 安装兼容k8s的docker版本yum install -y docker-ce-$&#123;docker_version&#125; docker-ce-cli-$&#123;docker_version&#125;#sed -i '/ExecStart=/a ExecStartPort=/usr/sbin/iptables -P FORWARD ACCEPT' /usr/lib/systemd/system/docker.service;mkdir -p /etc/docker;cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;  &quot;data-root&quot;: &quot;/export/docker-data-root&quot;,  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-opts&quot;: &#123;    &quot;max-size&quot;: &quot;100m&quot;  &#125;,  &quot;storage-driver&quot;: &quot;overlay2&quot;,  &quot;storage-opts&quot;: [    &quot;overlay2.override_kernel_check=true&quot;  ]&#125;EOFsystemctl daemon-reloadsystemctl restart dockersystemctl enable docker</code></pre><h3 id="安装-kubelet-kubeadm-kubectl">安装 kubelet kubeadm kubectl</h3><blockquote><p>阿里巴巴镜像点：</p><p><a href="https://developer.aliyun.com/mirror/kubernetes">https://developer.aliyun.com/mirror/kubernetes</a></p><p>google 官方安装文档 <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p></blockquote><p>设置软件源：</p><pre><code class="language-bash"># google 官方源cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearchenabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOF# 腾讯源cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.cloud.tencent.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0exclude=kubelet kubeadm kubectlEOF</code></pre><pre><code class="language-bash"># 确定 kube* 版本yum list kube* --showduplicates --disableexcludes=kubernetes# 选择要安装的版本full_verions=1.20.8-0# 安装 kube 管理组件和 kubeletyum install -y kubelet-$&#123;full_verions&#125; kubeadm-$&#123;full_verions&#125; kubectl-$&#123;full_verions&#125; --disableexcludes=kubernetessystemctl enable --now kubelet</code></pre><h2 id="高可用组件节点">高可用组件节点</h2><h3 id="安装负载均衡组件-haproxy-和-高可用组件-keepalived">安装负载均衡组件 haproxy 和 高可用组件 keepalived</h3><blockquote><p>k8s01，k8s02 执行</p></blockquote><ul><li>安装</li></ul><pre><code class="language-bash">yum install haproxy keepalived -y</code></pre><ul><li>haproxy配置</li></ul><pre><code class="language-bash">cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.defaultcat&gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal    log /dev/log local0    log /dev/log local1 notice    daemondefaults    mode                    http    log                     global    option                  httplog    option                  dontlognull    option http-server-close    option forwardfor       except 127.0.0.0/8    option                  redispatch    retries                 1    timeout http-request    10s    timeout queue           20s    timeout connect         5s    timeout client          20s    timeout server          20s    timeout http-keep-alive 10s    timeout check           10sfrontend apiserver    bind *:8443    mode tcp    option tcplog    default_backend apiserverbackend apiserver    option httpchk GET /healthz    http-check expect status 200    mode tcp    option ssl-hello-chk    balance     roundrobin        server k8sapivip 10.200.16.101:6443 check        server k8sapivip 10.200.16.102:6443 check        server k8sapivip 10.200.16.103:6443 checkEOF</code></pre><ul><li>keepalived 主配置</li></ul><blockquote><p>两节点配置不一样</p></blockquote><pre><code class="language-bash">cp /etc/keepalived/keepalived.conf  /etc/keepalived/keepalived.conf.default############## 放在 k8s01 ################cat&gt;/etc/keepalived/keepalived.conf&lt;&lt;EOFglobal_defs &#123;    router_id LVS_DEVEL&#125;vrrp_script check_apiserver &#123;  script &quot;/etc/keepalived/check_apiserver.sh&quot;  interval 3  weight -2  fall 10  rise 2&#125;vrrp_instance VI_1 &#123;    state MASTER  #     interface ens192  # 物理网卡名    virtual_router_id 51    priority 101  #    authentication &#123;        auth_type PASS        auth_pass 42    &#125;    virtual_ipaddress &#123;        10.200.16.100    &#125;    track_script &#123;        check_apiserver    &#125;&#125;EOF############## 放在 k8s02 ################cat&gt;/etc/keepalived/keepalived.conf&lt;&lt;EOFglobal_defs &#123;    router_id LVS_DEVEL&#125;vrrp_script check_apiserver &#123;  script &quot;/etc/keepalived/check_apiserver.sh&quot;  interval 3  weight -2  fall 10  rise 2&#125;vrrp_instance VI_1 &#123;    state BACKUP  #     interface ens192  # 物理网卡名    virtual_router_id 51    priority 100  #    authentication &#123;        auth_type PASS        auth_pass 42    &#125;    virtual_ipaddress &#123;        10.200.16.100    &#125;    track_script &#123;        check_apiserver    &#125;&#125;EOF</code></pre><ul><li>keepalived 检测脚本</li></ul><blockquote><p>k8s01和k8s02均需要</p></blockquote><pre><code class="language-bash">cat&gt;/etc/keepalived/check_apiserver.sh&lt;&lt;'EOF'#!/bin/sherrorExit() &#123;    echo &quot;*** $*&quot; 1&gt;&amp;2    exit 1&#125;APISERVER_VIP=10.200.16.100APISERVER_DEST_PORT=6443curl --silent --max-time 2 --insecure https://localhost:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://localhost:$&#123;APISERVER_DEST_PORT&#125;/&quot;if ip addr | grep -q $&#123;APISERVER_VIP&#125;; then    curl --silent --max-time 2 --insecure https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/&quot;fiEOFchmod a+x /etc/keepalived/check_apiserver.sh</code></pre><ul><li>启动keepalived和haproxy</li></ul><pre><code class="language-bash">systemctl start keepalived haproxysystemctl enable keepalived haproxy</code></pre><h2 id="控制平面节点">控制平面节点</h2><h3 id="拉取容器镜像">拉取容器镜像</h3><blockquote><p>k8s01，k8s02，k8s03 执行</p></blockquote><blockquote><p>鉴于网络问题，所以国内一般无法直接运行初始化命令，因此最好先自行安装好包</p></blockquote><pre><code class="language-bash"># 查看对应版本 k8s 所需包yum list --showduplicates kubeadm --disableexcludes=kubernetes | grep $&#123;master_verions&#125;===kubeadm.x86_64                       1.20.8-0                        @kuberneteskubeadm.x86_64                       1.20.0-0                        kuberneteskubeadm.x86_64                       1.20.1-0                        kuberneteskubeadm.x86_64                       1.20.2-0                        kuberneteskubeadm.x86_64                       1.20.4-0                        kuberneteskubeadm.x86_64                       1.20.5-0                        kuberneteskubeadm.x86_64                       1.20.6-0                        kuberneteskubeadm.x86_64                       1.20.7-0                        kuberneteskubeadm.x86_64                       1.20.8-0                        kubernetes</code></pre><pre><code class="language-bash">full_version=1.20.8kubeadm config images list --kubernetes-version=$&#123;full_version&#125;===k8s.gcr.io/kube-apiserver:v1.20.8k8s.gcr.io/kube-controller-manager:v1.20.8k8s.gcr.io/kube-scheduler:v1.20.8k8s.gcr.io/kube-proxy:v1.20.8k8s.gcr.io/pause:3.2k8s.gcr.io/etcd:3.4.13-0k8s.gcr.io/coredns:1.7.0</code></pre><p>根据上面的输出版本，修改下面脚本中 <code>pause</code> <code>etcd</code> <code>coredns</code> 的版本号</p><pre><code class="language-bash">cat&gt;images-pull.sh&lt;&lt;EOF#!/bin/bash# kubeadm config images listimages=(kube-apiserver:v$&#123;full_version&#125;kube-controller-manager:v$&#123;full_version&#125;kube-scheduler:v$&#123;full_version&#125;kube-proxy:v$&#123;full_version&#125;pause:3.2     # 修改我etcd:3.4.13-0 # 修改我coredns:1.7.0 # 修改我)for imageName in \$&#123;images[@]&#125;;do    docker pull registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;    docker tag registry.aliyuncs.com/google_containers/\$&#123;imageName&#125; k8s.gcr.io/\$&#123;imageName&#125;    docker rmi registry.aliyuncs.com/google_containers/\$&#123;imageName&#125;doneEOF</code></pre><pre><code class="language-bash">bash images-pull.sh</code></pre><h3 id="第一个控制平面节点安装（k8s01）">第一个控制平面节点安装（k8s01）</h3><p>(info: <a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/%EF%BC%89">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/）</a></p><h4 id="采取配置文件安装模式">采取配置文件安装模式</h4><pre><code class="language-bash">kubeadm config print init-defaults &gt; kubeadm.yaml.default</code></pre><p>根据生成的默认配置，来动态的调整，因为 k8s 版本变更很快，所以下面的配置不一定是正确的</p><pre><code class="language-bash">cat &gt; kubeadm.yaml &lt;&lt; EOF# kubeadm.yaml 将默认的配置进行修改apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.8272827282sksksu   # 改我，改成你自己随意的字符串  ttl: 24h0m0s  usages:  - signing  - authenticationkind: InitConfigurationlocalAPIEndpoint:  advertiseAddress: 10.200.16.101  # 改我，改成第一个控制平面节点的物理ip  bindPort: 6443nodeRegistration:  criSocket: /var/run/dockershim.sock  name: k8s01 # 改我，理论上它会自动获取配置所在节点的 hostname  taints:  - effect: NoSchedule    key: node-role.kubernetes.io/master---apiServer:  timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;controllerManagerExtraArgs:  address: 0.0.0.0  # 开放kube-controller-manager，便于之后prometheus监控scheduler: &#123;&#125;schedulerExtraArgs:  address: 0.0.0.0  # 开放kube-scheduller，便于之后prometheus监控controlPlaneEndpoint: &quot;k8sapi:8443&quot; # 新加我，k8sapi 是 apiserver 的负载均衡器的地址dns:  type: CoreDNSetcd:  local:    dataDir: /var/lib/etcdimageRepository: k8s.gcr.io # 如果你无法访问k8s.gcr.io，且没有提前下载阿里云镜像，你可以换成 registry.aliyuncs.com/google_containers 试试, 建议提前下载好kind: ClusterConfigurationkubernetesVersion: v$&#123;full_version&#125; # 改我，改成你所需安装的版本networking:  dnsDomain: cluster.local  serviceSubnet: 10.96.0.0/16 # 改我，定义 svc 的网段  podSubnet: 10.97.0.0/16 # 改我，定义 pod 的网段--- # 新加我apiVersion: kubeproxy.config.k8s.io/v1alpha1 # 新加我kind: KubeProxyConfiguration # 新加我mode: ipvs  # 新加我EOF# 进行初始化安装kubeadm init --config kubeadm.yaml</code></pre><h4 id="配置通过-kubectl-访问集群">配置通过 kubectl 访问集群</h4><pre><code class="language-bash">mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# kubectl 命令补全source &lt;(kubectl completion bash)# You should now deploy a pod network to the cluster.# Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:# https://kubernetes.io/docs/concepts/cluster-administration/addons/</code></pre><pre><code class="language-bash"># 添加用户上下文到配置里，方便切换用户kubectl config set-context admin --cluster=kubernetes --user=kubernetes-admin</code></pre><h4 id="调整-kube-controller-manager-和-scheduler-的配置">调整 kube-controller-manager 和 scheduler 的配置</h4><pre><code class="language-bash"># 检查状态kubectl get cs# 你可能会发现,出现服务连接拒绝问题controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refusedscheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused# 原因是这两个服务配置默认端口是0。至于为啥就不晓得了# 你需要注释掉两个配置里的端口（- --port=0），恢复为默认端口sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml# 重启 kubeletsystemctl restart kubelet# 再次检查kubectl get cs===NAME                 STATUS    MESSAGE             ERRORscheduler            Healthy   okcontroller-manager   Healthy   oketcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;</code></pre><h4 id="配置网络插件">配置网络插件</h4><p>网络插件常用的有两种</p><p>第一种是 flannel，涉及到更多的 iptables 规则</p><p>第二种是 calico，涉及到更多的路由规则</p><ul><li>flannel (本文档选用的插件)</li></ul><blockquote><p><a href="https://github.com/coreos/flannel">https://github.com/coreos/flannel</a> 代码托管地址</p></blockquote><pre><code class="language-bash">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改里面的 &quot;Network&quot;: &quot;10.244.0.0/16&quot;, 变更为你自己的 pod 网段，即kubeadm初始化阶段的 --pod-network-cidrkubectl apply -f  kube-flannel.yml</code></pre><ul><li>calico</li></ul><blockquote><p><a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart">https://docs.projectcalico.org/getting-started/kubernetes/quickstart</a> 安装文档</p></blockquote><pre><code class="language-bash">kubectl apply -f https://docs.projectcalico.org/manifests/tigera-operator.yamlwget https://docs.projectcalico.org/manifests/custom-resources.yaml# custom-resources.yaml 修改里面的网络为 pod 网段kubectl apply -f custom-resources.yaml</code></pre><blockquote><p>注意，当你使用了 calico 后， 会生成一些 cni 的配置，这些配置会导致你返回 flannel 的时候出现问题。例如无法创建 cni</p><p>你可以使用 find / -name ‘*calico*’ 找到所有信息，然后都删除</p></blockquote><p>默认kubeadm安装完后，禁止调度pod到master上。你可以通过下面的命令，关闭所有master节点的禁止调度</p><pre><code class="language-bash"> kubectl taint nodes --all node-role.kubernetes.io/master-</code></pre><h4 id="配置web控制台dashboard">配置web控制台dashboard</h4><blockquote><p><a href="https://github.com/kubernetes/dashboard">https://github.com/kubernetes/dashboard</a> 代码托管地址</p></blockquote><pre><code class="language-bash">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.3/aio/deploy/recommended.yaml &amp;&amp; mv recommended.yaml kubernetes-dashboard.yaml</code></pre><ul><li>如果你部署了 metallb，或者有云服务的 lb，那么只需要修改 kubernetes-dashboard.yaml 配置中的 svc 对象类型为  LoadBalancer</li></ul><pre><code class="language-yaml">kind: ServiceapiVersion: v1metadata:  labels:    k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboardspec:  ports:    - port: 443      targetPort: 8443  type: LoadBalancer   # 这里新加一条  selector:    k8s-app: kubernetes-dashboard</code></pre><p>之后通过 lb 地址访问即可</p><ul><li>如果你没有配置 metallb，也没有部署在云服务中（即没有云LB），那么需要修改 svc 类型为 NodePort，并且将 pod 的部署到固定的节点上，例如 k8s01</li></ul><pre><code class="language-bash"># 这里我们要修改一些东西# #---spec:  ports:    - port: 443      targetPort: 8443      nodePort: 30001  type: NodePort   # 这里新加一条#---# 还需要修改一下部署的位置，如果你集群中已经加入了多个节点，则会导致 pod 分发到其它节点上。这里我们强制分发到 master 上. 找到 kind: Deployment 配置，并修改两个 pod 的分发位置为 nodeName: &lt;master 节点主机名&gt;#---    spec:      nodeName: k8s01    # 这里新加一条      containers:        - name: kubernetes-dashboard          image: kubernetesui/dashboard:v2.0.3              spec:      nodeName: k8s01    # 这里新加一条      containers:        - name: dashboard-metrics-scraper          image: kubernetesui/metrics-scraper:v1.0.4          #---# 然后创建kubectl create -f kubernetes-dashboard.yaml</code></pre><p>之后通过 <a href="https://k8s01:30001">https://k8s01:30001</a> 访问</p><ul><li>获取访问web服务的token</li></ul><p>dashboard本身是一个pod，如果你想让pod去访问其它的k8s资源，则需要给pod创建一个服务账户(serviceaccount).</p><p>构建一个服务账户admin-user，并通过ClusterRoleBinding授权admin级别的ClusterRole对象</p><pre><code class="language-bash"># 添加访问 token# 官方文档： https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md# 通过下面内容创建 kube-db-auth.yamlcat &gt; kube-db-auth.yaml &lt;&lt; EOFapiVersion: v1kind: ServiceAccountmetadata:  name: admin-user  namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: admin-userroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:- kind: ServiceAccount  name: admin-user  namespace: kubernetes-dashboardEOFkubectl apply -f kube-db-auth.yaml# 通过下面命令拿到 tokenkubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '&#123;print $1&#125;')  </code></pre><p><img src="/posts/9602dad/image-20210711163654410.png" alt="image-20210711163654410"></p><h3 id="获取其他节点加入集群的命令（k8s01）">获取其他节点加入集群的命令（k8s01）</h3><h4 id="新加控制平面">新加控制平面</h4><pre><code class="language-bash"># 命令格式kubeadm join k8sapi:8443 \--token xxx \--discovery-token-ca-cert-hash xxx \--control-plane --certificate-key xxx</code></pre><h4 id="新加Node节点">新加Node节点</h4><pre><code class="language-bash"># 命令格式kubeadm join k8sapi:8443 \--token xxx \--discovery-token-ca-cert-hash xxx </code></pre><h4 id="命令重建">命令重建</h4><p>添加Node节点命令</p><pre><code class="language-bash"># 添加Node节点的命令add_node_command=`kubeadm token create --print-join-command`echo $&#123;add_node_command&#125;</code></pre><p>添加控制平面节点命令</p><pre><code class="language-bash"># 创建 --certificate-keyjoin_certificate_key=`kubeadm init phase upload-certs --upload-certs|tail -1`# 命令组合echo &quot;$&#123;add_node_command&#125; --control-plane --certificate-key $&#123;join_certificate_key&#125;&quot;</code></pre><h3 id="k8s02和k8s03作为控制平面添加到集群">k8s02和k8s03作为控制平面添加到集群</h3><pre><code class="language-bash">kubeadm join k8sapi:8443 \--token h77qhb.cjj8ig4t2v15dncm \--discovery-token-ca-cert-hash sha256:b7138bb76d19d3d7baf832e46559c400f4fe544d8f0ee81a87f \--control-plane --certificate-key fd996c7c0c2047c9c10a377f25a332bf4b5b00ca# sed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-scheduler.yamlsed -i '/- --port=0/d' /etc/kubernetes/manifests/kube-controller-manager.yaml# 重启 kubeletsystemctl restart kubelet# 根据提示，执行相关命令，一般都是下面的命令mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 等待一分钟，查看所有 pod 是否正常kubectl get pod -n kube-system===NAME                            READY   STATUS    RESTARTS   AGEcoredns-74ff55c5b-9gkgk         1/1     Running   0          2d13hcoredns-74ff55c5b-tndb5         1/1     Running   0          15detcd-k8s01                      1/1     Running   0          15detcd-k8s02                      1/1     Running   0          7m12setcd-k8s03                      1/1     Running   0          15dkube-apiserver-k8s01            1/1     Running   0          26hkube-apiserver-k8s02            1/1     Running   0          7m11skube-apiserver-k8s03            1/1     Running   0          26hkube-controller-manager-k8s01   1/1     Running   0          15dkube-controller-manager-k8s02   1/1     Running   0          82skube-controller-manager-k8s03   1/1     Running   1          15dkube-flannel-ds-42hng           1/1     Running   0          15dkube-flannel-ds-b42qj           1/1     Running   0          15dkube-flannel-ds-ss8w5           1/1     Running   0          15dkube-proxy-2xxpn                1/1     Running   0          15dkube-proxy-pg7j7                1/1     Running   0          15dkube-proxy-txh2t                1/1     Running   0          15dkube-scheduler-k8s01            1/1     Running   0          15dkube-scheduler-k8s02            1/1     Running   0          82skube-scheduler-k8s03            1/1     Running   0          15d</code></pre><h4 id="问题点">问题点</h4><p>如果etcd中曾经有k8s02和k8s03的节点信息，则你需要先从etcd中删除，否则加入的时候，会卡在检测etcd处，并最终报错.</p><p>删除etcd信息方式：</p><pre><code class="language-bash"># 输出 etcd 节点 idETCD=`docker ps|grep etcd|grep -v POD|awk '&#123;print $1&#125;'`docker exec \  -it $&#123;ETCD&#125; \  etcdctl \  --endpoints https://127.0.0.1:2379 \  --cacert /etc/kubernetes/pki/etcd/ca.crt \  --cert /etc/kubernetes/pki/etcd/peer.crt \  --key /etc/kubernetes/pki/etcd/peer.key \  member list===2f401672c9a538f1, started, k8s01, https://10.200.16.101:2380, https://10.200.16.101:2379, falsed6d9ca2a70f6638e, started, k8s02, https://10.200.16.102:2380, https://10.200.16.102:2379, falseee0e9340a5cfb4d7, started, k8s03, https://10.200.16.103:2380, https://10.200.16.103:2379, false# 假设这里我要删除 k8s03ETCD=`docker ps|grep etcd|grep -v POD|awk '&#123;print $1&#125;'`docker exec \  -it $&#123;ETCD&#125; \  etcdctl \  --endpoints https://127.0.0.1:2379 \  --cacert /etc/kubernetes/pki/etcd/ca.crt \  --cert /etc/kubernetes/pki/etcd/peer.crt \  --key /etc/kubernetes/pki/etcd/peer.key \  member remove ee0e9340a5cfb4d7</code></pre><h2 id="Node节点">Node节点</h2><blockquote><p>加入命令，位于 k8s01初始化命令尾部，worker加入的时候，不需要添加 --control-plane --certificate-key</p></blockquote><pre><code class="language-bash">kubeadm join k8sapi:8443 \--token h77qhb.cjj8ig4t2v15dncm \--discovery-token-ca-cert-hash sha256:b7138bb76d19d3d7baf832e46559c400f4fe544d8f0ee81a87f</code></pre><h2 id="其他内容">其他内容</h2><h3 id="容器中显示正确的可见资源">容器中显示正确的可见资源</h3><p>默认情况下,不管你如何设置资源限制,容器里的可见资源都等于节点资源.也就是说你在容器里查看/proc/meminfo显示的资源并不是你设置的.这个问题会带来很多麻烦.</p><p>因为很多程序的参数都是根据可见资源来设定的.例如nginx的auto, 或者jvm里的内存参数.</p><blockquote><p>从 Java 8u191开始, jvm 已经默认实现了容器感知( -XX:+UseContainerSupport). 因此无需安装下面的 lxcfs 方案.</p><p>并且, 在容器中建议只设置如下内存参数:</p><p>-XX:MaxRAMPercentage  最大堆内存=物理内存百分比, 建议为容器限制的50%-75% . 毕竟还需要预留其它内存.</p></blockquote><p>而社区常见的作法是用<code>lxcfs</code> 来提升资源的可见性.<code>lxcfs</code> 是一个开源的FUSE（用户态文件系统）实现来支持LXC容器, 它也可以支持Docker容器.</p><p>LXCFS通过用户态文件系统, 在容器中提供下列 <code>procfs</code> 的文件.</p><pre><code class="language-bash">/proc/cpuinfo/proc/diskstats/proc/meminfo/proc/stat/proc/swaps/proc/uptime</code></pre><p>与我们资源限制有关的, 主要是 cpuinfo 和 meminfo.</p><p>目前社区的做法如下:</p><ol><li><p>所有节点安装 fuse-libs 包.</p><pre><code class="language-bash">yum install -y fuse-libs</code></pre></li><li><p>集群部署 lxcfs 的 deployment 资源对象</p><pre><code class="language-bash">git clone https://github.com/denverdino/lxcfs-admission-webhook.gitcd lxcfs-admission-webhookvim deployment/lxcfs-daemonset.yaml=== 修正配置里的 apiVersion=== 当前git里的代码是陈旧的...代码里的版本在 1.18 已经被废弃). 具体归属于什么版本, 请参考k8s官方api文档 === https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#daemonset-v1-appsapiVersion:apps/v1  kubectl apply -f deployment/lxcfs-daemonset.yamlkubectl get pod#等待 lxcfs pod 状态成 running#如果你发现 pod 始终出错，那么执行下列命令：kubectl delete -f deployment/lxcfs-daemonset.yamlrm -rf /var/lib/lxcfskubectl apply -f deployment/lxcfs-daemonset.yamldeployment/install.shkubectl get secrets,pods,svc,mutatingwebhookconfigurations</code></pre></li><li><p>给相关namespace注入lxcfs，例如default</p><pre><code class="language-bash">kubectl label namespace default lxcfs-admission-webhook=enabled</code></pre></li><li><p>重启添加了资源限制的pod, 此时 /proc/cpuinfo 和 /proc/meminfo 将会正确显示.</p></li></ol><h3 id="集群指标监控服务">集群指标监控服务</h3><p>添加 metrics-server  <a href="https://github.com/kubernetes-sigs/metrics-server#configuration">https://github.com/kubernetes-sigs/metrics-server#configuration</a></p><pre><code class="language-bash">wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml# 修改metrics-server容器参数部分，添加额外的启动参数(arg)args:  - --kubelet-preferred-address-types=InternalIP  - --kubelet-insecure-tlskubectl apply -f components.yamlkubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;</code></pre><blockquote><p>kubectl top 指令需要指标才能输出</p></blockquote><hr><h2 id="kubectl-命令文档">kubectl 命令文档</h2><blockquote><p><a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands</a></p></blockquote><p>一些常用的命令</p><ol><li>kubectl explain</li></ol><blockquote><p>非常有用的命令，帮助你写配置文件</p></blockquote><p>输出配置某个字段的详细说明,例如 deployment.metadata</p><blockquote><p>其中标注有 -required- 的字段是必选字段</p></blockquote><pre><code class="language-bash">kubectl explain deployment.metadata</code></pre><ol start="2"><li>-o yaml --dry-run</li></ol><p>输出create命令的yaml配置</p><pre><code class="language-bash">kubectl create serviceaccount mysvcaccount -o yaml --dry-run</code></pre><ol start="3"><li>常用的工具容器</li></ol><pre><code class="language-bash">kubectl run cirros-$RANDOM -it --rm --restart=Never --image=cirros -- /bin/shkubectl run dnstools-$RANDOM -it --rm --restart=Never --image=infoblox/dnstools:latest </code></pre><h3 id="kubectl-调用的配置">kubectl 调用的配置</h3><pre><code class="language-yaml">➜   kubectl config viewapiVersion: v1clusters:- cluster:    certificate-authority-data: DATA+OMITTED    server: https://&lt;api_server&gt;:8443  name: kubernetes-qa- cluster:    certificate-authority-data: DATA+OMITTED    server: https://&lt;api_server&gt;:6443  name: kubernetes-prod  contexts:- context:    cluster: kubernetes-prod    user: kubernetes-admin  name: prod- context:    cluster: kubernetes-qa    user: user-kfktf786bk  name: qa  current-context: qa           ## 当前使用的配置kind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin  user:    client-certificate-data: REDACTED    client-key-data: REDACTED- name: user-kfktf786bk  user:    client-certificate-data: REDACTED    client-key-data: REDACTED</code></pre><h2 id="删除与清理">删除与清理</h2><pre><code class="language-bash"># 从集群里删除某个节点# master execkubectl drain &lt;NODE_ID&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;NODE_ID&gt;# worker execkubeadm resetiptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -Xipvsadm --clear;ip link set cni0 down &amp;&amp; ip link delete cni0;ip link set flannel.1 down &amp;&amp; ip link delete flannel.1;ip link set kube-ipvs0 down &amp;&amp; ip link delete kube-ipvs0;ip link set dummy0 down &amp;&amp; ip link delete dummy0;rm -rf /var/lib/cni/rm -rf $HOME/.kube;</code></pre><h2 id="节点一致性测试">节点一致性测试</h2><blockquote><p><a href="https://kubernetes.io/docs/setup/node-conformance/">https://kubernetes.io/docs/setup/node-conformance/</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞记录点</title>
      <link href="posts/e1096625/"/>
      <url>posts/e1096625/</url>
      
        <content type="html"><![CDATA[<h3 id="1-pv和pvc的绑定关系是终生的-也就是说你删除了当前pvc-pv依然不能被其它pvc所绑定">1. pv和pvc的绑定关系是终生的. 也就是说你删除了当前pvc, pv依然不能被其它pvc所绑定.</h3><p>删除pvc后, pv的状态将从bound转为Released.</p><p>如果你想让pv从新绑定到原来的pvc上, 则需要两个步骤:</p><ol><li>先创建原来的pvc.</li><li>删除当前pv并重新创建, 或者修改当前pv的配置, 删除spec.claimRef信息.</li></ol><h3 id="2-pod无法正常启动，报错：“mounting-“-var-lib-lxcfs-proc-loadavg-”">2. pod无法正常启动，报错：“mounting \“/var/lib/lxcfs/proc/loadavg\”</h3><p>to rootfs \&quot;/export/docker-data-root/overlay2/710d09a6715d88a01b417ba1a669dab69b67c3d57e576c1ae6d79aa03e1b294a/merged”</p><p>根据信息可知问题点应该是 lxcfs 问题。</p><p>查看故障pod所在节点的 lxcfs pod 日志，得到信息</p><p>“fuse: mountpoint is not empty<br>fuse: if you are sure this is safe, use the ‘nonempty’ mount option”</p><p>因此删除节点 lxcfs 目录，并重建 lxcfs pod</p><pre><code class="language-bash">rm -rf  /var/lib/lxcfs/ &amp;&amp; kubectl delete pod/lxcfs-hthgq</code></pre><h3 id="3-变更pv回收策略">3. 变更pv回收策略</h3><pre><code class="language-bash">kubectl patch pv &lt;your-pv-name&gt; -p '&#123;&quot;spec&quot;:&#123;&quot;persistentVolumeReclaimPolicy&quot;:&quot;Retain&quot;&#125;&#125;'</code></pre><h3 id="4-kubernetes-qos">4. kubernetes qos</h3><p>kubernetes 有三种策略，分别是Guaranteed和Burstable和BestEffort</p><p>Guaranteed：任何容器的cpu请求值和限制值必须一样。内存同样如此</p><p>Burstable：最少有一个容器的cpu请求值和限制值必须一样。内存同样如此</p><p>BestEffort： 没有任何限制</p><h3 id="5-kubelet无法启动，报错：kubelet-service-holdoff-time-over">5. kubelet无法启动，报错：<code>kubelet.service holdoff time over</code></h3><p>确认节点swap是否关闭</p><p>临时关闭<code>swapoff -a</code></p><p>永久关闭<code>注释/etc/fstab文件里的swap挂载行</code></p><h3 id="6-拉取私有镜像">6. 拉取私有镜像</h3><pre><code class="language-shell">kubectl create secret docker-registry &lt;secret_name&gt; --docker-server=&lt;FQDN:your-registry-server&gt; --docker-username=&lt;your-name&gt; --docker-password=&lt;your-pword&gt; --docker-email=&lt;your-email&gt;</code></pre><p>deployment 对象中调用：<code>deployment.spec.template.spec.imagePullSecrets: [&lt;secret_name&gt;]</code></p><h3 id="7-pv一直无法删除掉">7. pv一直无法删除掉</h3><pre><code class="language-bash"># 检查是否受保护kubectl describe pv &lt;pv&gt; | grep Finalizers # 重置策略为空kubectl patch pv &lt;pv&gt; -p '&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:null&#125;&#125;'</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prometheus☞搭建</title>
      <link href="posts/65820315/"/>
      <url>posts/65820315/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p>prometheus 监控的构建，相比 zabbix 来说，还是要麻烦一些。</p><p>当然，如果你完全熟悉之后，配置文件化也更容易被其他系统所修改。</p><h1>监控端组件</h1><p>搞之前，先创建一个网络</p><p>docker network create promsnet</p><h2 id="grafana">grafana</h2><blockquote><p>数据展示，部署在 proms 端</p></blockquote><pre><code class="language-bash">mkdir -p /export/docker-data-grafana/&#123;data,conf&#125;chmod 777 /export/docker-data-grafana/datadocker run --name grafana \--restart always \--network promsnet \--mount 'type=bind,src=/export/docker-data-grafana/data,dst=/var/lib/grafana' \--mount 'type=bind,src=/export/docker-data-grafana/conf,dst=/usr/share/grafana/conf' \-p 3000:3000 -d grafana/grafana:7.3.5</code></pre><blockquote><p>默认账户/初始密码都是admin</p><p>默认配置库是sqlite 3</p></blockquote><h3 id="配置">配置</h3><blockquote><p>/export/docker-data-grafana/conf/default.ini</p></blockquote><pre><code class="language-ini">[server]domain = 域名http_port = 端口[database]type = sqlite3host = 127.0.0.1:3306name = grafanauser = rootpassword =[smtp]enabled = truehost = smtp.feishu.cn:465user = password = skip_verify = truefrom_address = from_name = GrafanaAdmin</code></pre><h2 id="alertmanager-告警管理">alertmanager 告警管理</h2><p>告警管理器，接收 proms 发来的告警，并加以处理后发出</p><pre><code class="language-bash">docker volume create alertmanagerdocker run --name alertmanager -d \  --restart always \  --network promsnet \  -p 9093:9093 \  --mount 'type=volume,src=alertmanager,dst=/etc/alertmanager' \prom/alertmanager</code></pre><h3 id="配置-2">配置</h3><p>alertmanager.yml</p><pre><code class="language-bash">global:  resolve_timeout: 5mroute:  group_by: ['alertname']  group_wait: 10s  group_interval: 10s  repeat_interval: 1h  receiver: 'wechat'receivers:- name: 'wechat'  wechat_configs:  - corp_id: ''    to_user: ''    agent_id: ''    api_secret: ''    send_resolved: trueinhibit_rules:  - source_match:      severity: 'critical'    target_match:      severity: 'warning'    equal: ['alertname', 'dev', 'instance']</code></pre><p>上面是微信的，你也可以webhook方式，来走其它方式，例如飞书/钉钉</p><pre><code class="language-bash">global:  resolve_timeout: 5mroute:  group_by: ['instance']  group_wait: 10s  group_interval: 10s  repeat_interval: 1h  receiver: 'web.hook.prometheusalert'receivers:- name: 'web.hook.prometheusalert'  webhook_configs:  - url: ''   # 这里填写 webhook 调用inhibit_rules:  - source_match:      severity: 'critical'    target_match:      severity: 'warning'    equal: ['alertname', 'dev', 'instance']</code></pre><p>💁飞书/钉钉，可以用部署 <a href="https://gitee.com/feiyu563/PrometheusAlert">PrometheusAlert</a></p><h3 id="告警分组、收敛、静默">告警分组、收敛、静默</h3><p>分组的意思，就是将某一个组内同一时期的告警合并发送，例如根据实例来分组。</p><p>收敛的意思，就是告警A规则和告警B规则同时触发，但是告警A规则出现的时候，必然会触发告警B，此时只发告警A，例如MYSQL机器挂了，那么只需要发机器挂掉的告警，MYSQL的告警就没必要发送了。</p><p>静默的意思，就是用户已知这个时间点会触发告警规则，但是无需触发，例如高压力定时任务引起磁盘IO告警，虽然会触发平均指标告警，但是在用户认定的安全范围内，因而无需触发。</p><ul><li>分组</li></ul><p>根据 label 进行分组，同组的预警尽量一次性发出</p><pre><code class="language-yaml">route:  group_by: ['alertname'] # 告警分组  group_wait: 10s # 分组等待时间，也就是说第一个告警等待同组内其它告警来临的时间  group_interval: 5m # 分组发送不同告警规则的静默周期，以及发送失败的静默周期  repeat_interval: 1h # 分组发送相同告警规则的静默周期，如果时间结束，告警状态未变，将再次发送.</code></pre><ul><li>收敛</li></ul><p>举例说明：当主机挂了，此时只需要发送主机挂掉的预警，无需再发送因主机挂掉而产出的其它预警。</p><pre><code class="language-yaml">inhibit_rules:  - source_match:   # 匹配最底层的那个告警规则，比如服务器挂了      severity: 'critical'       target_match:   # 匹配当 source_match 触发的时候，无需告警的规则。      severity: 'warn'    equal: ['instance']   # 根据标签确定哪些匹配到 target_match 的需要忽略。例如，equal: instance 那么当 source_match 和 target_match 的 instance 值一样的时候，target_match 被忽略。</code></pre><blockquote><p>标签的 key ，可以是你在 proms.rule 中自定义，也可以是被监控端组件采集数据后发给 proms 的。</p></blockquote><h2 id="Prometheus-主程">Prometheus 主程</h2><pre><code class="language-bash">mkdir /export/docker-data-proms/&#123;data,conf&#125; -pchmod 777 /export/docker-data-proms/data</code></pre><h3 id="配置-3">配置</h3><h4 id="主配置">主配置</h4><p>/export/docker-data-proms/promethesu.yml</p><pre><code class="language-:"># my global configglobal:  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.  # scrape_timeout is set to the global default (10s).# Alertmanager configurationalerting:  alertmanagers:  - static_configs:    - targets:      - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files:  - &quot;/etc/prometheus/conf/rules/*.yml&quot;# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs:  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.#  - job_name: 'prometheus'#    static_configs:#    - targets: ['proms:9090']#  - job_name: &quot;grafana&quot;#    static_configs:#    - targets: ['grafana:3000']  - job_name: &quot;auto_discovery_dns&quot;    relabel_configs: # 数据获取前的进行的标签修改    - source_labels: [&quot;__address__&quot;]  # 获取原始标签      regex: &quot;(.*):9100&quot; # 匹配原始标签的值      replacement: &quot;$1&quot; # 提取 regex 中拿到的值      target_label: &quot;instance&quot; # 设定要修改的目标标签      action: replace # 将目标标签的值替换为 replacement 提取的值    - source_labels: [&quot;__address__&quot;]  # 获取原始标签      regex: &quot;(.*):10052&quot; # 匹配原始标签的值      replacement: &quot;$1&quot; # 提取 regex 中拿到的值      target_label: &quot;instance&quot; # 设定要修改的目标标签      action: replace # 将目标标签的值替换为 replacement 提取的值    metric_relabel_configs: # 落盘前的最后一次修改    - source_labels: [&quot;__name__&quot;] # 获取原始标签      regex: &quot;^go_.*|^process_.*&quot; # 匹配标签值      action: drop # 删除，不让数据落盘    dns_sd_configs: # 通过 dns srv 记录自动发现    - names: [&quot;_prometheus._tcp.zjk.pj&quot;]</code></pre><h5 id="原始标签和目标标签">原始标签和目标标签</h5><p>可以在下图位置【Status】-【Service Discovery】中看到详情</p><p><img src="/posts/65820315/image-20210316182617074.png" alt="image-20210316182617074"></p><h5 id="自动发现">自动发现</h5><p>dns srv 自动发现，需要一个内部的dns服务器，例如阿里云的云私有解析等。</p><p>如何添加 srv 记录，例如让proms发现节点服务 all.it.zjk.pj:9100 和 all.it.zjk.pj:10052</p><ol><li>添加私有域 zjk.pj</li><li>添加A记录 all.it.zjk.pj -&gt; 192.168.1.1</li><li>添加srv记录 _prometheus._tcp.zjk.pj -&gt; 10 10 9100 all.it.zjk.pj</li><li>添加srv记录 _prometheus._tcp.zjk.pj -&gt; 10 10 10052 all.it.zjk.pj</li></ol><p>关于A记录就不说了，srv记录里 <code>10 10 9100 all.it.zjk.pj</code>的意思是<code>优先级 权重 服务端口 服务器域名</code></p><p>最终，你可以在 http://&lt;prometheus_ip&gt;:9090/classic/targets 看到自动发现的节点信息。</p><h4 id="定义需要告警的指标规则">定义需要告警的指标规则</h4><p>将采集的数据指标通过 expr 进行运算，并通过 record 进行命名。</p><p>关于规则里的表达式语句的写法涉及到 PromQL，可以看一下文档。</p><p><a href="https://fuckcloudnative.io/prometheus/3-prometheus/basics.html">https://fuckcloudnative.io/prometheus/3-prometheus/basics.html</a></p><h5 id="主机监控指标规则">主机监控指标规则</h5><p>/export/docker-data-proms/conf/rules/node-exporter-record-rules.yml</p><pre><code class="language-bash"># node-exporter-record-rules.yml# 标签 job 关联主配置定义的任务 auto_discovery_dns，获取任务传递的数据，从而抽取信息定义 expr# 给 expr 表达式设置一个别名 record, 别名可以被其它 rules 调用groups:  - name: node_exporter-record    rules:    - expr: up&#123;job=~&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:up      labels:        desc: &quot;节点是否在线, 在线1,不在线0&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;    - expr: time() - node_boot_time_seconds&#123;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:node_uptime      labels:        desc: &quot;节点的运行时间&quot;        unit: &quot;s&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                              cpu                                                           #    - expr: (1 - avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:total:percent      labels:        desc: &quot;节点的cpu总消耗百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;idle&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:idle:percent      labels:        desc: &quot;节点的cpu idle百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;iowait&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:iowait:percent      labels:        desc: &quot;节点的cpu iowait百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;system&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:system:percent      labels:        desc: &quot;节点的cpu system百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=&quot;user&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:user:percent      labels:        desc: &quot;节点的cpu user百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (avg by (environment,instance) (irate(node_cpu_seconds_total&#123;job=&quot;auto_discovery_dns&quot;,mode=~&quot;softirq|nice|irq|steal&quot;&#125;[5m])))  * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:other:percent      labels:        desc: &quot;节点的cpu 其他的百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                    memory                                                  #    - expr: node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:total      labels:        desc: &quot;节点的内存总量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_memory_MemFree_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free      labels:        desc: &quot;节点的剩余内存量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; - node_memory_MemFree_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used      labels:        desc: &quot;节点的已使用内存量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; - node_memory_MemAvailable_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:actualused      labels:        desc: &quot;节点用户实际使用的内存量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: (1-(node_memory_MemAvailable_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:used:percent      labels:        desc: &quot;节点的内存使用百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: ((node_memory_MemAvailable_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125; / (node_memory_MemTotal_bytes&#123;job=&quot;auto_discovery_dns&quot;&#125;)))* 100* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:memory:free:percent      labels:        desc: &quot;节点的内存剩余百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                   load                                                     #    - expr: sum by (instance) (node_load1&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load1      labels:        desc: &quot;系统1分钟负载&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;    - expr: sum by (instance) (node_load5&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load5      labels:        desc: &quot;系统5分钟负载&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;    - expr: sum by (instance) (node_load15&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:load:load15      labels:        desc: &quot;系统15分钟负载&quot;        unit: &quot; &quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                 disk                                                       #    - expr: node_filesystem_size_bytes&#123;job=&quot;auto_discovery_dns&quot; ,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:total      labels:        desc: &quot;节点的磁盘总量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_filesystem_avail_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:free      labels:        desc: &quot;节点的磁盘剩余空间&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: node_filesystem_size_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125; - node_filesystem_avail_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:usage:used      labels:        desc: &quot;节点的磁盘使用的空间&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr:  (1 - node_filesystem_avail_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_size_bytes&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:used:percent      labels:        desc: &quot;节点的磁盘的使用百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: irate(node_disk_reads_completed_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:count:rate      labels:        desc: &quot;节点的磁盘读取速率&quot;        unit: &quot;次/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: irate(node_disk_writes_completed_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m])* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:count:rate      labels:        desc: &quot;节点的磁盘写入速率&quot;        unit: &quot;次/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (irate(node_disk_written_bytes_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:read:mb:rate      labels:        desc: &quot;节点的设备读取MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;auto_discovery_dns&quot;    - expr: (irate(node_disk_read_bytes_total&#123;job=&quot;auto_discovery_dns&quot;&#125;[1m]))/1024/1024* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:disk:write:mb:rate      labels:        desc: &quot;节点的设备写入MB速率&quot;        unit: &quot;MB/s&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                filesystem                                                  #    - expr:   (1 -node_filesystem_files_free&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_files&#123;job=&quot;auto_discovery_dns&quot;,fstype=~&quot;ext4|xfs&quot;&#125;) * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filesystem:used:percent      labels:        desc: &quot;节点的inode的剩余可用的百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                filefd                                                     #    - expr: node_filefd_allocated&#123;job=&quot;auto_discovery_dns&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:count      labels:        desc: &quot;节点的文件描述符打开个数&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: node_filefd_allocated&#123;job=&quot;auto_discovery_dns&quot;&#125;/node_filefd_maximum&#123;job=&quot;auto_discovery_dns&quot;&#125; * 100 * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:filefd_allocated:percent      labels:        desc: &quot;节点的文件描述符打开百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                network                                                    #    - expr: avg by (environment,instance,device) (irate(node_network_receive_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:bit:rate      labels:        desc: &quot;节点网卡eth0每秒接收的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_bytes_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:bit:rate      labels:        desc: &quot;节点网卡eth0每秒发送的比特数&quot;        unit: &quot;bit/s&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:packet:rate      labels:        desc: &quot;节点网卡每秒接收的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_packets_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:packet:rate      labels:        desc: &quot;节点网卡发送的数据包个数&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_receive_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netin:error:rate      labels:        desc: &quot;节点设备驱动器检测到的接收错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: avg by (environment,instance,device) (irate(node_network_transmit_errs_total&#123;device=~&quot;eth0|eth1|ens33|ens37&quot;&#125;[1m]))* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:netout:error:rate      labels:        desc: &quot;节点设备驱动器检测到的发送错误包的数量&quot;        unit: &quot;个/秒&quot;        job: &quot;auto_discovery_dns&quot;    - expr: node_tcp_connection_states&#123;job=&quot;auto_discovery_dns&quot;, state=&quot;established&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:established:count      labels:        desc: &quot;节点当前established的个数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;    - expr: node_tcp_connection_states&#123;job=&quot;auto_discovery_dns&quot;, state=&quot;time_wait&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:timewait:count      labels:        desc: &quot;节点timewait的连接数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;    - expr: sum by (environment,instance) (node_tcp_connection_states&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:network:tcp:total:count      labels:        desc: &quot;节点tcp连接总数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                process                                                    #    - expr: node_processes_state&#123;state=&quot;Z&quot;&#125;* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:process:zoom:total:count      labels:        desc: &quot;节点当前状态为zoom的个数&quot;        unit: &quot;个&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################                                other                                                    #    - expr: abs(node_timex_offset_seconds&#123;job=&quot;auto_discovery_dns&quot;&#125;)* on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:time:offset      labels:        desc: &quot;节点的时间偏差&quot;        unit: &quot;s&quot;        job: &quot;auto_discovery_dns&quot;##############################################################################################    - expr: count by (instance) ( count by (instance,cpu) (node_cpu_seconds_total&#123; mode='system'&#125;) ) * on(instance) group_left(nodename) (node_uname_info)      record: node_exporter:cpu:count</code></pre><h5 id="容器监控指标规则">容器监控指标规则</h5><p>/export/docker-data-proms/conf/rules/dockersInfo-record-rules.yml</p><pre><code class="language-yaml">groups:  - name: dockersInfo-record    rules:    - expr: count by (instance, name) (count_over_time(container_last_seen&#123;job=&quot;auto_discovery_dns&quot;, name!=&quot;&quot;, container_label_restartcount!=&quot;&quot;&#125;[15m]))      record: dockersInfo:container:restart      labels:        desc: &quot;15m周期内容器发生重启的次数&quot;        unit: &quot;&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                              cpu                                                           #    - expr: rate(container_cpu_usage_seconds_total&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125;[5m])  * 100      record: dockersInfo:container:cpu:total:percent      labels:        desc: &quot;容器的cpu总消耗百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;    - expr: rate(container_fs_io_time_seconds_total&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125;[5m])  * 100      record: dockersInfo:cpu:iowait:percent      labels:        desc: &quot;容器的cpu iowait百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;###############################################################################################                                    memory                                                  #    - expr: container_spec_memory_limit_bytes&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125;      record: dockersInfo:memory:total      labels:        desc: &quot;容器的内存总量&quot;        unit: byte        job: &quot;auto_discovery_dns&quot;    - expr: container_memory_usage_bytes&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125; / container_spec_memory_limit_bytes&#123;job=&quot;auto_discovery_dns&quot;, name!=''&#125; * 100      record: dockersInfo:memory:used:percent      labels:        desc: &quot;容器的内存使用百分比&quot;        unit: &quot;%&quot;        job: &quot;auto_discovery_dns&quot;</code></pre><h4 id="定义监控指标阈值规则">定义监控指标阈值规则</h4><h5 id="主机监控指标阈值">主机监控指标阈值</h5><p>/export/docker-data-proms/conf/rules/node-exporter-alert-rules.yml</p><pre><code class="language-bash"># node-exporter-alert-rules.yml# 定义告警规则# 通过前一个 rules 文件拿到定义的 record 别名来编写 expr 判断式# 这里定义的告警规则，在触发的时候，都会传递到 alertmanager，最后从传递的信息中抽取所需数据发送给目标人。groups:  - name: node-alert    rules:    - alert: node-down      expr: node_exporter:up == 0      for: 1m      labels:        severity: critical      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 宕机了&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-cpu-high      expr:  node_exporter:cpu:total:percent &gt; 80      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-cpu-iowait-high      expr:  node_exporter:cpu:iowait:percent &gt;= 12      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu iowait 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-load-load1-high      expr:  (node_exporter:load:load1) &gt; (node_exporter:cpu:count) * 1.2      for: 3m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; load1 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-memory-high      expr:  node_exporter:memory:used:percent &gt; 85      for: 3m      labels:        severity: info      annotations:        summary: &quot;内存使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-high      expr:  node_exporter:disk:used:percent &gt; 80      for: 3m      labels:        severity: info      annotations:        summary: &quot;&#123;&#123; $labels.device &#125;&#125;:&#123;&#123; $labels.mountpoint &#125;&#125; 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-read:count-high      expr:  node_exporter:disk:read:count:rate &gt; 3000      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; iops read 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-write-count-high      expr:  node_exporter:disk:write:count:rate &gt; 3000      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; iops write 使用率高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-read-mb-high      expr:  node_exporter:disk:read:mb:rate &gt; 60      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 读取字节数 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-disk-write-mb-high      expr:  node_exporter:disk:write:mb:rate &gt; 60      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 写入字节数 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-filefd-allocated-percent-high      expr:  node_exporter:filefd_allocated:percent &gt; 80      for: 10m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 打开文件描述符 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netin-error-rate-high      expr:  node_exporter:network:netin:error:rate &gt; 4      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包进入的错误速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netin-packet-rate-high      expr:  node_exporter:network:netin:packet:rate &gt; 35000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包进入速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-netout-packet-rate-high      expr:  node_exporter:network:netout:packet:rate &gt; 35000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 包流出速率 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-network-tcp-total-count-high      expr:  node_exporter:network:tcp:total:count &gt; 40000      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; tcp连接数量 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-process-zoom-total-count-high      expr:  node_exporter:process:zoom:total:count &gt; 10      for: 10m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 僵死进程数量 高于 &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: node-time-offset-high      expr:  node_exporter:time:offset &gt; 0.03      for: 2m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; &#123;&#123; $labels.desc &#125;&#125;  &#123;&#123; $value &#125;&#125;&#123;&#123; $labels.unit &#125;&#125;&quot;        grafana: &quot;http://jksgg.pengwin.com:3000/d/9CWBz0bik/node-exporter?orgId=1&amp;var-instance=&#123;&#123; $labels.instance &#125;&#125; &quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;</code></pre><h5 id="容器监控指标阈值">容器监控指标阈值</h5><p>/export/docker-data-proms/conf/rules/dockersInfo-alert-rules.yml</p><pre><code class="language-yaml">groups:  - name: container-alert    rules:    - alert: container-restart-times-high      expr: dockersInfo:container:restart &gt; 5      for: 1m      labels:        severity: warn      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; 15分钟内重启次数超过5次&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: container-cpu-usage-high      expr: dockersInfo:container:cpu:total:percent &gt; 90      for: 1m      labels:        severity: info      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; cpu 使用率持续超过90%.&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;    - alert: container-cpu-iowait-high      expr: dockersInfo:cpu:iowait:percent &gt; 10      for: 1m      labels:        severity: warn      annotations:        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; cpu iowait 持续超过10%&quot;        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;        type: &quot;aliyun_meta_ecs_info&quot;#    - alert: container-mem-usage-high#      expr: dockersInfo:memory:used:percent &gt; 80#      for: 1m#      labels:#        severity: warn#      annotations:#        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; 下的 container: &#123;&#123; $labels.name &#125;&#125; 内存使用率超过80%&quot;#        grafana: &quot;http://proms.pengwin.com:3000/d/Ss3q6hSZk/dockersinfo?orgId=1&amp;var-node=&#123;&#123; $labels.instance &#125;&#125;&quot;#        console: &quot;https://ecs.console.aliyun.com/#/server/&#123;&#123; $labels.instanceid &#125;&#125;/detail?regionId=cn-beijing&quot;#        cloudmonitor: &quot;https://cloudmonitor.console.aliyun.com/#/hostDetail/chart/instanceId=&#123;&#123; $labels.instanceid &#125;&#125;&amp;system=&amp;region=cn-beijing&amp;aliyunhost=true&quot;#        id: &quot;&#123;&#123; $labels.instanceid &#125;&#125;&quot;#        type: &quot;aliyun_meta_ecs_info&quot;</code></pre><h3 id="部署">部署</h3><h4 id="docker-方式">docker 方式</h4><pre><code class="language-bash">docker run -d \--name proms \--network promsnet \--restart always \-p 9090:9090 \--mount 'type=bind,src=/export/docker-data-proms/prometheus.yml,dst=/etc/prometheus/prometheus.yml'  \--mount 'type=bind,src=/export/docker-data-proms/data,dst=/prometheus' \--mount 'type=bind,src=/export/docker-data-proms/conf,dst=/etc/prometheus/conf' \prom/prometheus \--config.file=/etc/prometheus/prometheus.yml \--storage.tsdb.path=/prometheus \--storage.tsdb.retention=15d \--web.enable-admin-api</code></pre><p>⚠️<code>--web.enable-admin-api</code> 如果不开启，则无法使用api接口，也就无法自主的删除数据.</p><h4 id="二进制方式">二进制方式</h4><pre><code class="language-bash">wget https://github.com/prometheus/prometheus/releases/download/v2.20.0/prometheus-2.20.0.linux-amd64.tar.gzmkdir prometheustar xf prometheus-2.20.0.linux-amd64.tar.gz --strip-components 1 -C prometheusrm -rf prometheus-2.20.0.linux-amd64.tar.gzcd prometheus &amp;&amp; baseDir=`pwd`cp prometheus.yml&#123;,.bak&#125; #---cat &gt; /usr/lib/systemd/system/prometheus.service &lt;&lt;EOF[Unit]Description=prometheus server daemon[Service]Restart=on-failureExecStart=$&#123;baseDir&#125;/prometheus --config.file=$&#123;baseDir&#125;/prometheus.yml[Install]WantedBy=multi-user.targetEOF#---systemctl daemon-reloadsystemctl start prometheus</code></pre><h3 id="查看">查看</h3><p>被监控端状态：<a href="http://xxx:9090/targets">http://xxx:9090/targets</a></p><p>通过上述地址，你可以看到配置中 job_name 定义的被监控任务的端点状态</p><p><img src="/posts/65820315/image-20210311110821675.png" alt="image-20210311110821675"></p><h3 id="数据删除">数据删除</h3><pre><code class="language-bash"># match[] 表示匹配所有key, &#123;&#125;精确选中目标# 即删除包含&#123;instance=&quot;10.3.128.202:15692&quot;&#125;的所有数据curl -X POST -g 'http://127.0.0.1:9090/api/v1/admin/tsdb/delete_series?match[]=&#123;instance=~&quot;10.3.128.202:15692&quot;&#125;'# 添加删除时间范围curl -X POST -g 'http://127.0.0.1:9090/api/v1/admin/tsdb/delete_series?start=2021-05-30T00:00:00Z&amp;end=2021-06-07T23:59:59Z'</code></pre><h2 id="pushgateway-未验证">pushgateway (未验证)</h2><blockquote><p>push 模式下的网关</p><p>当前没有现成的推送模式的 node-exporter</p></blockquote><pre><code class="language-bash"># dockerdocker run -d --name=pushgateway -p 9091:9091 prom/pushgateway</code></pre><pre><code class="language-bash"># 一个推送模式的采集脚本示例# cat tcpestab.sh #!/bin/bash# 添加脚本到计划任务中，定时采集# pushgateway ippushgatewayIp=#获取主机名，常传输到Prometheus标签以主机名instance_name=`hostname -f | cut -d'.' -f1`#判断主机名不能是localhost不然发送过的数据不知道是那个主机的 if [ $instance_name == &quot;localhost&quot; ];thenecho &quot;Hostname must not localhost&quot;exit 1fi#自定义key，在Prometheus即可使用key查询label=&quot;count_estab_connections&quot; #获取TCP estab 连接数count_estab_connections=`netstat -an | grep -i 'established' | wc -l`#将数据发送到pushgateway固定格式echo &quot;$label $count_estab_connections&quot;  | curl --data-binary @- http://$pushgatewayIp:9091/metrics/job/pushgateway/instance/$instance_name</code></pre><h1>被监控端组件</h1><h2 id="node-exporter-物理节点监控组件">node-exporter 物理节点监控组件</h2><blockquote><p>宿主数据采集端，部署在被监控主机的9100端口</p></blockquote><h3 id="docker-不太建议">docker 不太建议</h3><pre><code class="language-bash">docker run -d \  --name=node-exporter \  --restart=always \  --net=&quot;host&quot; \  --pid=&quot;host&quot; \  -v &quot;/:/host:ro,rslave&quot; \  quay.io/prometheus/node-exporter:latest \  --path.rootfs=/host</code></pre><h3 id="yum-方式-推荐">yum 方式 推荐</h3><pre><code class="language-bash"># yum包 https://copr.fedorainfracloud.org/coprs/ibotty/prometheus-exporters/curl -Lo /etc/yum.repos.d/_copr_ibotty-prometheus-exporters.repo https://copr.fedorainfracloud.org/coprs/ibotty/prometheus-exporters/repo/epel-7/ibotty-prometheus-exporters-epel-7.repo &amp;&amp; yum install node_exporter -y</code></pre><h3 id="源码包">源码包</h3><pre><code class="language-bash">cd /usr/local/wget https://github.com/prometheus/node_exporter/releases/download/v1.0.1/node_exporter-1.0.1.linux-amd64.tar.gzmkdir node_exportertar xf node_exporter-1.0.1.linux-amd64.tar.gz --strip-components 1 -C node_exportermv node_exporter-1.0.1.linux-amd64.tar.gz srccd node_exporter &amp;&amp; baseDir=`pwd`#---cat &gt; /usr/lib/systemd/system/node_exporter.service &lt;&lt;EOF[Unit]Description=node_exporter server daemon[Service]Restart=on-failureExecStart=$&#123;baseDir&#125;/node_exporter[Install]WantedBy=multi-user.targetEOF</code></pre><h3 id="yum和源码的启动方式">yum和源码的启动方式</h3><pre><code class="language-bash">systemctl daemon-reloadsystemctl start node_exportersystemctl status node_exportercurl http://localhost:9100/metrics # 查看获取的监控数据systemctl enable node_exporter</code></pre><h2 id="cadvisor-容器监控组件">cadvisor 容器监控组件</h2><blockquote><p>容器数据采集端，部署在被监控容器所在宿主的10052端口</p></blockquote><pre><code class="language-bash">dockerRoot=`docker info | awk -F':'  '/Docker Root Dir/&#123;print $2&#125;'|sed 's@^ *@@g'`echo $dockerRootdocker run \  --restart=always \  --volume=/:/rootfs:ro \  --volume=/var/run:/var/run:rw \  --volume=/sys:/sys:ro \  --volume=$&#123;dockerRoot&#125;/:/var/lib/docker:ro \  --volume=/dev/disk/:/dev/disk:ro \  --publish=10052:8080 \  --privileged=true \  --detach=true \  --name=cadvisor \  google/cadvisor:latest</code></pre><h1>使用</h1><h2 id="添加-grafana-数据源-proms">添加 grafana 数据源 : proms</h2><p><img src="/posts/65820315/image-20201218135400147.png" alt="image-20201218135400147"></p><h2 id="添加-grafana-监控模板">添加 grafana 监控模板</h2><p><img src="/posts/65820315/image-20201218135502105.png" alt="image-20201218135502105"></p><p>node模板：<a href="https://grafana.com/grafana/dashboards/8919">https://grafana.com/grafana/dashboards/8919</a></p><p>docker模板：<a href="https://grafana.com/grafana/dashboards/10566">https://grafana.com/grafana/dashboards/10566</a></p>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞server安装注意事项</title>
      <link href="posts/ed865d2f/"/>
      <url>posts/ed865d2f/</url>
      
        <content type="html"><![CDATA[<ol><li><p>数据库编码一定要安装官方来命令来设置，例如zabbix5编码是 utf8和utf8_bin</p></li><li><p>最后web控制台安装界面，要确保zabbix-server可以访问设置的 hostname 的 10051 端口。如果hostname写的域名，那么要确保zabbix-server可以通过 hosts本地解析或者外网访问10051端口</p></li><li><p>zabbix-server 的 agent 要使用拉取模式，因为官方给的模板无法直接修正为推流模式</p></li><li><p>报警媒介设置</p><p>脚本设置</p><pre><code class="language-bash">脚本参数&#123;ALERT.SENDTO&#125;&#123;ALERT.SUBJECT&#125;&#123;ALERT.MESSAGE&#125;</code></pre><p>问题模板</p><pre><code class="language-bash">故障: &#123;TRIGGER.NAME&#125;【[骷髅]】=》&#123;TRIGGER.SEVERITY&#125; 事件ID : &#123;EVENT.ID&#125;【UTC+0】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;【告警等级】: &#123;TRIGGER.SEVERITY&#125;【主机信息1】: &#123;HOST.NAME&#125;【主机信息2】: &#123;HOSTNAME1&#125;【告警信息】: &#123;TRIGGER.NAME&#125;【告警项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;</code></pre><p>恢复模板</p><pre><code class="language-bash">恢复: &#123;TRIGGER.NAME&#125;【[OK]】=》&#123;TRIGGER.SEVERITY&#125; 事件ID : &#123;EVENT.ID&#125;【UTC+0】: &#123;EVENT.DATE&#125; &#123;EVENT.TIME&#125;【告警等级】: &#123;TRIGGER.SEVERITY&#125;【主机信息1】: &#123;HOST.NAME&#125;【主机信息2】: &#123;HOSTNAME1&#125;【告警信息】: &#123;TRIGGER.NAME&#125;【告警项目】: &#123;TRIGGER.KEY1&#125;【问题详情】: &#123;ITEM.NAME&#125;:&#123;ITEM.VALUE&#125;【当前状态】: &#123;TRIGGER.STATUS&#125;:&#123;ITEM.VALUE1&#125;</code></pre></li><li><p>预警用户添加超管权限</p></li><li><p>中文字体</p><pre><code class="language-bash">yum install google-noto-sans-simplified-chinese-fonts.noarch -yrm -rf /etc/alternatives/zabbix-web-fontln -s /usr/share/fonts/google-noto/NotoSansSC-Regular.otf /etc/alternatives/zabbix-web-font</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞从容器中访问宿主机docker命令</title>
      <link href="posts/3e0f2183/"/>
      <url>posts/3e0f2183/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>我的jenkins是运行在docker中，但是jenkins官方的镜像里却没有docker命令。</p><p>以至于无法在流水线中打包docker镜像。</p><h2 id="方法">方法</h2><p>首先，需要将docker命令、docker.sock文件以及相关依赖文件映射到容器内。</p><p>其次，以root用户访问容器，在容器中添加docker组，并且组id需要和宿主机中的docker组id一致。</p><p>最后，以root用户访问容器，并将jenkins用户加入到容器中的docker组中。</p><p>最最后，最关键的来了， 一定要重启一下 jenkins 容器。。。</p><h2 id="相关命令">相关命令</h2><pre><code class="language-bash"># 额外的映射文件（宿主机文件和容器内的映射路径，以实际情况为准）-v /var/run/docker.sock:/var/run/docker.sock -v /usr/bin/docker:/usr/bin/docker -v /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7# 以 root 用户访问 jenkins 容器（宿主机的组ID以实际情况为准）docker exec -it -u root jenkins /bin/bashgroupadd -g xxx dockerusermod -aG docker jenkins# 重启 jenkins 容器docker stop jenkins &amp;&amp; docker start jenkins</code></pre><h2 id="注意">注意</h2><p>在容器内执行的 groupadd 和 usermod 命令，需要在每次变更容器镜像后，重新执行，因为命令的相关结果都是容器内数据，清理后不会保留。</p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞ssh-MFA自动登陆</title>
      <link href="posts/5db0cd07/"/>
      <url>posts/5db0cd07/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>ssh 关联MFA后,安全度增加了很多,但是每次手动输入MFA动态口令比较麻烦.所以记录一下自动交互输入MFA口令</p><h2 id="命令安装">命令安装</h2><pre><code class="language-bash">sudo apt install oathtool gnupg2 expect</code></pre><blockquote><p>oathtool 是我们用来生成MFA口令的工具.</p><p>expect 用来编写交互程序</p></blockquote><h2 id="登陆交互脚本">登陆交互脚本</h2><pre><code class="language-bash">#!/usr/bin/expectset timeout 5set MFAToken &quot;我是MFA的TOKEN&quot;spawn 我是ssh命令expect &quot;MFA auth&quot;send &quot;[exec oathtool -b --totp $MFAToken]\r&quot;;interact</code></pre><blockquote><p>将脚本中的TOKEN和命令替换为自己的.</p></blockquote><h2 id="注意">注意</h2><p>如果你发现你登陆不了, 每次都验证错误, 那么你应该检查下你机器的时间是否正常.</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ssh </tag>
            
            <tag> mfa </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jdk☞安装和配置</title>
      <link href="posts/ac6b3b36/"/>
      <url>posts/ac6b3b36/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>oracle jdk 现在是商业产品了, 所以线上最好还是用 openjdk.</p><h2 id="安装">安装</h2><p><a href="https://adoptopenjdk.net/installation.html#linux-pkg">https://adoptopenjdk.net/installation.html#linux-pkg</a></p><p>rhel之类的，搜索【RPM installation on Centos, RHEL, or Fedora】位置，在添加了repo源之后，可以用 <code>yum list adoptopenjdk*</code>来查看源包含的版本</p><p>包命名示例：adoptopenjdk-11-openj9.x86_64，这里 openj9 指的是 jvm 版本 ，还有 hotspot 版本的 jvm</p><hr><p>ubuntu之类的，搜索【Deb installation on Debian or Ubuntu】位置，在添加了源之后，可以用<code>apt-show-versions -a adoptopenjdk* </code>来查看源包含的版本</p><h2 id="二进制安装">二进制安装</h2><p><a href="https://adoptopenjdk.net/installation.html?variant=openjdk11&amp;jvmVariant=hotspot#x64_linux-jdk">https://adoptopenjdk.net/installation.html?variant=openjdk11&amp;jvmVariant=hotspot#x64_linux-jdk</a></p><h2 id="二进制配置">二进制配置</h2><pre><code class="language-bash">tar xzf OpenJDK11U-jdk_x64_linux_hotspot_11.0.10_9.tar.gzexport PATH=$PWD/jdk-11.0.10+9/bin:$PATHjava -version</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> jdk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞fcgi中alias的使用</title>
      <link href="posts/58a1c0c7/"/>
      <url>posts/58a1c0c7/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>众所周知, nginx 在配置静态资源的时候, root 和 alias 分别是两种指定资源路径的方式.</p><p>如果：</p><p>​url 是 <code>http://xxx.com/god/a.jpg</code>，且 location 匹配的是 <code>/god/</code>.</p><p>则：</p><p>​当用 root 定位资源路径时, root 配置的值=域名<code>xxx.com</code> 部分所对应的路径, 此时 a.jpg 的物理路径就是  $root/god/a.jpg</p><p>​当用 alias 定位资源路径时, alias 配置的值=url<code>xxx.com/god/</code>部分所对应的路径, 此时 a.jpg 的物理路径就是 ${alias}a.jpg</p><p>即</p><pre><code class="language-bash">location ^~ /god/ &#123;    root /export/webapps/xxx.com;&#125;# 或者location ^~ /god/ &#123;    alias /export/webapps/xxx.com/god/;&#125;</code></pre><p>当我采用上述规则, 使用alias配置fcgi的时候,现实给了我暴击…妥妥的404了.</p><h2 id="在fcgi环境下-alias-的配置">在fcgi环境下, alias 的配置</h2><p>废话不多说, 直接上结果.</p><pre><code class="language-bash">location ^~ /god/ &#123;     index  index.php index.html index.htm;     root /export/webapps/xxx.com/;     location ~* &quot;\.php$&quot; &#123;         try_files      $uri =404;         fastcgi_pass   127.0.0.1:9000;         fastcgi_index  index.php;         include        fastcgi.conf;         fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;     &#125; &#125; location ^~ /god/ &#123;    index  index.php index.html index.htm;    alias /export/webapps/xxx.com/god/;    location ~* &quot;\.php$&quot; &#123;        try_files      $uri =404;        fastcgi_pass   127.0.0.1:9000;        include        fastcgi.conf;        fastcgi_param  SCRIPT_FILENAME  $request_filename;    &#125;&#125;</code></pre><blockquote><p>官方的例子:</p><p><a href="https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/#use-request-filename-for-script-filename">https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/#use-request-filename-for-script-filename</a></p></blockquote><p>关键点：</p><ol><li>当你用 alias 而不是 root 的时候, 你需要将$document_root$fastcgi_script_name替换成$request_filename</li><li>当你不在使用$fastcgi_script_name的时候，你需要显性的添加index，而不是fastcgi_index ,因为$request_filename并不能关联fastcgi_index。</li></ol><p>原因在于$document_root的值，来自于root或者alias，$fastcgi_script_name总是拿url_path</p><p>假设你访问 /god/api.php,  而这个文件的物理路径是/export/webapps/xxx.com/god/api.php。</p><p>此时，</p><p>当你用 alias 的时候</p><p>$document_root = alias = /export/webapps/xxx.com/god/</p><p>$fastcgi_script_name = /god/api.php</p><p>$document_root$fastcgi_script_name=/export/webapps/xxx.com/god//god/api.php</p><p>而$request_filename当前文件的请求路径，由root或者alias+uri相对路径</p>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> fcgi </tag>
            
            <tag> php </tag>
            
            <tag> alias </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞角色</title>
      <link href="posts/720afd15/"/>
      <url>posts/720afd15/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>角色的作用就是规范化，将一个playbook各部分分门别类的放置在规定好的目录中。就如同linux系统一样，/etc/就是放配置的，/bin 就是放程序的，/tmp 就是放临时文件的 …</p><p>ansible 会基于官方规定好的目录结构, 去自动加载目录中的文件. 当一个需求很复杂的时候, 我们就可以基于角色对需求进行分组.</p><p>最后, 如果你不按照这个规定来走, 那么ansible角色模块就找不到相关东西.</p><h2 id="角色结构">角色结构</h2><p>这是一个官方项目的例子</p><blockquote><p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html#">https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html#</a>  官方文档</p></blockquote><pre><code class="language-bash">site.ymlwebservers.ymlfooservers.ymlroles/    common/        tasks/        handlers/        files/        templates/        vars/        defaults/        meta/    webservers/        tasks/        defaults/        meta/</code></pre><p>例子中，common 和 webservers 就是两个角色<br>关于角色的目录，必须包含下面列举出来的目录之一，目录可以为空。但是如果你使用到了某个目录，那么部分目录需要包含main.yml文件。</p><ul><li>tasks 角色用到的主要任务列表</li><li>handlers 角色或者角色外用到的任务后续处理</li><li>defaults 角色默认变量</li><li>vars 角色其余变量，优先级大于 defaults 中定义的变量</li><li>files 角色部署中牵扯到的文件</li><li>templates 角色的jinja2模板</li><li>meta 角色的元数据信息，例如角色的属性(作者, 说明, 一些特殊功能等)</li></ul><p>最后，目录中main.yml用来存储主配置信息。在这里，你可以包含其它的子任务，在子任务中详细描述。<br>例如，在 tasks/main.yml 中，通过 import_tasks，包含其它子任务，就像下面这样</p><blockquote><p>main 包含 redhat 和 debian。当系统是 redhat 时，安装 httpd 包，当系统是 debian 时，安装 apache2 包</p></blockquote><pre><code class="language-yaml"># roles/xxx/tasks/main.yml- name: added in 2.4, previously you used 'include'  import_tasks: redhat.yml  when: ansible_facts['os_family']|lower == 'redhat'- import_tasks: debian.yml  when: ansible_facts['os_family']|lower == 'debian'# roles/xxx/tasks/redhat.yml- yum:    name: &quot;httpd&quot;    state: present# roles/xxx/tasks/debian.yml- apt:    name: &quot;apache2&quot;    state: present</code></pre><h2 id="使用角色">使用角色</h2><pre><code class="language-yaml">---# file: site.yml- include: webservers.yml- include: fooservers.yml</code></pre><pre><code class="language-yaml">---# file: webservers.yml- hosts: webservers  roles:    - common    - webservers</code></pre><p>只运行 webservers 角色，可以通过 site.yml 添加 limit 限制执行，也可以直接调用</p><pre><code class="language-bash">ansible-playbook site.yml --limit webserversansible-playbook webservers.yml</code></pre><h2 id="角色优先级">角色优先级</h2><p>角色中针对 tasks, handlers, vars 有一个优先级概念. 优先级大的会覆盖掉优先级小的配置.</p><p>优先级由大到小如下:</p><p><code>cli 层面参数 &gt; role-xxx-dir &gt; playbook-xxx &gt; role-defaults-dir</code></p><p>这里 cli 层面参数指的是 ansible-playbook -e vara=‘a’ test.play 这种外部传递变量的行为</p><p>这里 role-xxx-dir 指的是 role 特定目录里的配置, 例如 tasks, handlers, vars</p><p>这里 playbook-xxx 指的是直接写入 ploybook 中的部分</p><p>这里 role-defaults-dir 指的是角色目录中的 defaults 默认变量目录</p><h2 id="角色复制">角色复制</h2><p>如果你想让一个角色多次执行，就如同下面这样</p><pre><code class="language-yaml">---- hosts: webservers  roles:    - moo    - moo</code></pre><p>有下列两种方式:</p><ol><li>moo，拥有不同的参数</li><li>将<code>allow_duplicates: true</code>写入 moo/meta/main.yml</li></ol><h2 id="角色标签">角色标签</h2><p>我们一定要针对角色中的tasks添加标签(tags). 原因在于, 有些时候, 当我们只想修改部署很久的一堆机器的某个服务时, 我们只需要在执行角色的 playbook 的时候,追加 --tags 即可帮助我们执行项目中某一个任务，而不必执行所有任务.</p><p>比如, 我们有个部署项目, 其中有一个初始化角色( common ). common 中有众多的初始化任务, 其中一个任务是 ntp 服务的 安装/配置/启动/重启 .</p><p>ntp 服务配置如下:</p><pre><code class="language-yaml">---# file: roles/common/tasks/main.yml- name: be sure ntp is installed  yum: pkg=ntp state=installed  tags: ntp- name: be sure ntp is configured  template: src=ntp.conf.j2 dest=/etc/ntp.conf  notify:    - restart ntpd  tags: ntp- name: be sure ntpd is running and enabled  service: name=ntpd state=running enabled=yes  tags: ntp  ---# file: roles/common/handlers/main.yml- name: restart ntpd  service: name=ntpd state=restarted</code></pre><p>现在, 我们只想修改生产环境的 ntp 服务的配置, 那么只需要将 ntp.conf.j2 模板配置好之后, 重新执行这个playbooks即可. 就想下面这样:</p><pre><code class="language-bash">ansible-playbook -i production site.yml --tags ntp</code></pre><h2 id="写好的playbook">写好的playbook</h2><p><a href="https://galaxy.ansible.com/">https://galaxy.ansible.com/</a></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞模板</title>
      <link href="posts/724fda19/"/>
      <url>posts/724fda19/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>当你用ansible进行多机器的配置调整，且调整的东西都一模一样，此时你不会拒绝模板的诱惑。</p><p>ansible的模板是jinja2，所以jinja2的特性，在这里都可以用。</p><blockquote><p>模板中，不要出现任何你觉得模板会忽略的东西，包括但不限于空格</p></blockquote><h2 id="模块-template">模块 template</h2><p>参数：</p><ul><li>src 模板文件路径</li><li>dest 目的文件路径</li></ul><p>牵扯到目的路径，必然有权限参数</p><ul><li>owner 目的属主</li><li>group 目的属组</li><li>mode 目的权限</li></ul><p>覆盖与备份</p><ul><li>force 覆盖，yes / no</li><li>backup 备份， yes / no ， 若为 yes ，则目的重名文件会先改名</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - template:        src: ~/test.j2        dest: ~/test.info</code></pre><h2 id="模板分隔符">模板分隔符</h2><pre><code class="language-bash">&#123;&#123; &#125;&#125; 一般用来填充变量，可以是过滤器，也可以填充表达式，从而返回相应的值，例如 &#123;&#123; 1==1 &#125;&#125; 返回 True&#123;% %&#125; 一般用来填充控制语句&#123;# #&#125; 模板注释语句，并非渲染后会出现#  ... ## 这一种 ansible 貌似不支持，所以可以忽略</code></pre><h3 id="分隔符1">分隔符1</h3><pre><code class="language-jinja2"># &#123;&#123; &#125;&#125;&#123;# 普通变量 #&#125;&#123;&#123; foo.bar &#125;&#125;&#123;&#123; foo['bar'] &#125;&#125;&#123;# 以过滤器 lookup 为例 #&#125;&#123;&#123; lookup('file', '~/test.file') &#125;&#125;&#123;&#123; lookup('env', 'PATH' )&#125;&#125;</code></pre><p>最终目的文件，会输出<code>~/test.file</code> 内容和 <code>$PATH</code> 内容</p><blockquote><p>字符串拼接需要使用<code>~</code>，例如 <code>&quot;name:&quot;~name</code></p></blockquote><h3 id="分隔符2">分隔符2</h3><pre><code class="language-bash"># &#123;% %&#125;# 官网所有的控制列表https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures</code></pre><h4 id="条件控制语句-if">条件控制语句 if</h4><pre><code class="language-jinja2">&#123;% if 条件1 %&#125;  pass&#123;% elif 条件2 %&#125;  pass&#123;% else %&#125;  pass &#123;% endif %&#125;</code></pre><h4 id="循环语句-for">循环语句 for</h4><pre><code class="language-jinja2">&#123;% for i in 可迭代对象 %&#125;  &#123;&#123; i &#125;&#125;&#123;% endfor %&#125;</code></pre><blockquote><p>默认循环后，每一个循环单体独占一行，如果需要删除独占，则需要给第二个%}和第三个控制符{%加减号，最终变为-%}和{%-。</p></blockquote><p>关于字典类型，可以使用 iteritems() 函数，从而方便的获取到字典的 k 和 v。例如</p><pre><code class="language-jinja2">&#123;% for k,v in &#123;'name':'zhangsan', 'gender':'male'&#125;.iteritems() %&#125;  &#123;&#123; k &#125;&#125;:&#123;&#123; v &#125;&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后name:zhangsangender:male</code></pre><h4 id="条件和循环组合语句">条件和循环组合语句</h4><pre><code class="language-jinja2">&#123;% for i in 可迭代对象 if 条件 %&#125;  满足条件语句&#123;% else %&#125;  不满足条件语句&#123;% endfor %&#125;</code></pre><p>例如</p><pre><code class="language-jinja2">&#123;% for i in [1,2,3,4] if i>2 %&#125;&#123;&#123; i &#125;&#125;'s index is &#123;&#123; loop.index &#125;&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后3's index is 14's index is 2</code></pre><blockquote><p>loop.index 是循环体索引，这里可能会有个疑问。</p><p>正常情况下，3和4的索引应该就是3和4，之所以是1和2，原因在于当条件控制和循环控制位于同一行的时候，先行运算的是 <code>[1,2,3,4] if i&gt;2</code>，之后才开始走<code>for</code>循环。</p><p>如果你想输出原始循环体，则需要将条件控制语句另起一行，放在<code>for</code>循环内部</p></blockquote><pre><code class="language-jinja2">&#123;% for i in [1,2,3,4] %&#125;&#123;% if i>2 %&#125;&#123;&#123; i &#125;&#125;'s index is &#123;&#123; loop.index &#125;&#125;&#123;% endif %&#125;&#123;% endfor %&#125;</code></pre><pre><code class="language-bash"># 渲染后3's index is 34's index is 4</code></pre><blockquote><p>上述的 loop.index 只是jinja2的一种使用方式，其它方式具体可见官网文档</p><p><a href="https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures">https://jinja.palletsprojects.com/en/master/templates/#list-of-control-structures</a></p></blockquote><h4 id="宏-macro">宏 macro</h4><p>宏就是类似于函数的一个东西。</p><pre><code class="language-jinja2">&#123;# 编写宏 #&#125;&#123;% macro func() %&#125;函数体&#123;% endmacro %&#125;&#123;# 调用宏 #&#125;&#123;&#123; func() &#125;&#125;</code></pre><p>例如：</p><pre><code class="language-jinja2">&#123;% macro func(a,b,c=3,d=4) %&#125;&#123;# 宏编写的时候，宏参数，要遵循默认参数在后&#123;&#123; a &#125;&#125;&#123;&#123; b &#125;&#125;&#123;&#123; c &#125;&#125;&#123;&#123; d &#125;&#125;&#123;% endmacro %&#125;&#123;&#123; func(1,2,5) &#125;&#125;</code></pre><pre><code class="language-bash"># 渲染后1254</code></pre><blockquote><p>当给出参数超出了宏所定义的参数时，根据情况，宏会将多余的参数存在变量中，即：</p><p>超出的为非关键字参数，则存放在一个叫<code>varargs</code>的元组中</p><p>超出的为关键字参数，则存放在一个叫<code>kwargs</code>的字典中</p></blockquote><h4 id="call-方法">call 方法</h4><p>如同当前函数的装饰器，可以扩展当前宏的功能</p><pre><code class="language-jinja2">&#123;# 编写宏 func，并调用 caller #&#125;&#123;% macro func(a) %&#125;我有一个&#123;&#123; a &#125;&#125;。&#123;&#123; caller(a) &#125;&#125;&#123;% endmacro %&#125;&#123;# 编写宏 func_ext #&#125;&#123;% macro func_ext(a,b) %&#125;但&#123;&#123; b &#125;&#125;比&#123;&#123; a &#125;&#125;好吃。&#123;% endmacro %&#125;&#123;# 通过 call 关联 func，加载 func_ext #&#125;&#123;% call(a) func('汉堡') %&#125;&#123;&#123; func_ext(a,'三明治') &#125;&#125;&#123;% endcall %&#125;</code></pre><blockquote><p>caller是call的对象，因此caller也是可以给call传参</p></blockquote><pre><code class="language-bash"># 渲染后我有一个汉堡。但三明治比汉堡好吃</code></pre><h2 id="扩展">扩展</h2><blockquote><p>扩展官方文档，可见 <a href="https://jinja.palletsprojects.com/en/master/extensions/">https://jinja.palletsprojects.com/en/master/extensions/</a></p></blockquote><p>这里我只简单的说一下如何启动 <code>for</code> 循环中的 <code>break</code> 和 <code>continue</code>。</p><p><code>ansible</code> 中添加 <code>jinja2</code> 扩展，需要修改主配置文件 <code>/etc/ansible/ansible.cfg</code>，找到 <code>jinja2_extensions </code>，在后面追加扩展配置即可，每一个扩展用逗号<code>,</code>分割。</p><p><code>break</code>和<code>continue</code> 的扩展名叫：<code>jinja2.ext.loopcontrols</code></p>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-include</title>
      <link href="posts/6ce01f58/"/>
      <url>posts/6ce01f58/</url>
      
        <content type="html"><![CDATA[<h2 id="引入额外任务">引入额外任务</h2><pre><code class="language-yaml">tasks:  - include: add.yml</code></pre><h2 id="绑定-kv-对，从而改变额外任务里的变量">绑定 kv 对，从而改变额外任务里的变量</h2><pre><code class="language-yaml">tasks:  - include: add.yml    var1=hello    var2=world</code></pre><h2 id="绑定-tags-标记">绑定 tags 标记</h2><blockquote><p>可以通过tags执行相应的额外任务</p></blockquote><pre><code class="language-yaml:">tasks:  - include: add1.yml    tags: add1  - include: add2.yml    tags: add2</code></pre><pre><code class="language-bash">ansible-playbook test.play --tags add1 # 仅执行 add1.yml 任务</code></pre><h2 id="绑定-loop-循环">绑定 loop 循环</h2><pre><code class="language-yaml">tasks:  - include: add.yml    loop:      - [1,2,3]# add.yml- debug:  msg: &quot;loop-item: &#123;&#123; item &#125;&#125; in add.yml &quot;</code></pre><h2 id="绑定-when-条件">绑定 when 条件</h2><pre><code class="language-yaml">tasks:  - include: add.yml    when: 1 &lt; 2</code></pre><hr><p>ansible 在当前版本2.9中，推荐使用 import_tasks 和 include_tasks 来替换 include，include 未来有可能不在支持。（为啥总感觉 ansible 各种变呢）</p><p>import_tasks 静态任务导入，静态任务简单来说，就是不能从任务外传递变量到任务中。</p><p>include_tasks 动态任务导入。支持循环传递变量</p><p>import_tasks 绑定 when 的时候，会将 when 的条件一对一的应用到任务文件中列出的所有任务</p><p>include_tasks 绑定 when 的时候，会将 when 的条件仅应用到任务文件。即只要条件为真，任务文件里的所有任务都会执行。</p><p>关于新版写法，绑定 tags 的方式，和旧版差异比较大，例如</p><h4 id="include-tasks">include_tasks</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: test      include_tasks:         file: add.yml        apply:          tags:            - add      tags:        - always        # add.yml- debug:    msg: &quot;&#123;&#123; item &#125;&#125; is ok&quot;  loop: [1,2,3]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> include </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo☞本地搜索</title>
      <link href="posts/f5c218d/"/>
      <url>posts/f5c218d/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>安装完 hexo-generator-search 后，发现搜索结果始终是所有文章.</p><p>如果你用的也是 <a href="https://github.com/Molunerfinn/hexo-theme-melody">Melody</a> 主题，那么可以参考如下信息，来确认。</p><h2 id="效果图">效果图</h2><p><img src="/posts/f5c218d/image-20200518121421437.png" alt="image-20200518121421437"></p><h2 id="软件包">软件包</h2><blockquote><p>截至：2020.05.18，软件包如下</p></blockquote><pre><code class="language-bash">  &quot;dependencies&quot;: &#123;    &quot;gitalk&quot;: &quot;^1.6.2&quot;,    &quot;hexo&quot;: &quot;^4.0.0&quot;,    &quot;hexo-asset-image&quot;: &quot;^1.0.0&quot;,    &quot;hexo-deployer-git&quot;: &quot;^2.1.0&quot;,    &quot;hexo-generator-archive&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-category&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-index&quot;: &quot;^1.0.0&quot;,    &quot;hexo-generator-search&quot;: &quot;^2.4.0&quot;,    &quot;hexo-generator-tag&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-ejs&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-marked&quot;: &quot;^2.0.0&quot;,    &quot;hexo-renderer-pug&quot;: &quot;^1.0.0&quot;,    &quot;hexo-renderer-stylus&quot;: &quot;^1.1.0&quot;,    &quot;hexo-server&quot;: &quot;^1.0.0&quot;,    &quot;react&quot;: &quot;^15.3.1&quot;,    &quot;react-dom&quot;: &quot;^15.3.1&quot;  &#125;</code></pre><h2 id="config-yml">_config.yml</h2><blockquote><p>追加内容如下</p></blockquote><pre><code class="language-yaml">search:  path: search.xml  field: post  content: true</code></pre><h2 id="主题配置">主题配置</h2><blockquote><p>修改内容</p></blockquote><pre><code class="language-yaml">local_search:  enable: true</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞安装</title>
      <link href="posts/6209085/"/>
      <url>posts/6209085/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>此脚本用于安装 nginx;tengine;openresty. 安装版本为：</p><ul><li>nginx: 1.14</li><li>openresty: 1.15.8.3</li><li>tengine: 2.1.2 # 这是一个很古老的版本…</li></ul><h4 id="目录结构">目录结构</h4><p>因为是编译安装，所以产出目录均在 /usr/local/&lt;nginx/openresty/tengine&gt;，除了 logs 做了软链<code> /usr/local/xxx/logs -&gt; /export/logs/nginx</code></p><p><code>/usr/local/xxx/conf 目录结构</code></p><p><img src="/posts/6209085/image-20200515120037239.png" alt="image-20200515120037239"></p><pre><code class="language-bash"># 下面两个主配置文件会告诉你，相应的上下文配置，应该以什么结尾！！！include /usr/local/$&#123;NginxVer&#125;/nginx/conf/server/*.server;include /usr/local/$&#123;NginxVer&#125;/nginx/conf/upstream/*.upstream;</code></pre><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bashbasedir=/usr/local/srccd $basedirrunuser=`whoami`[[ $runuser == 'root' ]] || &#123;     echo &quot;ERROR:执行用户不是$runuser&quot; &amp;&amp; exit &#125;[[ -d /export/logs/nginx ]] || &#123;     echo &quot;/export/logs/nginx/目录不存在&quot; &amp;&amp; exit &#125;CpuNum=`cat /proc/cpuinfo | grep processor | wc -l`read -p &quot;输入安装的Nginx版本:(nginx;tengine;openresty):&quot; NginxVerread -p &quot;输入开发日常操作用户:&quot; KaifaUserread -p &quot;输入nginx worker用户:&quot; NginxWorkerUseruseradd -s /sbin/nologin $&#123;NginxWorkerUser&#125;usermod -a -G $&#123;KaifaUser&#125; $&#123;NginxWorkerUser&#125;cd /usr/local/srcrm -rf $&#123;NginxVer&#125; &amp;&amp; mkdir $&#123;NginxVer&#125;cat&gt;&gt;$basedir/test.com.server&lt;&lt;EOFserver &#123;    listen 80;    server_name test.com;    root /export/$&#123;NginxWorkerUser&#125;/test.com;    #charset koi8-r;    access_log logs/nginx-test.com.access.log main;    error_log logs/nginx-test.com.error.log;    # 关闭日志    location = /favicon.ico &#123;        log_not_found off;        access_log off;    &#125;    # 关闭日志    location = /robots.txt &#123;        auth_basic off;        allow all;        log_not_found off;        access_log off;    &#125;    # 拒绝探测网站根下的隐藏文件 Deny all attempts to access hidden files such as .htaccess, .htpasswd, .DS_Store (Mac).    location ~ /\. &#123;        deny all;        access_log off;        log_not_found off;    &#125;    location / &#123;        #######这个是一个thinkphp框架的伪静态规则，请忽略        if (!-e \$request_filename) &#123;           rewrite ^(.*)\$ /index.php?s=\$1 last;           break;        &#125;        #######        index index.php;    &#125;    #开启浏览器静态文件缓存    location ~ .*\.(html|htm|gif|jpg|jpeg|bmp|png|ico|txt|js|css)\$ &#123;         expires 3h;     &#125;     # 若php-fpm,请保留这里修改    location ~ \.php &#123;        fastcgi_pass 127.0.0.1：9000;        fastcgi_index index.php;        include fastcgi.conf;        fastcgi_connect_timeout 10s;        fastcgi_send_timeout 10s;        fastcgi_read_timeout 10s;        fastcgi_buffers 8 256k;                                   fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;    # 若 http，请保留这里修改    location / &#123;        proxy_pass http://127.0.0.1:8080;        proxy_connect_timeout 300ms;        proxy_send_timeout 300ms;        proxy_read_timeout 300ms;        proxy_max_temp_file_size 1024m;        proxy_set_header   Host         $host;        proxy_set_header   X-Real-IP    $remote_addr;        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;        proxy_buffers 256 4k;        proxy_intercept_errors on;    &#125;&#125;EOFcat&gt;&gt;nginx_status.server&lt;&lt;EOFserver &#123;    listen 80;    server_name 127.0.0.1;   # charset koi8-r;    access_log off;    location /server_status &#123;        stub_status on;        access_log off;        allow 127.0.0.1;        deny all;    &#125;&#125;EOF###################if [[ $NginxVer == 'nginx' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc    wget http://$&#123;NginxVer&#125;.org/download/$&#123;NginxVer&#125;-1.14.0.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-http_v2_module || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125; &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;        deny all;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/conf/upstream/*.upstream;&#125;EOFelif [[ $NginxVer == 'openresty' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc    wget https://openresty.org/download/openresty-1.15.8.3.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-http_v2_module || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125;/nginx &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/nginx/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;            return 444;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/nginx/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/nginx/conf/upstream/*.upstream;&#125;EOFelif [[ $NginxVer == 'tengine' ]];then    [[ -d /usr/local/$NginxVer ]] &amp;&amp; echo '/usr/local/$NginxVer 已存在' &amp;&amp; exit    yum install readline-devel pcre-devel openssl-devel gcc jemalloc-devel    wget http://tengine.taobao.org/download/tengine-2.1.2.tar.gz -O $&#123;NginxVer&#125;.tar.gz    tar xf $&#123;NginxVer&#125;.tar.gz --strip-components 1 -C $&#123;NginxVer&#125;    cd $&#123;NginxVer&#125; &amp;&amp; ./configure --prefix=/usr/local/$&#123;NginxVer&#125; --user=$&#123;NginxWorkerUser&#125; --group=$&#123;NginxWorkerUser&#125; --with-http_stub_status_module --with-http_ssl_module --with-http_gzip_static_module --with-pcre --with-jemalloc || exit    make    make install    cd /usr/local/$&#123;NginxVer&#125; &amp;&amp; rm -rf logs    ln -s /export/logs/nginx logs    cd /usr/local/$&#123;NginxVer&#125;/conf    mkdir &#123;location,ssl,upstream,server&#125;    mv $basedir/&#123;test.com.server,nginx_status.server&#125; server/    rm -rf nginx.conf    cat &gt;&gt;nginx.conf&lt;&lt;EOFuser $&#123;NginxWorkerUser&#125;;worker_processes auto;worker_rlimit_nofile 65535;events &#123;    use epoll;    worker_connections 65535; &#125;error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;http &#123;    include mime.types;    default_type application/octet-stream;    log_format main '\$remote_addr - \$remote_user [\$time_local] \$request_time \$host &quot;\$request&quot; '                      '\$status \$body_bytes_sent &quot;\$http_referer&quot; '                      '&quot;\$http_user_agent&quot; &quot;\$http_x_forwarded_for&quot; \$upstream_addr \$upstream_status';    access_log logs/access.log main;    sendfile on;    keepalive_timeout 65;    gzip on;    gzip_min_length 1k;    gzip_buffers 4 16k;    gzip_comp_level 2;    gzip_types text/plain application/x-javascript text/css text/javascript application/xml application/ms* application/vnd* application/postscript application/javascript application/json application/x-httpd-php application/x-httpd-fastphp;    gzip_vary off;    gzip_disable &quot;MSIE [1-6]\.&quot;;    #跨域访问    #add_header Access-Control-Allow-Origin *;     #add_header Access-Control-Allow-Headers X-Requested-With;    #add_header Access-Control-Allow-Methods GET,POST,OPTIONS;    server &#123;        listen 80 backlog=8092;        location / &#123;            deny all;        &#125;        error_page 500 502 503 504 /50x.html;        location = /50x.html &#123;            root html;        &#125;    &#125;    include /usr/local/$&#123;NginxVer&#125;/conf/server/*.server;    include /usr/local/$&#123;NginxVer&#125;/conf/upstream/*.upstream;&#125;EOFfi</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo☞图片显示404错误的解决办法</title>
      <link href="posts/662e9d4d/"/>
      <url>posts/662e9d4d/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>写作工具：typora</p><p>部署端：github</p><p>部署包：</p><pre><code># 你可以通过hexo的根目录下的package.json来确认版本&quot;hexo&quot;: &quot;^4.0.0&quot;,&quot;hexo-asset-image&quot;: &quot;^1.0.0&quot;</code></pre><p>目前网上大多数的博文描述的场景，均不是当前场景（截至到2020.05.14），所以博文里虽然展示都正常，但是按照博文的操作却会有路径问题，具体表现是图片前多了一级路径（路径应该是1级域名，<a href="http://xn--bvs393b.com">比如.com</a>，.io等，根据你的域名来定）</p><p>那么，请按照下面我的步骤来操作，如果还有问题，那就不是上述我所说的情况了。</p><h4 id="流程">流程</h4><ol><li><p>修改 hexo-asset-image，修改内容如图所示</p><p><img src="/posts/662e9d4d/image-20200901195338475.png" alt="image-20200901195338475">修改typora的图片存放路径，修改内容如图所示</p><p><img src="/posts/662e9d4d/image-20200514185631903.png" alt="image-20200514185631903"></p><blockquote><p>这不是必须的，但是我想没人会拒绝方便的操作。 typora 在进行如上操作后，就可以在你往文章里粘贴图片的时候，自动生成以文件名前缀命名的目录（效果就如同你开启了hexo的post_asset_folder: true参数），并将图片存放在此目录中。</p></blockquote></li><li><p>开启 hexo 的 _config.yml 中 post_asset_folder: true 参数配置</p></li></ol><h4 id="结论">结论</h4><p>经过上述操作，我想你已经可以在本地 md 文件和线上同时看到图片了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞日志切割脚本</title>
      <link href="posts/445cf088/"/>
      <url>posts/445cf088/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>本脚本用于将 nginx 日志进行时间周期切割，并 lzo 压缩，最终上传到 s3。<br>脚本分为三个函数，切割函数，压缩上传函数，删除函数，需要执行哪个，就填写相对应变量。<br>详情可以看脚本注释。</p><blockquote><p>请务必执行前，确认安装了 lzop 和 jq 命令 ，且机器是 aws EC2</p></blockquote><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bash# by zyh# time: 2019-12-13# warning: 使用之前 yum install -y lzop jq# crontab (执行时间周期需要和切割时间周期一致) 重要!!!!!!!# */10 * * * * root bash /export/shell/nginxlog2s3/start.sh &gt; /export/shell/nginxlog2s3/start.log 2&gt;&amp;1# 标识日志名前缀localtag=`curl -sq http://169.254.169.254/latest/dynamic/instance-identity/document/ | jq -r .&quot;accountId&quot;,.&quot;availabilityZone&quot;,.&quot;privateIp&quot; | sed 'N;N;s@\n@_@g'`# ----------------------------------人为变量填写开始区域----------------------------------# 切割时间周期，定位切割后日期的初始写入时间（仅适用于连续切割，且不适用于第一次切割）todaytime=$(date -d &quot;-10 mins&quot; +%Y%m%d)todayhour=$(date -d &quot;-10 mins&quot; +%H)todaytimestr=$(date  -d &quot;-10 mins&quot; +%s)# 企业微信机器人wx_api=''# nginx 日志目录 logs 所在路径, 备份日志目录是 logs/logsbak# 例如日志目录是 /usr/local/nginx/logs，则填写 /usr/loca/nginx, 则切割后本地备份路径是 /usr/local/nginx/logs/logsbaknginx_base=# 日志位于S3的根路径，例如 s3://xxx/logs/xxxdays/nginxS3Base=&quot;&quot;# MvLogList=&quot;a.log b.log c.log&quot;  需要切割的日志，这是必须的MvLogList=&quot;&quot;# LzopS3LogList=&quot;a.log b.log c.log&quot; 需要压缩并上传S3的日志，如果你需要执行此步骤# S3目录格式：$&#123;S3Base&#125;/$&#123;日志名&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/ LzopS3LogList=&quot;&quot;# DeleteLocalLog=&quot;a.log b.log c.log&quot; 需要本地设置保留时间的日志，如果你需要执行此步DeleteLocalLog=&quot;&quot;# 本地保存时间deletetime=$(date -d &quot;72 hours ago&quot; +%s)# ----------------------------------人为变量填写结束区域----------------------------------# 日志原始路径nginx_logs=&quot;$&#123;nginx_base&#125;/logs&quot;# 日志位于本地的切割后备份路径backup_logs=&quot;$&#123;nginx_logs&#125;/logsbak&quot;[[ -d $&#123;backup_logs&#125; ]] || mkdir -p $&#123;backup_logs&#125;# nginx pid 文件路径nginx_pid=&quot;$&#123;nginx_logs&#125;/nginx.pid&quot;[[ -f $&#123;nginx_pid&#125; ]] || &#123;  echo &quot;$&#123;nginx_pid&#125; is not exist!!!!&quot; &amp;&amp; exit&#125;mvlog()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  [[ -d $&#123;backup_logs&#125;/$&#123;NginxLogName&#125; ]] || mkdir -p $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  mv $&#123;nginx_logs&#125;/$&#123;NginxLogName&#125; $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125; &amp;&amp; echo &quot;MV: $&#123;nginx_logs&#125;/$&#123;NginxLogName&#125; to $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;&quot;&#125;lzops3log()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  S3Path=$2  cd $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  lzop $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125; &amp;&amp; aws s3 cp $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo $&#123;S3Path&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/ --quiet &amp;&amp; rm -rf $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo &amp;&amp; echo &quot;UPLOAD: $&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo to $&#123;S3Path&#125;/$&#123;todaytime&#125;/$&#123;todayhour&#125;/&quot; || curl &quot;$wx_api&quot; -H 'Content-Type: application/json' -d '&#123;&quot;msgtype&quot;: &quot;markdown&quot;,&quot;markdown&quot;: &#123;&quot;content&quot;: &quot;# `'&quot;$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;todaytime&#125;$&#123;todayhour&#125;_$&#123;todaytimestr&#125;.lzo&quot;'` 日志上传失败!!!!!!&quot;&#125;&#125;'&#125;deletelocallog()&#123;  [[ -z $1 ]] &amp;&amp; continue  NginxLogName=$1  cd $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;  for logname in `ls`;do    if [[ $&#123;deletetime&#125; -ge $&#123;logname##*_&#125; ]];then      rm -rf $&#123;logname&#125; &amp;&amp; echo &quot;DELETE: $&#123;backup_logs&#125;/$&#123;NginxLogName&#125;/$&#123;localtag&#125;_$&#123;NginxLogName&#125;_$&#123;deletetime&#125;&quot;    fi  done&#125;#MVfor i in $&#123;MvLogList&#125;;do  mvlog $&#123;i&#125;done#nginx log reloadkill -USR1 `cat $&#123;nginx_pid&#125;`#lzop and to s3for i in $&#123;LzopS3LogList&#125;;do  lzops3log $&#123;i&#125; $&#123;S3Base&#125;/$&#123;i&#125;done#Deletefor i in $&#123;DeleteLocalLog&#125;;do  deletelocallog $&#123;i&#125;done</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> log </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞安装</title>
      <link href="posts/65ab632a/"/>
      <url>posts/65ab632a/</url>
      
        <content type="html"><![CDATA[<h4 id="docker-安装">docker 安装</h4><p><a href="https://registry.hub.docker.com/_/redis">Docker Hub</a></p><pre><code class="language-bash">redisName=redisversionTag=6dataPath=/export/docker-data-redismkdir -p $&#123;dataPath&#125;redis.conf 请参考官方默认配置文档，默认配置文档地址： https://redis.io/topics/configwget -P $&#123;dataPath&#125; https://raw.githubusercontent.com/redis/redis/$&#123;versionTag&#125;.0/redis.confdocker run --name  $&#123;redisName&#125;_$&#123;versionTag&#125; --mount &quot;type=bind, src=$&#123;dataPath&#125;, dst=/data&quot; -p 6379:6379 --restart always -d redis:$&#123;versionTag&#125; redis-server /data/redis.conf</code></pre><h4 id="编译安装">编译安装</h4><pre><code class="language-shell">#!/bin/bash# by zyh# 2018-06-21# DownUrl: redis源码包# RedisBaseDir： redis安装路径# RedisPort: redis端口# RedisMaxMem：redis内存限制# ZabbixBase: zabbix 根路径# 安装完毕后，会输出# 1. redis信息# 2. zabbix需要额外手动添加的命令， 并在zabbix_server_web里，给机器关联上 &lt;Template Redis Auto Discovert Active mode&gt; 模板# 3. monit需要额外手动添加的配置BaseDir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`DownUrl=http://download.redis.io/releases/redis-stable.tar.gzRedisMaxMem=1gRedisPort=6379RedisBaseDir=/export/redis_$&#123;RedisPort&#125;ZabbixBase=/etc/zabbixif [[ $&#123;ZabbixBase&#125; == '/usr/local/zabbix' ]];then    ZabbixShell=$&#123;ZabbixBase&#125;/shell    ZabbixEtc=$&#123;ZabbixBase&#125;/etc/zabbix_agentd.conf.d/redis.confelse    ZabbixShell=/etc/zabbix/shell    ZabbixEtc=/etc/zabbix/zabbix_agentd.d/redis.conffiRedisBaseName=$&#123;RedisBaseDir##*/&#125;export TOP_PID=$$trap 'exit 1' TERMexit_script()&#123;    kill -s TERM $TOP_PID&#125;yum install gcc-c++ -y[[ -d $&#123;RedisBaseDir&#125; ]] &amp;&amp; echo &quot;$&#123;RedisBaseDir&#125;已存在&quot; &amp;&amp; exit_scriptss -tnalp | grep redis | awk '&#123;print $4&#125;' | awk -F':' '&#123;print $2&#125;' | while read line;do    [[ $line -eq $&#123;RedisPort&#125; ]] &amp;&amp; echo &quot;$&#123;RedisPort&#125;已被占用&quot; &amp;&amp; exit_scriptdonecd $&#123;BaseDir&#125; &amp;&amp; mkdir rediswget $&#123;DownUrl&#125; -O redis.tar.gztar xf redis.tar.gz --strip-components 1 -C rediscd redismake PREFIX=$&#123;RedisBaseDir&#125; installmkdir $&#123;RedisBaseDir&#125;/&#123;etc,data,logs&#125;cat&gt;$&#123;RedisBaseDir&#125;/etc/redis.conf &lt;&lt;EOFbind 0.0.0.0protected-mode yesport $&#123;RedisPort&#125;tcp-backlog 511timeout 0tcp-keepalive 300daemonize yessupervised nopidfile $&#123;RedisBaseDir&#125;/redis.pidloglevel warninglogfile &quot;$&#123;RedisBaseDir&#125;/logs/redis.log&quot;databases 16always-show-logo yessave 900 1save 300 10save 60 10000stop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesdbfilename redis_$&#123;RedisPort&#125;.rdbdir $&#123;RedisBaseDir&#125;/data/slave-serve-stale-data yesslave-read-only yesrepl-diskless-sync norepl-diskless-sync-delay 5repl-disable-tcp-nodelay noslave-priority 100rename-command FLUSHDB GOD_FLUSHDBrename-command FLUSHALL GOD_FLUSHALLrename-command CONFIG GOD_CONFIGrename-command KEYS GOD_KEYSmaxmemory $&#123;RedisMaxMem&#125;maxmemory-policy allkeys-lrulazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noslave-lazy-flush noappendonly noappendfilename &quot;appendonly.aof&quot;appendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yesaof-use-rdb-preamble nolua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128latency-monitor-threshold 0notify-keyspace-events &quot;&quot;hash-max-ziplist-entries 512hash-max-ziplist-value 64list-max-ziplist-size -2list-compress-depth 0set-max-intset-entries 512zset-max-ziplist-entries 128zset-max-ziplist-value 64hll-sparse-max-bytes 3000activerehashing yesclient-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60hz 10aof-rewrite-incremental-fsync yesEOFcat&gt;$&#123;RedisBaseDir&#125;/redis.sh &lt;&lt; EOF#!/bin/bash# Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.BASEDIR=$&#123;RedisBaseDir&#125;REDISPORT=$&#123;RedisPort&#125;EXEC=\$BASEDIR/bin/redis-serverCLIEXEC=\$BASEDIR/bin/redis-cliPIDFILE=\$BASEDIR/redis.pidCONF=&quot;\$BASEDIR/etc/redis.conf&quot;case &quot;\$1&quot; in    start)        [[ -f \$PIDFILE ]] &amp;&amp; kill -0 \`cat \$PIDFILE\` 2&gt;&gt;\$BASEDIR/crash.log &amp;&amp; echo &quot;\$PIDFILE exists, process is already running or crashed&quot; || &#123;                echo &quot;Starting Redis server...&quot;                \$EXEC \$CONF        &#125;        ;;    stop)        if [ ! -f \$PIDFILE ]        then                echo &quot;\$PIDFILE does not exist, process is not running&quot;        else                PID=\$(cat \$PIDFILE)                echo &quot;Stopping ...&quot;                \$CLIEXEC -p \$REDISPORT shutdown                while [ -x /proc/\$&#123;PID&#125; ]                do                    echo &quot;Waiting for Redis to shutdown ...&quot;                    sleep 1                done                echo &quot;Redis stopped&quot;        fi        ;;    *)        echo &quot;Please use start or stop as first argument&quot;        ;;esacEOFchmod u+x $&#123;RedisBaseDir&#125;/redis.sh#修改内核参数grep -q net.core.somaxconn /etc/sysctl.conf || echo &quot;net.core.somaxconn = 511&quot; &gt;&gt; /etc/sysctl.confgrep -q vm.overcommit_memory /etc/sysctl.conf || &#123;    echo &quot;vm.overcommit_memory = 1&quot; &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -p&#125;grep -q '/sys/kernel/mm/transparent_hugepage/enabled' /etc/rc.local || &#123;    echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled    echo 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' &gt;&gt; /etc/rc.local&#125;#修改zabbix监控cat&gt;$&#123;ZabbixEtc&#125;&lt;&lt;EOF##redis monitorUserParameter=redis_info[*],$&#123;ZabbixShell&#125;/redis_info.sh \$1 \$2 \$3 \$4UserParameter=ip_port_discovery[*],$&#123;ZabbixShell&#125;/ip_port_discovery.sh \$1EOFmkdir $&#123;ZabbixShell&#125;cat&gt;$&#123;ZabbixShell&#125;/redis_info.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashREDISPATH=&quot;/export/redis/bin/redis-cli&quot;HOST=$1PORT=$2REDIS_INFO=&quot;$REDISPATH -h $HOST -p $PORT info&quot;[[ $# -ne 3 ]] &amp;&amp; [[ $# -ne 4 ]] &amp;&amp; &#123; exit&#125;if [[ $# -eq 3 ]];thencase $3 inexist) result=`$REDISPATH -h $HOST -p $PORT ping 2&gt;/dev/null |grep -c PONG` echo $result;;cluster)        result=`$REDIS_INFO|/bin/grep cluster|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;uptime_in_seconds)        result=`$REDIS_INFO|/bin/grep uptime_in_seconds|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;connected_clients)        result=`$REDIS_INFO|/bin/grep connected_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_longest_output_list)        result=`$REDIS_INFO|/bin/grep client_longest_output_list|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_biggest_input_buf)        result=`$REDIS_INFO|/bin/grep client_biggest_input_buf|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;blocked_clients)        result=`$REDIS_INFO|/bin/grep blocked_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#内存maxmemory)        result=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory)        result=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_rss)        result=`$REDIS_INFO|/bin/grep -w used_memory_rss|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk -F'k' '&#123;print $1&#125;'`        echo $result;;used_memory_pct) A=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` B=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` result=`echo $A $B | awk '&#123;printf&quot;%0.2f&quot;,$1/$2&#125;'`        echo $result;;used_memory_peak)        result=`$REDIS_INFO|/bin/grep used_memory_peak|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_lua)        result=`$REDIS_INFO|/bin/grep used_memory_lua|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;mem_fragmentation_ratio)        result=`$REDIS_INFO|/bin/grep mem_fragmentation_ratio|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#rdbrdb_changes_since_last_save)        result=`$REDIS_INFO|/bin/grep rdb_changes_since_last_save|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_bgsave_in_progress)        result=`$REDIS_INFO|/bin/grep rdb_bgsave_in_progress|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_save_time)        result=`$REDIS_INFO|/bin/grep rdb_last_save_time|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_bgsave_status)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_last_bgsave_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;rdb_current_bgsave_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_current_bgsave_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#rdbinfoaof_enabled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_enabled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_scheduled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_scheduled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_last_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_current_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_last_bgrewrite_status)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_bgrewrite_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;#aofinfoaof_current_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_base_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_base_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_rewrite)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_rewrite&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_bio_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_bio_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_delayed_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_delayed_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#statstotal_connections_received)        result=`$REDIS_INFO|/bin/grep -w &quot;total_connections_received&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;total_commands_processed)        result=`$REDIS_INFO|/bin/grep -w &quot;total_commands_processed&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;instantaneous_ops_per_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;instantaneous_ops_per_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;rejected_connections)        result=`$REDIS_INFO|/bin/grep -w &quot;rejected_connections&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;expired_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;expired_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;evicted_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;evicted_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_hits)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_hits&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_misses)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_misses&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_patterns)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_patterns&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;latest_fork_usec)        result=`$REDIS_INFO|/bin/grep -w &quot;latest_fork_usec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;connected_slaves)        result=`$REDIS_INFO|/bin/grep -w &quot;connected_slaves&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;master_link_status)        result=`$REDIS_INFO|/bin/grep -w &quot;master_link_status&quot;|awk -F':' '&#123;print $2&#125;'|/bin/grep -c up`        echo $result;;master_last_io_seconds_ago)        result=`$REDIS_INFO|/bin/grep -w &quot;master_last_io_seconds_ago&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;master_sync_in_progress)        result=`$REDIS_INFO|/bin/grep -w &quot;master_sync_in_progress&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;slave_priority)        result=`$REDIS_INFO|/bin/grep -w &quot;slave_priority&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;#cpuused_cpu_sys)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_sys_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;*) echo &quot;argu error&quot;;;esac#db0:key   elif [[ $# -eq 4 ]];thencase $4 inkeys)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;keys&quot; | awk -F'=|,' '&#123;print $2&#125;'`        echo $result;;expires)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;expires&quot; | awk -F'=|,' '&#123;print $4&#125;'`        echo $result;;avg_ttl)        result=`$REDIS_INFO|/bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;avg_ttl&quot; | awk -F'=|,' '&#123;print $6&#125;'`        echo $result;;*)     echo &quot;argu error&quot; ;;esacfiEOFcat&gt;$&#123;ZabbixShell&#125;/ip_port_discovery.sh&lt;&lt;&quot;EOF&quot;#!/bin/bash#$1是要发现的进程部分词汇portarray=(`sudo ss -tnlp|egrep -i &quot;$1&quot;|awk &#123;'print $4'&#125;|awk -F':' '&#123;if ($NF~/^[0-9]*$/) print $NF&#125;'|sort|uniq`)iparray=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`length=$&#123;#portarray[@]&#125;printf &quot;&#123;\n&quot;printf '\t'&quot;\&quot;data\&quot;:[&quot;for ((i=0;i&lt;$length;i++))  do     printf '\n\t\t&#123;'     printf &quot;\&quot;&#123;#IP&#125;\&quot;:\&quot;$&#123;iparray&#125;\&quot;,\&quot;&#123;#TCP_PORT&#125;\&quot;:\&quot;$&#123;portarray[$i]&#125;\&quot;&#125;&quot;     if [ $i -lt $[$length-1] ];then                printf ','     fi  doneprintf &quot;\n\t]\n&quot;printf &quot;&#125;\n&quot;EOFchmod a+x $&#123;ZabbixShell&#125;/redis_info.shchmod a+x $&#123;ZabbixShell&#125;/ip_port_discovery.shsed -i '2s#/export/redis#'&quot;$&#123;RedisBaseDir&#125;&quot;'#' $&#123;ZabbixShell&#125;/redis_info.shecho '--------------------------------------------------我是 redis 信息-----------------------------------------------------------'echo 'redis相关信息如下：'echo &quot;根路径：$&#123;RedisBaseDir&#125;启动脚本：$&#123;RedisBaseDir&#125;/redis.sh&quot;echo '--------------------------------------------------我是 zabbix 监控信息----------------------------------------------------------'echo '请先安装 zabbix.'echo 'zabbix监控因使用了ss命令，故而需要开启sudo相关信息'echo &quot;#zabbix用户可以以sudo执行sszabbix ALL = NOPASSWD: /usr/sbin/ss#zabbix用户使用sudo无需ttyDefaults:zabbix    !requiretty&quot;echo '---------------------------------------------------我是 monit 监控信息----------------------------------------------------------'echo '请先安装 monit.'echo 'monit配置文件如下:'echo &quot;check process $&#123;RedisBaseName&#125; with pidfile $&#123;RedisBaseDir&#125;/redis.pid  start program = \&quot;$&#123;RedisBaseDir&#125;/redis.sh start\&quot;  stop program = \&quot;$&#123;RedisBaseDir&#125;/redis.sh stop\&quot;if changed pid then alert&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix ☞ nginx</title>
      <link href="posts/b35b5965/"/>
      <url>posts/b35b5965/</url>
      
        <content type="html"><![CDATA[<blockquote><p>确保相关目录地址如下：</p><p>/etc/zabbix/shell</p><p>/etc/zabbix/zabbix_agentd.d</p><p>/usr/local/nginx/conf/server</p></blockquote><ul><li>1，nginx增加配置 server_status.server</li></ul><pre><code class="language-bash:">cat &gt; /usr/local/nginx/conf/server/server_status.server &lt;&lt; 'EOF'server &#123;    listen       80;    server_name  127.0.0.1;    #charset koi8-r;    access_log  off;    location /server_status &#123;        stub_status on;        access_log off;        allow 127.0.0.1;        deny all;    &#125;&#125;EOF/usr/local/nginx/sbin/nginx -t &amp;&amp; /usr/local/nginx/sbin/nginx -s reload</code></pre><hr><ul><li>2，<a href="http://xn--nginx-e86hk14jmnl025b.sh">添加脚本nginx.sh</a>  (确保a+x权限)</li></ul><pre><code class="language-bash:">mkdir /etc/zabbix/shell -p;cat &gt; /etc/zabbix/shell/nginx.sh &lt;&lt; 'EOF'#!/bin/bash  function active &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Active' | awk '&#123;print $NF&#125;'&#125;function reading &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Reading' | awk '&#123;print $2&#125;'&#125;function writing &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt;/dev/null| grep 'Writing' | awk '&#123;print $4&#125;'&#125;function waiting &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| grep 'Waiting' | awk '&#123;print $6&#125;'&#125;function accepts &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $1&#125;'&#125;function handled &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $2&#125;'&#125;function requests &#123;/usr/bin/curl &quot;http://127.0.0.1/server_status&quot; 2&gt; /dev/null| awk NR==3 | awk '&#123;print $3&#125;'&#125;function qps &#123;        NGINX_STATUS_URL=&quot;http://127.0.0.1/server_status&quot;        #若是tnginx，则最后应输出d[length(d)-1]        requestold=`/usr/bin/curl -s $&#123;NGINX_STATUS_URL&#125; | /usr/bin/awk '/server accepts handled requests/&#123;getline a;split(a,d);print d[length(d)]&#125;'`        TimeWait=1        sleep $TimeWait        requestnew=`/usr/bin/curl -s $&#123;NGINX_STATUS_URL&#125; | /usr/bin/awk '/server accepts handled requests/&#123;getline a;split(a,d);print d[length(d)]&#125;'`        if [ $requestnew -gt 0 ];then                QPS=`echo &quot;( $requestnew - $requestold ) / $TimeWait&quot; | /usr/bin/bc`        fi        echo $QPS&#125;# Run the requested function  $1EOFchmod a+x /etc/zabbix/shell/nginx.sh</code></pre><hr><ul><li>3，配置zabbix客户端zabbix_agentd.conf</li></ul><pre><code class="language-bash:">cat &gt; /etc/zabbix/zabbix_agentd.d/nginx.conf &lt;&lt; 'EOF'#monitor nginx  UserParameter=nginx.accepts,/etc/zabbix/shell/nginx.sh acceptsUserParameter=nginx.handled,/etc/zabbix/shell/nginx.sh handledUserParameter=nginx.requests,/etc/zabbix/shell/nginx.sh requestsUserParameter=nginx.connections.active,/etc/zabbix/shell/nginx.sh activeUserParameter=nginx.connections.reading,/etc/zabbix/shell/nginx.sh readingUserParameter=nginx.connections.writing,/etc/zabbix/shell/nginx.sh writingUserParameter=nginx.connections.waiting,/etc/zabbix/shell/nginx.sh waitingUserParameter=nginx.connections.qps,/etc/zabbix/shell/nginx.sh qpsEOF</code></pre><hr><ul><li>4，在服务的对应主机上添加模板</li></ul>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞web监控</title>
      <link href="posts/3d20f83e/"/>
      <url>posts/3d20f83e/</url>
      
        <content type="html"><![CDATA[<h2 id="1-构建-zabbix-agentd-端配置">1. 构建 zabbix_agentd 端配置</h2><p>选择一个主机用于发起监控</p><pre><code class="language-bash"># 目录结构[root@ip-10-230-10-105 zabbix]# pwd/etc/zabbix[root@ip-10-230-10-105 zabbix]# tree &#123;etc,shell&#125;etc├── zabbix_agentd.conf├── zabbix_agentd.conf.bak└── zabbix_agentd.conf.d    └── http_status.conf # 我是 zabbix_agentd 数据项配置shell└── web    ├── http_status.py # 我是自动发现脚本 + 数据采集脚本    └── WEB.txt  # 我是自动发现的数据源2 directories, 5 files</code></pre><pre><code class="language-bash"># zabbix_agentd 配置 http_status.confUserParameter=web.site.code[*],/usr/bin/python /etc/zabbix/shell/web/http_status.py web_site_code $1UserParameter=web.site.discovery,/usr/bin/python /etc/zabbix/shell/web/http_status.py web_site_discovery</code></pre><pre><code class="language-bash"># 自动发现规则配置文件 WEB.txt# 一行一个监控地址# get 原样写入# post 模仿get多加一个?https://abc.com??&lt;post_kv&gt;https://abc.com?&lt;get_kv&gt;https://abc.com</code></pre><pre><code class="language-python">#!/usr/bin/env python#encoding=utf-8# python2.7# 自动发现脚本 + 数据采集脚本 http_status.py# 请将我添加 o+x 权限# ConfigParser 模块是需要安装的import urllib2, sys, json, ConfigParser, os a1 = sys.argv[1]def web_site_code(args):    response = None    WEB_TXT = &quot;%s/WEB.txt&quot; % (os.path.dirname(os.path.realpath(__file__)))    with open(WEB_TXT, 'ro') as f:        for line in f.readlines():            line = line.strip()            if args in line:                if &quot;??&quot; in line:                    line = line.split(&quot;??&quot;)                elif &quot;?&quot; in line:                    line = line.split(&quot;?&quot;)                else:                    pass                try:                    try:                        response = urllib2.urlopen(line[0], data=line[1], timeout=5)                        print response.code                    except IndexError:                        response = urllib2.urlopen(line[0], timeout=5)                        print response.code                    finally:                        response = urllib2.urlopen(line, timeout=5)                        print response.code                except urllib2.URLError,e:                    if hasattr(e, 'code'):                        print e.code                    elif hasattr(e, 'reason'):                        print 500                    else:                        print 500                finally:                    if response:                        response.close()                    exit()def web_site_discovery():    Dict = &#123;&quot;data&quot;:[]&#125;    WEB_TXT = &quot;%s/WEB.txt&quot; % (os.path.dirname(os.path.realpath(__file__)))    for line in open(WEB_TXT):        if &quot;??&quot; in line:            line = line.strip('\n').split(&quot;??&quot;)            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line[0]&#125;        elif &quot;?&quot; in line:            line = line.strip('\n').split(&quot;?&quot;)            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line[0]&#125;        else:            line = line.strip('\n')            line = &#123;&quot;&#123;#SITENAME&#125;&quot;:line&#125;        Dict[&quot;data&quot;].append(line)    print json.dumps(Dict, indent=2)if a1 == 'web_site_code':    url = sys.argv[2]    web_site_code(url)elif a1 == 'web_site_discovery':    web_site_discovery()</code></pre><blockquote><p>{ #SITENAME} 自动发现脚本输出的重要变量 servername 地址，将会用于 web 控制台配置</p></blockquote><h2 id="2-构建-web-控制台配置">2. 构建 web 控制台配置</h2><ul><li><p>选择你配置好的zabbix_agentd端的主机所在的 web 控制台配置项，添加一个自动发现规则</p><p><img src="/posts/3d20f83e/image-20210524165435984.png" alt="image-20210524165435984"></p></li><li><p>在自动发现规则里，构建监控项原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20210524165539770.png" alt="image-20210524165539770"></p><p>在自动发现规则里，构建触发器原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20210524165609439.png" alt="image-20210524165609439"></p><blockquote><p>100秒以内，状态为200的次数少于3次告警</p></blockquote></li><li><p>在自动发现规则里，构建图形原型，内容如图：</p><p><img src="/posts/3d20f83e/image-20210524165635429.png" alt="image-20210524165635429"></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞agent和proxy（yum）</title>
      <link href="posts/1587f6e8/"/>
      <url>posts/1587f6e8/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://repo.zabbix.com/zabbix/">https://repo.zabbix.com/zabbix/</a></p></blockquote><h2 id="导入-zabbix-源">导入 zabbix 源</h2><pre><code class="language-bash：">rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm</code></pre><h2 id="agent-安装">agent 安装</h2><pre><code class="language-shell">yum install zabbix-agent -ysystemctl enable zabbix-agent</code></pre><h2 id="agent主动模式配置文件">agent主动模式配置文件</h2><h3 id="变量">变量</h3><pre><code class="language-bash"># 主机名前缀name_prefix=# 自动发现用的元数据host_meta_data=''# server或者proxy地址server_proxy_addr=# 本机ip## 若是 aws： local_ip=`curl -sq http://169.254.169.254/latest/meta-data/local-ipv4`local_ip=</code></pre><pre><code>mv /etc/zabbix/zabbix_agentd.conf /etc/zabbix/zabbix_agentd.conf.bak;cat &gt; /etc/zabbix/zabbix_agentd.conf.xxx &lt;&lt; EOFHostname=$&#123;name_prefix&#125;_$&#123;local_ip&#125;StartAgents=0ServerActive=$&#123;server_proxy_addr&#125;PidFile=/var/run/zabbix/zabbix_agentd.pidLogFile=/var/log/zabbix/zabbix_agentd.log#DebugLevel=4Include=/etc/zabbix/zabbix_agentd.d/*.conf#被监控端到服务器获取监控项的周期RefreshActiveChecks=60#被监控端存储监控信息的空间大小BufferSize=1000MaxLinesPerSecond=200#超时时间Timeout=10#自动发现用的元信息HostMetadata=$&#123;host_meta_data&#125;EOFvi  /etc/zabbix/zabbix_agentd.confsystemctl start zabbix-agent</code></pre><blockquote><p>如果无法被 zabbix_server 发现，则可能是上述生成的配置有特殊字符。</p></blockquote><h2 id="proxy-安装命令">proxy 安装命令</h2><pre><code class="language-bash:">yum install zabbix-proxy-mysql -y# 解压数据库文件, 并自行导入zcat /usr/share/doc/zabbix-proxy-mysql-*/schema.sql.gz &gt; schema.sql</code></pre><h2 id="proxy-配置">proxy 配置</h2><pre><code class="language-bash">mv /etc/zabbix/zabbix_proxy.conf /etc/zabbix/zabbix_proxy.conf.bak;cat &gt; /etc/zabbix/zabbix_proxy.conf  &lt;&lt; 'EOF'Server=ServerPort=10051Hostname=LogFile=/var/log/zabbix/zabbix_proxy.logPidFile=/var/run/zabbix/zabbix_proxy.pidDBHost=DBPort=DBName=DBUser=DBPassword=Timeout=4LogSlowQueries=3000ConfigFrequency=60DataSenderFrequency=60StartDiscoverers=5CacheSize=128MStartDBSyncers=20HistoryCacheSize=256MHistoryIndexCacheSize=32MEOFvi /etc/zabbix/zabbix_proxy.conf</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zabbix☞redis</title>
      <link href="posts/6c093771/"/>
      <url>posts/6c093771/</url>
      
        <content type="html"><![CDATA[<h2 id="zabbix-模板">zabbix 模板</h2><ol><li>自动发现规则</li></ol><p><img src="/posts/6c093771/image-20200520152740815.png" alt="image-20200520152740815"></p><ol start="2"><li><p>过滤器</p><p><img src="/posts/6c093771/image-20200520152922570.png" alt="image-20200520152922570"></p></li><li><p>监控项原型</p><table><thead><tr><th>名称</th><th>键值</th><th>间隔</th><th>历史记录</th><th>趋势</th><th>类型</th></tr></thead><tbody><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; connected_clients[客户端连接数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,connected_clients]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; maxmemory[redis配置的内存上限]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,maxmemory]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; mem_fragmentation_ratio[内存碎片率]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; redis_instantaneous_ops_per_sec[每秒执行的命令个数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,instantaneous_ops_per_sec]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; redis 存活状态</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,exist]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; rejected_connections[被拒绝的客户端连接数]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,rejected_connections]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_sys[redis-master sys-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_sys]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_sys_children[redis-children sys-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_sys_children]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_user[redis-master user-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_user]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_cpu_user_children[redis-children user-cpu占用]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_cpu_user_children]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory[redis层面已使用内存-不含碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_pct[操作系统层面已使用内存百分比]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_peak[操作系统层面已使用内存历史峰值-含内存碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_peak]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125; used_memory_rss[操作系统层面已使用内存-含内存碎片]</code></td><td><code>redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_rss]</code></td><td>30s</td><td>30d</td><td>90d</td><td>Zabbix客户端(主动式)</td></tr></tbody></table></li><li><p>触发器原型</p><table><thead><tr><th>严重性</th><th>名称</th><th>表达式</th></tr></thead><tbody><tr><td>一般严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  内存碎片化超过50%, 剩余可用内存低于30%</code></td><td><code>&#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio].last()&#125;&gt;1.5  and &#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  可用内存低, 存在使用交换分区</code></td><td><code>&#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,mem_fragmentation_ratio].last()&#125;&lt;1  and &#123;Template Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>一般严重</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis  操作系统层面内存占用百分比过高</code></td><td><code>&#123;Template  Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,used_memory_pct].last()&#125;&gt;0.7</code></td></tr><tr><td>灾难</td><td><code>&#123;#IP&#125;:&#123;#TCP_PORT&#125; Redis 端口无法访问</code></td><td><code>&#123;Template  Redis Auto Discovert Active  mode:redis_info[&#123;#IP&#125;,&#123;#TCP_PORT&#125;,exist].last()&#125;&lt;&gt;1</code></td></tr></tbody></table></li><li><p>图形原型，就不详细写了，这里只列出我自己的分类</p><table><thead><tr><th>名称</th><th>宽</th><th>高</th><th>图形类别</th></tr></thead><tbody><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis 连接数监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis 其他监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis qps 监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis mem 监控</code></td><td>900</td><td>200</td><td>正常</td></tr><tr><td><code>&#123;#IP&#125;,&#123;#TCP_PORT&#125;  redis cpu 监控</code></td><td>900</td><td>200</td><td>正常</td></tr></tbody></table></li></ol><h2 id="脚本">脚本</h2><p>脚本会生成自动发现进程脚本和redis检测脚本</p><p>zabbix的redis配置路径：ZabbixEtc</p><p>zabbix的脚本路径：ZabbixShell</p><p>redis的cli命令路径：RedisCli</p><p>自动发现脚本：ip_port_discovery.sh</p><ul><li>bash ip_port_discovery.sh 进程名或者端口</li></ul><p>redis检测脚本：redis_info.sh</p><ul><li>bash redis_info.sh ip port item</li></ul><pre><code class="language-redis">ZabbixEtc=/etc/zabbix/zabbix_agentd.dZabbixShell=/etc/zabbix/shellRedisCli='docker exec -t redis redis-cli'[[ -d $&#123;ZabbixShell&#125; ]] || mkdir -p $&#123;ZabbixShell&#125;[[ -z $&#123;ZabbixEtc&#125; ]] &amp;&amp; [[ -z $&#123;ZabbixShell&#125; ]] || [[ -z $&#123;RedisCli&#125; ]] &amp;&amp; exitcat&gt;$&#123;ZabbixEtc&#125;/redis.conf&lt;&lt;EOF##redis monitorUserParameter=redis_info[*],$&#123;ZabbixShell&#125;/redis_info.sh \$1 \$2 \$3 \$4UserParameter=ip_port_discovery[*],$&#123;ZabbixShell&#125;/ip_port_discovery.sh \$1EOFcat&gt;$&#123;ZabbixShell&#125;/redis_info.sh&lt;&lt;&quot;EOF&quot;#!/bin/bashRedisCli=HOST=$1PORT=$2REDIS_INFO=&quot;$RedisCli -h $HOST -p $PORT info&quot;[[ $# -ne 3 ]] &amp;&amp; [[ $# -ne 4 ]] &amp;&amp; &#123; exit&#125;if [[ $# -eq 3 ]];thencase $3 inexist) result=`$RedisCli -h $HOST -p $PORT ping 2&gt;/dev/null |grep -c PONG` echo $result;;cluster)        result=`$REDIS_INFO|/bin/grep cluster|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;uptime_in_seconds)        result=`$REDIS_INFO|/bin/grep uptime_in_seconds|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;connected_clients)        result=`$REDIS_INFO|/bin/grep connected_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_longest_output_list)        result=`$REDIS_INFO|/bin/grep client_longest_output_list|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;client_biggest_input_buf)        result=`$REDIS_INFO|/bin/grep client_biggest_input_buf|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;blocked_clients)        result=`$REDIS_INFO|/bin/grep blocked_clients|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#内存maxmemory)        result=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory)        result=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_rss)        result=`$REDIS_INFO|/bin/grep -w used_memory_rss|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk -F'k' '&#123;print $1&#125;'`        echo $result;;used_memory_pct) A=`$REDIS_INFO|/bin/grep used_memory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` B=`$REDIS_INFO|/bin/grep maxmemory|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'` result=`echo $A $B | awk '&#123;printf&quot;%0.2f&quot;,$1/$2&#125;'`        echo $result;;used_memory_peak)        result=`$REDIS_INFO|/bin/grep used_memory_peak|awk -F&quot;:&quot; '&#123;print $NF&#125;'|awk 'NR==1'`        echo $result;;used_memory_lua)        result=`$REDIS_INFO|/bin/grep used_memory_lua|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;mem_fragmentation_ratio)        result=`$REDIS_INFO|/bin/grep mem_fragmentation_ratio|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;#rdbrdb_changes_since_last_save)        result=`$REDIS_INFO|/bin/grep rdb_changes_since_last_save|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_bgsave_in_progress)        result=`$REDIS_INFO|/bin/grep rdb_bgsave_in_progress|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_save_time)        result=`$REDIS_INFO|/bin/grep rdb_last_save_time|awk -F&quot;:&quot; '&#123;print $NF&#125;'`        echo $result;;rdb_last_bgsave_status)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_last_bgsave_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;rdb_current_bgsave_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;rdb_current_bgsave_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#rdbinfoaof_enabled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_enabled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_scheduled)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_scheduled&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_last_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_current_rewrite_time_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_rewrite_time_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result            ;;aof_last_bgrewrite_status)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_last_bgrewrite_status&quot; | awk -F':' '&#123;print $2&#125;' | /bin/grep -c ok`        echo $result;;#aofinfoaof_current_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_current_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_base_size)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_base_size&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_rewrite)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_rewrite&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_rewrite_buffer_length)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_rewrite_buffer_length&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_pending_bio_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_pending_bio_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;aof_delayed_fsync)        result=`$REDIS_INFO|/bin/grep -w &quot;aof_delayed_fsync&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;#statstotal_connections_received)        result=`$REDIS_INFO|/bin/grep -w &quot;total_connections_received&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;total_commands_processed)        result=`$REDIS_INFO|/bin/grep -w &quot;total_commands_processed&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;instantaneous_ops_per_sec)        result=`$REDIS_INFO|/bin/grep -w &quot;instantaneous_ops_per_sec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;rejected_connections)        result=`$REDIS_INFO|/bin/grep -w &quot;rejected_connections&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;expired_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;expired_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;evicted_keys)        result=`$REDIS_INFO|/bin/grep -w &quot;evicted_keys&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_hits)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_hits&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;keyspace_misses)        result=`$REDIS_INFO|/bin/grep -w &quot;keyspace_misses&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_channels)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_channels&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;pubsub_patterns)        result=`$REDIS_INFO|/bin/grep -w &quot;pubsub_patterns&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;latest_fork_usec)        result=`$REDIS_INFO|/bin/grep -w &quot;latest_fork_usec&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;connected_slaves)        result=`$REDIS_INFO|/bin/grep -w &quot;connected_slaves&quot; | awk -F':' '&#123;print $2&#125;'`        echo $result;;master_link_status)        result=`$REDIS_INFO|/bin/grep -w &quot;master_link_status&quot;|awk -F':' '&#123;print $2&#125;'|/bin/grep -c up`        echo $result;;master_last_io_seconds_ago)        result=`$REDIS_INFO|/bin/grep -w &quot;master_last_io_seconds_ago&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;master_sync_in_progress)        result=`$REDIS_INFO|/bin/grep -w &quot;master_sync_in_progress&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;slave_priority)        result=`$REDIS_INFO|/bin/grep -w &quot;slave_priority&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;#cpuused_cpu_sys)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_sys_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_sys_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;used_cpu_user_children)        result=`$REDIS_INFO|/bin/grep -w &quot;used_cpu_user_children&quot;|awk -F':' '&#123;print $2&#125;'`        echo $result;;*) echo &quot;argu error&quot;;;esac#db0:key   elif [[ $# -eq 4 ]];thencase $4 inkeys)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;keys&quot; | awk -F'=|,' '&#123;print $2&#125;'`        echo $result;;expires)        result=`$REDIS_INFO| /bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;expires&quot; | awk -F'=|,' '&#123;print $4&#125;'`        echo $result;;avg_ttl)        result=`$REDIS_INFO|/bin/grep -w &quot;db0&quot;| /bin/grep -w &quot;$1&quot; | /bin/grep -w &quot;avg_ttl&quot; | awk -F'=|,' '&#123;print $6&#125;'`        echo $result;;*)     echo &quot;argu error&quot; ;;esacfiEOFcat&gt;$&#123;ZabbixShell&#125;/ip_port_discovery.sh&lt;&lt;&quot;EOF&quot;#!/bin/bash#$1是要发现的进程部分词汇portarray=(`sudo ss -tnlp|egrep -i &quot;$1&quot;|awk &#123;'print $4'&#125;|awk -F':' '&#123;if ($NF~/^[0-9]*$/) print $NF&#125;'|sort|uniq`)iparray=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`length=$&#123;#portarray[@]&#125;printf &quot;&#123;\n&quot;printf '\t'&quot;\&quot;data\&quot;:[&quot;for ((i=0;i&lt;$length;i++))  do     printf '\n\t\t&#123;'     printf &quot;\&quot;&#123;#IP&#125;\&quot;:\&quot;$&#123;iparray&#125;\&quot;,\&quot;&#123;#TCP_PORT&#125;\&quot;:\&quot;$&#123;portarray[$i]&#125;\&quot;&#125;&quot;     if [ $i -lt $[$length-1] ];then                printf ','     fi  doneprintf &quot;\n\t]\n&quot;printf &quot;&#125;\n&quot;EOFchmod a+x $&#123;ZabbixShell&#125;/redis_info.shchmod a+x $&#123;ZabbixShell&#125;/ip_port_discovery.shsed -i 's#RedisCli=#RedisCli=\&quot;'&quot;$&#123;RedisCli&#125;&quot;'\&quot;#' $&#123;ZabbixShell&#125;/redis_info.shecho '------------------------------------我是 zabbix 监控信息----------------------------------'echo '编辑 visudo，添加如下信息'echo &quot;#zabbix用户可以以sudo执行sszabbix ALL = NOPASSWD: /usr/sbin/ss#zabbix用户使用sudo无需ttyDefaults:zabbix    !requiretty&quot;echo '若redis运行在docker中，执行如下命令'echo 'usermod -a -G docker zabbix'</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
          <category> zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞logrotate安装</title>
      <link href="posts/421d605e/"/>
      <url>posts/421d605e/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>logrotate 可以帮助我们进行日志切割，搭配 cron 服务，就可以自动的进行轮转</p><h4 id="logrotate-版本更新">logrotate 版本更新</h4><blockquote><p>确保 logrotate 支持小时级别的管理，替换/usr/sbin/logrotate,并附加x权限，我这里有一个二进制版本<a href="D:%5Czyh.cool%5Csource_posts%5C%E6%97%A5%E5%BF%97%5C%E5%85%B6%E5%AE%83%5C%E6%97%A5%E5%BF%97%E2%98%9Elogrotate%E5%AE%89%E8%A3%85%5Clogrotate">logrotate</a></p><p>或者也可以直接去 github 上拉取https://github.com/logrotate/logrotate</p></blockquote><h4 id="添加-logrotate-配置">添加 logrotate 配置</h4><pre><code class="language-bash"># 添加所需切割的日志配置cat &gt; /etc/logrotate.d/nginx &lt;&lt; 'EOF'/usr/local/nginx/logs/access.log &#123;  # 定义日志位置 hourly    # 按照小时切割 rotate 2  # 最多保留两份切割日志 missingok nocompress sharedscripts postrotate  /bin/kill -USR1 `cat /usr/local/nginx/logs/nginx.pid 2&gt;/dev/null` 2&gt;/dev/null || true endscript&#125;EOF</code></pre><h4 id="添加-crontab-配置">添加 crontab 配置</h4><pre><code class="language-bash"># 添加logrotate执行脚本cp /etc/cron.daily/logrotate /etc/cron.hourly/</code></pre><h4 id="重载-crond-服务">重载 crond 服务</h4><pre><code class="language-bash">systemctl reload crond</code></pre>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> logrotate </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 微信预警</title>
      <link href="posts/f9b08c30/"/>
      <url>posts/f9b08c30/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># conf.ini[wechat]corpid = [app]it = &lt;app_agent_id&gt;:&lt;app_secret&gt;[group]it = usera|userb</code></pre><pre><code class="language-python">#!/usr/bin/env python3&quot;&quot;&quot;@author: zyh@contact: aaa103439@hotmail.com@software: vscode@file: sendchat.py@time: 2020/02/05&quot;&quot;&quot;import sys, os, requests, pathlib, json, configparserimport loggingimport logging.handlersfrom datetime import datetimeclass PySendchat():    def __init__(self, corpid, agentid, secret, touser, content):        self.corpid=corpid        self.agentid=agentid        self.secret=secret        self.touser=touser        self.content=content        LOG_PATHDIR=os.path.dirname(os.path.abspath(__file__))        LOG_FILENAME = '&#123;0&#125;/sendchat.log'.format(LOG_PATHDIR)        self.my_logger = logging.getLogger('SendChat')        self.my_logger.setLevel(logging.INFO)        # Add the log message handler to the logger        handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=102400000, backupCount=5)        # create formatter and add it to the handlers        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')        handler.setFormatter(formatter)        self.my_logger.addHandler(handler)    def gettoken(self):        self.my_logger.info('-----------------------------------------------------------------------------------------')        pwd=os.path.dirname(os.path.abspath(__file__))        tokenfile='&#123;0&#125;/wechat.&#123;1&#125;'.format(pwd,self.agentid)        if pathlib.Path(tokenfile).exists():            tokenfilectime=os.path.getctime(tokenfile)            currenttime=datetime.now().timestamp()            dtime=currenttime-tokenfilectime            self.my_logger.info('&#123;0&#125; lived &#123;1&#125;s.'.format(tokenfile, dtime))            if dtime &gt;= 7200:                try:                    os.remove(tokenfile)                    self.my_logger.info('Token file &#123;0&#125;: delete success'.format(tokenfile))                except Exception as e:                    self.my_logger.error('Token file:&#123;0&#125; delete error.Reason:&#123;1&#125;'.format(tokenfile,e))                    exit        # check token file        try:            tokensize = os.path.getsize(tokenfile)        except Exception as e:            self.my_logger.info('Token file is not exist.Reason:&#123;0&#125;'.format(e))            tokensize = 0        # get token from token file        if tokensize != 0:            with open(tokenfile, 'rb') as fd:                token = fd.read() # get token success                self.my_logger.info('Get token from token file.')            jsonObject = json.loads(token.decode(encoding='utf8'))            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token        # get token from weixin api        else:            try:                self.my_logger.info('New Token Create.')                f = requests.get('https://qyapi.weixin.qq.com/cgi-bin/gettoken?corpid=&#123;0&#125;&amp;corpsecret=&#123;1&#125;'.format(self.corpid, self.secret))                token = f.content                self.my_logger.info('Get token from weixin api.')                jsonObject = json.loads(token.decode(encoding='utf8'))                errcode=int(jsonObject.get(&quot;errcode&quot;))                if errcode != 0:                    errmsg=jsonObject.get(&quot;errmsg&quot;)                    self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(errmsg))                    exit()            except Exception as e:                self.my_logger.error('Get token error!Reason:&#123;0&#125;'.format(e))                exit()            try:                self.my_logger.info('Write token to &#123;0&#125;.'.format(tokenfile))                with open(tokenfile, 'wb') as fd:                    fd.write(token)            except Exception as e:                self.my_logger.error('Write &#123;0&#125; error!Reason:&#123;1&#125;'.format(tokenfile,e))                exit()            access_token = jsonObject.get(&quot;access_token&quot;)            return access_token    def sendmsg(self):        accessToken = self.gettoken()        self.my_logger.info('Token:&#123;0&#125;'.format(accessToken))        sendMapDirectroy = &#123;&#125;        sendMapDirectroy[&quot;agentid&quot;] = self.agentid        sendMapDirectroy[&quot;touser&quot;] = self.touser        sendMapDirectroy[&quot;msgtype&quot;] = &quot;text&quot;        sendMapDirectroy[&quot;safe&quot;] = &quot;0&quot;        contentDirectory = &#123;&#125;        sendMapDirectroy[&quot;text&quot;] = contentDirectory        contentDirectory[&quot;content&quot;] = self.content        bodyStr = json.dumps(sendMapDirectroy, ensure_ascii=False).encode(encoding=&quot;utf-8&quot;)        try:            f = requests.post(url=&quot;https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s&quot; % accessToken,                          data=bodyStr, timeout=5)            self.my_logger.info(f.content)        except Exception as e:            self.my_logger.error('Send chat network error!Reason:&#123;0&#125;'.format(e))if __name__ == '__main__':    appname = sys.argv[1]    content = sys.argv[3]    # read conf.ini    conf = configparser.ConfigParser()    conf_path = os.path.dirname(os.path.abspath(__file__))    conf_ini = &quot;&#123;0&#125;/conf.ini&quot;.format(conf_path)    if pathlib.Path(conf_ini).exists():        conf.read(conf_ini)        corpid = conf.get(&quot;wechat&quot;, &quot;corpid&quot;)        appinfo = conf.get(&quot;app&quot;, appname)        agentid = appinfo.split(':')[0]        secret = appinfo.split(':')[1]        groupname = conf.get(&quot;group&quot;, appname)        touser = groupname.split(':')[0]        chatobj = PySendchat(corpid, agentid, secret, touser, content)    else:        print('conf.ini error')        exit()    try:        chatobj.sendmsg()    except:        chatobj.my_logger.error(&quot;Send chat failure!&quot;)</code></pre><p>eg:</p><p>python <a href="http://sendchat.py">sendchat.py</a> it ‘’ &lt;预警内容&gt;</p><blockquote><p>创建好app，并关联用户到app</p><p>执行上述命令，会将预警内容通过&lt;app_agent_id&gt; 应用发送给用户usera和userb</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 企业微信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控☞企业微信机器人</title>
      <link href="posts/8c33f887/"/>
      <url>posts/8c33f887/</url>
      
        <content type="html"><![CDATA[<h4 id="shell">shell</h4><pre><code class="language-bash">wechatUrl=wechatData=curl $&#123;wechatUrl&#125; -H 'Content-Type: application/json' -d '&#123;&quot;msgtype&quot;: &quot;markdown&quot;,&quot;markdown&quot;: &#123;&quot;content&quot;: &quot;`'&quot;$wechatData&quot;'`&quot;&#125;&#125;'</code></pre><h4 id="python">python</h4><pre><code class="language-python">import json, requestswechatData=&#123;&quot;msgtype&quot;: &quot;text&quot;,&quot;text&quot;: &#123;&quot;content&quot;: &quot;&quot;&#125;&#125;wechatData['text']['content']='广州今日天气：29度，大部分多云，降雨概率：60%'wechatData['text']['mentioned_list']=[&quot;zyh&quot;]  # all 代表群组所有人wechatData=json.dumps(wechatData)wechatUrl=requests.post(url=wechatUrl, headers=&#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;, data=wechatData, timeout=5)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 企业微信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 远程磁盘检测</title>
      <link href="posts/1fac36d/"/>
      <url>posts/1fac36d/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">#!/bin/bash # 文件名：disklog.sh # 用途：监视远程系统的磁盘使用情况 BaseDir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $BaseDirlogfile=&quot;disk.log&quot; if [[ -n $1 ]];then     logfile=$1 fishellexecuser=`whoami`if [[ $shellexecuser == root ]];then    rm -rf /root/.ssh/known_hostselse    rm -rf /home/$shellexecuser/.ssh/known_hostsfiprintf &quot;%-8s %-14s %-9s %-8s %-6s %-6s %-6s %s\r\n&quot; &quot;Date&quot; &quot;IP ADDRESS&quot; &quot;Device&quot; &quot;Capacity&quot; &quot;Used&quot; &quot;Free&quot; &quot;Percent&quot; &quot;Status&quot; &gt; $logfile ##################### 手动填写区# 提供远程主机IP地址列表 1.1.1.1 2.2.2.2 3.3.3.3IP_LIST=# 监控阈值(百分比) 只填写数字 1 到 100DiskPct=# 执行用户UserName=''# 执行用户所需私钥, 此文件需要与脚本同级目录PemName=''# 企业微信bot机器人地址wx_api=''##################### 手动填写区for ip in $IP_LIST;do     ssh -i $PemName -o StrictHostKeyChecking=no $&#123;UserName&#125;@$ip 'df -H' | grep ^/dev/ &gt; /tmp/$$.df     while read line;do         cur_date=`date  &quot;+%F_%R&quot;`        printf &quot;%-8s %-14s &quot; $cur_date $ip         echo $line | awk '&#123; printf(&quot;%-9s %-8s %-6s %-6s %-8s&quot;, $1,$2,$3,$4,$5); &#125;'         pusg=$(echo $line | egrep -o &quot;[0-9]+%&quot;)         pusg=$&#123;pusg/\%/&#125;;         if [ $pusg -lt $DiskPct ];then             echo OK        else             echo ALERT         fi     done &lt; /tmp/$$.df     rm -rf /tmp/$$.dfdone &gt;&gt; $&#123;logfile&#125;sed -n '1p' $&#123;logfile&#125; &gt; alert.logawk '$NF == &quot;ALERT&quot;&#123;print $0&#125;' $&#123;logfile&#125; &gt;&gt; alert.log#sed -i '1i &quot;磁盘阈值：'&quot;$DiskPct&quot;'&quot;' alert.logcontent=`cat alert.log`grep -q 'ALERT' alert.log &amp;&amp; &#123;curl &quot;$wx_api&quot;  -H 'Content-Type: application/json'  \-d '   &#123;        &quot;msgtype&quot;: &quot;text&quot;,        &quot;text&quot;: &#123;            &quot;content&quot;: &quot;'&quot;$content&quot;'&quot;        &#125;   &#125;'&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 磁盘 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控 ☞ 邮件预警</title>
      <link href="posts/f8f9b4d0/"/>
      <url>posts/f8f9b4d0/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># $1 收件人# $2 主题# $3 内容smtpServer=  # smtp 服务器地址，例如 smtp.gmail.com:xxxsendUserEmail='it@abc.com'sendUserPassword=  # 一般发件人邮箱密码都是专用密码，并非web密码/usr/local/bin/sendEmail -f $&#123;sendUserEmail&#125; -t $1 -u &quot;$2&quot; -m &quot;$3&quot; -s $&#123;smtpServer&#125; -xu $&#123;sendUserEmail&#125; -xp $&#123;sendUserPassword&#125; -o message-charset=utf-8</code></pre><pre><code class="language-python">#!/usr/bin/python#coding:utf-8# by zyh# python sendmail.py '收件人邮箱地址'  '主题' '邮件内容' '抄送邮箱地址' '附件绝对路径'import smtplibfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.multipart import MIMEMultipartimport sysreload(sys)sys.setdefaultencoding('utf8')mail_host = 'smtp服务器地址'  # 例如 smtp.gmail.com:587mail_user = '发件人邮箱'      # 例如 it@gmail.commail_pass = '发件人密码'def send_mail(*args):    argsNum=len(args[0])    if argsNum == 4:        filename,to_list, subject, content=args[0]    elif argsNum == 5:        filename,to_list, subject, content, cc_list=args[0]    elif argsNum == 6:        filename,to_list, subject, content, cc_list, subfile=args[0]    me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;    #初始化邮件对象 msg    msg = MIMEMultipart()    msg['From'] = me    msg['to'] = to_list    emailList=to_list    # 主题    msg['Subject'] = Header(subject, 'utf-8').encode()    # 内容    msg.attach(MIMEText(content, 'plain', 'utf-8'))    # 抄送    if 'cc_list' in vars():        msg['Cc'] = cc_list        emailstring = to_list+','+cc_list        emailList=emailstring.split(&quot;,&quot;)    # 附件    if 'subfile' in vars():        att1 = MIMEText(open((&quot;%s&quot;%subfile),'rb').read(), 'base64', 'utf8')        att1[&quot;Content-Type&quot;] = 'text/plain'        att1[&quot;Content-Disposition&quot;] = 'attachment; filename='+subfile.split('/')[-1] # 这里的filename可以任意写，写什么名字，邮件中显示什么名字        msg.attach(att1)    # 发送    try:        print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)        print &quot;connect mail server suesscc&quot;        s.starttls()        s.login(mail_user,mail_pass)        print &quot;login mail server suesscc&quot;        s.sendmail(me,emailList,msg.as_string())        s.close()        return True    except Exception,e:        print &quot;%s\t%s&quot;%(to_list,str(e))        return Falseif __name__ == &quot;__main__&quot;:    if len(sys.argv) &lt; 4:        exit()    send_mail(sys.argv)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 监控 </tag>
            
            <tag> 邮件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-过滤器</title>
      <link href="posts/f43dca37/"/>
      <url>posts/f43dca37/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>不管是过滤器，lookup，query，with_xxx，很多都是获取我们想要的信息。</p><h4 id="过滤器">过滤器</h4><blockquote><p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html">https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html</a></p></blockquote><blockquote><p>处理变量值，从而获取想要的信息.</p><p>过滤器本身是 jinja2 或者 ansible 官方定义的</p></blockquote><h4 id="简例">简例</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    vara: abcde    varb: [1,2,3,A,b,C,d]    varc: 123    vard: [ 1,2,3,[4,5,6],4,5 ]  tasks:    - name: show upper      debug:        msg: &quot;&#123;&#123; vara | upper&#125;&#125;&quot;</code></pre><blockquote><p>简例中的 upper 即是过滤器，它可以将 vara 中的所有字母元素大写，最终输出 ABCDE</p></blockquote><h4 id="常用的过滤器">常用的过滤器</h4><pre><code class="language-yaml">#将字符串开头和结尾的空格去除msg: &quot;&#123;&#123; vara | trim &#125;&#125;&quot;#返回字符串或列表长度,length与count等效,可以写为countmsg: &quot;&#123;&#123; varb | length &#125;&#125;&quot;# 绝对值msg: &quot;&#123;&#123; varc | abs &#125;&#125;&quot;# 排序(降序排序)msg: &quot;&#123;&#123; varb | sort(reverse=true) &#125;&#125;&quot;# 将列表中第一层嵌套列表元素展开并入列表中,并取出新列表中的最大元素msg: &quot;&#123;&#123; vard | flatten(levels=1) | max &#125;&#125;&quot;# 随机返回一个元素msg: &quot;&#123;&#123; varb | random &#125;&#125;&quot;# 去重msg: &quot;&#123;&#123; vard | unique &#125;&#125;&quot;# 并集msg: &quot;&#123;&#123; varb | union(vard) &#125;&#125;&quot;# 交集msg: &quot;&#123;&#123; varb | intersect(vard) &#125;&#125;&quot;# 补集，取出存在于 varb，但不存在于 vard 中的元素msg: &quot;&#123;&#123; varb | difference(vard) &#125;&#125;&quot;# 去除两个列表交集后的元素msg: &quot;&#123;&#123; varb | symmetric_difference(vard) &#125;&#125;&quot;# 变量未定义，返回默认值 newmsg: &quot;&#123;&#123; vare | default('new')&#125;&#125;&quot;# 变量未定义或者定义但为空，返回默认值 newmsg: &quot;&#123;&#123; vare | default('new', boolean=true)&#125;&#125;&quot;# 变量未定义时，忽略某个参数file: xxxx  mode=&#123;&#123; vare | default(omit)&#125;&#125;&quot;  # 若 vare 不存在，则忽略mode参数</code></pre><h4 id="json-query">json_query</h4><blockquote><p>获取特定数据</p></blockquote><pre><code>1. 查询字符串可用变量代替，增加可读性 loop: &quot;&#123;&#123; domain_definition | json_query(server_name_cluster1_query) &#125;&#125;&quot; vars:    server_name_cluster1_query: &quot;domain.server[?cluster=='cluster1'].port&quot;</code></pre><ol start="2"><li>查询条件</li></ol><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    users:    - name: zhangsan      gender: male    - name: lisi      gender: female  tasks:    - name: test 1      debug:        msg: &quot;&#123;&#123; users | json_query('[?name==`zhangsan`].gender') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;male&quot;    ]&#125;</code></pre><h4 id="map">map</h4><blockquote><p>映射</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars:    users:    - name: zhangsan      gender: male    - name: lisi      gender: female  tasks:    - name: test 1      debug:        msg: &quot;&#123;&#123; users|map(attribute='name') | list &#125;&#125;&quot;    - name: test 2      debug:        msg: &quot;&#123;&#123; users | json_query('[*].name') &#125;&#125;&quot;</code></pre><pre><code class="language-bash"># test 2 是采用 json_query 方式，test1和test2结果一样ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;zhangsan&quot;,         &quot;lisi&quot;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-文本文件操作</title>
      <link href="posts/4235ddf4/"/>
      <url>posts/4235ddf4/</url>
      
        <content type="html"><![CDATA[<h4 id="var-定义">var 定义</h4><blockquote><p>通过变量，修改playbook</p><p>可直接写入 playbook， 也可以写入文件，然后 playbook 通过 vars_files 引用</p><p>关键词:</p><ul><li>vars</li><li>vars_files # 一次性加载文件内部数据，不支持文件动态修改或添加新变量</li></ul></blockquote><h4 id="简例">简例</h4><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  vars_files: ~/vars.yml  vars:    father: Zhang San  tasks:    - name: test vars      shell: echo &quot;&#123;&#123; father &#125;&#125; - &#123;&#123; children.son_name &#125;&#125; success&quot; &gt;&gt; ~/son.log</code></pre><pre><code class="language-yaml">children:  son_name: Zhang Xiaosan</code></pre><pre><code class="language-bash"># son.log 内容Zhang San - Zhang Xiaosan success</code></pre><h4 id="var-注册">var 注册</h4><blockquote><p>当我们想将某个任务的结果写入一个变量的时候，我们可以用register来进行注册</p><p>关键词:</p><ul><li>register</li><li>debug<ul><li>var 输出变量值</li><li>msg 输出字符串</li></ul></li></ul></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  tasks:    - name: test register      shell: echo &quot;register success&quot; &gt; ~/register.log      register: resultInfo    - name: show register result      debug:        msg: &quot;Oh my god&quot;        var: resultInfo</code></pre><h4 id="var-交互">var 交互</h4><blockquote><p>提供一个用户输入信息的机会，和 shell 里面的 read -p 一致。</p><p>关键词：</p><ul><li>vars_prompt</li></ul></blockquote><pre><code class="language-bash">---- hosts: localhost  remote_user: zyh  vars_prompt:    - name: &quot;fatherName&quot;      prompt: &quot;What's your father name&quot;      default: ZhangSan      private: no  tasks:    - name: show father name      shell: echo &quot;&#123;&#123; fatherName &#125;&#125;&quot; &gt; ~/prompt.log</code></pre><blockquote><p>private yes=隐藏输入内容 no=显示输入内容</p><p>default 默认值</p><p>encrypt “sha512_crypt” 将变量值加密，一般用于传递密码，比如传递给 user 模块的 password 参数</p></blockquote><h4 id="var-命令行传入">var 命令行传入</h4><blockquote><p>一般用于临时强制覆盖playbook中定义好的变量</p><p>关键词：</p><ul><li>-e 或者 --extra-vars<ul><li>参数后面，可以跟随多个变量kv对，每一个kv对用空格隔开</li><li>参数后面，@filePath 可以传入变量文件，文件中的变量均可以被引用</li></ul></li></ul></blockquote><pre><code class="language-bash">ansible-play test.play -e &quot;fatherName=Laowang&quot;</code></pre><h4 id="var-作用域">var 作用域</h4><table><thead><tr><th>创建方式</th><th>调用位置</th><th>作用域</th></tr></thead><tbody><tr><td>vars</td><td>play和tasks</td><td>当前play或者当前tasks，无法跨主机</td></tr><tr><td>set_fact</td><td>tasks</td><td>跨play，默认无法跨主机</td></tr><tr><td>register</td><td>tasks</td><td>跨play，默认无法跨主机</td></tr></tbody></table><p>若要使得 set_fact 和 register 跨主机使用，则需要引入内置变量 <code>hostvars</code> 例如 hostvars.&lt;主机名&gt;.&lt;变量名&gt;</p><blockquote><p>其它内置变量：</p><ul><li><p>ansible_version # 版本</p></li><li><p>hostvars # 存储play中的所有主机变量</p></li><li><p>play_hosts # 存储play中的所有主机名</p></li><li><p>inventory_hostname  # 存储当前主机名</p></li><li><p>inventory_hostname_short  # 存储当前主机名简称（其实就是获取主机名第一级，例如001.localhost，那么获取的就是001）</p></li><li><p>groups # 存储所有分组信息，包括all和ungrouped</p></li><li><p>group_names # 存储当前play中主机的所属组名</p></li><li><p>inventory_dir # 存储主机清单文件所在路径</p></li></ul></blockquote><h4 id="var-动态获取新变量">var 动态获取新变量</h4><blockquote><p>关键词：include_vars</p><p>用于任务重载变量文件，从而获取任务期间变量文件修改的数据</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  vars_files: ~/test.yaml  tasks:    - name: get varb - max      debug:        msg: &quot;&#123;&#123; varb |  max &#125;&#125;&quot;    - name: lineinfile      lineinfile:        regexp: ^varb        line: &quot;varb: [1,2,3,4]&quot;        path: ~/test.yaml    - include_vars: ~/test.yaml    - name: get varb - max again      debug:        msg: &quot;&#123;&#123; varb |  max &#125;&#125;&quot;</code></pre><pre><code class="language-bash">#### test.yaml 变量文件初始内容:vara: 123                                                                 varb: [1,2,3]  #### 最终结果：第一次 get varb 任务输出 3, 第二次 get varb 任务输出 4</code></pre><blockquote><p>include_vars 模块常用参数：</p><ul><li><p>file 读取某个变量文件</p></li><li><p>dir 读取某个目录的所有变量文件</p></li><li><p>depth 递归层深，仅在 dir 启用的时候有意义</p></li><li><p>files_matching 正则匹配文件名，仅在 dir 启用的时候有意义</p></li><li><p>ignore_files 忽略某个列表，列表中的元素可以为正则表达式</p></li><li><p>name: 变量 x  将读取的文件内容集中复制给变量 x，例如上例中变量 x 为 {vara: 123, varb: [1,2,3,4]}</p></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞输出个性化开机状态</title>
      <link href="posts/3988a5f2/"/>
      <url>posts/3988a5f2/</url>
      
        <content type="html"><![CDATA[<h4 id="前沿">前沿</h4><p>觉得默认的登陆不够给力，无法忽悠机器，用wower的话来说，就是先祖忽悠着你</p><h4 id="效果图在此">效果图在此</h4><p><img src="/posts/3988a5f2/image-20200515110044304.png" alt="image-20200515110044304"></p><h4 id="脚本在此">脚本在此</h4><blockquote><p>将脚本放置到 /etc/profile.d/status.sh</p></blockquote><pre><code class="language-bash">#!/bin/bash# Author: zyh# 需先安装 toilet 和 cowsay 命令# yum install epel-release -y# yum install https://rpmfind.net/linux/openmandriva/4.1/repository/x86_64/unsupported/release/toilet-0.2-3-omv4000.x86_64.rpm cowsay -yuser=$USERhome=$HOME## blue to echofunction blue()&#123;    echo -e &quot;\033[34m[Info] $1\033[0m&quot;    &#125;## green to echofunction green()&#123;    echo -e &quot;\033[32m[Success] $1\033[0m&quot;    &#125;## Errorfunction red()&#123;    echo -e &quot;\033[31m\033[01m[Error] $1\033[0m&quot;    &#125;# warningfunction yellow()&#123;    echo -e &quot;\033[33m\033[01m[Warn] $1\033[0m&quot;    &#125;## Error to warning with blinkfunction bred()&#123;    echo -e &quot;\033[31m\033[01m\033[05m[Error] $1\033[0m&quot;    &#125;# Error to warning with blinkfunction byellow()&#123;    echo -e &quot;\033[33m\033[01m\033[05m[Warn] $1\033[0m&quot;    &#125;publicip=`curl -s http://169.254.169.254/latest/meta-data/public-ipv4`localip=`curl -s http://169.254.169.254/latest/meta-data/local-ipv4`echo -e &quot;$publicip $localip&quot; | cowsay -f tux | toilet -f term  --gay# * Check if we're somewhere in /homeif [ ! -d $&#123;home&#125; ];then    return 0fi# * Calculate last loginlastlog=`lastlog -u $&#123;user&#125; | grep $&#123;user&#125; | awk '&#123;for(i=3;i&lt;=NF;++i) printf(&quot;%s &quot;,$i)&#125;'`# * Print Outputecho &quot; ::::::::::::::::::::::::::::::::::-STATUS-::::::::::::::::::::::::::::::::::&quot;#  * Check RAM Usagesfree_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemAvailable/&#123;free=$2&#125;END&#123;print free/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;' /proc/meminfo)app_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemFree/&#123;free=$2&#125;/Buffers/&#123;buffers=$2&#125;/^Cached/&#123;cached=$2&#125;END&#123;print (total-free-buffers-cached)/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;'  /proc/meminfo)all_mem_usages=$(awk '/MemTotal/&#123;total=$2&#125;/MemFree/&#123;free=$2&#125;END&#123;print (total-free)/1024&quot;/&quot;total/1024&quot; MB&quot;&#125;'  /proc/meminfo)blue &quot; Free Memory : $&#123;free_mem_usages&#125;&quot;blue &quot; Application Memory Usages : $&#123;app_mem_usages&#125;&quot;blue &quot; System Memory Usages : $&#123;all_mem_usages&#125;&quot;# * Check Disk Usagesdiskusages=$(df -PH | awk '&#123;printf &quot;%-40s%-15s%-15s%-15s%-15s%-15s\n&quot;, $1,$2,$3,$4,$5,$6&#125;')blue &quot; Disk Usages :&quot;echo &quot;$&#123;diskusages&#125;&quot; | toilet -f term --metal -w 200# * Check Load Averageloadaverage=$(top -n 1 -b | grep &quot;load average:&quot; | awk '&#123;print $(NF-2) $(NF-1) $NF&#125;')blue &quot; Load Average: $loadaverage&quot;</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>问题记录</title>
      <link href="posts/1ddb9d4e/"/>
      <url>posts/1ddb9d4e/</url>
      
        <content type="html"><![CDATA[<h3 id="Q-包管理器安装软件出现">Q:包管理器安装软件出现</h3><pre><code class="language-bash">insserv:  loop involving service xxx at depth 2</code></pre><h4 id="A-删除-xxx-服务，如果-xxx-服务已删除，清理-xxx-服务的启动脚本-etc-init-d-xxx">A:删除 xxx 服务，如果 xxx 服务已删除，清理 xxx 服务的启动脚本 /etc/init.d/xxx</h4><hr><h3 id="Q-crontab-如何修改时区">Q:crontab 如何修改时区</h3><h4 id="A-在crontab文件最上方添加命令，例如芝加哥时区">A:在crontab文件最上方添加命令，例如芝加哥时区</h4><pre><code class="language-bash">TZ='America/Chicago'                                          CRON_TZ='America/Chicago'  `</code></pre><hr><h3 id="Q-su-user-c-“command”-命令出错">Q:su - user -c “command” 命令出错</h3><h4 id="A-需要用户开启登陆权限，即-etc-passwd-中不能使-sbin-nologin">A:需要用户开启登陆权限，即 /etc/passwd 中不能使 /sbin/nologin</h4><hr><h3 id="Q-zabbix-自动发现异常，表面看不出问题">Q:zabbix 自动发现异常，表面看不出问题</h3><h4 id="A-zabbix-agent-端开启-debug-模式，配置加入参数-DebugLevel-4">A:zabbix-agent 端开启 debug 模式，配置加入参数 DebugLevel=4</h4><hr><h3 id="Q-adminer-无效的CSRF令牌-Invalid-CSRF-token">Q:adminer: 无效的CSRF令牌(Invalid CSRF token)</h3><h4 id="A-nginx-worker-user-用户无法访问-php-session-目录">A:nginx worker user 用户无法访问 php session 目录.</h4><pre><code class="language-bash">chgrp $&#123;nginxWorkerUser&#125; $&#123;phpSessionDir&#125;# 如果是yum或者apt安装,那么php的session一般是 /var/lib/php/session</code></pre><hr><h3 id="Q-python3-No-module-named-‘PIL’">Q:python3 : No module named ‘PIL’</h3><h4 id="A-pip3-install-pillow">A:pip3 install pillow</h4><hr><h3 id="Q-python-workon-命令找不到">Q:python: workon 命令找不到</h3><h4 id="A">A:</h4><pre><code class="language-bash:">pip install virtualenvwrapperecho 'export PATH=~/.local/bin:$PATH' &gt;&gt; ~/.bashrc # 根据所用shell来决定文件路径</code></pre><hr><h3 id="Q-jira配置163的smtp连接超时-但是服务器终端telnet正常">Q:jira配置163的smtp连接超时,但是服务器终端telnet正常</h3><h4 id="A-2">A:</h4><p><img src="/posts/1ddb9d4e/image-20200917111208620.png" alt="image-20200917111208620"></p><h3 id="Q-tomcat-添加-pid-文件">Q: tomcat 添加 pid 文件</h3><h4 id="A-catalina-sh-文件的-PRGDIR-dirname-PRG-下面添加新行-CATALINA-PID-PRGDIR-tomcat-pid-此时将在-bin-目录下创建-tomcat-pid">A:  <a href="http://catalina.sh">catalina.sh</a> 文件的 <code>PRGDIR=dirname &quot;$PRG&quot;</code>下面添加新行 <code>CATALINA_PID=$PRGDIR/tomcat.pid</code> 此时将在 bin/ 目录下创建 tomcat.pid</h4>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 问题记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-tags</title>
      <link href="posts/960fc783/"/>
      <url>posts/960fc783/</url>
      
        <content type="html"><![CDATA[<h4 id="tags的定义">tags的定义</h4><blockquote><p>tags 可以让你在执行playbook的时候，有选择地执行某些任务，因此 tags 是 tasks 下的关键词</p></blockquote><pre><code class="language-bash">ansible-playbook test.play &lt;--tags-args&gt;</code></pre><h4 id="tags的参数">tags的参数</h4><blockquote><ul><li>–tags=tag_name  执行具有 tag_name 任务</li><li>–skip-tags=tag_name 忽略具有 tag_name 任务</li><li>–list-tags 输出所有</li></ul></blockquote><blockquote><p>tag_name 内置值 ：</p><ul><li>tagged 有tag的task，表示执行具有标记的任务</li><li>untagged 没有tag的task，表示执行不具有标记的任务</li><li>all 所有task，表示执行所有任务</li></ul></blockquote><h4 id="tags的内置标记">tags的内置标记</h4><blockquote><ul><li>always 总是执行某个 task</li><li>never 永远不执行某个 task</li></ul></blockquote><h4 id="tags-的位置">tags 的位置</h4><blockquote><p>位于play或者tasks都可以，本身具有继承属性，也就是tasks里的tags会继承play的tags</p></blockquote><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  tags: father  tasks:    - name: test tag son      tags: son,children      shell: echo &quot;son is here!&quot; &gt; ~/son.log    - name: test tag daughter      tags: daughter,children      shell: echo &quot;daughter is here!&quot; &gt; ~/daughter.log</code></pre><pre><code class="language-bash">ansible-playbook test.play --tags=son # 只会生成 son.logansible-playbook test.play --tags=father  # 因继承机制，会生成 son.log 和 daughter.logansible-playbook test.play --tags=children # 因都含有，同样会生成 son.log 和 daughter.log</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-错误捕获</title>
      <link href="posts/c4126d5a/"/>
      <url>posts/c4126d5a/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>类似于 python 中的 try…except…finally，ansible 可以用 block…rescue…always</p><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - block:        - shell: mkdir /file      rescue:        - debug:            msg: &quot;No operation permission&quot;      always:        - debug:            msg: &quot;Task End!&quot;</code></pre><blockquote><p>创建 file 目录失败，则输出&quot;No operation permission&quot;, 最终总是输出“Task End!”</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-条件判断</title>
      <link href="posts/29683589/"/>
      <url>posts/29683589/</url>
      
        <content type="html"><![CDATA[<h4 id="关键词">关键词</h4><ul><li>when</li></ul><h4 id="运算符">运算符</h4><ul><li>==  !=  &gt;  &lt;  &gt;=  &lt;=</li><li>and or not</li><li>( ) 组合，例如 ( a and b ) or c</li></ul><blockquote><p>ansible 某个 task 报错，会导致任务终止，而ignore_errors: true 可以忽略某个任务的条件不满足</p></blockquote><h4 id="is-语句-或者-is-not-语句">is 语句 或者 is not 语句</h4><pre><code class="language-yaml">tasks:  - name: show is xxx    debug:      msg: &quot;xxx is ok&quot;    when: var is xxx</code></pre><blockquote><p>判断文件</p><ul><li>xxx 是 exists ，表示若 var 存在，条件为真</li><li>xxx 是 file, 表示若 var 是文件，条件为真</li><li>xxx 是 directory， 表示若 var 是目录，条件为真</li><li>xxx 是 link，表示若 var 是软连接，条件为真</li><li>xxx 是 mount，表示若 var 是挂载点，条件为真</li></ul></blockquote><blockquote><p>判断变量</p><ul><li>若 xxx 是 defined, 表示若 var 已定义，条件为真</li><li>若 xxx 是 undefined， 表示若 var 未定义，条件为真</li><li>若 xxx 是 none， 表示若 var 是空，条件为真</li></ul></blockquote><blockquote><p>判断任务状态</p><ul><li>若 xxx 是 success， 若 var 为某任务返回结果，则任务状态成功，条件为真</li><li>若 xxx 是 failure， 若 var 为某任务返回结果，则任务状态失败，条件为真</li><li>若 xxx 是 change，若 var 为某任务返回结果，则任务状态改变，条件为真</li><li>若 xxx 是 skip， 若 var 为某任务返回结果，则任务被忽略，条件为真</li></ul></blockquote><blockquote><p>判断字符串</p><ul><li><p>若 xxx 是 string，若 var 是字符串，条件为真</p></li><li><p>若 xxx 是 lower，若 var 是纯小写，条件为真</p></li><li><p>若 xxx 是 upper，若 var 是纯大写，条件为真</p></li></ul></blockquote><blockquote><p>判断数字</p><ul><li><p>若 xxx 是 number， 若 var 是数字，条件为真。 var: “123” ,这里 var 是字符串，不是数字</p></li><li><p>若 xxx 是 even，若 var 是偶数，条件为真</p></li><li><p>若 xxx 是 odd， 若 var 是奇数，条件为真</p></li><li><p>若 xxx 是 divisibleby(num), 若 var 可以被 num 整除，条件为真</p></li></ul></blockquote><blockquote><p>判断集合</p><ul><li>若 xxx 是 subset(list)，若 var 是 list 的子集，条件为真</li><li>若 xxx 是 superset(list), 若 var 是 list 的父集，条件为真</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-循环</title>
      <link href="posts/14bc184e/"/>
      <url>posts/14bc184e/</url>
      
        <content type="html"><![CDATA[<h4 id="常见的循环">常见的循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show debug info      debug:        msg: &quot;&#123;&#123; item &#125;&#125;&quot;      with_items:        - [ a,b ]        - [ A,B, [ D,E,F ]]</code></pre><blockquote><ul><li><p>with_list 输出最表层元素，在简例中，会输出 [ a,b ] 和 [ A,B, [ D,E,F ] ]</p></li><li><p>with_item 递归输出所有层元素</p></li><li><p>with_together 合并两个列表，元素按照对应下标结合，如果某一方列表元素缺失，则用null代替</p></li><li><p>with_indexed_items 最表层所有列表合并为一个新列表并循环。item由{ list.index: list.value } 构成。在简例中，新列表是[ a,b,A,B, [ D,E,F ]]</p><pre><code class="language-yaml"></code></pre></li></ul><p>msg: “Index:&#123;&#123; item.0 &#125;&#125;, Value:&#123;&#123; item.1 &#125;&#125;”</p><pre><code>```bashok: [localhost] =&gt; (item=[0, u'a']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:0, Vaule:a&quot;&#125;ok: [localhost] =&gt; (item=[1, u'b']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:1, Vaule:b&quot;&#125;ok: [localhost] =&gt; (item=[2, u'A']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:2, Vaule:A&quot;&#125;ok: [localhost] =&gt; (item=[3, u'B']) =&gt; &#123;    &quot;msg&quot;: &quot;Index:3, Vaule:B&quot;&#125;ok: [localhost] =&gt; (item=[4, [u'D', u'E', u'F']]) =&gt; &#123;    &quot;msg&quot;: &quot;Index:4, Vaule:[u'D', u'E', u'F']&quot;&#125;</code></pre><ul><li>with_random_choice 随机输出一个最表层列表元素，简例中输出 [a,b] 或者 [ A,B, [ D,E,F ]]</li></ul></blockquote><h4 id="dict-字典循环">dict 字典循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show debug info      debug:        msg: &quot;Name:&#123;&#123; item.key &#125;&#125;, gender:&#123;&#123; item.value &#125;&#125;&quot;      with_dict:        Zhangsan: male        Lisi: female</code></pre><blockquote><p>输出所有字典</p></blockquote><h4 id="sequence-序列循环">sequence 序列循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show sequence info      debug:        msg: &quot;&#123;&#123; item &#125;&#125;&quot;      with_sequence:        start=1        end=5        stride=2        format=&quot;I'm %0.4f&quot;</code></pre><blockquote><ul><li>with_sequence 获取奇偶数，start和end是起止点，stride 是步长（步长可以为负值），format是格式化</li></ul></blockquote><h4 id="nested-嵌套循环">nested 嵌套循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  remote_user: zyh  gather_facts: no  tasks:    - name: show nested info      debug:        msg: &quot;mkdir /mnt/&#123;&#123; item.0 &#125;&#125;/&#123;&#123; item.1 &#125;&#125;&quot;      with_nested:        - [ a,b ]        - [ A,B,C ]</code></pre><blockquote><p>两个列表做笛卡尔积, 例如构建环境目录</p></blockquote><h4 id="subelements-子元素循环">subelements 子元素循环</h4><ul><li>简例</li></ul><pre><code class="language-yaml">---- hosts: localhost  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                    age: 18                    content:                            - eating                            - sleeping                            - play ogre                  - name: Maris                    gender: female                    age: 20                    content:                            - eating                            - sleeping                            - shopping  tasks:          - name: test vars 2            debug:                    msg: &quot;&#123;&#123; item.0.name &#125;&#125; - &#123;&#123; item.0.gender &#125;&#125; - &#123;&#123; item.1 &#125;&#125;&quot;            with_subelements:              - &quot;&#123;&#123; users &#125;&#125;&quot;              - content</code></pre><blockquote><p>分解 , 选中列表内某一个列表元素 content, 与作为一个临时整体的剩余元素构建笛卡尔积，形成 item</p></blockquote><h4 id="file-文件循环">file 文件循环</h4><pre><code class="language-yaml">with_file:  /mnt/a.ini  /mnt/b.ini</code></pre><blockquote><p>始终循环获取ansible主机里文件的内容。（与目标主机无关）</p></blockquote><h4 id="fileglob-寻找通配符匹配的文件">fileglob 寻找通配符匹配的文件</h4><pre><code class="language-yaml">with_fileglob:  - /home/zyh/test/dirA/*  - /home/zyh/test/dirB/[0-9].ini</code></pre><blockquote><p>始终循环获取ansible主机指定目录中匹配的文件名和路径。（与目标主机无关）</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞管理静态文件</title>
      <link href="posts/6a4df400/"/>
      <url>posts/6a4df400/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://docs.djangoproject.com/zh-hans/3.1/howto/static-files/">管理静态文件（比如图片、JavaScript、CSS） | Django 文档 | Django (djangoproject.com)</a></p><p><a href="https://docs.djangoproject.com/zh-hans/3.1/ref/views/">内置视图 | Django 文档 | Django (djangoproject.com)</a></p><h2 id="通用设置">通用设置</h2><p>项目.settings.py中</p><pre><code class="language-python">STATIC_URL = '/static/'if DEBUG:    STATICFILES_DIRS = [        BASE_DIR / &quot;static&quot;,    ]else:    STATIC_ROOT = BASE_DIR / &quot;static&quot;MEDIA_URL = '/media/'MEDIA_ROOT = BASE_DIR / &quot;media</code></pre><h2 id="DEBUG开启">DEBUG开启</h2><p>无需其它改动</p><h2 id="DEBUG关闭">DEBUG关闭</h2><p>项目.urls.py中</p><pre><code class="language-python">from django.views.static import servefrom django.urls import path, include, re_pathfrom . import settingsif not settings.DEBUG:    urlpatterns += [        re_path(r'^media/(?P&lt;path&gt;.*)$', serve, &#123;            'document_root': settings.MEDIA_ROOT,        &#125;),        re_path(r'^static/(?P&lt;path&gt;.*)$', serve, &#123;            'document_root': settings.STATIC_ROOT,        &#125;),    ]</code></pre><h2 id="部署静态文件">部署静态文件</h2><pre><code class="language-bash">python manage.py collectstatic --noinput</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞记忆点</title>
      <link href="posts/3f8b5448/"/>
      <url>posts/3f8b5448/</url>
      
        <content type="html"><![CDATA[<h2 id="git文件">.git文件</h2><pre><code class="language-bash"># Django #*.log*.pot*.pyc__pycache__mediastatic# Backup files #*.bak# other.vscode.gitdblog</code></pre><h2 id="Dockerfile">Dockerfile</h2><pre><code class="language-bash">FROM python:3.8-alpineLABEL maintainer=&quot;aaa103439@hotmail.com&quot;WORKDIR /app COPY . /appRUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories &amp;&amp; \    apk update &amp;&amp; \    apk add gcc musl-dev jpeg-dev zlib-dev freetype-dev lcms2-dev openjpeg-dev tiff-dev tk-dev tcl-dev libffi-dev openssl-dev &amp;&amp; \    pip install --upgrade pip &amp;&amp; pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/CMD [&quot;sh&quot;,&quot;/app/start.sh&quot;]</code></pre><h2 id="生成-settings-SECRET-KEY">生成 settings.SECRET_KEY</h2><pre><code class="language-python">#!/usr/bin/env python# coding=utf-8from django.core.management.utils import  get_random_secret_keyprint(&quot;务必妥善保管好此密码&quot;)print(get_random_secret_key())</code></pre><h2 id="调用-fontawesome-free-图标库">调用 fontawesome-free 图标库</h2><pre><code class="language-bash">pip install fontawesome-free</code></pre><p><code>settings.INSTALLED_APPS</code> 中加入<code>fontawesome-free</code></p><h2 id="外键">外键</h2><p>外键，顾名思义不是自己的字段，而是引用其它表的字段。</p><blockquote><p>外键不是通过字段名定义的，而是通过【外键约束】定义</p></blockquote><p>外键可以实现一对多、多对多和一对一的关系。</p><p>外键既可以通过数据库来约束，也可以不设置约束，仅依靠应用程序的逻辑来保证。</p><p>学生表【student】有主键【sid】，成绩表也有主键【sid】, 学生表和成绩表是1对1的关系</p><p><img src="/posts/3f8b5448/image-20210716114155366.png" alt="image-20210716114155366"></p><p>如果没有【外键约束】的规则建立，则两个表在数据库看来是没有关系的，即：要保持两个表的关系，则需要应用程序去保证逻辑正确，也就是说，应用程序不能插入相同学号的数据到表里。</p><p>若应用程序不想去维持，则就需要数据库【外键约束】去维持。</p><p>在没有django的时候，你需要直接写入SQL语句去实现【外键约束】，例如</p><pre><code class="language-sql">ALTER TABLE scoresADD CONSTRAINT fk_student_sidFOREIGN KEY (student_sid)REFERENCES student (sid);</code></pre><p>django创建外键约束的方式如下，成绩表的【sid】字段定义如下</p><pre><code class="language-python">sid = models.OneToOneField('student', null=True, on_delete=models.CASCADE)</code></pre><h3 id="一对一-models-OneToOneField">一对一 models.OneToOneField()</h3><p>常用于“扩展”另一个模型的主键，就如同上述例子，扩展了学生的额外属性【成绩】</p><h3 id="一对多-models-ForeignKey">一对多 models.ForeignKey()</h3><p>常用于A包含B，外键定义在B表中，引用A表的主键</p><h3 id="多对多-ManyToManyField">多对多 ManyToManyField()</h3><p>你中有我，我中有你，一个班有多个老师，一个老师可以教多个班</p><h3 id="查询">查询</h3><h3 id="on-delete">on_delete</h3><p>字面意思，当删除的时候，应该怎么做</p><p><code>CASCADE</code>：删除</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-系统相关</title>
      <link href="posts/84eb900/"/>
      <url>posts/84eb900/</url>
      
        <content type="html"><![CDATA[<h4 id="cron">cron</h4><blockquote><p>crontab 计划任务</p><p>参数介绍：</p><p>name 计划任务注释，多次操作同名任务，只会修改，而不会新加</p><p>时间参数：</p><ul><li><p>minute hour day month weekday</p></li><li><p>special_time : @reboot @yearly @monthly @weekly @daily @hourly (每xxx执行)</p></li></ul><p>user 添加到指定用户计划任务中</p><p>job 计划任务执行命令</p><p>state 当值为absent时，指删除任务. 只需指定 name.</p><p>disabled 注释任务，若任务信息和之前不一致，会同时修改任务</p><p>backup 先备份再操作, 备份文件位于 /tmp/crontabxxxx</p></blockquote><pre><code class="language-bash">ansible localhost -m cron -a &quot;name='test cron module' user=zyh special_time=hourly job='ls /home/zyh &gt; /home/zyh/cron.log 2&gt;&amp;1'&quot;ansible localhost -m cron -a &quot;name='test cron module' state=absent&quot;</code></pre><h4 id="service">service</h4><blockquote><p>调用远程系统自身的服务管理模块，例如 centos6 的 service ，或者 centos7 的 systemctl</p><p>参数介绍:</p><p>name 服务名</p><p>state 执行动作 started, stopped, restarted, reloaded</p><p>enabled 开机自启动</p></blockquote><h4 id="user">user</h4><blockquote><p>用户管理</p><p>常用参数介绍：</p><p>name 用户名</p><p>group 用户组 groups 用户附加组</p><ul><li>append 额外附加用户附加组</li></ul><p>shell 指定默认shell，比如/usr/sbin/nologin</p><p>state 值为 absent 表示删除用户，值为 present 表示用户必须存在</p><ul><li>remove 删除用户时，同时删除用户家目录</li></ul><p>password 用户密码。（需要传递加密密码，不能是明文密码）</p><pre><code class="language-python">import crypt:passwd=print(crypt.crypt(passwd))</code></pre><p>generate_ssh_key 相当于远程执行 ssh-keygen 命令（不加任何参数，一路回车）。若已经存在~/.ssh/{id_rsa, id_rsa.pub}, 则不执行</p><ul><li>ssh_key_file 自定义私钥名和私钥存放路径, 公钥也会在自定义路径下生成</li></ul></blockquote><h4 id="group">group</h4><blockquote><p>管理用户组</p><p>参数介绍：</p><p>name 组名</p><p>state 组状态, 值为 absent 指删除(组本身并非用户主要组)</p></blockquote><h4 id="setup">setup</h4><blockquote><p>获取机器信息</p><p>参数介绍：</p><p>gather_subset 获取某个子集（all, min, hardware, network, virtual, ohai, facter）</p><p>filter 获取某个集合的某个key</p><p>fact_path 自定义信息存放目录</p></blockquote><pre><code class="language-ini"># setup 默认会搜索目标主机/etc/ansible/facts.d 下的自定义信息,例如 family.ini[family]father=Zhangsanson=Zhangxiaosan</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-handle</title>
      <link href="posts/b0929188/"/>
      <url>posts/b0929188/</url>
      
        <content type="html"><![CDATA[<h4 id="功能">功能</h4><p>用一个短路判断来说，就是两者是串联关系，handlers 用来处理任务后续</p><p>tasks &amp;&amp; handlers</p><p>tasks &amp;&amp; handlers - listen （handlers 组）</p><h4 id="handlers-的-playbook-样本">handlers 的 playbook 样本</h4><pre><code class="language-yaml">---- hosts: localhost  become: true  remote_user: root  tasks:  - name: check nginx    shell: /usr/sbin/nginx -t    notify:      log        - meta: flush_handlers     handlers:  - name: log    shell: echo &quot;nginx check success&quot; &gt; ~/playbook.log</code></pre><blockquote><p>多个tasks的时候，tasks后面的 <code>meta: flush_handlers</code> 可以让tasks执行完，立马执行关联的handlers。否则handlers会在所有tasks执行完后，才开始执行</p></blockquote><h4 id="handlers-listen-的-playbook-样本">handlers-listen 的 playbook 样本</h4><pre><code class="language-yaml">---- hosts: localhost  become: true  remote_user: root  tasks:  - name: check nginx    shell: /usr/sbin/nginx -t    notify:      log group    handlers:  - name: log1    listen: log group    shell: echo &quot;nginx check success 1&quot; &gt; ~/playbook.log  - name: log2    listen: log group    shell: echo &quot;nginx check success 2&quot; &gt;&gt; ~/playbook.log</code></pre><blockquote><p>handlers 通过 listen 绑定在一起， tasks 关联 liasten 绑定 handlers 组，最终 playbook.log 将会写入两行信息</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> handle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞备份</title>
      <link href="posts/2c2d50a0/"/>
      <url>posts/2c2d50a0/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>本文主要记录 redis 的两种数据磁盘固化方式。<br>涉及到相关参数，简单的命令操作等。</p><h4 id="rdb">rdb</h4><blockquote><p>通过fork一个子进程来存储某一时刻redis数据(bgsave方式)。rdb持久化是默认方式。</p><p>特点：</p><ul><li><p>小幅度丢失数据(取决于save或者bgsave命令的执行周期)，这里我们不说save，应该不会用到这个</p></li><li><p>恢复速度快</p></li></ul></blockquote><pre><code class="language-bash"># 压缩rdb文件rdbcompression yes# rdb 文件名称dbfilename redis-6379.rdb# rdb文件保存目录dir /redis/data/</code></pre><ul><li>数据自动写入策略 (满足下列规则，就执行bgsave)</li></ul><pre><code class="language-bash"># 900s内至少达到一条写命令save 900 1# 300s内至少达至10条写命令save 300 10# 60s内至少达到10000条写命令save 60 10000</code></pre><ul><li>数据人工写入策略 (每10分钟计划任务调用一次bgsave)</li></ul><pre><code class="language-bash">*/10 * * * * root /export/redis/bin/redis-cli -h 127.0.0.1 bgsave &gt;&gt; /export/redis/bgsave.log 2&gt;&amp;1  </code></pre><blockquote><p>bgsave 因需要fork子进程，所以需要额外预留空闲的物理内存，在overcommit_memory=1开启的情况下，预留内存大小 &gt; 周期变化数据大小</p></blockquote><h4 id="aof">aof</h4><blockquote><p>记录的是redis每一次的写入操作记录</p><p>特点：</p><ul><li>恢复速度慢</li><li>丢失数据小</li></ul></blockquote><pre><code class="language-bash"># 开启aof机制appendonly yes# aof文件名appendfilename &quot;appendonly.aof&quot;# 写入策略,always表示每个写操作都保存到aof文件中,也可以是everysec或no。 everysec 每秒一次appendfsync everysec# 默认不重写aof文件,意思就是每次 appendfsync 就压缩整合aof文件，避免aof过大，不推荐开启，影响性能no-appendfsync-on-rewrite no# 保存目录dir /redis/data/</code></pre><ul><li>人工计划任务重写</li></ul><pre><code class="language-bash">*/10 * * * * root /export/redis/bin/redis-cli -h 127.0.0.1 bgrewriteaof &gt;&gt; /export/redis/bgrewriteaof.log 2&gt;&amp;1</code></pre><ul><li>aof 因服务器挂掉损坏可以修复</li></ul><pre><code class="language-bash">redis-check-aof -fix file.aof</code></pre><h4 id="结论">结论</h4><p>rdb 或者 aof 根据业务二选一即可，没必要都开启，但是不管是哪一种，都可以在人工计划任务之后，复刻一份备份文件到云端对象存储中。</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 备份 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-软件包管理</title>
      <link href="posts/3d9b9ec1/"/>
      <url>posts/3d9b9ec1/</url>
      
        <content type="html"><![CDATA[<h4 id="apt-repository">apt_repository</h4><blockquote><p>ubuntu 下：</p><p>repo 指定库地址，例如 nginx 地址 ppa:nginx/stable</p><p>state 值为 absent 时为删除</p></blockquote><pre><code class="language-bash">ansible localhost -m apt_repository -a &quot;repo=ppa:nginx/stable&quot; -b --ask-become-pass</code></pre><pre><code class="language-bash">localhost | CHANGED =&gt; &#123;    &quot;changed&quot;: true,     &quot;repo&quot;: &quot;ppa:nginx/stable&quot;,     &quot;state&quot;: &quot;present&quot;&#125;#( 04/24/20@ 3:10PM )( zyh@zyh ):~   cat  /etc/apt/sources.list.d/ppa_nginx_stable_bionic.listdeb http://ppa.launchpad.net/nginx/stable/ubuntu bionic main</code></pre><h4 id="apt">apt</h4><blockquote><p>常用参数：</p><p>name 包名</p><p>state 包状态 （absent-删除，latest-最新包,  present-默认安装） latest 相当于升级包</p><p>upgrade 升级 （yes，dist，full，no-默认）</p><p><a href="https://docs.ansible.com/ansible/latest/modules/apt_module.html#apt-module">https://docs.ansible.com/ansible/latest/modules/apt_module.html#apt-module</a></p></blockquote><pre><code class="language-bash">ansible localhost -m apt -a &quot;name=nginx state=present&quot; -b --ask-become-pass</code></pre><pre><code class="language-bash">localhost | CHANGED =&gt; &#123;    &quot;cache_update_time&quot;: 1587713114,     &quot;cache_updated&quot;: false,     &quot;changed&quot;: true,     &quot;stderr&quot;: &quot;&quot;,     &quot;stderr_lines&quot;: [],     &quot;stdout&quot;: &quot;Reading package lists...\nBuilding dependency tree.......</code></pre><h4 id="yum-repository">yum_repository</h4><blockquote><p>name 仓库名</p><p>baseurl 仓库地址</p><p>enabled （yes-默认，no)</p><p>gpgcheck (yes, no)</p><p>gpgcakey 指定 gpg ca 公钥</p><p>state (present-默认，absent-删除)</p></blockquote><pre><code class="language-bash">ansible localhost -m yum_repository -a &quot;name=epel baseurl=https://download.fedoraproject.org/pub/epel/$releasever/$basearch/&quot;</code></pre><h4 id="yum">yum</h4><blockquote><p>name 包名</p><p>state (absent-删除，present-安装-默认值，latest-更新)</p><p>disable_gpg_check 关闭gpg检查（用于源gpg检查没有的情况）</p><p>enablerepo 安装包的时候，先临时启用某个源</p><p>disablerepo 安装包的时候，先临时禁用某个源</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-命令调用</title>
      <link href="posts/1e274151/"/>
      <url>posts/1e274151/</url>
      
        <content type="html"><![CDATA[<h4 id="shell">shell</h4><blockquote><p>远程执行一个命令</p><p>部分参数解析：</p><p>chdir 远程工作目录</p><p>executable 远程执行shell，需要绝对路径</p></blockquote><pre><code class="language-bash">ansible localhost -m shell -a &quot;chdir=/ ls&quot;</code></pre><pre><code class="language-bash">localhost | CHANGED | rc=0 &gt;&gt;binbootdevetchome...</code></pre><h4 id="script">script</h4><blockquote><p>远程执行一个ansible主机环境的脚本</p><p>部分参数解析：</p><p>chdir 远程工作目录</p></blockquote><pre><code class="language-bash">cat test.sh#!/bin/bashfor i in `ls /export`;do        echo $idone#-------------ansible -i hosts test -m script -a &quot;/home/zyh/test.sh&quot;</code></pre><pre><code class="language-bash">10.200.10.212 | CHANGED =&gt; &#123;    &quot;changed&quot;: true,     &quot;rc&quot;: 0,     &quot;stderr&quot;: &quot;Shared connection to 10.200.10.212 closed.\r\n&quot;,     &quot;stderr_lines&quot;: [        &quot;Shared connection to 10.200.10.212 closed.&quot;    ],     &quot;stdout&quot;: &quot;jdk1.8.0_191\r\njdk8\r\nsen\r\n&quot;,     &quot;stdout_lines&quot;: [        &quot;jdk1.8.0_191&quot;,         &quot;jdk8&quot;,         &quot;sen&quot;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞常用模块-文本文件操作</title>
      <link href="posts/4235ddf4/"/>
      <url>posts/4235ddf4/</url>
      
        <content type="html"><![CDATA[<h2 id="文本操作">文本操作</h2><h4 id="file">file</h4><blockquote><p>path 文件对象地址<br>state 文件类型或者动作状态 （touch: 针对文件, directory：针对目录, link：针对软连接, hard：针对硬链接)</p><p>src 软硬链接的源文件</p><p>owner 属主</p><p>group 属组</p><p>mode 数字权限</p><p>recurse 递归操作</p></blockquote><h4 id="blockinfile">blockinfile</h4><blockquote><p>在指定位置，插入文本块，并在文本块开头和结尾添加标记. 标记用来确认文本块的位置，一些参数会通过标记位置来修改文本块。</p><p>注释格式:</p><p># BEGIN xxx</p><p># END xxx</p><p>参数简介：</p><p>path 文件对象地址</p><p>block 需要添加的文本块</p><p>marker 自定义标记 xxx 部分，如果存在相同标记，则优先处理相同标记的文本块。</p><p>state 状态为absent时，删除标记包括的文本块</p><p>insertafter 正则或者EOF，插入匹配的指定行之后</p><p>insertbefore 正则或者BOF，插入匹配的指定行之前</p><p>backup 先备份，再操作，备份文件后缀是时间戳</p><p>create 文件不存在，则创建</p></blockquote><pre><code class="language-bash"># 这条命令中，如果marker标记已经存在，则insertafter将无效-m blockinfile -a 'path= block=&quot; &quot; marker=&quot;#&#123;mark&#125; xxx&quot; insertafter=&quot;正则&quot;'</code></pre><h4 id="lineinfile">lineinfile</h4><blockquote><p>根据指定的内容，进行替换或删除</p><p>参数简介：</p><p>path 文件对象地址。</p><p>line 指定行内容（在没有正则的情况下，需要全匹配）。</p><p>regexp 通过正则匹配行，并将此行替换成 line 指定的内容，regexp有额外扩展参数，例如 backref。</p><ul><li><p>line  若line匹配到某行，则不修改，若无匹配，则添加line至末尾。</p></li><li><p>regexp + line  若有regexp匹配到某行，替换匹配行为line；如果没有匹配，则将line追加到行尾。此时，regexp不支持分组。</p></li><li><p>regexp + backrefs （true）+ line  若有regexp匹配到某行，替换匹配行为line；如果没有匹配，则保持源文件不变；此时，regexp支持分组。</p></li></ul><p>state 状态为absent时，删除匹配行</p><p>insertafter 正则或者EOF，插入匹配的指定行之后</p><p>insertbefore 正则或者BOF，插入匹配的指定行之前</p><p>backup 先备份，再操作</p><p>create 文件不存在，则创建</p></blockquote><pre><code class="language-bash">-m lineinfile -a 'path= line=&quot; &quot; insertafter=&quot;正则&quot;'</code></pre><h4 id="replace">replace</h4><blockquote><p>替换文件对象中符合匹配的字符串</p><p>path 文件对象地址</p><p>regexp 正则匹配</p><p>replace 替换后的字符串</p><p>backup 先备份，再操作</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-aws</title>
      <link href="posts/15aa7d2e/"/>
      <url>posts/15aa7d2e/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://github.com/ansible/ansible/blob/stable-2.9/contrib/inventory/">https://github.com/ansible/ansible/blob/stable-2.9/contrib/inventory/</a></p><p><a href="https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html">https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html</a></p><p><a href="https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html#inventory-script-example-aws-ec2">https://docs.ansible.com/ansible/latest/user_guide/intro_dynamic_inventory.html#inventory-script-example-aws-ec2</a> （深坑，脚本404，找到了脚本，各种错误，请扔一边）</p></blockquote><h3 id="前言">前言</h3><p>通过 ansible 获取大区下 ec2 资源信息</p><h3 id="授权">授权</h3><pre><code class="language-shell">export AWS_ACCESS_KEY_ID='AK123'export AWS_SECRET_ACCESS_KEY='abc123'export EC2_INI_PATH=ec2.ini</code></pre><h3 id="库存-inventory">库存(inventory)</h3><pre><code class="language-ini">[local]localhost</code></pre><h3 id="Playbook">Playbook</h3><pre><code class="language-yaml">---  - name: test ec2    hosts: local    gather_facts: no   # 我们要这信息干什么？我们是有目标的    connection: local # 木有定义资源    tasks:      - name: get ec2 info        ec2_instance_info:          region: cn-north-1        register: data_output      - name: show ec2 info        debug:          msg: &quot;&#123;&#123; data_output|json_query('instances[*].network_interfaces[*].private_ip_address') &#125;&#125;&quot;</code></pre><h3 id="执行">执行</h3><pre><code class="language-shell">ansible-playbook -i hosts ec2.yml</code></pre><h3 id="输出">输出</h3><pre><code class="language-shell">TASK [show ec2 info] ******************************************************************************************************************************************************ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        [            &quot;10.100.10.250&quot;        ],         [            &quot;10.100.10.252&quot;        ],         [            &quot;10.100.10.210&quot;        ],         [            &quot;10.100.10.251&quot;        ]    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>windows 包管理工具 scoop</title>
      <link href="posts/22b82d75/"/>
      <url>posts/22b82d75/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-powershell">Set-ExecutionPolicy RemoteSigned -scope CurrentUseriwr -useb get.scoop.sh | iexscoop install aria2scoop config aria2-max-connection-per-server 16scoop config aria2-split 16scoop config aria2-min-split-size 1Mscoop bucket add extrasscoop install Terminus</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> windows </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows </tag>
            
            <tag> scoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jenkins☞安装和基本配置</title>
      <link href="posts/d9cdb70/"/>
      <url>posts/d9cdb70/</url>
      
        <content type="html"><![CDATA[<ul><li><p>jenkins</p><blockquote><p><a href="https://github.com/jenkinsci/docker/blob/master/README.md">https://github.com/jenkinsci/docker/blob/master/README.md</a></p></blockquote><pre><code class="language-bash">jenkinsTag=jenkinsDns=jenkinsDomain=gitlabDomain=gitlabWanIP=docker volume create jenkins_homedocker run --name jenkins --hostname $&#123;jenkinsDomain&#125; --add-host $&#123;gitlabDomain&#125;:$&#123;gitlabWanIP&#125; --restart always --mount 'type=volume,src=jenkins_home,dst=/var/jenkins_home' --mount 'type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock' --mount 'type=bind,src=/usr/bin/docker,dst=/usr/bin/docker' --mount 'type=bind,src=/usr/lib64/libltdl.so.7,dst=/usr/lib/x86_64-linux-gnu/libltdl.so.7' -p 8080:8080 -p 50000:50000 --dns $&#123;jenkinsDns&#125; -d jenkins/jenkins:$&#123;jenkinsTag&#125;</code></pre><pre><code class="language-bash"># 让 jenkins 用户可以调用 docker 命令，以及安装 ansiblecat /etc/group | grep dockerdocker exec -it -u root jenkins /bin/bashcd /var/jenkins_home/[[ -d src ]] || mkdir srccd srccurl https://bootstrap.pypa.io/get-pip.py -o get-pip.pymkdir ~/.pip/cat &gt; ~/.pip/pip.conf &lt;&lt; EOF[global]index-url = https://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.comEOFpython get-pip.pypip install awsclipip install ansiblegroupadd -g &lt;docker-group-id&gt; dockerusermod -aG docker jenkinsexitdocker stop jenkins &amp;&amp; docker start jenkins</code></pre></li><li><p>用户和角色管理</p><ol><li><p>安装插件 Role-based Authorization Strategy</p></li><li><p>启用插件 Configure Global Security 中启用 Role-Based Strategy 策略</p><p><img src="/posts/d9cdb70/image-20200515180703925.png" alt="image-20200515180703925"></p></li><li><p>配置全局角色和项目角色 Manage and Assign Roles - Manage Roles</p><p>全局角色<strong>Global roles</strong> 设置两个： admin 和 read</p><p><img src="/posts/d9cdb70/image-20200515181449965.png" alt="image-20200515181449965"></p><p>项目角色<strong>Project roles</strong>：每一个项目设置一个</p><p>Pattern: <code>.*\.&lt;项目名&gt; </code></p><p>权限: 看图</p><p><img src="/posts/d9cdb70/image-20200515181359533.png" alt="image-20200515181359533"></p></li><li><p>创建项目用户</p></li><li><p>分配角色 Manage and Assign Roles - Assign Roles</p><p>给管理员分配 admin，给项目用户分配 read 和 cp (cp是我设置的项目角色)</p><p><img src="/posts/d9cdb70/image-20200515181740598.png" alt="image-20200515181740598"></p></li></ol></li><li><p>安装配置文件插件 Config File Provider</p></li><li><p>自动安装 git ，jdk，maven （这种方式只有在进行了一次构建后，才会安装）</p></li><li><p>maven 私服配置文件</p><ol><li><p>通过 Config File Provider 添加一个项目maven配置</p></li><li><pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;          xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;  &lt;servers&gt;    &lt;server&gt;     &lt;id&gt;&lt;Repository Policy名&gt;&lt;/id&gt;     &lt;username&gt;&lt;/username&gt;     &lt;password&gt;&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;     &lt;id&gt;&lt;Repository Policy名&gt;&lt;/id&gt;      &lt;username&gt;&lt;/username&gt;      &lt;password&gt;&lt;/password&gt;    &lt;/server&gt;  &lt;/servers&gt;  &lt;profiles&gt;  &lt;profile&gt;&lt;id&gt;&lt; 仓库名 &gt;&lt;/id&gt;&lt;repositories&gt;  &lt;repository&gt;&lt;id&gt;&lt;maven 仓库组或仓库ID&gt;&lt;/id&gt;&lt;url&gt;&lt;maven 私服具体仓库组或仓库地址&gt;&lt;/url&gt;&lt;releases&gt;  &lt;enabled&gt;true&lt;/enabled&gt;&lt;/releases&gt;&lt;snapshots&gt;  &lt;enabled&gt;true&lt;/enabled&gt;&lt;/snapshots&gt;  &lt;/repository&gt;&lt;/repositories&gt;  &lt;/profile&gt;  &lt;/profiles&gt;  &lt;activeProfiles&gt; &lt;activeProfile&gt;项目名&lt;/activeProfile&gt;  &lt;/activeProfiles&gt;&lt;/settings&gt;</code></pre></li><li><p>然后构建项目的时候，构建环境-Provide Configuration files-Files，并且Build-高级-Settings file-Provided settings.xml</p></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> jenkins </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞安装-包管理方式</title>
      <link href="posts/b73a61bf/"/>
      <url>posts/b73a61bf/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>介绍如何通过yum或者apt-get安装php和php-fpm<br>适合php7.2</p><h4 id="centos">centos</h4><blockquote><p>安装源 <a href="https://webtatic.com/">https://webtatic.com/</a></p></blockquote><pre><code class="language-bash">yum install epel-releaserpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpmyum install gcc-c++ geoip-devel -yyum install php72w-cli php72w-devel mod_php72w php72w-fpm php72w-opcache php72w-gd php72w-bcmath php72w-xml -y# php72w-lzo php72w-yaf 没有直接的包mkdir /export/logs/php -pcd /etc/php-fpm.d/ &amp;&amp; mv www.conf www.conf.bakwebName=wwwcat &gt; www.conf &lt;&lt; EOF[$&#123;webName&#125;]user = webappsgroup = webappslisten = 0.0.0.0:9000pm = staticpm.max_children = 20pm.max_requests = 1024pm.status_path = /php-fpm_statusrequest_slowlog_timeout = 2sslowlog = /export/logs/php/php-slow.logphp_admin_value[error_log] = /export/logs/php/www-error.logphp_admin_flag[log_errors] = onphp_value[session.save_handler] = filesphp_value[session.save_path]    = /var/lib/php/sessionphp_value[soap.wsdl_cache_dir]  = /var/lib/php/wsdlcacheEOF# 安装源中没有的模块, 假设模块是rediscd /usr/local/srcphpModule=yafwget https://pecl.php.net/get/$&#123;phpModule&#125; &amp;&amp; mkdir $&#123;phpModule&#125;-src &amp;&amp; tar xf $&#123;phpModule&#125; --strip-components 1 -C $&#123;phpModule&#125;-srccd $&#123;phpModule&#125;-src &amp;&amp; phpize &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make installcat &gt; /etc/php.d/$&#123;phpModule&#125;.ini &lt;&lt; EOF; Enable zip extension moduleextension=$&#123;phpModule&#125;.soEOF</code></pre><h4 id="ubuntu">ubuntu</h4><pre><code class="language-bash">add-apt-repository ppa:ondrej/phpapt-get install php7.2 php7.2-dev php7.2-fpm php7.2-mysql php7.2-curl php7.2-json php7.2-mbstring php7.2-xml  php7.2-intl php7.2-yac php7.2-yaf php7.2-redis php7.2-lzo php7.2-geoip php7.2-pecl php7.2-pear php7.2-dev php7.2-gd php7.2-zip php7.2-xml php7.2-bcmath</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞基础信息</title>
      <link href="posts/d3e413b7/"/>
      <url>posts/d3e413b7/</url>
      
        <content type="html"><![CDATA[<h3 id="安装">安装</h3><pre><code class="language-shell">apt-get install python3-pippip3 install ansible --user -i https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><blockquote><p>pip 安装方式，不会生成默认配置<br><a href="https://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg">https://raw.githubusercontent.com/ansible/ansible/devel/examples/ansible.cfg</a></p></blockquote><h3 id="关闭-known-hosts-检查">关闭 known_hosts 检查</h3><pre><code class="language-ini"># /etc/ansible/ansible.cfg or ~/.ansible.cfg[defaults]host_key_checking = False</code></pre><h3 id="库存和变量">库存和变量</h3><pre><code class="language-ini"># /etc/ansible/hosts 默认位置，但可自定义，并通过 -i 来调用############################################################################# 单主机mail.example.com# http_port 主机变量[webservers]www[01:50].example.comhttp_port=80[dbservers]db-[a:f].example.comansible_connection=ssh        ansible_ssh_user=mysql# 组变量 ==&gt; 组名:vers[dbservers:vars]mysql_port=3306# 嵌套组 ==&gt; 父组:children[webproject:children]webserversdbservers</code></pre><blockquote><p>中括号表示分组，可以用组名代替组资源 ;</p></blockquote><h3 id="结构化变量">结构化变量</h3><blockquote><p>采用 yaml 配置，格式：</p><pre><code class="language-yaml">---  变量:值</code></pre></blockquote><pre><code class="language-shell">/etc/ansible/group_vars/&lt;组名&gt; # &lt;组名&gt;文件或路径下的文件，均会被认为是&lt;组名&gt;变量/etc/ansible/host_vars/&lt;主机名&gt; # &lt;主机名&gt;文件或路径下的文件，均会被认为是&lt;组名&gt;变量</code></pre><h4 id="常用的变量">常用的变量</h4><pre><code class="language-shell">ansible_ssh_host      将要连接的远程主机名.与你想要设定的主机的别名不同的话,可通过此变量设置.ansible_ssh_port      ssh端口号.如果不是默认的端口号,通过此变量设置.ansible_ssh_user      默认的 ssh 用户名ansible_ssh_pass      ssh 密码(这种方式并不安全,我们强烈建议使用 --ask-pass 或 SSH 密钥)ansible_sudo_pass      sudo 密码(这种方式并不安全,我们强烈建议使用 -b --ask-become-pass)ansible_sudo_exe (new in version 1.8)      sudo 命令路径(适用于1.8及以上版本)ansible_connection      与主机的连接类型.比如:local, ssh 或者 paramiko. Ansible 1.2 以前默认使用 paramiko.1.2 以后默认使用 'smart','smart' 方式会根据是否支持 ControlPersist, 来判断'ssh' 方式是否可行.ansible_ssh_private_key_file      ssh 使用的私钥文件.适用于有多个密钥,而你不想使用 SSH 代理的情况.ansible_shell_type      目标系统的shell类型.默认情况下,命令的执行使用 'sh' 语法,可设置为 'csh' 或 'fish'.ansible_python_interpreter      目标主机的 python 路径.适用于的情况: 系统中有多个 Python, 或者命令路径不是&quot;/usr/bin/python&quot;,比如  \*BSD, 或者 /usr/bin/python      不是 2.X 版本的 Python.我们不使用 &quot;/usr/bin/env&quot; 机制,因为这要求远程用户的路径设置正确,且要求 &quot;python&quot; 可执行程序名不可为 python以外的名字(实际有可能名为python26).      与 ansible_python_interpreter 的工作方式相同,可设定如 ruby 或 perl 的路径....</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>工具库</title>
      <link href="posts/3827a60a/"/>
      <url>posts/3827a60a/</url>
      
        <content type="html"><![CDATA[<p><a href="https://encycolorpedia.cn/323e4e">十六进制颜色代码表，图表，调色板，绘图&amp;油漆</a></p><p><a href="http://gosspublic.alicdn.com/ram-policy-editor/index.html">阿里云 ram 策略生成器</a></p><p><a href="https://develop.aliyun.com/tools/sdk?#/python">阿里云SDK频道</a></p><p><a href="https://app.xuty.tk/static/app/index.html">表情锅</a></p><p><a href="https://whoer.net/zh">测试国外出口ip</a></p><p><a href="http://www.ip-api.com/">测试国外出口ip</a></p><p><a href="https://www.wondercv.com/">超级简历WonderCV - HR推荐简历模板,智能简历制作工具,专业中英文简历模板免费下载</a></p><p><a href="https://help.aliyun.com/knowledge_detail/50270.html?spm=a2c4g.11186623.6.621.483534bfFo31Sm">各地区管局备案规则</a></p><p><a href="https://www.ipplus360.com/">更精准的全球IP地址定位平台_IP问问 -埃文科技(ipplus360.com)</a></p><p><a href="http://xn--eqrt2g.xn--vuq861b/">工信部-域名.信息</a></p><p><a href="http://explainshell.com/">解析命令 explainshell.com - match command-line arguments to their help text</a></p><p><a href="https://haveibeenpwned.com/Passwords">密码泄露检测</a></p><p><a href="https://tuna.moe/">清华大学 TUNA 协会 - Home</a></p><p><a href="http://zh.thetimenow.com/time-zone-converter.php">时区转换生成</a></p><p><a href="https://help.aliyun.com/document_detail/116378.html?spm=a2c4g.11186623.2.17.6f36578flUOrcy#concept-188715">使用redis-shake迁移RDB文件内的数据</a></p><p><a href="https://www.17ce.com/">网站测速|网站速度测试|网速测试|电信|联通|网通|全国|监控|CDN|PING|DNS 17CE.COM</a></p><p><a href="https://devhints.io/">语言/工具语法常用摘要</a></p><p><a href="https://ipchaxun.com/">域名反查ip</a></p><p><a href="https://www.whatsmydns.net/">域名解析检查</a></p><p><a href="https://intodns.com/">域名状态报告</a></p><p><a href="https://github.com/ireaderlab/alex">alex:web压力测试工具</a></p><p><a href="http://www.kammerl.de/ascii/AsciiSignature.php">Ascii Text / Signature Generator motd动态开机提醒</a></p><p><a href="https://www.json2yaml.com/convert-yaml-to-json">Convert YAML to JSON</a></p><p><a href="https://csr.chinassl.net/generator-csr.html">CSR文件生成工具-中国数字证书CHINASSL</a></p><p><a href="https://apps.evozi.com/apk-downloader/">gp apk 下载</a></p><p><a href="http://tool.520101.com/wangluo/ipjisuan/">ip地址在线计算器</a></p><p><a href="https://ipv6-test.com/validate.php">IPv6 站点测试</a></p><p><a href="http://grokconstructor.appspot.com/do/match#result">logstash grok 测试</a></p><p><a href="https://github.com/DoubleLabyrinth/MobaXterm-keygen">MobaXterm-keygen 密钥</a></p><p><a href="http://msdn.itellyou.cn/">MSDN, 我告诉你</a></p><p><a href="https://api.aliyun.com/#/cli">OpenAPI Explorer</a></p><p><a href="https://www.pyman.com.cn/">pyman网址导航</a></p><p><a href="http://rpm.pbone.net/">RPM Search</a></p><p><a href="https://www.cnblogs.com/zhaoruiqing/articles/12870209.html">Typora的Emoji指令</a></p><p><a href="https://paste.ubuntu.com/">Ubuntu Pastebin</a></p><p><a href="http://www.92csz.com/study/UnixToolbox-zh_CN.html">Unix Toolbox - 中文版</a></p><p><a href="https://www.dotcom-tools.com/">Website Performance Test Tools</a></p>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞admin管理视图</title>
      <link href="posts/86059ffb/"/>
      <url>posts/86059ffb/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://docs.djangoproject.com/zh-hans/3.2/ref/contrib/admin/">Django 管理站点 | Django 文档 | Django (djangoproject.com)</a></p><h2 id="基本使用">基本使用</h2><p>若要在管理视图中看到某个模型，则需要先注册</p><p><code>&lt;app&gt;.admin</code></p><p>例如注册<code>Author</code></p><pre><code class="language-python">from django.contrib import adminfrom myproject.myapp.models import Authorclass AuthorAdmin(admin.ModelAdmin):    list_display = []    list_filter = []    search_fileds = ()admin.site.register(Author, AuthorAdmin)</code></pre><p>其中类<code>AuthorAdmin</code>通过继承<code>admin.ModelAdmin</code>获取默认的一些方法，这些默认方法可以查看：</p><p><a href="https://docs.djangoproject.com/zh-hans/3.2/ref/contrib/admin/#custom-template-options">自定义|Django 管理站点 | Django 文档 | Django (djangoproject.com)</a></p><h2 id="自定义">自定义</h2><h3 id="自定义侧边栏过滤器">自定义侧边栏过滤器</h3><p><a href="https://docs.djangoproject.com/zh-hans/3.2/ref/contrib/admin/#django.contrib.admin.ModelAdmin.list_filter">list_filter | Django 管理站点 | Django 文档 | Django (djangoproject.com)</a></p><p>例如资源表有字段<code>user</code>,<code>user</code>是 User 表的外键，确保过滤器里仅显示拥有资源的用户。</p><p>则需要设置一个元组，第一个元素是字段<code>user</code>，第二个元素是<code>admin.RelatedOnlyFieldListFilter</code>。</p><p>第二个元素继承自 <code>django.contrib.admin.FieldListFilter</code></p><pre><code class="language-python">    list_filter = (        ('user', admin.RelatedOnlyFieldListFilter),    )</code></pre><blockquote><p>列表过滤器通常只有在过滤器有多个选择时才会出现</p></blockquote><h3 id="过滤展示数据">过滤展示数据</h3><p>例如登录用户如果是超级用户，则展示所有数据；</p><p>登录用户如果是普通用户，则展示自己的数据</p><pre><code class="language-python">class MyModelAdmin(admin.ModelAdmin):    def get_queryset(self, request):        qs = super().get_queryset(request)         if request.user.is_superuser:            return qs        return qs.filter(user=request.user)</code></pre><p>这里利用<code>super()</code>调用父类<code>ModelAdmin</code>的<code>get_queryset</code>方法返回<a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/querysets/#queryset-api">QuerySet</a>的所有模型实例.</p><p><code>QuerySet</code>有很多细化方法，例如上述代码里的<code>filter</code>，关于细化方法里的参数格式，则参考<a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/querysets/#field-lookups">Field | QuerySet</a></p><h3 id="表格保存前后添加额外逻辑">表格保存前后添加额外逻辑</h3><p><code>save_model</code>方法被赋予 <code>HttpRequest</code>、一个模型实例、一个 <code>ModelForm</code> 实例和一个基于是否添加或更改对象的布尔值。覆盖这个方法可以进行保存前或保存后的操作。</p><p>例如保存前，自动将模型<code>user</code>字段设置为登录用户</p><pre><code class="language-python">    def save_model(self, request, obj, form, change):        obj.user = request.user  # 保存前的逻辑        super().save_model(request, obj, form, change)        pass # 保存后的逻辑</code></pre><h3 id="设定表格操作权限">设定表格操作权限</h3><p>设定普通登录用户也拥有表格修改和删除权限</p><pre><code class="language-python">    def has_change_permission(self, request, obj=None):        if not obj:            return True # So they can see the change list page        if request.user.is_superuser or obj.user == request.user:            return True        else:            return False        has_delete_permission = has_change_permission  </code></pre><p><code>if not obj</code>返回<code>True</code>，确保模型没有数据的时候，用户也可以看到</p><p><code>if request.user.is_superuser or obj.user == request.user:</code>，确保超级用户和登录登录用户，可以自行修改</p><p>最后其它用户没有权限访问页面</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞restful简单使用</title>
      <link href="posts/dd4c9efa/"/>
      <url>posts/dd4c9efa/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>官方快速教程：<a href="https://www.django-rest-framework.org/tutorial/quickstart/">Quickstart - Django REST framework (django-rest-framework.org)</a></p><h2 id="安装">安装</h2><pre><code class="language-bash">pip install djangorestframework</code></pre><h2 id="配置">配置</h2><p><code>settings.INSTALLED_APPS</code>加入<code>'rest_framework.authtoken','rest_framework'</code></p><blockquote><p>其中<code>rest_framework.authtoken</code>是简单的 Token 认证</p></blockquote><p><code>settings.py</code> 中加入以下代码：</p><pre><code class="language-python">REST_FRAMEWORK = &#123;    'DEFAULT_AUTHENTICATION_CLASSES': [        'rest_framework.authentication.BasicAuthentication',        'rest_framework.authentication.SessionAuthentication',        'rest_framework.authentication.TokenAuthentication'  # 开启 Token 认证    ],    'DEFAULT_PERMISSION_CLASSES': [        'rest_framework.permissions.IsAuthenticated', # 全局接口权限：开启全局认证    ],    # 'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.PageNumberPagination', # 接口分页    # 'PAGE_SIZE': 10&#125;</code></pre><h2 id="具体">具体</h2><p><img src="/posts/dd4c9efa/image-20210805152047616.png" alt="image-20210805152047616"></p><p>大概的界面就如上图所示：</p><ol><li>右上角是一个登录UI</li><li>正面是请求地址/方法之类的，以及返回的数据</li></ol><h3 id="关于右上角的登录UI">关于右上角的登录UI</h3><pre><code class="language-python"># 项目根APP里的 urls.pyfrom django.urls import path, includeurlpatterns += [    path('api-auth/', include('rest_framework.urls')), ]</code></pre><h3 id="大致的一个流程">大致的一个流程</h3><p>Request☞DjangoUrLs☞AppRootUrL☞APIUrL☞APIView☞API序列化☞DjangoModel</p><p>假设你的API地址是 /api-user/user</p><h3 id="DjangoUrLs路由">DjangoUrLs路由</h3><blockquote><p>项目根路由</p></blockquote><pre><code class="language-python">urlpatterns += [    path('api-user/', include(loginurls, namespace=&quot;login&quot;)),]</code></pre><h3 id="AppRoutUrL">AppRoutUrL</h3><blockquote><p>loginurls文件：App根路由</p></blockquote><pre><code class="language-python">app_name=&quot;login&quot;from rest_framework.decorators import api_viewfrom rest_framework.response import Responsefrom rest_framework.reverse import reverse@api_view(['GET'])def api_root(request, format=None):    return Response(&#123;        'users': reverse('login:user-list', request=request, format=format),        'snippets': reverse('login:asset-list', request=request, format=format)    &#125;)# API根路由urlpatterns += [    path('', api_root),]</code></pre><h3 id="APIURL">APIURL</h3><pre><code class="language-python">user_list = views.UserViewSet.as_view(&#123;    'get': 'list'&#125;)user_detail = views.UserViewSet.as_view(&#123;    'get': 'retrieve'&#125;)urlpatterns += [    path('user/', user_list, name='user-list'),    path('user/&lt;int:pk&gt;/', user_detail, name='user-detail')]</code></pre><h3 id="APIViews">APIViews</h3><pre><code class="language-python">from login import models as loginmodelsfrom login.serializers import UserSerializerfrom django.contrib.auth.models import Userfrom rest_framework import permissionsfrom rest_framework import viewsetsclass UserViewSet(viewsets.ModelViewSet):    &quot;&quot;&quot;    This viewset automatically provides `list` and `retrieve` actions.    &quot;&quot;&quot;    queryset = User.objects.all()    serializer_class = UserSerializer    permission_classes = [permissions.IsAdminUser]</code></pre><h3 id="API序列化">API序列化</h3><pre><code class="language-python">from rest_framework import serializersfrom login import models as loginmodelsfrom django.contrib.auth.models import Userclass UserSerializer(serializers.HyperlinkedModelSerializer):    # 显示的定义两个数据字段的查找方法，一个是user的关联外键 user_asset，一个是 HyperlinkedModelSerializer 所需的链接字段 url    user_asset = serializers.HyperlinkedRelatedField(     # user_asset 是 User主表的外键名(related_name)        view_name=&quot;login:asset-detail&quot;, read_only=True    )    url = serializers.HyperlinkedIdentityField(        view_name='login:user-detail',   # 默认 HyperlinkedModelSerializer 的 url 字段查找的视图名是 &#123;model_name&#125;-detail。 这里显示指定视图    )    class Meta:        model = User        fields = ['url', 'id', 'username', 'user_asset']  # api返回的数据</code></pre><h2 id="简单的Token认证">简单的Token认证</h2><blockquote><p>通过API拿到数据，一般我们会开启认证，例如Token方式</p></blockquote><pre><code class="language-python">&quot;&quot;&quot;Token URL&quot;&quot;&quot;from login import views as loginviewsurlpatterns += [    path('api/gettoken/', loginviews.CustomAuthToken.as_view()), # 通过发起POST获取token，&#123;&quot;username&quot;: &quot;xxx&quot;, &quot;password&quot;:xxx&#125;]</code></pre><pre><code class="language-python">&quot;&quot;&quot;views&quot;&quot;&quot;from rest_framework.authtoken.views import ObtainAuthTokenfrom rest_framework.authtoken.models import Tokenfrom rest_framework.response import Responseclass CustomAuthToken(ObtainAuthToken):    #throttle_classes =     def post(self, request, *args, **kwargs):        serializer = self.serializer_class(data=request.data,                                        context=&#123;'request': request&#125;)        serializer.is_valid(raise_exception=True)        user = serializer.validated_data['user']        token, created = Token.objects.get_or_create(user=user)        return Response(&#123;            'token': token.key,            'user_id': user.pk,            'email': user.email        &#125;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jquery☞伪进度条</title>
      <link href="posts/5d527900/"/>
      <url>posts/5d527900/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>点击-&gt;触发脚本jquery-&gt;脚本访问url-&gt;django view(show_progress)-&gt;view返回数据给脚本jquery-&gt;脚本jquery拿到进度数据百分比-&gt;更新进度条-&gt;重复访问url-&gt;直至进度条数据100%-&gt;终止访问url.</p><h2 id="具体步骤">具体步骤</h2><h3 id="点击">点击</h3><blockquote><p>点击提交按钮，通过id “submit” 找到脚本</p></blockquote><pre><code class="language-html">        &lt;div class=&quot;item1&quot;&gt;          &lt;a href=&quot;&#123;% url 'assets:ssllist' %&#125;&quot;&gt;返回&lt;/a&gt;          &lt;button id=&quot;submit&quot; class=&quot;mybtn&quot; type=&quot;submit&quot;&gt;提交&lt;/button&gt;        &lt;/div</code></pre><h3 id="脚本jquery">脚本jquery</h3><pre><code class="language-javascript">    &lt;script&gt;    $(function () &#123;      $('#submit').on('click', function () &#123;     // 触发 click 动作，执行 function        $('#prog_out').attr(&quot;class&quot;,&quot;progress&quot;);      var sitv = setInterval(function()&#123;    // 添加一个间隔期函数setInterval，它有两个参数，参数1是执行函数function，参数2是间隔时间              var show_progress_url = &quot;&#123;% url 'assets:showprogress' %&#125;&quot;; // 变量 show_progress_url                $.getJSON(show_progress_url, function(res)&#123;    // 访问 show_progress_url，并将view视图的结果res传递给 function 函数，并执行 function(res)                 if(res === 100)&#123;       // 若 view视图返回100，则修改动作条颜色为 success，并且清空计时器并退出                  $('#prog_out').attr(&quot;class&quot;, &quot;progress progress-bar bg-success&quot;);                  clearInterval(sitv);                &#125;                console.log(&quot;currect progresspct:&quot;+res);  // 打印当前进度                  $('#prog_in').width(res + '%');     // 改变进度条进度，注意这里是内层的div， res是后台返回的进度                  $('#prog_in').val(res);   //改变进度条的值                &#125;);              &#125;,           1000);   // 间隔期函数setInterval,第二个参数，每1秒查询一次后台进度      &#125;);    &#125;)    &lt;/script&gt;</code></pre><h3 id="脚本访问URL">脚本访问URL</h3><p>通过上述JS脚本，访问show_progress_url变量</p><h3 id="django-view视图">django view视图</h3><pre><code class="language-python">def show_progress(request):    if request.user.is_authenticated:        loginuser = request.user.username        #progresspct = cache.get_or_set(loginuser,0)        print('当前创建证书的用户是:' + loginuser)        progresspct = cache.get(loginuser)        print(&quot;百分比:&#123;0&#125;&quot;.format(progresspct))        if progresspct == 100:            print('删除缓存')            cache.delete(loginuser)            return JsonResponse(progresspct, safe=False)        progresspct += 1        cache.set(loginuser,progresspct)        return JsonResponse(progresspct, safe=False)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> jquery </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jquery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python☞json</title>
      <link href="posts/876c5852/"/>
      <url>posts/876c5852/</url>
      
        <content type="html"><![CDATA[<h2 id="Json与python之间类型关系">Json与python之间类型关系</h2><table><thead><tr><th><strong>JSON类型</strong></th><th><strong>python类型</strong></th></tr></thead><tbody><tr><td><strong>{}</strong></td><td><strong>dict</strong></td></tr><tr><td><strong>[]</strong></td><td><strong>list</strong></td></tr><tr><td><strong>“string”</strong></td><td><strong>str</strong></td></tr><tr><td><strong>“123456”</strong></td><td><strong>int或float</strong></td></tr><tr><td><strong>true/false</strong></td><td><strong>True/False</strong></td></tr><tr><td><strong>null</strong></td><td><strong>None</strong></td></tr></tbody></table><h2 id="序列化">序列化</h2><pre><code class="language-python">import jsonjson.dumps(python_dict)  # 将py类型转为json字符串类型</code></pre><h2 id="反序列化">反序列化</h2><pre><code class="language-python">import jsonjson.loads(json_str)  # 将json字符串类型转为py类型</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> json </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>django☞验证码</title>
      <link href="posts/d60c38c/"/>
      <url>posts/d60c38c/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><a href="https://django-simple-captcha.readthedocs.io/en/latest/advanced.html#configuration-toggles">高级主题 — Django Simple Captcha 0.5.14 文档 (django-simple-captcha.readthedocs.io)</a></p><h2 id="安装">安装</h2><pre><code class="language-python">pip install django-simple-captcha</code></pre><p><code>settings.INSTALLED_APPS</code>加入<code>captcha</code></p><h2 id="配置">配置</h2><p><code>settings.py</code>末尾中加入以下代码：</p><pre><code class="language-python"># 验证码设置CAPTCHA_FIELD_TEMPLATE = &quot;captcha/field.html&quot; # 字段排列CAPTCHA_TEXT_FIELD_TEMPLATE = &quot;captcha/text_field.html&quot; # 文本输入框#CAPTCHA_HIDDEN_FIELD_TEMPLATE = &quot;captcha/hidden_field.html&quot; CAPTCHA_IMAGE_TEMPLATE = &quot;captcha/image.html&quot; # 验证码图标框CAPTCHA_CHALLENGE_FUNCT = 'captcha.helpers.random_char_challenge' # 验证码类型CAPTCHA_IMAGE_SIZE=(100,36) # 验证码尺寸CAPTCHA_TIMEOUT=1#CAPTCHA_OUTPUT_FORMAT=u'%(text_field)s %(hidden_field)s %(image)s'</code></pre><h2 id="具体">具体</h2><p>以<code>app:login</code>为例，在其根目录构建以下内容</p><ol><li>构建<code>templates/captcha</code>目录，并创建<code>field.html</code>、<code>text_field.html</code>、<code>image.html</code>模板</li></ol><pre><code class="language-bash">❯ cat field.html&#123;&#123; text_field &#125;&#125;&#123;&#123; hidden_field &#125;&#125;&#123;&#123; image &#125;&#125;❯ cat text_field.html&lt;input id=&quot;id_&#123;&#123; name &#125;&#125;_1&quot; name=&quot;&#123;&#123; name &#125;&#125;_1&quot; type=&quot;text&quot; placeholder=&quot;captcha&quot; /&gt;❯ cat image.html&lt;img src=&quot;&#123;&#123; image &#125;&#125;&quot; alt=&quot;&#123;&#123; name &#125;&#125;&quot; class=&quot;&#123;&#123; name &#125;&#125;&quot;/&gt;</code></pre><ol start="2"><li>构建<code>static/login/js</code>目录，并创建<code>captcha.js</code>，用于动态刷新验证码</li></ol><pre><code class="language-bash">❯ cat captcha.js$('img.captcha').click(function() &#123;  $.getJSON('/captcha/refresh/',function(json) &#123; // This should update your captcha image src and captcha hidden input      console.log(json);      $(&quot;img.captcha&quot;).attr(&quot;src&quot;,json.image_url);      $(&quot;#id_captcha_0&quot;).val(json.key);    &#125;);    return false;&#125;);</code></pre><ol start="3"><li>创建<code>form</code>表单调用<code>captcha</code></li></ol><pre><code>❯ cat forms.py&quot;&quot;&quot;导入表单模块&quot;&quot;&quot;from django import formsfrom captcha.fields import CaptchaField, CaptchaTextInputclass UserForm(forms.Form):    &quot;&quot;&quot;    用户表单    &quot;&quot;&quot;    username = forms.CharField(label='用户名', max_length=128, widget=forms.TextInput(attrs=&#123;'class': 'form-control'&#125;))    password = forms.CharField(label='密码', max_length=256, widget=forms.PasswordInput(attrs=&#123;'class': 'form-control'&#125;))    captcha = CaptchaField(label='验证码')</code></pre><ol start="4"><li>构建<code>templates/login</code>目录，并创建<code>login.html</code>模板调用<code>caphtcha</code></li></ol><pre><code class="language-html">&#123;% load static %&#125;&lt;!doctype html&gt;&lt;html lang=&quot;en&quot;&gt;  &lt;head&gt;    &lt;!-- Required meta tags --&gt;    &lt;meta charset=&quot;utf-8&quot;&gt;    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, shrink-to-fit=no&quot;&gt;    &lt;!-- 上述meta标签*必须*放在最前面，任何其他内容都*必须*跟随其后！ --&gt;    &lt;!-- Bootstrap CSS --&gt;    &lt;link href=&quot;&#123;% static 'login/css/login.css' %&#125;&quot; rel=&quot;stylesheet&quot; /&gt;    &lt;link href=&quot;&#123;% static 'fontawesome_free/css/all.min.css' %&#125;&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;    &lt;title&gt;登录&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;    &lt;div id=&quot;container&quot;&gt;      &lt;h1&gt;登录&lt;/h1&gt;      &#123;% if login_form.captcha.errors %&#125;        &#123;% for error in login_form.captcha.errors %&#125;        <small>😢😢😢&#123;&#123; error &#125;&#125;😢😢😢</small>        &#123;% endfor %&#125;      &#123;% elif message %&#125;        <small>😢😢😢&#123;&#123; message &#125;&#125;😢😢😢</small>      &#123;% else %&#125;        <small>😊😊😊欢迎您的到来😊😊😊</small>      &#123;% endif %&#125;      &lt;form class=&quot;form&quot; action=&quot;/login/&quot; method=&quot;post&quot;&gt;&#123;% csrf_token %&#125;        &lt;!-- 判断用户输入信息，并返回错误信息 --&gt;        &#123;% for item in login_form %&#125;          &#123;% if item.html_name == "username" %&#125;          <div class="item">            <i class="far fa-user"></i>            <input id="&#123;&#123; item.id_for_label &#125;&#125;" name="&#123;&#123; item.html_name &#125;&#125;" type="&#123;&#123; item.field.widget.input_type &#125;&#125;" placeholder="&#123;&#123; item.html_name &#125;&#125;">            <text>            &#123;% for error in item.errors %&#125;                &#123;&#123; error &#125;&#125;            &#123;% endfor %&#125;            &lt;/text&gt;          &lt;/div&gt;          &#123;% endif %&#125;          &#123;% if item.html_name == "password" %&#125;          <div class="item">            <i class="fas fa-key"></i>            <input id="&#123;&#123; item.id_for_label &#125;&#125;" name="&#123;&#123; item.html_name &#125;&#125;" type="&#123;&#123; item.field.widget.input_type &#125;&#125;" placeholder="&#123;&#123; item.html_name &#125;&#125;">            <text>            &#123;% for error in item.errors %&#125;                &#123;&#123; error &#125;&#125;            &#123;% endfor %&#125;            </text>          </div>          &#123;% endif %&#125;          &#123;% if item.html_name == "captcha" %&#125;          <div class="item">            <i class="fas fa-robot"></i>            &#123;&#123; login_form.captcha &#125;&#125;            </div>            &#123;% endif %&#125;        &#123;% endfor %&#125;        &lt;div class=&quot;item1&quot;&gt;          &lt;button type=&quot;submit&quot;&gt;Go~~~~&lt;/button&gt;          &lt;a href=&quot;/register/&quot;&gt;Register&lt;/a&gt;        &lt;/div&gt;      &lt;/form&gt;    &lt;/div&gt;    &lt;script src=&quot;https://cdn.bootcss.com/jquery/3.5.1/jquery.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;https://cdn.bootcss.com/popper.js/1.16.0/umd/popper.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;https://cdn.bootcss.com/twitter-bootstrap/4.5.3/js/bootstrap.min.js&quot;&gt;&lt;/script&gt;    &lt;script src=&quot;&#123;% static 'login/js/captcha.js' %&#125;&quot;&gt;&lt;/script&gt;  &lt;/body&gt;&lt;/html&gt;</text></div></code></pre><ol start="5"><li>构建<code>static/login/css</code>目录，并创建<code>login.css</code></li></ol><pre><code class="language-bash">❯ cat login.cssbody &#123;background: url('../image/backend.jpg') center center no-repeat;background-attachment: fixed;background-size: cover;&#125;input::-webkit-input-placeholder&#123;    color:#000000;&#125;input::-moz-placeholder&#123;   /* Mozilla Firefox 19+ */    color:#000000;&#125;input:-moz-placeholder&#123;    /* Mozilla Firefox 4 to 18 */    color:#000000;&#125;input:-ms-input-placeholder&#123;  /* Internet Explorer 10-11 */    color:#000000;&#125;#container &#123;    width: 35%;    height: 400px;    margin: 0 auto;    margin-top: 15%;    padding: 20px 50px;    text-align: center;    background: #ffffff50;&#125;#container .alert &#123;    margin: 0 auto;    width: 350px;    text-align: left;&#125;#container .form &#123;    margin-top: 30px;&#125;#container .form .item &#123;    margin-top: 15px;    position: relative;&#125;#container .form .item i &#123;    font-size: 20px;&#125;#container .form .item input &#123;    border: 0;    border-bottom: 2px solid #000;    padding: 10px 30px;    background: #ffffff00;    font-size: 20px;    padding-left: 11px;&#125;#container .form .item text &#123;    position: absolute;    text-size: 5px;    text-align: left;    padding-top: 10px;&#125;#container .form .item img &#123;    position: absolute;&#125;#container .form .item1 &#123;    display: flex;    justify-content: space-between;    margin-top: 15px;    padding-top: 5%;    padding-left: 22%;    padding-right: 22%;&#125;#container .form .item1 button &#123;    width: 180px;    height: 30px;    font-size: 20px;    font-weight: 600;    color: #ffffff;    background-image: linear-gradient(60deg, #64b3f4 0%, #c2e59c 100%);    border-radius: 15px;    border: 0;&#125;#container .form .item1 a &#123;    text-decoration:none;    width: 100px;    font-size: 20px;    font-weight: 600;    color: #ffffff;    background-image: linear-gradient(60deg, #64b3f4 0%, #c2e59c 100%);    border-radius: 15px;    border: 0;&#125;</code></pre><ol start="6"><li>添加路由</li></ol><p><code>项目.urls.urlpatterns</code>中添加<code>path('captcha/', include('captcha.urls')),</code></p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> django </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞时间</title>
      <link href="posts/cd32aa48/"/>
      <url>posts/cd32aa48/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>本文中描述的并不能涵盖所有时间设置命令，只是记录了经常用到的一些。</p><h2 id="时间同步">时间同步</h2><pre><code>sudo yum erase ntp*sudo yum -y install chronysudo service chronyd startsed -i '1i server 10.200.16.101 prefer iburst' /etc/chrony.conf</code></pre><blockquote><p>10.200.16.101 替换成ntp服务器</p></blockquote><h2 id="时区修改">时区修改</h2><p>如果你用的是7以上的centos或者ubuntu</p><p>那么可以用 timedatectl 命令来设置时区</p><p>timedatectl --list-timezones</p><p>timedatectl --set-timezone Asia/Shanghai</p><hr><h2 id="没有timedatectl命令的话">没有timedatectl命令的话</h2><h4 id="修改会话时区">修改会话时区</h4><pre><code class="language-bash">echo &quot;TZ='UTC+0'; export TZ&quot; &gt;&gt; ~/.bash_profile</code></pre><blockquote><p>需要注意以下几点：</p><ol><li>UTC8 表示西8区</li><li>tzselect 可以帮你查看时区有哪些</li><li>UTC 方式，无法识别冬令时和夏令时，所以建议用地区名称，例如 asia/shanghai</li></ol></blockquote><h4 id="修改crontab时区">修改crontab时区</h4><p>在 crontab 用户配置最上面加入，例如添加芝加哥时区</p><pre><code class="language-bash">TZ='America/Chicago'CRON_TZ='America/Chicago'</code></pre><blockquote><p>关于时区设置方面，不建议修改配置，因为不够灵活</p></blockquote><h4 id="修正时间-写入硬件时钟">修正时间, 写入硬件时钟</h4><pre><code class="language-bash">yum install ntp -yntpdate cn.ntp.org.cnhwclock -wecho '0 12 * * * /usr/sbin/ntpdate cn.ntp.org.cn &gt; /dev/null 2&gt;&amp;1' &gt;&gt; /etc/crontab</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> 时区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日志☞ELK简单部署-容器方式</title>
      <link href="posts/3475106c/"/>
      <url>posts/3475106c/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><ul><li>各组件总下载页: <a href="https://www.elastic.co/cn/downloads/">https://www.elastic.co/cn/downloads/</a></li><li>容器下载页: <a href="https://www.docker.elastic.co">https://www.docker.elastic.co</a></li></ul><ol><li>Elasticsearch 搜索分析 <a href="https://www.elastic.co/cn/downloads/elasticsearch">https://www.elastic.co/cn/downloads/elasticsearch</a></li><li>Logstash 转换输出 <a href="https://www.elastic.co/cn/downloads/logstash">https://www.elastic.co/cn/downloads/logstash</a></li><li>Filebeat 收集 <a href="https://www.elastic.co/cn/downloads/beats/filebeat">https://www.elastic.co/cn/downloads/beats/filebeat</a></li><li>Kibana 展示 <a href="https://www.elastic.co/cn/downloads/kibana">https://www.elastic.co/cn/downloads/kibana</a></li></ol><h4 id="数据过程">数据过程:</h4><p>Filebeat ☞ Logstash ☞ Elasticsearch (master node) + data node ☞ Kibana</p><h4 id="安装步骤">安装步骤</h4><pre><code class="language-bash"># dockeryum install docker -y 或者 yum install docker-ce -yyum install python3-pip -ypip3 install docker-compose 或者 pip install docker-composesystemctl enable docker# 修改 docker 默认数据目录vi /etc/docker/daemon.json&#123;&quot;data-root&quot;: &quot;/export/docker-data-root&quot;&#125;systemctl start docker</code></pre><pre><code class="language-yml"># elasticsearch 具体安装命令和版本请以下载页中对应的docker安装方式页里命令为基准sysctl -a | grep  vm.max_map_count  # 查看是否过小, 如果过小执行下一条echo 'vm.max_map_count=262144' &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -pmkdir -p /export/docker-compose-data/es;touch /export/docker-compose-data/docker-compose.yml;touch /export/docker-compose-data/es/es01.yml;touch /export/docker-compose-data/es/es02.yml;mkdir -p /export/docker-compose-data/kibana;touch /export/docker-compose-data/kibana/kibana.yml;mkdir -p /export/docker-compose-data/logstashtouch /export/docker-compose-data/logstash/logstash.yml;</code></pre><pre><code class="language-bash"># es01 master 和 data# es02 datacat &gt; /export/docker-compose-data/es/es01.yml &lt;&lt; EOFnode.master: truenode.data: truehttp.port: 9200network.host: 0.0.0.0http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; xpack.security.enabled: falseEOFcat &gt; /export/docker-compose-data/es/es02.yml &lt;&lt; EOFnode.master: falsenode.data: truehttp.port: 9200network.host: 0.0.0.0http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; xpack.security.enabled: falseEOFcat &gt; /export/docker-compose-data/kibana/kibana.yml &lt;&lt; EOFxpack.monitoring.ui.container.elasticsearch.enabled: truei18n.locale: zh-CNEOFcat &gt; /export/docker-compose-data/logstash/logstash.yml &lt;&lt; EOFnode.name: logstashhttp.host: 0.0.0.0http.port: 9600log.level: infoconfig.reload.automatic: trueconfig.reload.interval: 10sconfig.support_escapes: falseEOF</code></pre><h4 id="编写-logstash-管道文件">编写 logstash 管道文件</h4><pre><code class="language-yml"># 示例 /export/docker-compose-data/logstash/pipeline/es-curator.confinput &#123;file &#123;path =&gt; &quot;/mnt/info.log&quot;type =&gt; &quot;es-curator&quot;start_position =&gt; &quot;beginning&quot;&#125;&#125;output &#123;if [type] == &quot;es-curator&quot; &#123;elasticsearch &#123;hosts=&gt; [&quot;es01:9200&quot;]index=&gt; &quot;es-curator-%&#123;+YYYY-MM-dd&#125;&quot;&#125;&#125;&#125;</code></pre><h4 id="启动服务docker-compose配置文件">启动服务docker-compose配置文件</h4><blockquote><p><a href="https://docs.docker.com/compose/compose-file/compose-file-v2/">https://docs.docker.com/compose/compose-file/compose-file-v2/</a></p><p>本配置文件参考 2.3 版本, 请勿使用 3.x 版本, 因为它的资源层 deploy, docker-compose命令不支持</p></blockquote><pre><code class="language-yml">cat &gt; /export/docker-compose-data/docker-compose.yml &lt;&lt; EOFversion: '2.3'services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1    container_name: es01    environment:      - node.name=es01      - cluster.initial_master_nodes=es01      - cluster.name=es-sen      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms1536m -Xmx1536m&quot;    ulimits:      memlock:        soft: -1        hard: -1      nofile:        soft: 65536        hard: 65536    volumes:      - esdata01:/usr/share/elasticsearch/data      - /export/docker-compose-data/es/es01.yml:/usr/share/elasticsearch/config/elasticsearch.yml    ports:      - 9200:9200    networks:      - esnet    cpus: 0.5    mem_limit: 3G    memswap_limit: 3G    mem_reservation: 3G    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9200&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failure  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.1    container_name: es02    environment:      - node.name=es02      - discovery.seed_hosts=es01      - cluster.initial_master_nodes=es01      - cluster.name=es-sen      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms1536m-Xmx1536m&quot;    ulimits:      memlock:        soft: -1        hard: -1      nofile:        soft: 65536        hard: 65536    volumes:      - esdata02:/usr/share/elasticsearch/data      - /export/docker-compose-data/es/es02.yml:/usr/share/elasticsearch/config/elasticsearch.yml    networks:      - esnet    cpus: 1    mem_limit: 3G    memswap_limit: 3G    mem_reservation: 3G    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9200&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failure  kibana:    image: docker.elastic.co/kibana/kibana:7.4.1    container_name: kibana    environment:      SERVER_NAME: kibana.sen-sdk.com      SERVER_HOST: 0.0.0.0      ELASTICSEARCH_HOSTS: http://es01:9200    volumes:      - /export/docker-compose-data/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml    ports:      - 5601:5601    networks:      - esnet    cpus: 0.5    mem_limit: 512m    memswap_limit: 512m    mem_reservation: 512m    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:5601&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failure  logstash:    image: docker.elastic.co/logstash/logstash:7.4.1    container_name: logstash    volumes:      - /export/docker-compose-data/logstash/pipeline/:/usr/share/logstash/pipeline/      - /export/docker-compose-data/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml      - logstash:/usr/share/logstash/data    ports:      - &quot;5044:5044&quot;      - &quot;9600:9600&quot;    networks:      - esnet    cpus: 0.5    mem_limit: 1g    memswap_limit: 1g    mem_reservation: 1g    healthcheck:      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9600&quot;]      interval: 1m30s      timeout: 10s      retries: 3      start_period: 120s    restart: on-failurevolumes:  esdata01:    driver: local  esdata02:    driver: local  logstash:    driver: localnetworks:  esnet:EOF</code></pre><h4 id="启动-docker-compose">启动 docker-compose</h4><pre><code class="language-bash">cd /export/docker-compose-data/docker-compose up -d# docker-compose up -d --no-recreate</code></pre>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> docker </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx ☞ 基本认证</title>
      <link href="posts/18de0eed/"/>
      <url>posts/18de0eed/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># 安装 htpasswd 工具yum install httpd-tools -y# 生成密码文件htpasswd -bc /usr/local/nginx/conf/.passwd usera pwd # 创建用户usera, 并写入 .passwdhtpasswd -b /usr/local/nginx/conf.passwd userb pwd  # 追加用户 userbhtpasswd -D /usr/local/nginx/conf/.passwd usera # 删除用户# nginx配置server &#123;    listen       80;    server_name  xxx.com;    index index.html;    location /auth &#123;        auth_basic &quot;nginx auth&quot;;        auth_basic_user_file /usr/local/nginx/conf/.passwd;        alias /export/webapps/xxx.com;    &#125;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞软raid创建</title>
      <link href="posts/a7611bc1/"/>
      <url>posts/a7611bc1/</url>
      
        <content type="html"><![CDATA[<h2 id="linux-☞-软-raid-创建">linux ☞ 软 raid 创建</h2><ul><li>创建</li></ul><pre><code class="language-bash"># 磁盘分区fdisk /dev/sdafdisk /dev/sdb# 构建raid0# --level raid级别# --raid-devices 盘数# --chunk 条带深度，决定了数据分割的标准单位大小，数值越小，则数据越分散，性能越低(如若没有特殊优化需求，建议选默认值即可)mdadm -Cv /dev/md0 --level=0 --raid-devices=2 /dev/sda1 /dev/sdb1# 已上配置中, 也可以不分区, 直接进行 raid 构建mdadm --create --verbose /dev/md0 --level=0 --name=MY_RAID --raid-devices=number_of_volumes device_name1 device_name2# 观察和等待阵列初始化cat /proc/mdstat# 观察初始化后的阵列信息mdadm --detail /dev/md0# 格式化 （加卷标）mke2fs -t ext4 -L raid0 /dev/md0# 写入配置# 不同的操作系统 mdadm.conf 位置不同, 具体以 man mdadm.conf 为准mdadm --detail --scan | tee -a /etc/mdadm.conf# echo &quot;DEVICE /dev/sda1 /dev/sdb1 &quot; &gt;&gt; /etc/mdadm/mdadm.conf# mdadm -Ds &gt;&gt; /etc/mdadm/mdadm.conf# 创建新的 Ramdisk Image 以为新的 RAID 配置正确地预加载块储存设备模块sudo dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)# 写入挂载 （用卷标挂载，有些系统重启后，设备名会从md0变成md127）echo &quot;LABEL=raid0 /data ext4 defaults,nofail 0 2&quot; &gt;&gt; /etc/fstabmkdir /datamount -a# 确认挂载成功df -h</code></pre><ul><li>删除</li></ul><pre><code class="language-bash"># 删除/etc/fstab的挂载信息$ mdadm -S /dev/md0$ mdadm --misc --zero-superblock /dev/sda$ mdadm --misc --zero-superblock /dev/sdb# 删除/etc/mdadm/mdadm.conf文件中添加的DEVICE行和ARRAY行</code></pre><ul><li>额外信息</li></ul><pre><code class="language-bash">#2T以上大小分区parted /dev/sda mklabel gpt mkpart primary1 0% 100%partprobe</code></pre><ul><li>关于云</li></ul><pre><code class="language-bash">当使用云端磁盘构建raid的时候,且又想进行 raid 备份,则务必先停止io操作,停止io操作的方法最好是 umount 或者停机. 否则会导致 raid 数据完整性出现问题.</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> raid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞s3 初始化安装</title>
      <link href="posts/2c5a64ff/"/>
      <url>posts/2c5a64ff/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>脚本的目的：</p><ol><li>创建S3</li><li>添加生命周期</li><li>创建iam规则</li></ol><blockquote><p>s3官方授权示例：<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/example-policies-s3.html">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/example-policies-s3.html</a></p></blockquote><h3 id="主体脚本">主体脚本</h3><pre><code class="language-bash">#!/bin/bash# http://docs.amazonaws.cn/general/latest/gr/rande.html#s3_region# us-east-1 us-west-1 等# 需要s3权限和iam权限# 需要先编写 s3-lifecycle.json#桶名read -p &quot;输入s3桶名=&quot; S3BucketName#所属项目read -p &quot;输入项目名=&quot; TeamNameread -p &quot;输入AWS_ACCESS_KEY_ID=&quot; AWS_ACCESS_KEY_IDread -p &quot;输入AWS_SECRET_ACCESS_KEY=&quot; AWS_SECRET_ACCESS_KEYread -p &quot;输入AWS_DEFAULT_REGION=&quot; AWS_DEFAULT_REGIONexport AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_IDexport AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEYexport AWS_DEFAULT_REGION=$AWS_DEFAULT_REGIONS3LocationConstraint=$&#123;AWS_DEFAULT_REGION&#125;[[ $&#123;S3LocationConstraint&#125; == 'us-east-1' ]] &amp;&amp; aws s3api create-bucket --bucket $&#123;S3BucketName&#125; --region $&#123;S3LocationConstraint&#125; || aws s3api create-bucket --bucket $&#123;S3BucketName&#125; --region $&#123;S3LocationConstraint&#125; --create-bucket-configuration LocationConstraint=$&#123;S3LocationConstraint&#125;aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'conf/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'data/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'backup/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/7days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/15days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/30days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/60days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/90days/'aws s3api put-object --bucket $&#123;S3BucketName&#125; --key 'logs/longlasting/'aws s3api put-bucket-tagging --bucket $&#123;S3BucketName&#125; --tagging &quot;TagSet=[&#123;Key=Team,Value=$&#123;TeamName&#125;&#125;]&quot;aws s3api put-bucket-lifecycle-configuration --bucket $&#123;S3BucketName&#125; --lifecycle-configuration file://s3-lifecycle.jsoncat &gt; s3-program.rule &lt;&lt; EOF&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:GetObject&quot;,                &quot;s3:PutObject&quot;,                &quot;s3:DeleteObject&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/7days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/15days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/30days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/60days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/90days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/longlasting/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/conf/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/data/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/backup/*&quot;            ]        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListBucket&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;        &#125;    ]&#125;EOFcat &gt; s3-local.rule &lt;&lt; EOF&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListBucket&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:PutObject&quot;,                &quot;s3:GetObject&quot;,                &quot;s3:DeleteObject&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/7days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/15days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/30days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/60days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/90days/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/logs/longlasting/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/conf/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/data/*&quot;,                &quot;arn:aws:s3:::$&#123;S3BucketName&#125;/backup/*&quot;            ],            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor2&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:GetBucketLocation&quot;,            &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;S3BucketName&#125;&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor3&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListAllMyBuckets&quot;,            &quot;Resource&quot;: &quot;*&quot;,            &quot;Condition&quot;: &#123;                &quot;ForAnyValue:IpAddress&quot;: &#123;                    &quot;aws:SourceIp&quot;: [                        &quot;1.1.1.1/32&quot;                    ]                &#125;            &#125;        &#125;    ]&#125;EOFaws iam create-policy --policy-name s3-$&#123;S3BucketName&#125;-role --description &quot;For role use only!!!!!!!!!!!!&quot; --policy-document  file://s3-program.ruleaws iam create-policy --policy-name s3-$&#123;S3BucketName&#125;-local --description &quot;Limit the source IP!!!!!!!!!!!!&quot; --policy-document file://s3-local.ruleecho &quot;程序用户规则: s3-$&#123;S3BucketName&#125;-role 已生成&quot;echo &quot;本地用户规则：s3-$&#123;S3BucketName&#125;-local 已生成&quot;echo &quot;请将 local 规则关联到对应个人用户或组&quot;echo &quot;请将 role 规则关联到角色&quot;</code></pre><h3 id="生命周期规则-s3-lifecycle-json">生命周期规则 s3-lifecycle.json</h3><blockquote><p>规则说明：</p><ol><li>所有对象，30天之后转为ONEZONE_IA;</li><li>logs 前缀单独定义：<ul><li>days 路径下的对象保存对应的天数</li><li>longlasting 路径下的对象永久保存，但是 90 天之后的对象转换为 GLACIER</li></ul></li></ol></blockquote><pre><code class="language-shell">&#123;  &quot;Rules&quot;: [      &#123;          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;&quot;          &#125;,           &quot;Status&quot;: &quot;Enabled&quot;,           &quot;Transitions&quot;: [              &#123;                  &quot;Days&quot;: 30,                   &quot;StorageClass&quot;: &quot;ONEZONE_IA&quot;              &#125;          ],           &quot;NoncurrentVersionTransitions&quot;: [              &#123;                  &quot;NoncurrentDays&quot;: 30,                   &quot;StorageClass&quot;: &quot;ONEZONE_IA&quot;              &#125;          ],           &quot;ID&quot;: &quot;30days_onezone_ia&quot;      &#125;,      &#123;          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/longlasting/&quot;          &#125;,           &quot;Status&quot;: &quot;Enabled&quot;,           &quot;Transitions&quot;: [              &#123;                  &quot;Days&quot;: 90,                   &quot;StorageClass&quot;: &quot;GLACIER&quot;              &#125;          ],           &quot;ID&quot;: &quot;logs_90day_glacier&quot;      &#125;,       &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/7days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 7          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_7days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;:  1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/15days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 15          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_15days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;:  1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/30days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 30          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_30days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/60days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 60          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_60days_before&quot;      &#125;,      &#123;          &quot;Status&quot;: &quot;Enabled&quot;,          &quot;NoncurrentVersionExpiration&quot;: &#123;              &quot;NoncurrentDays&quot;: 1          &#125;,          &quot;Filter&quot;: &#123;              &quot;Prefix&quot;: &quot;logs/90days/&quot;          &#125;,          &quot;Expiration&quot;: &#123;              &quot;Days&quot;: 90          &#125;,          &quot;AbortIncompleteMultipartUpload&quot;: &#123;              &quot;DaysAfterInitiation&quot;: 7          &#125;,          &quot;ID&quot;: &quot;logs_delete_90days_before&quot;      &#125;  ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> s3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker☞01安装</title>
      <link href="posts/b7cf7b1f/"/>
      <url>posts/b7cf7b1f/</url>
      
        <content type="html"><![CDATA[<h2 id="依赖">依赖</h2><pre><code class="language-bash">yum install -y yum-utils \  device-mapper-persistent-data \  lvm2</code></pre><h2 id="仓库">仓库</h2><blockquote><p>官方 repo: <a href="https://download.docker.com/linux/centos/docker-ce.repo">https://download.docker.com/linux/centos/docker-ce.repo</a></p></blockquote><pre><code class="language-bash:">yum-config-manager \    --add-repo \    http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</code></pre><h2 id="安装">安装</h2><pre><code class="language-bash">yum install docker-ce -y### 安装 compose ， 可选yum install python3-pip -ypip3 install --upgrade pip &amp;&amp; pip3 install docker-composepip3 install docker-compose### 开机自启动systemctl enable docker</code></pre><h3 id="安装-runlike-命令">安装 runlike 命令</h3><p>runlike命令可以输出docker启动的时候的指令</p><pre><code class="language-bash">pip3 install runlikerunlike &lt;container_id&gt; # 输出run指令不过需要注意的是，这个命令并不完美，比如输出不了mount参数，因为你还需要通过docker inspect &lt;container_id&gt;来查看mount指令</code></pre><h2 id="修改-docker-默认数据目录">修改 docker 默认数据目录</h2><pre><code class="language-bash">[[ -f /etc/docker/daemon.json ]] &amp;&amp; mv /etc/docker/daemon.json /etc/docker/daemon.json.default || &#123; mkdir -p /etc/docker/ &amp;&amp;  touch /etc/docker/daemon.json &#125;cat &gt;  /etc/docker/daemon.json &lt;&lt; EOF&#123;  &quot;registry-mirrors&quot;: [&quot;http://hub-mirror.c.163.com&quot;],  &quot;data-root&quot;: &quot;/export/docker-data-root&quot;&#125;EOF</code></pre><h2 id="启动">启动</h2><pre><code class="language-bash">systemctl start docker</code></pre><h2 id="镜像库">镜像库</h2><pre><code class="language-bash">docker hubquay.io</code></pre>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s☞01常见术语概念</title>
      <link href="posts/aaf9b7/"/>
      <url>posts/aaf9b7/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><p><a href="https://kubernetes.io/zh/docs/concepts/overview/components/">Kubernetes 组件 | Kubernetes</a></p><p><a href="https://kubernetes.io/zh/docs/reference/glossary/?all=true">词汇表 | Kubernetes</a></p><h2 id="结构图">结构图</h2><p><img src="https://d33wubrfki0l68.cloudfront.net/2475489eaf20163ec0f54ddc1d92aa8d4c87c96b/e7c81/images/docs/components-of-kubernetes.svg" alt="Kubernetes 组件"></p><ol><li><p>Control Plane 是 k8s 的 master 节点，负责协调集群中的所有活动，例如调度应用程序、维护应用程序的所需状态、扩展应用程序和滚动更新。</p><blockquote><p>control plane 和 master 是同义</p></blockquote></li><li><p>Node 是 k8s 的工作节点，也就是实际主要跑容器的节点。</p></li></ol><h2 id="服务">服务</h2><p>Control Plane 节点组件服务</p><ol><li>kube-apiserver  是k8s 的 api 服务，是集群入口，提供 http rset 服务。</li><li>kube-controller-manager 维护集群状态（故障检测/滚动升级/node扩展等），是集群所有资源对象的自动化管理中心。</li><li>kube-scheduler 是负责 pod 的调度, 即如何将 pod 分发到 node</li><li>etcd 注册中心，保存集群状态。</li></ol><p>Node 节点组件服务</p><ol><li><p>kubelet 是节点代理程序，部署在 node 上。它是k8s master和k8s node之间的纽带。处理 pod 创建/启动/监控/重启/销毁等工作。</p><p>开启 register-node = true 的情况下 会自动向 apiserver 服务注册自己。</p></li><li><p>kube-proxy 是节点的网络代理，是 service 资源对象的一部分功能实现，解决节点上各资源的通信</p></li></ol><h2 id="资源">资源</h2><ol><li><p>Namespace</p><p>命名空间是一个逻辑概念，起到了隔离作用。通过它，你可以找到它内部的其它资源对象，例如 pod，svc，deployment等</p></li><li><p>Pod</p><p>pod 内包含多个容器，所以多个容器共享以下资源。</p><ul><li>PID命名空间: pod内的进程能互相看到PID</li><li>网络命名空间: pod中的多个容器共享一个ip (唯一)</li><li>IPC命名空间: pod中的多个容器之间可以互相通信</li><li>UTS命名空间: pod中的多个容器共享一个主机名 (唯一)</li><li>存储卷: pod多个容器可以共同访问pod定义的存储卷</li></ul></li><li><p>Label</p><p>label用于表示资源，从而方便的被其它资源找到。它很重要。</p><p>标签定义 <code>key: value</code></p><p>选择器: <code>key &lt;= !=&gt; value</code>  <code>key &lt;not&gt; in (value1, value2)</code></p><p>ReplicaSet  和 Service 通过 selector 选择器来选择 Pod 对象, 从而精细化的将 Pod 进行分组。一旦某个 pod 的 Label 被修改，那么这个 pod 将从 ReplicaSet 中脱离，而 ReplicaSet 会重新创建新的 pod 补足 ReplicaSet 定义的副本数。</p></li><li><p>Deployment</p><p>无状态副本集。用来部署一个 pod 组，它通过 ReplicaSet 来进行缩放， 并需要 pod 模板创建 pod， 需要 Label 监控 pod。Deployment 通过 ReplicaSet 严格执行配置所定义的pod副本数量。应该始终使用 ReplicaSet 来创建 pod，因为通过 ReplicaSet 可以方便的控制 pod。Deployment 可以简单的比喻为 aws 的目标组或者nginx的upstream组。</p></li><li><p>Satefulset</p><p>有状态副本集。</p></li><li><p>Service</p><p>svc 用来构建一个负载配置。可以比喻为aws的elb内网负载均衡器或者nginx的service proxy配置。</p><p>svc 通过 pod 定义的 label 发现一组 pod。这些 pod 本身有 endpoint 地址。</p><p>svc 创建后，会拿到一个集群内部ip和dns。kube-proxy 服务会将 svc 的 ip 和 pod 的 endpoint 地址关联起来。</p><p>最终，其它容器可以通过这个ip和dns来访问svc关联的pod资源。</p></li><li><p>Ingress</p><p>管理集群服务外部访问的API对象，典型的访问方式是 HTTP</p></li></ol><h2 id="服务组件流程图">服务组件流程图</h2><blockquote><ul><li>apiserver 负责 etcd 存储的所有操作，且只有 apiserver 才直接操作 etcd 集群</li><li>apiserver 对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件均通过 apiserver 进行通信<ul><li>controller manager、scheduler、kube-proxy 和 kubelet 等均通过 apiserver watch API 监测资源变化情况，并对资源作相应的操作</li><li>所有需要更新资源状态的操作均通过 apiserver 的 REST API 进行</li></ul></li><li>apiserver 也会直接调用 kubelet API（如 logs, exec, attach 等），默认不校验 kubelet 证书，但可以通过 <code>--kubelet-certificate-authority</code> 开启（而 GKE 通过 SSH 隧道保护它们之间的通信）</li></ul></blockquote><p><img src="/posts/aaf9b7/image-20200823102314477.png" alt="image-20200823102314477"></p><p><img src="/posts/aaf9b7/image-20200823103640732.png" alt="image-20200823103640732"></p><blockquote><p>以一个pod的构建为例:</p><ul><li>用户通过 REST API 创建一个 Pod</li><li>apiserver 将其写入 etcd</li><li>scheduluer 检测到未绑定 Node 的 Pod，开始调度并更新 Pod 的 Node 绑定</li><li>kubelet 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod</li><li>kubelet 通过 container runtime 取到 Pod 状态，并更新到 apiserver 中</li></ul></blockquote><p><img src="/posts/aaf9b7/image-20200823102430144.png" alt="image-20200823102430144"></p>]]></content>
      
      
      <categories>
          
          <category> 容器 </category>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql☞安装.</title>
      <link href="posts/b824edd8/"/>
      <url>posts/b824edd8/</url>
      
        <content type="html"><![CDATA[<p><a href="https://hub.docker.com/_/mysql?tab=description">https://hub.docker.com/_/mysql?tab=description</a></p><pre><code class="language-bash">mkdir -p /export/docker-data-mysql/&#123;config,logs,mysql,mysql-files&#125;docker run --name mysql8 \-p 3306:3306 \--restart=always \--mount 'type=bind,src=/export/docker-data-mysql/config,dst=/etc/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/logs,dst=/var/log/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/mysql,dst=/var/lib/mysql' \--mount 'type=bind,src=/export/docker-data-mysql/mysql-files,dst=/var/lib/mysql-files' \-e MYSQL_ROOT_PASSWORD=123456 \-d mysql:8 \--character-set-server=utf8mb4 \--collation-server=utf8mb4_general_ci</code></pre><blockquote><p>-d mysql:5.6</p></blockquote><p>开启远程访问</p><pre><code class="language-sql">alter user 'root'@'%' identified with mysql_native_password by '123456';</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> mysql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞EC2国内外之间迁移</title>
      <link href="posts/2e304ad6/"/>
      <url>posts/2e304ad6/</url>
      
        <content type="html"><![CDATA[<ul><li><p>aws 国内和国外之间无法直接复制 AMI，所以经咨询 aws 技术人员，就有了这么一道手工饭。。。</p></li><li><p>命令步骤如下：</p><pre><code class="language-shell">#源机器上：dd if=/dev/源根盘 of=/非根挂载点/文件A bs=1M#aws EC服务器上：dd if=/文件A of=/dev/盘B bs=1M oflag=direct#盘B打快照，再根据盘B快照生成AMI</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ec2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞rds-mysql参数</title>
      <link href="posts/438aa722/"/>
      <url>posts/438aa722/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash"># 下列是非默认值character_set_client: utf8character_set_connection: utf8character_set_database: utf8character_set_filesystem: utf8character_set_results: utf8character_set_server: utf8collation_connection: utf8_general_cislow_query_log: 1long_query_time: 10log_output: FILE# 下列是默认值explicit_defaults_for_timestamp: '1'innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'innodb_file_per_table: '1'innodb_flush_method: O_DIRECTbinlog_cache_size: '32768'binlog_format: MIXED</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞利用cf服务初始化aws环境</title>
      <link href="posts/ddc4c969/"/>
      <url>posts/ddc4c969/</url>
      
        <content type="html"><![CDATA[<blockquote><p>因公司小项目多，账户多，每次构建aws初始化环境均很麻烦，且每个人操作的时候，想法都不一样，规则不统一，导致环境信息无法通用.</p><p>所以准备使用 cf 来构建一个通用模板，保持环境一致性.</p><p>之前一直没有使用，是因为 cf 本身编写不是太方便，然而我低估了 aws 的牛逼，仔细看了文档才知道， 有个 cf 本身有个堆栈，可以直接复刻当前环境.</p><p>这个工具就是 <code>cloudformer</code></p><p>官方文档是: <a href="https://docs.aws.amazon.com/zh_cn/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html#launch-cloudformer">https://docs.aws.amazon.com/zh_cn/AWSCloudFormation/latest/UserGuide/cfn-using-cloudformer.html#launch-cloudformer</a></p></blockquote><h2 id="那么步骤来了">那么步骤来了~</h2><ul><li><p>先通过 web 控制台生成一个适合本地化的通用环境，比如 vpc，子网，安全组，数据库子网组，参数组，s3以及相应的s3生命周期规则等等</p></li><li><p>构建 cloudformer 堆栈，并通过 cloudformer 服务生成当前环境模板</p></li><li><p>将当前环境模板的一些非通用内容，换成变量，并构建成 cf 参数或者脚本</p></li><li><p>最后通过执行脚本，并输入变量值，来生成新模板</p></li><li><p>在新环境里调用新模板来初始化环境</p></li></ul><h2 id="下面是我贴出的一个基础环境生成脚本">下面是我贴出的一个基础环境生成脚本</h2><ul><li><p>vpc: x.x.0.0/16</p></li><li><p>prod: x.x.128.0/17</p><ul><li>prod 子网默认有四个, 分别是 EC2 两个可用区, Database 两个可用区</li></ul></li><li><p>qa: x.x.0.0/19</p><ul><li>qa 子网默认有四个, 分别是 EC2 两个可用区, Database 两个可用区</li></ul></li><li><p>prod 和 qa 通过安全组来分割</p><ul><li><p>prod 规则示例如下(qa一样):</p></li><li><p>Prod-EC2-common:</p><table><thead><tr><th>规则名</th><th>协议</th><th>开放端口</th><th>源ip或者其它安全组ID</th><th></th></tr></thead><tbody><tr><td>所有流量</td><td>全部</td><td>全部</td><td>10.130.128.0/17</td><td></td></tr></tbody></table></li></ul><p>| SSH      | TCP  | 22       | ${src_allow_ssh_ip}  |      |</p><ul><li><p>Prod-Database-common:</p><table><thead><tr><th>规则名</th><th>协议</th><th>开放端口</th><th>源ip或者其它安全组ID</th><th></th></tr></thead><tbody><tr><td>自定义 TCP 规则</td><td>TCP</td><td>1025 - 65535</td><td>${src_allow_rds_ip}</td><td></td></tr><tr><td>自定义 TCP 规则</td><td>TCP</td><td>1025 - 65535</td><td>Prod-EC2-common-Group-ID</td><td></td></tr></tbody></table></li></ul></li><li><p>SNS 默认预警主题</p></li><li><p>mysql 子网组</p></li><li><p>redis 子网组</p></li><li><p>s3以及初始化生命周期规则</p></li><li><p>vpc 路由表不会添加默认路由, 还请自行添加(Internet 网关会自动生成)</p></li></ul><pre><code class="language-bash">#!/bin/bash# 本脚本用来生成一份基础环境的 cf 模板# 网络A和B段，例如 10.10 那么 vpc 就是 10.10.0.0/16VpcCidrBlockAB=# 大区区域, 例如 新加坡 那么区域就写 ap-southeast-1Region=# 项目名，用于区分资源以及添加 Team 标签 (只能是字母数字组合)TagTeamValue=# 允许访问EC2的22号端口的ip (指的是非vpc内网)，通常应该是堡垒机或者公司外网src_allow_ssh_ip=# 允许访问数据库端口的ip (指的是非vpc内网)，通常应该是堡垒机或者公司外网src_allow_rds_ip=# 默认 SNS 服务主体名 SNSThemeNameSNSThemeName=$&#123;TagTeamValue&#125;# 默认 SNS 服务信息接收用户邮箱, cf 模板执行完后，需要用户邮箱读取邮件自行确认消息SNSEmail=# 设置 vpc 任意两个可用区 字母标识, 比如 a bAvailabilityZoneOne=aAvailabilityZoneTwo=b######## 本模板, 不会添加 vpc 默认路由, 请自行加  #########cat &gt; cf-common.yml &lt;&lt; EOFAWSTemplateFormatVersion: 2010-09-09Resources:  vpc12345:    Type: 'AWS::EC2::VPC'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.0.0/16      InstanceTenancy: default      EnableDnsSupport: 'true'      EnableDnsHostnames: 'true'      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 84f85642-9390-4126-b347-95ca2ebf1c15  subnet0qa0database0$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.3.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-database-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 864ecf93-1ce6-4b4d-9b29-c3d89f07141b  subnet0prod0database0$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.131.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-database-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 7ef6daea-3bcd-4913-ab55-ba4f7bced319  subnet0qa0ec20$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.2.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-ec2-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: bc2b5a08-f7b8-4699-a7ba-5c9c6c021bd8  subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.4.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-database-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: b4738f0a-11a6-4d75-b9f0-a24072e53228  subnet0prod0ec20$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.129.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-ec2-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 2ffb24f6-49fd-4d3a-bdfe-69e90e228517  subnet0prod0ec20$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.128.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-ec2-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 9a2789fc-b7d2-4532-811b-be3455ab7d7d  subnet0qa0ec20$&#123;AvailabilityZoneOne&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.1.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneOne&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-qa-ec2-$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 5c6c2638-a7b5-4185-a7a3-08d5901c252a  subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;:    Type: 'AWS::EC2::Subnet'    Properties:      CidrBlock: $&#123;VpcCidrBlockAB&#125;.132.0/24      AvailabilityZone: $&#123;Region&#125;$&#123;AvailabilityZoneTwo&#125;      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-prod-database-$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 00a78a8b-fd72-4613-bf09-8319b876ba2b  igw088b65cb430bc5ca0:    Type: 'AWS::EC2::InternetGateway'    Properties:      Tags:        - Key: Name          Value: $&#123;TagTeamValue&#125;-internet-gw    Metadata:      'AWS::CloudFormation::Designer':        id: e60ac13b-274d-46db-ac2a-5afaf975e1eb  dopt8128ece5:    Type: 'AWS::EC2::DHCPOptions'    Properties:      DomainName: $&#123;Region&#125;.compute.internal      DomainNameServers:        - AmazonProvidedDNS    Metadata:      'AWS::CloudFormation::Designer':        id: 909e6481-ccca-40fe-a1b1-4abe15a2ba18  acl03e81a4f65756520d:    Type: 'AWS::EC2::NetworkAcl'    Properties:      VpcId: !Ref vpc12345    Metadata:      'AWS::CloudFormation::Designer':        id: 93e06538-fe9e-49a7-8549-e2efb97dfab7  dbsubnet$&#123;TagTeamValue&#125;prod:    Type: 'AWS::RDS::DBSubnetGroup'    Properties:      DBSubnetGroupDescription: $&#123;TagTeamValue&#125;-prod      SubnetIds:        - !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 16096ba6-c5de-4a73-a083-07b2c0573362  dbsubnet$&#123;TagTeamValue&#125;qa:    Type: 'AWS::RDS::DBSubnetGroup'    Properties:      DBSubnetGroupDescription: $&#123;TagTeamValue&#125;-qa      SubnetIds:        - !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 186893f8-59ef-4d56-880f-d62294d25780  dbpg$&#123;TagTeamValue&#125;prod:    Type: 'AWS::RDS::DBParameterGroup'    Properties:      Description: $&#123;TagTeamValue&#125;-prod-mysql5.6      Family: mysql8.0      Parameters:        binlog_cache_size: '32768'        binlog_format: MIXED        character_set_client: utf8mb4        character_set_connection: utf8mb4        character_set_database: utf8mb4        character_set_filesystem: utf8mb4        character_set_results: utf8mb4        character_set_server: utf8mb4        collation_connection: utf8mb4_general_ci        explicit_defaults_for_timestamp: '1'        innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'        innodb_file_per_table: '1'        innodb_flush_method: O_DIRECT    Metadata:      'AWS::CloudFormation::Designer':        id: 87a954ed-d1b2-4626-87fe-f2515f4479b4  dbpg$&#123;TagTeamValue&#125;qa:    Type: 'AWS::RDS::DBParameterGroup'    Properties:      Description: $&#123;TagTeamValue&#125;-qa-mysql5.6      Family: mysql8.0      Parameters:        binlog_cache_size: '32768'        binlog_format: MIXED        character_set_client: utf8        character_set_connection: utf8        character_set_database: utf8        character_set_filesystem: utf8        character_set_results: utf8        character_set_server: utf8        collation_connection: utf8_general_ci        explicit_defaults_for_timestamp: '1'        innodb_buffer_pool_size: '&#123;DBInstanceClassMemory*3/4&#125;'        innodb_file_per_table: '1'        innodb_flush_method: O_DIRECT    Metadata:      'AWS::CloudFormation::Designer':        id: 6c4c2e1e-ec99-4e7f-b254-0f09145a0d87  cachesubnet$&#123;TagTeamValue&#125;databaseprod:    Type: 'AWS::ElastiCache::SubnetGroup'    Properties:      Description: '$&#123;TagTeamValue&#125;_prod'      SubnetIds:        - !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;        - !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: bdfb3d49-78d8-40df-84b6-63bf65616f5c  cachesubnet$&#123;TagTeamValue&#125;databaseqa:    Type: 'AWS::ElastiCache::SubnetGroup'    Properties:      Description: '$&#123;TagTeamValue&#125;_qa'      SubnetIds:        - !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;        - !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 70367616-0a11-4bc9-9373-a23be1690a59  topicIT:    Type: 'AWS::SNS::Topic'    Properties:      DisplayName: $&#123;SNSThemeName&#125;      Subscription:        - Endpoint: $&#123;SNSEmail&#125;          Protocol: email    Metadata:      'AWS::CloudFormation::Designer':        id: 343cc3f4-4380-47e5-850e-cc3fb560f22a  $&#123;TagTeamValue&#125;qacommon:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-qa-common      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  QA-EC2-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: d31db020-e702-41e5-8ccc-d71a5a26f565  $&#123;TagTeamValue&#125;prodcommon:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-prod-common      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  Prod-EC2-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: f54456ef-e984-4b00-b04d-64b16a2057ea  ITLinshi:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: IT-Linshi      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  I'm god    Metadata:      'AWS::CloudFormation::Designer':        id: 182b7e60-f895-4519-8845-4d32a8591340  $&#123;TagTeamValue&#125;proddatabase:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-prod-database      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  Prod-database-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: 44f0485a-2575-4eab-82b3-bc7507cd5820  $&#123;TagTeamValue&#125;qadatabase:    Type: 'AWS::EC2::SecurityGroup'    Properties:      GroupDescription: $&#123;TagTeamValue&#125;-qa-database      VpcId: !Ref vpc12345      Tags:        - Key: Name          Value:  QA-database-common,Don't modify me    Metadata:      'AWS::CloudFormation::Designer':        id: f903102a-c394-4301-a08d-5625293ac23f  snspolicyIT:    Type: 'AWS::SNS::TopicPolicy'    Properties:      Topics:        - !Ref topicIT      PolicyDocument:        Version: 2008-10-17        Id: __default_policy_ID        Statement:          - Sid: __default_statement_ID            Effect: Allow            Principal:              AWS: '*'            Action:              - 'SNS:GetTopicAttributes'              - 'SNS:SetTopicAttributes'              - 'SNS:AddPermission'              - 'SNS:RemovePermission'              - 'SNS:DeleteTopic'              - 'SNS:Subscribe'              - 'SNS:ListSubscriptionsByTopic'              - 'SNS:Publish'              - 'SNS:Receive'            Resource: !Ref topicIT            Condition:              StringEquals:                'AWS:SourceOwner': '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 66a45d1a-3c99-4ef8-9c4e-db9ba717e17e  acl1:    Type: 'AWS::EC2::NetworkAclEntry'    Properties:      CidrBlock: 0.0.0.0/0      Egress: 'true'      Protocol: '-1'      RuleAction: allow      RuleNumber: '100'      NetworkAclId: !Ref acl03e81a4f65756520d    Metadata:      'AWS::CloudFormation::Designer':        id: de76822e-cddc-40d9-ba6f-4de20d1188ec  acl2:    Type: 'AWS::EC2::NetworkAclEntry'    Properties:      CidrBlock: 0.0.0.0/0      Protocol: '-1'      RuleAction: allow      RuleNumber: '100'      NetworkAclId: !Ref acl03e81a4f65756520d    Metadata:      'AWS::CloudFormation::Designer':        id: eb6e0bf9-3c84-4991-a037-64e2a368650b  subnetacl1:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0ec20$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 2801f8a7-901b-43b6-b4d9-9275d5639b86  subnetacl2:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0ec20$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 67641cac-39f6-4ee1-8410-60eaba963c27  subnetacl3:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0ec20$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 4b480f9d-e7b8-4034-b253-1c254fd83758  subnetacl4:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 1792de63-b1da-4c31-bab9-9925de98de6c  subnetacl5:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: e62f3e81-63f0-4f5e-b3e0-aa733ba6f323  subnetacl6:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0ec20$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 28228fcf-5649-4e5e-bf03-d636ca28bffe  subnetacl8:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneTwo&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: 00ab0d37-3e37-427e-ac06-e94663719fcf  subnetacl9:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0qa0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: ee93485d-3df4-416d-ab61-4f77c46441f1  subnetacl10:    Type: 'AWS::EC2::SubnetNetworkAclAssociation'    Properties:      NetworkAclId: !Ref acl03e81a4f65756520d      SubnetId: !Ref subnet0prod0database0$&#123;AvailabilityZoneOne&#125;    Metadata:      'AWS::CloudFormation::Designer':        id: a4208016-e7cf-4acf-a718-104a25fef4c3  gw1:    Type: 'AWS::EC2::VPCGatewayAttachment'    Properties:      VpcId: !Ref vpc12345      InternetGatewayId: !Ref igw088b65cb430bc5ca0    Metadata:      'AWS::CloudFormation::Designer':        id: 3071b8a9-f4ab-4d61-83d8-d2792481e135  dchpassoc1:    Type: 'AWS::EC2::VPCDHCPOptionsAssociation'    Properties:      VpcId: !Ref vpc12345      DhcpOptionsId: !Ref dopt8128ece5    Metadata:      'AWS::CloudFormation::Designer':        id: 69a63971-39fa-45c9-8a17-cc730730083d  ingress1:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: '-1'      CidrIp: $&#123;VpcCidrBlockAB&#125;.0.0/19  ingress2:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: '-1'      CidrIp: $&#123;VpcCidrBlockAB&#125;.128.0/17  ingress3:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      SourceSecurityGroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      SourceSecurityGroupOwnerId: '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 8972689f-9301-4538-b493-bd1956559ecd  ingress4:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      SourceSecurityGroupId: !Ref $&#123;TagTeamValue&#125;qacommon      SourceSecurityGroupOwnerId: '705452591009'    Metadata:      'AWS::CloudFormation::Designer':        id: 236a8060-da9c-4be7-af29-fb6758a764a9  ingress5:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref ITLinshi      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  ingress6:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: tcp      FromPort: '22'      ToPort: '22'      CidrIp: $&#123;src_allow_ssh_ip&#125;  ingress8:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: tcp      FromPort: '22'      ToPort: '22'      CidrIp: $&#123;src_allow_ssh_ip&#125;  ingress11:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      CidrIp: $&#123;src_allow_rds_ip&#125;  ingress14:    Type: 'AWS::EC2::SecurityGroupIngress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: tcp      FromPort: '1025'      ToPort: '65535'      CidrIp: $&#123;src_allow_rds_ip&#125;  egress1:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qacommon      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress2:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;prodcommon      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress3:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref ITLinshi      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress4:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;proddatabase      IpProtocol: '-1'      CidrIp: 0.0.0.0/0  egress5:    Type: 'AWS::EC2::SecurityGroupEgress'    Properties:      GroupId: !Ref $&#123;TagTeamValue&#125;qadatabase      IpProtocol: '-1'      CidrIp: 0.0.0.0/0Description: ''EOF</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ec2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab☞批量修改用户密码</title>
      <link href="posts/5edc3545/"/>
      <url>posts/5edc3545/</url>
      
        <content type="html"><![CDATA[<pre><code>export OldToken=export OldDomain=# 导出 用户ID 用户名 用户状态 用户邮箱curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=1&quot;  | jq '.[] | .id,.username,.state,.email' | sed 'N;N;N;s#\n# #g;s#&quot;##g' | grep 'active' &gt;&gt; user.id;curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=2&quot;  | jq '.[] | .id,.username,.state,.email' | sed 'N;N;N;s#\n# #g;s#&quot;##g' | grep 'active' &gt;&gt; user.id;# 修改用户密码, 并生成密码记录文件 user.passwordwhile read userid username state email;do    Pwd=`cat /proc/sys/kernel/random/uuid | cut -d'-' -f1`    curl --request  PUT --header &quot;PRIVATE-TOKEN:$&#123;OldToken&#125;&quot; --data &quot;password=$&#123;username&#125;@$&#123;Pwd&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users/$&#123;userid&#125;&quot;    echo &quot;$&#123;email&#125; $&#123;username&#125; $&#123;username&#125;@$&#123;Pwd&#125;&quot; &gt;&gt; user.passworddone &lt; user.id</code></pre><pre><code>#!/usr/bin/python #coding:utf-8   import smtplib from email.mime.text import MIMEText import sys  mail_host = 'smtp.gmail.com:587'mail_user = 'it-ops@altamob.com'mail_pass = 'hoqmefjrmkvkukth'def send_mail(to_list,subject,content):     me = mail_user+&quot;&lt;&quot;+mail_user+&quot;&gt;&quot;     msg = MIMEText(content)     msg['Subject'] = subject     msg['From'] = me     msg['to'] = to_list          try:     print &quot;start sendmail&quot;        s = smtplib.SMTP(mail_host)    print &quot;connect mail server suesscc&quot;     s.starttls()        s.login(mail_user,mail_pass)     print &quot;login mail server suesscc&quot;        s.sendmail(me,to_list,msg.as_string())         s.close()         return True     except Exception,e:         print str(e)         return False      if __name__ == &quot;__main__&quot;:     send_mail(sys.argv[1], sys.argv[2], sys.argv[3])</code></pre><pre><code>while read email username pwd;do python sendmail.py &quot;$&#123;email&#125;&quot; &quot;gitlab $&#123;username&#125;新密码&quot; &quot;$&#123;pwd&#125;&quot;;done &lt; user.password</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab☞大版本迁移</title>
      <link href="posts/a38d0013/"/>
      <url>posts/a38d0013/</url>
      
        <content type="html"><![CDATA[<h1>前言</h1><ul><li>现在 gitlab 官方包升级很容易，直接 yum 之类的就可以，之所以有这篇文章，是因为我要从 v7 升级到 v11, 数据库也从 mysql 转变为了 pgsql;而且 v7 当时用的是 bitnami 安装的。所以无奈之下，就准备用 api 进行迁移，因而遗漏之处是肯定有的。</li></ul><h1>正文</h1><ul><li><p>Token变量定义[Token不同版本获取方式不一样，但是总归都是从web用户界面能直接拿到的], Token需要是最大权限用户，且拥有所有的项目权限</p><pre><code>export OldToken=su2GwpNeVQexport NewToken=yTVZTPeussexport OldDomain=git.a.comexport NewDomain=git.b.com</code></pre><blockquote><p>gitlab 不同版本的 api 版本不一样，本文档是从 7 迁移到 11，所以 api 版本也是从 v3 到 v4<br>gitlab 的 api 默认一次最多获取100条信息，所以需要进行分页获取<br>per_page 每页信息条数 page 页数<br>所以信息太多的自行分页即可</p></blockquote></li><li><p>迁移用户</p><blockquote><p>下面代码，并不迁移用户的公钥信息</p></blockquote><pre><code>mkdir create_user &amp;&amp; cd create_user;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=$i&quot; | jq '.[] |.name, .username, .state, .email, .can_create_group, .can_create_project' | sed 'N;N;N;N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; users.list;while read name username state email can_create_group can_create_project;do    curl --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; --data &quot;name=$name&amp;username=$username&amp;password=$&#123;username&#125;@123456&amp;state=$state&amp;email=$email&amp;can_create_group=$can_create_group&amp;can_create_project=$can_create_project&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users&quot;done &lt; users.listcd ..</code></pre></li><li><p>创建项目组</p><pre><code>mkdir create_group &amp;&amp; cd create_group;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/groups?per_page=100&amp;page=$i&quot; | jq '.[] |.name, .path' | sed 'N;s#\n# #;s#&quot;##g';done &gt; group.list;echo 'repo repo' &gt;&gt; group.list;cat group.list | while read name path;do curl --request POST --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; --data &quot;name=$name&amp;path=$path&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups&quot;;donecd ..</code></pre></li><li><p>创建项目到对应组</p><pre><code>mkdir create_project &amp;&amp; cd create_project;</code></pre><pre><code>for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.name, .namespace.name, .description' | sed 'N;N;s#\n#\t#g;s# #-#g;s#&quot;##g';done  &gt;&gt; project.list;mv  project.list project.list.tmp;cat project.list.tmp | sort | uniq &gt; project.list;rm -rf project.list.tmp;</code></pre><pre><code># 这里 pre_page 不起作用for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups?page=$i&quot; | jq '.[] | .id, .name' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; group.id</code></pre><pre><code>cat project.list | while read projectname groupname des;do    newgroupid=`egrep &quot;\b$&#123;groupname&#125;\b&quot; ./group.id | awk '&#123;print $1&#125;'`    curl --request POST --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot;  --data &quot;name=$&#123;projectname&#125;&amp;namespace_id=$&#123;newgroupid&#125;&amp;description=$des&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/projects&quot;donecd ..</code></pre></li><li><p>添加用户到组</p><pre><code>mkdir add_user_group &amp;&amp; cd add_user_group;</code></pre><pre><code>for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/groups?page=$i&quot; | jq '.[] | .id, .name' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; group.old.id;cp ../create_project/group.id ./group.new.id;#curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups&amp;page=1&quot; | jq '.[] | .id, .name' | sed 'N;s#\n# #g;s#&quot;##g' &gt;&gt; group.new.id;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot;  | jq '.[] | .id,.username' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; new_user.id;</code></pre><pre><code>cat ./group.old.id | while read groupid groupname;do    curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; http://$&#123;OldDomain&#125;/api/v3/groups/$&#123;groupid&#125;/members | jq '.[] | .id, .username, .access_level' |sed 'N;N;s#\n# #g;s#&quot;##g' | while read nameid username level;do        newid=`egrep &quot;\b$&#123;username&#125;\b&quot; ./new_user.id| awk '&#123;print $1&#125;'`        newgid=`egrep &quot;\b$&#123;groupname&#125;\b&quot; ./group.new.id|awk '&#123;print $1&#125;'`curl --request POST --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; --data &quot;user_id=$&#123;newid&#125;&amp;access_level=$&#123;level&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/groups/$&#123;newgid&#125;/members    donedonecd ..</code></pre></li><li><p>添加用户到项目</p><pre><code>mkdir add_user_project &amp;&amp; cd add_user_project;</code></pre><pre><code>for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.id, .name, .namespace.name' | sed 'N;N;s#\n# #g;s#&quot;##g';done  &gt;&gt; project.old.id;mv  project.old.id project.old.id.tmp;cat project.old.id.tmp | sort | uniq &gt; project.old.id;rm -rf project.old.id.tmp;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.id, .name, .namespace.name' | sed 'N;N;s#\n# #g;s#&quot;##g';done  &gt;&gt; project.new.id;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot;  | jq '.[] | .id,.username' | sed 'N;s#\n# #g;s#&quot;##g';done &gt;&gt; new_user.id;</code></pre><pre><code>cat ./project.old.id | while read projectid projectname groupname;do    curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; http://$&#123;OldDomain&#125;/api/v3/projects/$&#123;projectid&#125;/members | jq '.[] | .id, .username, .access_level' |sed 'N;N;s#\n# #g;s#&quot;##g' | while read nameid username level;do        newid=`egrep &quot;\b$&#123;username&#125;\b&quot; ./new_user.id| awk '&#123;print $1&#125;'`        newpid=`egrep &quot;\b$&#123;projectname&#125; $&#123;groupname&#125;\b&quot; ./project.new.id|awk '&#123;print $1&#125;'`        #echo &quot;$projectid $projectname $groupname $username &gt; $newpid $projectname $groupname&quot;curl --request POST --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; --data &quot;user_id=$&#123;newid&#125;&amp;access_level=$&#123;level&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/projects/$&#123;newpid&#125;/members    donedonecd ..</code></pre></li><li><p>代码迁移</p><pre><code>mkdir sync_code &amp;&amp; cd sync_code;#for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot;  | jq '.[] |.name, .namespace.name, .description' | sed 'N;N;s#\n#\t#g;s# #-#g;s#&quot;##g';done  &gt;&gt; project.list;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/projects?per_page=100&amp;page=$i&quot; | jq '.[] |.path_with_namespace' | sed 's#&quot;##g';done &gt;&gt; project.path;</code></pre><pre><code># 将本代码框的代码保存到 start.sh 并执行(确保sync_code_base变量路径准确)num=1sync_code_base=/export/sync/sync_codenohup while read line;do     echo &quot;$num: -------------------------$line-------------------------&quot; &gt;&gt; $&#123;sync_code_base&#125;/sync.log    [[ -d $&#123;line%%/*&#125; ]] || mkdir $&#123;line%%/*&#125;    cd $&#123;line%%/*&#125;    git clone --mirror git@$&#123;OldDomain&#125;:$&#123;line&#125;.git &gt;&gt; $&#123;sync_code_base&#125;/sync.log 2&gt;&amp;1 || echo &quot;clone:$line&quot; &gt;&gt; $&#123;sync_code_base&#125;/clone_error.log    cd $&#123;line##*/&#125;.git    git remote set-url origin git@$&#123;NewDomain&#125;:$&#123;line&#125;.git    git push -f origin &gt;&gt; $&#123;sync_code_base&#125;/sync.log 2&gt;&amp;1 || echo &quot;sync:$line&quot; &gt;&gt; $&#123;sync_code_base&#125;/sync_error.log    cd /export/sync/sync_code    let num++done &lt; $&#123;sync_code_base&#125;/project.path &amp;</code></pre><pre><code>cd ..</code></pre></li><li><p>初始化新用户密码</p><pre><code>mkdir init_password; cd init_password;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot; |  jq '.[] |.id, .name, .username' |  sed 'N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; user.id;cat user.id | while read id name nameuser;do  curl --request PUT --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; --data &quot;password=$&#123;nameuser&#125;@123&quot; http://$&#123;NewDomain&#125;/api/v4/users/$&#123;id&#125;;donecd ..</code></pre></li><li><p>同步用户公钥信息</p><pre><code>mkdir sync_user_pub &amp;&amp; cd sync_user_pub;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; &quot;http://$&#123;OldDomain&#125;/api/v3/users?per_page=100&amp;page=$i&quot;  |  jq '.[] |.id, .name, .username' |  sed 'N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; old_users.id;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users?per_page=100&amp;page=$i&quot;  |  jq '.[] |.id, .name, .username' |  sed 'N;N;s#\n# #g;s#&quot;##g';done &gt;&gt; new_users.id;cat old_users.id | while read old_current_id name username;do    new_current_id=`grep $username new_users.id | awk '&#123;print $1&#125;'`    curl -s --header &quot;PRIVATE-TOKEN: $&#123;OldToken&#125;&quot; http://$&#123;OldDomain&#125;/api/v3/users/$&#123;old_current_id&#125;/keys | jq '.[]|.title,.key' | sed 'N;s/\n/\t/;s/&quot;//g' | while read  current_title current_key;do        curl --request POST --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; --data-urlencode &quot;title=$&#123;current_title&#125;&quot; --data-urlencode &quot;key=$&#123;current_key&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/users/$&#123;new_current_id&#125;/keys&quot;    donedone</code></pre></li><li><p>如果出现push故障，则删除新服务器的项目, 并重新push</p><pre><code>sync_code_base=/export/sync/sync_codecd $&#123;sync_code_base&#125;# 准备好 delete.path 文件，内部一行一个 project.path_with_namespacefor i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/projects?per_page=100&amp;page=$i&quot; | jq '.[]|.id, .path_with_namespace'| sed 'N;s#\n#\t#g;s# #-#g;s#&quot;##g';done &gt; project-path-id.allwhile read line;do grep $line project-path-id.all ;done &lt; delete.path &gt; delete.path.idwhile read id path;do curl -s --request DELETE --header &quot;PRIVATE-TOKEN:$&#123;NewToken&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/projects/$&#123;id&#125;;sleep 5;cd $&#123;path&#125;.git;git push -f origin ;cd $&#123;sync_code_base&#125;;done &lt; delete.path.id</code></pre></li><li><p>删除新服务器的项目组</p><pre><code>mkdir delete_group &amp;&amp; cd delete_group;for i in `seq 1 15`;do curl -s --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; &quot;http://$&#123;NewDomain&#125;/api/v4/groups?per_page=100&amp;page=$i&quot; | jq '.[] | .id';done &gt;&gt; group.id;cat group.id | while read id;do curl -s --request DELETE --header &quot;PRIVATE-TOKEN: $&#123;NewToken&#125;&quot; http://$&#123;NewDomain&#125;/api/v4/groups/$&#123;id&#125;;done;</code></pre></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx ☞ 泛域名_变量截取</title>
      <link href="posts/78ceb386/"/>
      <url>posts/78ceb386/</url>
      
        <content type="html"><![CDATA[<pre><code class="language-bash">server &#123;    listen       80;    listen       [::]:80;    server_name ~^(?&lt;userName&gt;.*)\.apple\.com\.cn$;    root /export/webapps/apple.com/$userName;    access_log  /var/log/nginx/access.log  main;    #开启浏览器静态文件缓存    location ~ .*\.(html|htm|gif|jpg|jpeg|bmp|png|ico|txt|js|css)$ &#123;        expires      3h;    &#125;        # https://&lt;username&gt;.apple.com/api -&gt; /export/webapps/apple.com/api.php #####    location ~* ^/(api|event_api)$ &#123;        root /export/webapps/apple.com;        rewrite ^/(.*)$ /$1.php break;        fastcgi_pass     127.0.0.1:9001;        fastcgi_index    index.php;        include      fastcgi.conf;        fastcgi_connect_timeout    600s;        fastcgi_send_timeout       600s;        fastcgi_read_timeout       600s;        fastcgi_buffers 8 256k;        fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;        # https://&lt;username&gt;.apple.com/&lt;uri&gt; -&gt; /export/webapps/apple.com/&lt;username&gt;/&lt;uri&gt;.php    location ~* ^/[0-9a-zA-Z]+$ &#123;        rewrite ^/(.*)$ /$1.php break;        fastcgi_pass     127.0.0.1:9001;        fastcgi_index    index.php;        include      fastcgi.conf;        fastcgi_connect_timeout    600s;        fastcgi_send_timeout       600s;        fastcgi_read_timeout       600s;        fastcgi_buffers 8 256k;        fastcgi_buffer_size 256k;        fastcgi_busy_buffers_size 256k;        fastcgi_intercept_errors on;    &#125;&#125;</code></pre><blockquote><p>location 优先级从上往下依次递减：<br>location =      仅匹配字符串自身<br>location ^~    匹配某个字符串开头的uri<br>location ~      正则匹配，区分大小写<br>location ~*    正则匹配，不区分大小写<br>location /      表示匹配“域名/之后的uri”，再比如localtion /images，表示匹配“域名/images之后的uri</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞安装-编译方式</title>
      <link href="posts/3321a2dc/"/>
      <url>posts/3321a2dc/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>用于编译安装php7.1</p><h4 id="目录结构">目录结构</h4><p>根目录：/usr/local/php<br>日志目录：/usr/local/php/var/log -&gt; /export/logs/php</p><h4 id="脚本在此">脚本在此</h4><pre><code class="language-bash">#!/bin/bash# 脚本, 适用于 php 7.1basedir=/usr/local/srccd $basedirrunuser=`whoami`[[ $runuser == 'root' ]] || &#123;        echo &quot;ERROR:执行用户不是$runuser&quot; &amp;&amp; exit&#125;#初始化服务器环境[[ -d /export/logs/php ]] || &#123;        echo &quot;/export/logs/php目录不存在&quot; &amp;&amp; exit&#125;yum install libmcrypt-devel ncurses-devel recode-devel aspell-devel curl-devel readline-devel openldap-devel enchant-devel pcre-devel net-snmp-devel libicu-devel libtool-ltdl-devel libjpeg-devel libpng-devel libxml2-devel bzip2-devel freetype-devel gcc-c++ mysql-develcp -p /usr/lib64/libldap* /usr/libln -s /usr/lib64/mysql /usr/lib/mysqlwget http://sg2.php.net/distributions/php-7.1.25.tar.gz -O php.tar.gzrm -rf php &amp;&amp; mkdir phptar xf php.tar.gz --strip-components 1 -C phpcd php &amp;&amp; ./configure --prefix=/usr/local/php --with-config-file-path=/usr/local/php/etc --with-config-file-scan-dir=/usr/local/php/etc/php.d --with-curl --with-mysqli=mysqlnd --with-pdo-mysql=mysqlnd --with-openssl --enable-mbstring --with-freetype-dir --with-jpeg-dir --with-png-dir --with-gd --with-libxml-dir --with-zlib --with-mcrypt --with-bz2 --enable-sysvshm --enable-sysvsem --enable-soap --with-recode --with-snmp --with-readline --enable-intl --enable-dba --enable-bcmath --with-enchant --with-pspell --enable-xml --enable-sockets --enable-exif --enable-inline-optimization --enable-fpm || exitmake &amp;&amp; make install || exitcp sapi/fpm/init.d.php-fpm /etc/rc.d/init.d/php-fpmchkconfig --add php-fpmchkconfig php-fpm offchmod u+x /etc/rc.d/init.d/php-fpmcp php.ini-production /usr/local/php/etc/php.inicd /usr/local/php/var &amp;&amp; rm -rf logln -s /export/logs/php logcat&gt;&gt;/usr/local/php/etc/php.ini&lt;&lt;EOF; 关闭php无用日志信息error_reporting = E_COMPILE_ERROR|E_RECOVERABLE_ERROR|E_ERROR|E_CORE_ERROR; 开启php opcache 缓存功能[opcache]zend_extension=opcache.so; 启动操作码缓存opcache.enable=1; 针对支持CLI版本PHP启动操作码缓存 一般被用来测试和调试opcache.enable_cli=0; 共享内存大小，单位为MBopcache.memory_consumption=128; 存储临时字符串缓存大小，单位为MB，PHP5.3.0以前会忽略此项配置opcache.interned_strings_buffer=8; 缓存文件数最大限制，命中率不到100%，可以试着提高这个值opcache.max_accelerated_files=4000; 一定时间内检查文件的修改时间, 这里设置检查的时间周期, 默认为 2, 单位为秒opcache.revalidate_freq=60; 开启快速停止续发事件，依赖于Zend引擎的内存管理模块，一次释放全部请求变量的内存，而不是依次释放内存块opcache.fast_shutdown=1;启用检查 PHP 脚本存在性和可读性的功能，无论文件是否已经被缓存，都会检查操作码缓存,可以提升性能。 但是如果禁用了 opcache.validate_timestamps选项， 可能存在返回过时数据的风险。opcache.enable_file_override=1EOFcat&gt;&gt;/usr/local/php/etc/php-fpm.conf&lt;&lt;EOF[global]log_level =errordaemonize = yesevents.mechanism = epollrlimit_files = 10240emergency_restart_threshold = 60emergency_restart_interval = 60s[fcgi]user = webappsgroup = webappslisten = 0.0.0.0:9000pm = staticpm.max_children = 100pm.max_requests = 1024request_slowlog_timeout = 1sslowlog = /usr/local/php/var/log/php-slow.logpm.status_path = /php-fpm_statusEOF</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab☞安装及备份</title>
      <link href="posts/3f38f454/"/>
      <url>posts/3f38f454/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://docs.gitlab.com/omnibus/docker/">https://docs.gitlab.com/omnibus/docker/</a></p><p><a href="https://hub.docker.com/r/gitlab/gitlab-ce">https://hub.docker.com/r/gitlab/gitlab-ce</a></p><p><a href="https://docs.gitlab.com/14.0/ee/update/index.html#community-to-enterprise-edition">Upgrading GitLab | GitLab</a></p></blockquote><h2 id="安装">安装</h2><pre><code class="language-bash"># 确保 /export 存在mkdir -p /export/docker-data-gitlab/&#123;config, logs, data&#125;gitlabtag=# createdomainName=docker pull gitlab/gitlab-ce:$&#123;gitlabtag&#125;docker run --detach   --hostname $&#123;domainName&#125;   --publish 443:443 --publish 80:80 --publish 2222:22   --name gitlab   --restart always   --volume /export/docker-data-gitlab/config:/etc/gitlab   --volume /export/docker-data-gitlab/logs:/var/log/gitlab   --volume /export/docker-data-gitlab/data:/var/opt/gitlab   gitlab/gitlab-ce:$&#123;gitlabtag&#125;# startdocker start gitlab# stopdocker stop gitlab</code></pre><h2 id="基本配置">基本配置</h2><pre><code class="language-bash"># configure# https://docs.gitlab.com/omnibus/settings/README.htmlcp /export/docker-data-gitlab/config/gitlab.rb /export/docker-data-gitlab/config/gitlab.rb.bak</code></pre><pre><code class="language-bash">## /export/docker-data-gitlab/config/gitlab.rb# 决定各个位置 url 链接内容external_url 'http://$&#123;domainName&#125;'# 决定各个位置 ssh 链接内容gitlab_rails['gitlab_shell_ssh_port'] = 2222gitlab_rails['gitlab_email_enabled'] = truegitlab_rails['gitlab_email_from'] = 'ew@xxx.com'gitlab_rails['gitlab_email_display_name'] = 'GitLab Admin'gitlab_rails['gitlab_email_reply_to'] = 'ew@xxx.com'gitlab_rails['gitlab_email_subject_suffix'] = 'GitLab'gitlab_rails['smtp_enable'] = truegitlab_rails['smtp_address'] = &quot;smtp.gmail.com&quot;gitlab_rails['smtp_port'] = 587gitlab_rails['smtp_user_name'] = &quot;ew@xxx.com&quot;gitlab_rails['smtp_password'] = &quot;&quot;gitlab_rails['smtp_domain'] = &quot;smtp.gmail.com&quot;gitlab_rails['smtp_authentication'] = &quot;login&quot;gitlab_rails['smtp_enable_starttls_auto'] = truegitlab_rails['smtp_tls'] = falsegitlab_rails['smtp_openssl_verify_mode'] = 'peer'</code></pre><h2 id="备份配置">备份配置</h2><p>备份到AWS-S3，role 授权</p><pre><code class="language-bash">## /export/docker-data-gitlab/config/gitlab.rbgitlab_rails['backup_upload_connection'] = &#123;  'provider' =&gt; 'AWS',  'region' =&gt; '&lt;region-id&gt;',  #'aws_access_key_id' =&gt; 'AKIAKIAKI',  #'aws_secret_access_key' =&gt; 'secret123'  # If using an IAM Profile, don't configure aws_access_key_id &amp; aws_secret_access_key  'use_iam_profile' =&gt; true&#125;# 备份到S3上的根路径 bucket/Path gitlab_rails['backup_upload_remote_directory'] = '&lt;桶名&gt;/backup/gitlab'</code></pre><pre><code class="language-json"># s3策略&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;VisualEditor0&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;s3:PutObject&quot;,                &quot;s3:GetObject&quot;,                &quot;s3:ListBucket&quot;            ],            &quot;Resource&quot;: [                &quot;arn:aws:s3:::&lt;桶名&gt;&quot;,                &quot;arn:aws:s3:::&lt;桶名&gt;/backup/gitlab/*&quot;            ]        &#125;,        &#123;            &quot;Sid&quot;: &quot;VisualEditor1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: &quot;s3:ListAllMyBuckets&quot;,            &quot;Resource&quot;: &quot;*&quot;        &#125;    ]&#125;</code></pre><p>备份到阿里云-OSS，AK授权，暂不支持 role 授权</p><pre><code class="language-bash">## /export/docker-data-gitlab/config/gitlab.rbgitlab_rails['backup_upload_connection'] = &#123;'provider' =&gt; 'aliyun','aliyun_accesskey_id' =&gt; 'AK123','aliyun_accesskey_secret' =&gt; 'secret123','aliyun_oss_bucket' =&gt; '&lt;桶名&gt;','aliyun_region_id' =&gt; '&lt;region-id&gt;','aliyun_oss_endpoint' =&gt; 'http://oss-&lt;region-id&gt;-internal.aliyuncs.com'&#125;gitlab_rails['backup_upload_remote_directory'] = 'backup/gitlab'</code></pre><p>⚠️</p><blockquote><h3 id="需要注意的是，截止-fog-aliyun-0-3-19-版本，aliyun-oss-endpoint-指定内网地址的时候，依然走的是公网的-endpoint，会消耗公网流量">需要注意的是，截止 fog-aliyun: 0.3.19 版本，aliyun_oss_endpoint 指定内网地址的时候，依然走的是公网的 endpoint，会消耗公网流量</h3><p><a href="https://gitlab.com/gitlab-org/gitlab/-/blob/da46c9655962df7d49caef0e2b9f6bbe88462a02/Gemfile#L122">https://gitlab.com/gitlab-org/gitlab/-/blob/da46c9655962df7d49caef0e2b9f6bbe88462a02/Gemfile#L122</a><br><a href="https://rubygems.org/gems/fog-aliyun">https://rubygems.org/gems/fog-aliyun</a></p></blockquote><pre><code class="language-json">&#123;    &quot;Version&quot;: &quot;1&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;oss:*&quot;            ],            &quot;Resource&quot;: [                &quot;acs:oss:*:*:&lt;桶名&gt;/backup/gitlab/*&quot;            ]        &#125;,        &#123;            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Action&quot;: [                &quot;oss:Get*&quot;,                &quot;oss:List*&quot;            ],            &quot;Resource&quot;: &quot;*&quot;        &#125;    ]&#125;</code></pre><h2 id="备份计划，触发备份">备份计划，触发备份</h2><ol><li>阿里云 （role授权）</li></ol><pre><code class="language-bash"># 配置 role 到服务器上cat &gt; /etc/profile.d/ecs_role.sh &lt;&lt; EOFRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`ramRoleName=`curl -sq http://100.100.100.200/latest/meta-data/ram/security-credentials/`aliyun configure set --profile ecsRamRoleProfile  --mode EcsRamRole --ram-role-name $&#123;ramRoleName&#125; --region $&#123;Region&#125;EOF</code></pre><p>添加脚本到crontab</p><pre><code class="language-bash">#!/bin/bashsource /etc/profile.d/ecs_role.shRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;basedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $basedirTime=`date +%Y%m%d`docker exec -t gitlab gitlab-backup createtar zcf $&#123;Time&#125;.config.tar.gz /export/docker-data-gitlab/configaliyun oss cp $&#123;Time&#125;.config.tar.gz oss://桶名/backup/gitlab/ -e $&#123;Endpoint&#125; --force</code></pre><ol start="2"><li><p>aws (role授权)</p><p>添加脚本到crontab</p></li></ol><pre><code class="language-bash">#!/bin/bashbasedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`cd $basedirTime=`date +%Y%m%d`docker exec -t gitlab gitlab-backup createtar zcf $&#123;Time&#125;.config.tar.gz /export/docker-data-gitlab/configaws s3 cp $&#123;Time&#125;.config.tar.gz s3://桶名/backup/gitlab/</code></pre><h2 id="重载配置服务">重载配置服务</h2><p>docker exec -it gitlab gitlab-ctl reconfigure</p><h2 id="升级">升级</h2><blockquote><p>大版本升级可能失败，特别是数据库也升级的情况下</p></blockquote><pre><code class="language-bash">docker stop gitlabdocker rm gitlabgitlabtag=domainName=docker pull gitlab/gitlab-ce:$&#123;gitlabtag&#125;docker run --detach --hostname $&#123;domainName&#125;  --publish 443:443 --publish 80:80 --publish 2222:22   --name gitlab   --restart always   --volume /export/docker-data-gitlab/config:/etc/gitlab   --volume /export/docker-data-gitlab/logs:/var/log/gitlab   --volume /export/docker-data-gitlab/data:/var/opt/gitlab   gitlab/gitlab-ce:$&#123;gitlabtag&#125;</code></pre><h2 id="Oauth">Oauth</h2><p><a href="https://docs.gitlab.com/ee/administration/auth/README.html">GitLab authentication and authorization | GitLab</a></p><h2 id="大问题">大问题</h2><p>暂无</p><h2 id="其它">其它</h2><p>若没有直接采用oss作为存储路径，即本地压缩后，再用脚本上传</p><pre><code class="language-bash">#!/bin/bashbasedir=`cd &quot;$(dirname &quot;$0&quot;)&quot;; pwd`# 确定gitlab备份文件地址mv /path/to/backup/*backup.tar $&#123;basedir&#125;/ls $&#123;basedir&#125;/*.tar | awk -F '/|_' '&#123;print $4&#125;' | while read line;do        Old7day=`date -d &quot;-7 days&quot; &quot;+%s&quot;`        if [ $line -lt $Old7day ];then              rm -rf $&#123;basedir&#125;/$&#123;line&#125;*.tar &amp;&amp; echo &quot;Delete old7day file $&#123;basedir&#125;/$&#123;line&#125;*.tar&quot;        fidonecd ~ &amp;&amp; source $&#123;basedir&#125;/role.confRegion=`curl -sq http://100.100.100.200/latest/meta-data/region-id`Endpoint=&quot;http://oss-$&#123;Region&#125;-internal.aliyuncs.com&quot;BucketName=&lt;存储桶&gt;echo &quot;Start: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; oss://$&#123;BucketName&#125;/backup/&quot;aliyun oss sync $&#123;basedir&#125;/ oss://$&#123;BucketName&#125;/backup/ --exclude='*.log' --update --delete --force -e $&#123;Endpoint&#125; --checkpoint-dir=/tmp/ossutil_checkpoint --output-dir=/tmp/ossutil_outputecho &quot;End: `date &quot;+%Y%m%d %H%M%S&quot;` --&gt; oss://$&#123;BucketName&#125;/backup/&quot;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞EC2-GPU实例安装显卡驱动</title>
      <link href="posts/2e07203a/"/>
      <url>posts/2e07203a/</url>
      
        <content type="html"><![CDATA[<blockquote><p>以下命令均在 ubuntu 用户中执行</p></blockquote><h2 id="安装-awscli-到-ubuntu-用户下">安装 awscli 到 ubuntu 用户下</h2><pre><code class="language-bash:">sudo apt-get update -ysudo apt-get install python3-pip -ypip3 install awscli --upgrade --user</code></pre><h2 id="更新系统内核和gcc，以及禁用开源驱动">更新系统内核和gcc，以及禁用开源驱动</h2><pre><code class="language-bash:">sudo apt-get upgrade -y linux-awssudo rebootsudo apt-get install -y gcc make linux-headers-$(uname -r)cat &lt;&lt; EOF | sudo tee --append /etc/modprobe.d/blacklist.confblacklist vga16fbblacklist nouveaublacklist rivafbblacklist nvidiafbblacklist rivatvEOFsudo vi /etc/default/grub     GRUB_CMDLINE_LINUX=&quot;rdblacklist=nouveau&quot;sudo update-grubsudo reboot</code></pre><h2 id="配置-aws-key-拉取-G3-系列驱动">配置 aws key 拉取 G3 系列驱动</h2><pre><code class="language-bash:">aws configure# 根据提示输入相关数据，这里 aws key 可以用任意 iam 用户的</code></pre><h2 id="下载-G3-驱动">下载 G3 驱动</h2><pre><code class="language-bash:"># G3实例直接从aws库里下载，其它需要自行去官网下载https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/install-nvidia-driver.html#obtain-nvidia-GRID-driver-linuxaws s3 cp --recursive s3://ec2-linux-nvidia-drivers/latest/ .</code></pre><h2 id="安装驱动">安装驱动</h2><pre><code class="language-bash:">sudo /bin/sh ./NVIDIA-Linux-x86_64*.run# 可能会出现 gcc 版本检测，忽略掉版本检测，直接安装sudo reboot</code></pre><h2 id="启用-GRUD-虚拟应用程序，默认启用-GRUD-虚拟工作站">启用 GRUD 虚拟应用程序，默认启用 GRUD 虚拟工作站</h2><pre><code class="language-bash:"># 具体步骤看文档https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/activate_grid.html</code></pre><h2 id="优化G3，开启全功率，关闭autoboost-（P3-实例上的-GPU-不支持-autoboost-功能。）">优化G3，开启全功率，关闭autoboost （P3 实例上的 GPU 不支持 autoboost 功能。）</h2><pre><code class="language-bash:">sudo nvidia-persistencedsudo nvidia-smi --auto-boost-default=0# P2 实例：sudo nvidia-smi -ac 2505,875# P3 实例：sudo nvidia-smi -ac 877,1530# G3 实例：sudo nvidia-smi -ac 2505,1177</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> ubuntu </tag>
            
            <tag> gpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fpm☞静态配置</title>
      <link href="posts/d1d65c30/"/>
      <url>posts/d1d65c30/</url>
      
        <content type="html"><![CDATA[<blockquote><p>QPS 800-1000</p><p>2*4 机器</p></blockquote><pre><code class="language-ini">[www01]user = webappsgroup = webappslisten = /usr/local/php/var/log/php-fpm-www01.socklisten.owner = webappslisten.group = webappslisten.backlog = 10240listen.mode = 0666listen.allowed_clients = 127.0.0.1pm = staticpm.max_children = 300pm.max_requests = 1024request_slowlog_timeout = 1request_terminate_timeout = 1slowlog = /usr/local/php/var/log/php-slow.logpm.status_path = /php-fpm_status</code></pre>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> fpm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> php-fpm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx☞日志切割脚本</title>
      <link href="posts/445cf088/"/>
      <url>posts/445cf088/</url>
      
        <content type="html"><![CDATA[<h2 id="upstream-prematurely-closed-connection-while-reading-response-header-from-upstream">upstream prematurely closed connection while reading response header from upstream</h2><p>汉译：当从上游读取响应头时，上游提前关闭连接</p><p>经确认，此错误一般是开启长链接后，长链接超时关闭时，恰好有数据发生</p><p>因此，解决方案有两个：</p><ol><li>nginx 将长链接的超时设置为 0 ，即关闭长链接</li></ol><pre><code class="language-bash">keepalive_timeout 0; # 此值默认为0，默认位于nginx主配置</code></pre><ol start="2"><li>关闭 nginx 长链接超时主动关闭的功能</li></ol><pre><code class="language-bash">proxy_http_version 1.1; # 支持长链接proxy_set_header Connection &quot;&quot;; # nginx不会主动关闭长连接，此值默认是 close</code></pre><blockquote><p>nginx 添加自定义 header 后，就会覆盖掉相应的默认值</p><p><a href="http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header">Module ngx_http_proxy_module (nginx.org)</a></p><table><thead><tr><th style="text-align:left">Syntax:</th><th><code>proxy_set_header field value;</code></th></tr></thead><tbody><tr><td style="text-align:left">Default:</td><td><code>proxy_set_header Host $proxy_host; proxy_set_header Connection close;</code></td></tr><tr><td style="text-align:left">Context:</td><td><code>http</code>, <code>server</code>, <code>location</code></td></tr></tbody></table></blockquote>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>openvpn☞安装</title>
      <link href="posts/7c1abbe1/"/>
      <url>posts/7c1abbe1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>openvpn 一般用于小公司远程连接公司办公网络环境，此脚本需要一个linux系统的主机。<br>至于土豪公司，可以无视。<br>文档包含安装脚本，创建用户脚本，删除用户脚本。使用的时候，安装脚本和创建用户脚本，需要自行修改一些变量。</p><blockquote><p>需要注意的是，发现移动网络连接其它运营商网络的时候，不稳定。<br>比如我这边，移动网络连接电信网络（openvpn所在网络），就容易丢包。</p></blockquote><h2 id="安装">安装</h2><p>以下命令需逐行执行，不能复制批量执行，因为中间需要自行添加变量，以及手动输入一些信息</p><pre><code class="language-bash:">yum install dockerdocker pull kylemanna/openvpnOVPN_DATA=&quot;/root/ovpn-data&quot;IP=&quot;服务器主网卡ip&quot;  # 自行修改PORT=  # 容器映射的宿主机端口mkdir $&#123;OVPN_DATA&#125;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_genconfig -u tcp://$&#123;IP&#125;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn ovpn_initpkidocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn easyrsa build-client-full CLIENTNAME nopassdocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient CLIENTNAME &gt; $&#123;OVPN_DATA&#125;/CLIENTNAME.ovpndocker run --name openvpn -v $&#123;OVPN_DATA&#125;:/etc/openvpn -d -p $&#123;PORT&#125;:1194 --privileged kylemanna/openvpn</code></pre><h2 id="容器生成的服务器主配置">容器生成的服务器主配置</h2><p>/root/ovpn-data/openvpn.conf</p><p>下面是一个改后的示例配置</p><pre><code class="language-bash">server 192.168.255.0 255.255.255.0verb 3# 变更为容器里默认生成的key /etc/openvpn/pki/private/&lt;修改我&gt;.keyca /etc/openvpn/pki/ca.crt# 变更为容器里默认生成的cert /etc/openvpn/pki/issued/&lt;修改我&gt;.crtdh /etc/openvpn/pki/dh.pemtls-auth /etc/openvpn/pki/ta.keykey-direction 0keepalive 10 60persist-keypersist-tunproto tcp# Rely on Docker to do port mapping, internally always 1194port 1194dev tun0status /tmp/openvpn-status.loguser nobodygroup nogroupcomp-lzo no### Route Configurations Belowroute 192.168.254.0 255.255.255.0### Push Configurations Belowpush &quot;block-outside-dns&quot;push &quot;comp-lzo no&quot;### 关闭DNS推送，客户端走本地DNS,需要通知关闭客户端的全局路由 redirect-gateway def1#push &quot;dhcp-option DNS 223.5.5.5&quot;#push &quot;dhcp-option DNS 114.114.114.114&quot;### 添加客户端需要走openvpn的网络路由，例如访问openvpn所在内网的某个网段，10.100.0.0/16push &quot;route 10.100.0.0 255.255.0.0&quot;</code></pre><h3 id="服务端dns推送与客户端全局路由的关系">服务端dns推送与客户端全局路由的关系</h3><p>如果客户端删除了全局路由 redirect-gateway def1；则服务端要么关闭 dns 推送，要么开启 dns 推送的同时，开启 dns 的相关路由。</p><p>如果客户端开启了全局路由 redirect-gateway def1；则服务端最好开启 dns 推送，但无需开启 dns 路由。</p><p>示例如下：</p><p>服务端配置示例如下：开启 dns 推送和 dns 路由</p><pre><code class="language-bash">push &quot;dhcp-option DNS 223.5.5.5&quot;push &quot;dhcp-option DNS 114.114.114.114&quot;push &quot;route 223.5.5.5 255.255.255.255&quot;push &quot;route 114.114.114.114 255.255.255.255&quot;</code></pre><p>客户端配置示例如下：关闭全局路由</p><pre><code class="language-bash"># redirect-gateway def1</code></pre><h2 id="创建用户脚本">创建用户脚本</h2><blockquote><p>需要注意的是，下列脚本中，是需要自行修改ip的。</p></blockquote><pre><code class="language-bash:">#!/bin/bash# 如果是nat后的内网ip，则需要修改配置文件里的ip为外网ip# bash xxx.sh &lt;用户名&gt;[[ -z $1 ]] &amp;&amp; exitCLIENTNAME=$1OVPN_DATA=&quot;/root/ovpn-data&quot;docker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm -it kylemanna/openvpn easyrsa build-client-full $&#123;CLIENTNAME&#125; nopassdocker run -v $&#123;OVPN_DATA&#125;:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient $CLIENTNAME &gt; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '1a comp-lzo' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '2a tun-mtu 1500' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i '3a auth-nocache' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnsed -i &quot;s/1194/&lt;外网端口&gt;/&quot; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn # 修改我sed -i &quot;s/&lt;服务器内网IP&gt;/&lt;客户端连接的外网IP&gt;/&quot; $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn  # 修改我read -p '是否关闭全局路由，（y）关闭全局路由，仅当客户端匹配下发路由时才走openvpn.[确保服务端没有开启DNS推送，若开启则添加对应的DNS路由]；（n）开启&gt;全局路由，客户端所有流量均走openvpn.[确保服务端开启了DNS推送]：' yn[[ $yn == 'y' ]] &amp;&amp; sed -i '/redirect-gateway def1/d' $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpnmkdir -pv $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv /root/ovpn-data/pki/private/$&#123;CLIENTNAME&#125;.key $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv /root/ovpn-data/pki/issued/$&#123;CLIENTNAME&#125;.crt $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;mv $&#123;OVPN_DATA&#125;/$&#123;CLIENTNAME&#125;.ovpn $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;cd $&#123;OVPN_DATA&#125;/users/tar zcf $&#123;CLIENTNAME&#125;.tar.gz $&#123;CLIENTNAME&#125;echo &quot;`pwd`/$&#123;CLIENTNAME&#125;.tar.gz&quot;echo &quot;################################################&quot;cat $&#123;OVPN_DATA&#125;/users/$&#123;CLIENTNAME&#125;/$&#123;CLIENTNAME&#125;.ovpn#sz $&#123;CLIENTNAME&#125;.tar.gz</code></pre><h2 id="删除用户脚本">删除用户脚本</h2><pre><code class="language-bash:">#!/bin/bash# bash xxx.sh &lt;用户名&gt;[[ -z $1 ]] &amp;&amp; exitCLIENTNAME=$1#read -p '输入用户名（字母组成）:' CLIENTNAMEOVPN_DATA=&quot;/root/ovpn-data&quot;rm -rf /root/ovpn-data/pki/private/$&#123;CLIENTNAME&#125;.key*rm -rf /root/ovpn-data/pki/reqs/$&#123;CLIENTNAME&#125;.reqrm -rf /root/ovpn-data/pki/issued/$&#123;CLIENTNAME&#125;.crtsed -i &quot;/$&#123;CLIENTNAME&#125;/d&quot; /root/ovpn-data/pki/index.txtcd /root/ovpn-data/usersrm -rf $&#123;CLIENTNAME&#125;rm -f $&#123;CLIENTNAME&#125;.tar.gz</code></pre>]]></content>
      
      
      <categories>
          
          <category> 其它 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
            <tag> openvpn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞调整内核参数</title>
      <link href="posts/28361833/"/>
      <url>posts/28361833/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>redis 内存占用爆炸. 所以网上找了一个</p><pre><code class="language-python"># encoding: utf-8&quot;&quot;&quot;author: yangyi@youzan.comtime: 2018/4/26 下午4:34func: 获取数据库中没有设置ttl的 key&quot;&quot;&quot;import redisimport argparseimport timeimport sysclass ShowProcess:    &quot;&quot;&quot;    显示处理进度的类    调用该类相关函数即可实现处理进度的显示    &quot;&quot;&quot;    i = 0 # 当前的处理进度    max_steps = 0 # 总共需要处理的次数    max_arrow = 100 # 进度条的长度    # 初始化函数，需要知道总共的处理次数    def __init__(self, max_steps):        self.max_steps = max_steps        self.i = 0    # 显示函数，根据当前的处理进度i显示进度    # 效果为[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;]100.00%    def show_process(self, i = None):        if i is not None:            self.i = i        else:            self.i += 1        num_arrow = int(self.i * self.max_arrow / self.max_steps) # 计算显示多少个'&gt;'        num_line = self.max_arrow - num_arrow # 计算显示多少个'-'        percent = self.i * 100.0 / self.max_steps # 计算完成进度，格式为xx.xx%        process_bar = '[' + '&gt;' * num_arrow + ' ' * num_line + ']'\                      + '%.2f' % percent + '%' + '\r' # 带输出的字符串，'\r'表示不换行回到最左边        sys.stdout.write(process_bar) # 这两句打印字符到终端        sys.stdout.flush()    def close(self, words='done'):        print ''        print words        self.i = 0def check_ttl(redis_conn, no_ttl_file, dbindex):    start_time = time.time()    no_ttl_num = 0    keys_num = redis_conn.dbsize()    print &quot;there are &#123;num&#125; keys in db &#123;index&#125; &quot;.format(num=keys_num, index=dbindex)    process_bar = ShowProcess(keys_num)    with open(no_ttl_file, 'a') as f:        for key in redis_conn.scan_iter(count=1000):            process_bar.show_process()            if redis_conn.ttl(key) == -1:                no_ttl_num += 1    #            if no_ttl_num &lt; 1000:                f.write(key+'\n')            else:                continue    process_bar.close()    print &quot;cost time(s):&quot;, time.time() - start_time    print &quot;no ttl keys number:&quot;, no_ttl_num    print &quot;we write keys with no ttl to the file: %s&quot; % no_ttl_filedef main():    parser = argparse.ArgumentParser()    parser.add_argument('-p', type=int, dest='port', action='store', help='port of redis ')    parser.add_argument('-d', type=str, dest='db_list', action='store', default=0,                        help='ex : -d all / -d 1,2,3,4 ')    args = parser.parse_args()    port = args.port    if args.db_list == 'all':        db_list = [i for i in xrange(0, 16)]    else:        db_list = [int(i) for i in args.db_list.split(',')]    for index in db_list:        try:            pool = redis.ConnectionPool(host='127.0.0.1', port=port, db=index)            r = redis.StrictRedis(connection_pool=pool)        except redis.exceptions.ConnectionError as e:            print e        else:            no_ttl_keys_file = &quot;/tmp/&#123;port&#125;_&#123;db&#125;_no_ttl_keys.txt&quot;.format(port=port, db=index)            check_ttl(r, no_ttl_keys_file, index)if __name__ == '__main__':    main()</code></pre><ol><li>请勿在redis服务器上执行</li><li>修改脚本中 127.0.0.1 为 redis 服务器地址</li><li>脚本执行命令: <code>python nottlkey.py -d all -p 6379  </code></li><li>添加ttl命令：<code>cat no_ttl_keys.txt  |  xargs -i -t ./redis-cli -h &lt;redis_ip&gt; -n &lt;database_num&gt; expire &lt;ttl_time&gt;</code></li></ol>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>redis☞调整内核参数</title>
      <link href="posts/28361833/"/>
      <url>posts/28361833/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>之前安装完redis，启动redis，总会告诉你让你默认修改一些参数，不知其原因，但每次都招办。<br>后来遇到一些redis内存使用紧张的时候，从而导致 redis-bgsave 失败，再一次想到了这个问题。<br>经过查阅资料，发现官方文档里告诉了原因，在此记录一下。如有不对之处，还请指出。</p><h4 id="我是问题">我是问题</h4><p>redis-bgsave与overcommit_memory的关系。<br>当剩余物理内存低于当前redis所用内存的时候，overcommit_memory=1的意义</p><h4 id="官方解释在此">官方解释在此</h4><blockquote><p><a href="https://redis.io/topics/faq">https://redis.io/topics/faq</a></p></blockquote><h2 id="Background-saving-fails-with-a-fork-error-under-Linux-even-if-I-have-a-lot-of-free-RAM">Background saving fails with a fork() error under Linux even if I have a lot of free RAM!</h2><p>Short answer: : <code>echo 1 &gt; /proc/sys/vm/overcommit_memory</code></p><p>And now the long one:</p><p>Redis background saving schema relies on the copy-on-write semantic of fork in modern operating systems: Redis forks (creates a child process) that is an exact copy of the parent. The child process dumps the DB on disk and finally exits. In theory the child should use as much memory as the parent being a copy, but actually thanks to the copy-on-write semantic implemented by most modern operating systems the parent and child process will <em>share</em> the common memory pages. A page will be duplicated only when it changes in the child or in the parent. Since in theory all the pages may change while the child process is saving, Linux can’t tell in advance how much memory the child will take, so if the setting is set to zero fork will fail unless there is as much free RAM as required to really duplicate all the parent memory pages, with the result that if you have a Redis dataset of 3 GB and just 2 GB of free memory it will fail.<code>overcommit_memory</code></p><p>Setting to 1 tells Linux to relax and perform the fork in a more optimistic allocation fashion, and this is indeed what you want for Redis.<code>overcommit_memory</code></p><p>A good source to understand how Linux Virtual Memory works and other alternatives for and is this classic from Red Hat Magazine, <a href="https://people.redhat.com/nhorman/papers/rhel3_vm.pdf">“Understanding Virtual Memory”</a>. You can also refer to the <a href="http://man7.org/linux/man-pages/man5/proc.5.html">proc(5)</a> man page for explanations of the available values.<code>overcommit_memory``overcommit_ratio</code></p><h4 id="翻译后的大致意思">翻译后的大致意思</h4><p>官方的FAQ，给人一种这么个意思。 如果你不设置overcommit_memory=1，那么COW机制将无法使用，所以当空余内存小于当前redis占用内存时，redis-bgsave 因为无法申请到足够的内存，将导致分配内存失败。<br>而COW机制的意思就是：父子进程公用内存页，因此只会copy变化的数据。因此，在COW机制下，redis-bgsave只需要申请到支撑变化数据的内存即可。<br>至于是否是因为启用 overcommit_memory=1 从而使得COW机制起作用，限于这是内核层面的东西</p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞s3跨账户访问</title>
      <link href="posts/7cac5d21/"/>
      <url>posts/7cac5d21/</url>
      
        <content type="html"><![CDATA[<ul><li>规则如下：</li></ul><pre><code class="language-:">存储桶策略,授权其它账户的某个 iam 资源访问此存储桶arn:aws:iam::&lt;aws_account_id&gt;:&lt;type&gt;/&lt;name&gt;</code></pre><ul><li>示例配置如下：</li></ul><pre><code class="language-json:">&#123;    &quot;Version&quot;: &quot;2012-10-17&quot;,    &quot;Id&quot;: &quot;Policy1564021125924&quot;,    &quot;Statement&quot;: [        &#123;            &quot;Sid&quot;: &quot;object1&quot;,            &quot;Effect&quot;: &quot;Allow&quot;,            &quot;Principal&quot;: &#123;                &quot;AWS&quot;: &quot;arn:aws:iam::&lt;account-id&gt;:role/&lt;role-name&gt;&quot;            &#125;,            &quot;Action&quot;: [                &quot;s3:Get*&quot;,                &quot;s3:List*&quot;            ],            &quot;Resource&quot;: &quot;arn:aws:s3:::&lt;open-bucket&gt;/&lt;open-path&gt;/*&quot;        &#125;    ]&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cloudformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aws☞rds-mysql日志</title>
      <link href="posts/604c5952/"/>
      <url>posts/604c5952/</url>
      
        <content type="html"><![CDATA[<h2 id="常规-错误-慢-日志">常规/错误/慢/日志</h2><pre><code class="language-bash"># 修改参数组slow_query_log：要创建慢速查询日志，请设置为 1。默认值为 0。general_log(会产生大量日志)：要创建常规日志，请设置为 1。默认值为 0。</code></pre><p>long_query_time：要防止在慢速查询日志中记录快速运行的查询，请指定需要记录的最短查询执行时间值，以秒为单位。默认值为 10 秒；最小值为 0。<br>如果 log_output = FILE，则可以指定精确到微秒的浮点值, 即可记录 0.1s<br>如果 log_output = TABLE，则必须指定精确到秒的整数值。<br>系统只记录执行时间超过 long_query_time 值的查询。</p><p>log_queries_not_using_indexes(会产生大量日志)：要将所有不使用索引的查询记录到慢速查询日志，请设置为 1。默认值为 0。将记录不使用索引的查询，即使它们的执行时间小于 long_query_time 参数的值。</p><p>log_output option：您可为 log_output 参数指定下列选项之一。</p><p>TABLE（默认, 不建议, 影响数据库本身性能）– 将一般查询写入 mysql.general_log 表，将慢速查询写入 mysql.slow_log 表。</p><p>FILE(推荐,也是必须,否则无法推送到cloudwatch)– 将一般查询日志和慢速查询日志写入文件系统。日志文件每小时轮换一次。默认记录24小时.</p><p>NONE– 禁用日志记录</p><h2 id="审计日志">审计日志</h2><pre><code># 修改选项组, 添加额外选项MARIADB_AUDIT_PLUGIN </code></pre><h2 id="二进制日志">二进制日志</h2><blockquote><p>二进制日志一般我们用来恢复数据, 默认 rds 会尽量销毁二进制日志, 来保留磁盘可用空间.</p><p>二进制日志默认格式是   mixed</p></blockquote><p>配置二进制日志的方法:</p><pre><code class="language-bash"># 将二进制日志保留24小时call mysql.rds_set_configuration('binlog retention hours', 24);</code></pre><p>访问二进制日志的方法如下:</p><pre><code class="language-bash">mysqlbinlog \    --read-from-remote-server \    --host=&lt;mysql_rds_instance_address&gt; \    --port=&lt;mysql_port&gt; \    --user ReplUser \    --password \    --raw \    --result-file=/tmp/ \    binlog.00098</code></pre><blockquote><ul><li><p>指定  <code>--read-from-remote-server</code>  选项。</p></li><li><p><code>--host</code>：指定该实例所在的终端节点中的 DNS 名称。</p></li><li><p><code>--port</code>：指定该实例使用的端口。</p></li><li><p><code>--user</code>：指定已授予了复制从属实例权限的 MySQL 用户。</p></li><li><p><code>--password</code>：指定用户的密码，或忽略密码值以让实用程序提示您输入密码。</p></li><li><p>要按二进制格式下载文件，请指定  <code>--raw</code>  选项。</p></li><li><p><code>--result-file</code>：指定用于接收原始输出的本地文件。</p></li><li><p>指定一个或多个二进制日志文件的名称。要获取可用日志的列表，请使用 SQL 命令 SHOW BINARY LOGS。</p></li><li><p>要流式传输二进制日志文件，请指定  <code>--stop-never</code>  选项。</p></li></ul></blockquote><hr><h2 id="参考文档">参考文档:</h2><ol><li><a href="https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html#Appendix.MySQL.CommonDBATasks.Logs">https://docs.aws.amazon.com/zh_cn/AmazonRDS/latest/UserGuide/USER_LogAccess.Concepts.MySQL.html#Appendix.MySQL.CommonDBATasks.Logs</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 公有云 </category>
          
          <category> aws </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rds </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞centos7重置root密码</title>
      <link href="posts/13364d65/"/>
      <url>posts/13364d65/</url>
      
        <content type="html"><![CDATA[<ol><li>开机按ECS，进入启动项</li></ol><p><img src="/posts/13364d65/image-20210706143953672.png" alt="image-20210706143953672"></p><ol start="2"><li><p>按e，编辑第一条</p></li><li><p>将 root 挂载，从 ro 变更为 rw，并在最后添加 init=/bin/sh</p></li></ol><p><img src="/posts/13364d65/image-20210706144107672.png" alt="image-20210706144107672"></p><ol start="4"><li>按 ctrl+x，进入系统，并执行 <code>passwd</code> 命令进行修改</li></ol>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞系统内核优化</title>
      <link href="posts/f41d0763/"/>
      <url>posts/f41d0763/</url>
      
        <content type="html"><![CDATA[<h2 id="内核优化">内核优化</h2><pre><code class="language-bash"># 内核参数# 系统级别上限， 即整个系统所有进程单位时间可打开的文件描述符数量fs.file-max = 6553500# 三次握手请求频次net.ipv4.tcp_syn_retries = 5# 放弃回应一个TCP请求之前，需要尝试多少次net.ipv4.tcp_retries1 = 3# 三次握手应答频次net.ipv4.tcp_synack_retries = 2# 三次握手完毕， 没有数据沟通的情况下， 空连接存活时间net.ipv4.tcp_keepalive_time = 60# 探测消息发送次数net.ipv4.tcp_keepalive_probes = 3# 探测消息发送间隔时间net.ipv4.tcp_keepalive_intvl = 15net.ipv4.tcp_retries2 = 5net.ipv4.tcp_fin_timeout = 5# 系统处理不属于任何进程的TCP链接net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_max_orphans = 327680# 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。net.core.netdev_max_backlog = 10240# 对于还未获得对方确认的连接请求，可保存在队列中的最大数目net.ipv4.tcp_max_syn_backlog = 10240# 定义了系统中每一个端口最大的监听队列的长度 net.core.somaxconn=10240#最大timewait数net.ipv4.tcp_max_tw_buckets = 20000net.ipv4.ip_local_port_range=1024 65500# 开启时间戳net.ipv4.tcp_timestamps=1# 针对客户端有效，必须在开启时间戳的前提下net.ipv4.tcp_tw_reuse = 1# 开启 iptables 后， 链路追踪上限和超时时间, 若没有使用 iptables，则无效net.netfilter.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_tcp_timeout_established = 150## tcp栈内存使用， 单位是内存页， 一页=4KB#net.ipv4.tcp_mem = 524288 786432 1310720## socket读写缓冲区大小，单位是字节#net.ipv4.tcp_rmem = 4096 4096 16777216#net.ipv4.tcp_wmem = 4096 4096 16777216##最低内存和缓冲区回收倾向（此参数有一定风险）#vm.min_free_kbytes=409600#vm.vfs_cache_pressure=200</code></pre><pre><code class="language-bash">#修改/etc/security/limits.conf# 单会话级别，可打开的所有文件描述符上限* soft nofile 655350* hard nofile 655350# 单会话级别， 可打开的所有进程上限* soft nproc 10240* hard nproc 10240</code></pre><h2 id="开启-iptables-后">开启 iptables 后</h2><h3 id="查看当前链路表数量命令">查看当前链路表数量命令</h3><blockquote><p>$ sysctl net.netfilter.nf_conntrack_count</p><p>net.netfilter.nf_conntrack_count = 601032</p></blockquote><h3 id="查看连接数最高的10个IP：可以查封某个ip-或者判断是谁导致的">查看连接数最高的10个IP：可以查封某个ip, 或者判断是谁导致的</h3><p>$ awk -F’=’ ‘{c[$2]++}END{for ( i in c) print i,c[i]}’ /proc/net/nf_conntrack | head -10</p>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell☞if判定</title>
      <link href="posts/241ce96c/"/>
      <url>posts/241ce96c/</url>
      
        <content type="html"><![CDATA[<pre><code>-a FILE：存在则为真；否则则为假；-e FILE: 存在则为真；否则则为假；-f FILE: 存在并且为普通文件，则为真；否则为假；-d DIR: 存在并且为目录，则为真；否则为假；-L/-h FILE: 存在并且为符号链接文件，则为真；否则为假；-b: 存在并且为块设备，则为真；否则为假；-c: 存在并且为字符设备，则为真；否则为假-S: 存在并且为套接字文件，则为真；否则为假-p: 存在并且为命名管道，则为真；否则为假-s FILE: 存在并且为非空文件则为值，否则为假；-r FILE：文件可读为真，否则为假-w FILE：文件可写为真，否则为假</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux☞df与du数据不一致的原因</title>
      <link href="posts/a3dfc0e8/"/>
      <url>posts/a3dfc0e8/</url>
      
        <content type="html"><![CDATA[<h2 id="linux-☞-df与du数据不一致的原因">linux ☞ df与du数据不一致的原因</h2><ul><li><h3 id="问题表现：">问题表现：</h3></li></ul><p>df 数据比 du 数据小，而且是发生在删除文件之后。</p><hr><p>这个问题牵扯到 linux 系统是将如何确认和回收数据的.</p><ul><li><h3 id="空间是如何被判定占用的">空间是如何被判定占用的</h3></li></ul><p>linux文件系统判定空间是否被占用，是查看空间的 imap 是否为1，而 imap 是否为1，则取决于占用空间的文件的 inode 节点的 Links 是否为0。而每当程序调用文件且没有关闭，那么此文件的 inode 的 Links 就会加 1。</p><p>如果 Links 不为0，则 imap 就不会是 0 ，则这份空间就无法被再次调用.<br>另外，每一个文件，默认 inode  Links 是1</p><ul><li><h3 id="系统是如何删除一个文件的">系统是如何删除一个文件的</h3></li></ul><p>文件系统首先在父目录文件里找到所要删除的文件名，将此文件信息从父目录里清除掉，如若清除后，Links 为0 ，则删除 inode 节点，将 imap 就置为0，空间可以再次被利用。但是，如果父目录文件信息清除后，有程序在调用文件，则 Links 不为0，那么 inode 无法被删除， imap 也无法置为0，结果就是文件看着没了，但是空间还是没有释放。</p><hr><p>df 和 du 的最大区别就是：</p><p>df 是根据 inode 的 Links 来确认空间是否被占用，并进而统计</p><p>du 是根据目录的文件信息来确认空间是否被占用，并进而统计</p><ul><li><h3 id="如何释放被占用的空间">如何释放被占用的空间</h3></li></ul><p>从上面可知，我们只需要将被删除文件的 inode Links 降成 0 即可，也就是关闭此文件的调用程序.</p><p>如何查找调用程序？</p><pre><code class="language-bash">lsof -n | grep rm_file_name # 获取到程序 pid，并将其杀死即可</code></pre>]]></content>
      
      
      <categories>
          
          <category> 系统 </category>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ansible☞playbook-lookup插件</title>
      <link href="posts/25732c33/"/>
      <url>posts/25732c33/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>lookup 插件可以配合 loop 循环实现 with_x 循环</p><h2 id="lookup-查询插件">lookup 查询插件</h2><blockquote><p>query 插件与 lookup 插件一样，区别在于 query 默认情况下，启用了wantlist=true。<br>wantlist 的意思是将返回的字符串构成一个列表<br>另外，query 可以简写为q</p></blockquote><h5 id="loop-关键词">loop 关键词</h5><p>用于将一个列表进行循环，默认循环单体变量是 item. 官方用来替代 with_xxx</p><p>用来遍历 lookup 结果集</p><blockquote><p>自定义loop循环单体变量为 xxx</p><pre><code class="language-yaml">loop_control:  loop_var: xxx</code></pre><p>loop_control 还有其它控制，比如</p><ul><li>index_var: var_name 循环索引.</li><li>pause: time 循环间隔时间</li><li>label: var_name 去掉 var_name 之外的不相干信息输出</li></ul></blockquote><h2 id="常用参数示例">常用参数示例</h2><h4 id="file-参数">file 参数</h4><blockquote><p>多个文件内容合并在一起，或以字符串逗号分隔输出，或以列表形式输出</p></blockquote><h4 id="ini-参数">ini 参数</h4><blockquote><p>获取 ini 配置信息</p></blockquote><pre><code class="language-ini"># ~/test.ini[testA]a1=zhangsana2=lisi[testB]b1=wangwu</code></pre><pre><code class="language-yaml">---- hosts: localhost  gather_facts: no  remote_user: zyh  tasks:    - name: get ini      debug:        msg: &quot;&#123;&#123; q('ini', 'a1 section=testA file=~/test.ini') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; &#123;    &quot;msg&quot;: [        &quot;zhangsan&quot;    ]&#125;</code></pre><blockquote><p>当配置是 properties，可以追加 type=properties</p></blockquote><h4 id="dict参数">dict参数</h4><pre><code class="language-bash"># 结果集 [&#123;key: xxx, value: xxx&#125;, &#123;key: xxx, value: xxx&#125;]</code></pre><pre><code class="language-yaml">---- hosts: local  gather_facts: no  connection: local  vars:    users:      male: Bob      female: Maris  tasks:    - name: test vars 1      debug:        msg: &quot;&#123;&#123; item.key &#125;&#125;: &#123;&#123; item.value &#125;&#125;&quot;      loop: &quot;&#123;&#123; lookup('dict', users) &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=&#123;'key': u'male', 'value': u'Bob'&#125;) =&gt; &#123;    &quot;msg&quot;: &quot;male: Bob&quot;&#125;ok: [localhost] =&gt; (item=&#123;'key': u'female', 'value': u'Maris'&#125;) =&gt; &#123;    &quot;msg&quot;: &quot;female: Maris&quot;&#125;</code></pre><h4 id="subelements参数">subelements参数</h4><pre><code class="language-bash"># 写法：&#123;&#123; lookup('subelements',list,'content') &#125;&#125;# 一个由相同结构字典组成的列表list，将字典中某一个元素key（值必须是列表）与字典剩余的元素（剩余的元素作为一个整体新字典），构建笛卡尔积。从而形成 item。每一个字典拆分组合后的 item 构建结果集 items# 结果集items=[[&#123;list.0.剩余kv&#125;,  list.0.key.0], [&#123;list.0.剩余kv&#125;,  list.0.key.1], [&#123;list.1.剩余kv&#125;,  list.1.key.0], [&#123;list.1.剩余kv&#125;,  list.1.key.1], ]</code></pre><pre><code class="language-yaml">---- hosts: local  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                    age: 18                    content:                            - eating                            - sleeping                            - play ogre                  - name: Maris                    gender: female                    age: 20                    content:                            - eating                            - sleeping                            - shopping  tasks:          - name: test vars 2            debug:                    msg: &quot;&#123;&#123; item.0.name &#125;&#125; - &#123;&#123; item.0.gender &#125;&#125; - &#123;&#123; item.1 &#125;&#125;&quot;            loop: &quot;&#123;&#123; lookup('subelements',users,'content') &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'eating']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - eating&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'sleeping']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - sleeping&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'male', u'age': 18, u'name': u'Bob'&#125;, u'play ogre']) =&gt; &#123;    &quot;msg&quot;: &quot;Bob - male - play ogre&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'eating']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - eating&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'sleeping']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - sleeping&quot;&#125;ok: [localhost] =&gt; (item=[&#123;u'gender': u'female', u'age': 20, u'name': u'Maris'&#125;, u'shopping']) =&gt; &#123;    &quot;msg&quot;: &quot;Maris - female - shopping&quot;&#125;</code></pre><blockquote><p>若有多个key需要附加，需要 nested 与  include_tasks 的组合实现</p></blockquote><h4 id="nested参数">nested参数</h4><blockquote><p>将多个列表进行笛卡尔积运算</p><ol><li>test3.yml 中拿到 users循环单体 user</li><li>针对循环单体 user引入附加任务 test3_1.yml</li><li>通过lookup插件nested，将 user字典中各key的value相互遍历，构建新列表 item</li></ol></blockquote><pre><code class="language-yaml">################ test3.yml---- hosts: local  gather_facts: no  connection: local  vars:          users:                  - name: Bob                    gender: male                           age: 18                    content:                            - eating                               - sleeping                             - play ogre                    specialty:                            - english                              - game                       - name: Maris                            gender: female                         age: 20                    content:                            - eating                               - sleeping                             - shopping                     specialty:                            - history                              - cooking    tasks:          - include_tasks: test3_1.yml             loop: &quot;&#123;&#123; users &#125;&#125;&quot;            loop_control:                    loop_var: user ################ test3_1.yml- loop: &quot;&#123;&#123; lookup('nested',user.name, user.age, user.content, user.specialty) &#125;&#125;&quot;  debug:    msg: &quot;name:&#123;&#123; item.0 &#125;&#125;, age:&#123;&#123; item.1 &#125;&#125;, &#123;&#123; item.2 &#125;&#125;, &#123;&#123; item.3 &#125;&#125;&quot;</code></pre><pre><code class="language-bash">ok: [localhost] =&gt; (item=[u'Bob', 18, u'eating', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, eating, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'eating', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, eating, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'sleeping', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, sleeping, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'sleeping', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, sleeping, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'play ogre', u'english']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, play ogre, english&quot;&#125;ok: [localhost] =&gt; (item=[u'Bob', 18, u'play ogre', u'game']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Bob, age:18, play ogre, game&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'eating', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, eating, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'eating', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, eating, cooking&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'sleeping', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, sleeping, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'sleeping', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, sleeping, cooking&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'shopping', u'history']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, shopping, history&quot;&#125;ok: [localhost] =&gt; (item=[u'Maris', 20, u'shopping', u'cooking']) =&gt; &#123;    &quot;msg&quot;: &quot;name:Maris, age:20, shopping, cooking&quot;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 自动化 </category>
          
          <category> ansible </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ansible </tag>
            
            <tag> lookup </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
